*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/456) train vimp2_845_05312013_VZ
(1/456) train vimp2_823_05202013_AJ
(2/456) train vimp2_915_07112013_LC
(3/456) train vimp2_901_07052013_AS
(4/456) train vimp2_ctrl_911_07082013_TTO
(5/456) train vimp2_ctrl_925_07152013_LS
(6/456) train vimp2_869_06142013_BL
(7/456) train vimp2_ANON724_03272013
(8/456) train vimp2_819_05172013_DS
(9/456) train vimp2_ctrl_918_07112013_TQ
(10/456) train vimp2_ctrl_902_07052013_SI
(11/456) train vimp2_ANON606_20130110
(12/456) train vimp2_943_07242013_PA
(13/456) train vimp2_824_05212013_JS
(14/456) train vimp2_ANON624_20130117
(15/456) train vimp2_ctrl_920_07122013_SW
(16/456) train vimp2_884_06272013_TS
(17/456) train vimp2_668_02282013_CD
(18/456) train vimp2_964_08092013_TG
(19/456) train vimp2_ctrl_921_07122013_MP
(20/456) train vimp2_972_08152013_DC
(21/456) train vimp2_988_08302013_CB
(22/456) train vimp2_ANON702_03152013
(23/456) train vimp2_ANON714_03222013
(24/456) train vimp2_972_08152013_DC_Aug0_Rot_-1_sd0
(25/456) train vimp2_972_08152013_DC_Aug0_Rot_1_sd2
(26/456) train vimp2_972_08152013_DC_Aug0_Rot_-7_sd1
(27/456) train vimp2_972_08152013_DC_Aug1_Rot_2_sd0
(28/456) train vimp2_972_08152013_DC_Aug1_Rot_5_sd1
(29/456) train vimp2_972_08152013_DC_Aug1_Rot_6_sd2
(30/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd1
(31/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd2
(32/456) train vimp2_972_08152013_DC_Aug2_Rot_-5_sd0
(33/456) train vimp2_972_08152013_DC_Aug3_Rot_2_sd1
(34/456) train vimp2_972_08152013_DC_Aug3_Rot_-3_sd0
(35/456) train vimp2_972_08152013_DC_Aug3_Rot_-6_sd2
(36/456) train vimp2_972_08152013_DC_Aug4_Rot_-2_sd2
(37/456) train vimp2_972_08152013_DC_Aug4_Rot_3_sd0
(38/456) train vimp2_972_08152013_DC_Aug4_Rot_6_sd1
(39/456) train vimp2_972_08152013_DC_Aug5_Rot_-1_sd2
(40/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd0
(41/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd1
(42/456) train vimp2_988_08302013_CB_Aug0_Rot_-3_sd2
(43/456) train vimp2_988_08302013_CB_Aug0_Rot_4_sd0
(44/456) train vimp2_988_08302013_CB_Aug0_Rot_-5_sd1
(45/456) train vimp2_988_08302013_CB_Aug1_Rot_-2_sd2
(46/456) train vimp2_988_08302013_CB_Aug1_Rot_-3_sd0
(47/456) train vimp2_988_08302013_CB_Aug1_Rot_-6_sd1
(48/456) train vimp2_988_08302013_CB_Aug2_Rot_-2_sd0
(49/456) train vimp2_988_08302013_CB_Aug2_Rot_4_sd1
(50/456) train vimp2_988_08302013_CB_Aug2_Rot_-5_sd2
(51/456) train vimp2_988_08302013_CB_Aug3_Rot_2_sd2
(52/456) train vimp2_988_08302013_CB_Aug3_Rot_-3_sd0
(53/456) train vimp2_988_08302013_CB_Aug3_Rot_3_sd1
(54/456) train vimp2_988_08302013_CB_Aug4_Rot_-2_sd0
(55/456) train vimp2_988_08302013_CB_Aug4_Rot_-6_sd1
(56/456) train vimp2_988_08302013_CB_Aug4_Rot_7_sd2
(57/456) train vimp2_988_08302013_CB_Aug5_Rot_-1_sd0
(58/456) train vimp2_988_08302013_CB_Aug5_Rot_-6_sd1
(59/456) train vimp2_988_08302013_CB_Aug5_Rot_7_sd2
(60/456) train vimp2_ANON702_03152013_Aug0_Rot_-2_sd1
(61/456) train vimp2_ANON702_03152013_Aug0_Rot_-3_sd2
(62/456) train vimp2_ANON702_03152013_Aug0_Rot_-4_sd0
(63/456) train vimp2_ANON702_03152013_Aug1_Rot_-4_sd1
(64/456) train vimp2_ANON702_03152013_Aug1_Rot_-5_sd2
(65/456) train vimp2_ANON702_03152013_Aug1_Rot_-7_sd0
(66/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd0
(67/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd2
(68/456) train vimp2_ANON702_03152013_Aug2_Rot_-7_sd1
(69/456) train vimp2_ANON702_03152013_Aug3_Rot_-1_sd2
(70/456) train vimp2_ANON702_03152013_Aug3_Rot_-3_sd0
(71/456) train vimp2_ANON702_03152013_Aug3_Rot_-6_sd1
(72/456) train vimp2_ANON702_03152013_Aug4_Rot_-2_sd0
(73/456) train vimp2_ANON702_03152013_Aug4_Rot_-3_sd2
(74/456) train vimp2_ANON702_03152013_Aug4_Rot_-7_sd1
(75/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd0
(76/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd2
(77/456) train vimp2_ANON702_03152013_Aug5_Rot_4_sd1
(78/456) train vimp2_ANON714_03222013_Aug0_Rot_-1_sd2
(79/456) train vimp2_ANON714_03222013_Aug0_Rot_-2_sd1
(80/456) train vimp2_ANON714_03222013_Aug0_Rot_4_sd0
(81/456) train vimp2_ANON714_03222013_Aug1_Rot_0_sd0
(82/456) train vimp2_ANON714_03222013_Aug1_Rot_1_sd1
(83/456) train vimp2_ANON714_03222013_Aug1_Rot_-6_sd2
(84/456) train vimp2_ANON714_03222013_Aug2_Rot_-2_sd0
(85/456) train vimp2_ANON714_03222013_Aug2_Rot_4_sd1
(86/456) train vimp2_ANON714_03222013_Aug2_Rot_6_sd2
(87/456) train vimp2_ANON714_03222013_Aug3_Rot_2_sd0
(88/456) train vimp2_ANON714_03222013_Aug3_Rot_-3_sd2
(89/456) train vimp2_ANON714_03222013_Aug3_Rot_-7_sd1
(90/456) train vimp2_ANON714_03222013_Aug4_Rot_1_sd0
(91/456) train vimp2_ANON714_03222013_Aug4_Rot_-2_sd1
(92/456) train vimp2_ANON714_03222013_Aug4_Rot_4_sd2
(93/456) train vimp2_ANON714_03222013_Aug5_Rot_1_sd0
(94/456) train vimp2_ANON714_03222013_Aug5_Rot_6_sd1
(95/456) train vimp2_ANON714_03222013_Aug5_Rot_-7_sd2
(96/456) train vimp2_668_02282013_CD_Aug0_Rot_7_sd0
(97/456) train vimp2_668_02282013_CD_Aug1_Rot_-1_sd0
(98/456) train vimp2_668_02282013_CD_Aug2_Rot_-4_sd0
(99/456) train vimp2_668_02282013_CD_Aug3_Rot_3_sd0
(100/456) train vimp2_668_02282013_CD_Aug4_Rot_-5_sd0
(101/456) train vimp2_668_02282013_CD_Aug5_Rot_-6_sd0
(102/456) train vimp2_819_05172013_DS_Aug0_Rot_1_sd0
(103/456) train vimp2_819_05172013_DS_Aug1_Rot_5_sd0
(104/456) train vimp2_819_05172013_DS_Aug2_Rot_-4_sd0
(105/456) train vimp2_819_05172013_DS_Aug3_Rot_2_sd0
(106/456) train vimp2_819_05172013_DS_Aug4_Rot_5_sd0
(107/456) train vimp2_819_05172013_DS_Aug5_Rot_4_sd0
(108/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd0
(109/456) train vimp2_823_05202013_AJ_Aug1_Rot_7_sd0
(110/456) train vimp2_823_05202013_AJ_Aug2_Rot_3_sd0
(111/456) train vimp2_823_05202013_AJ_Aug3_Rot_-5_sd0
(112/456) train vimp2_823_05202013_AJ_Aug4_Rot_-3_sd0
(113/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd0
(114/456) train vimp2_824_05212013_JS_Aug0_Rot_-2_sd0
(115/456) train vimp2_824_05212013_JS_Aug1_Rot_6_sd0
(116/456) train vimp2_824_05212013_JS_Aug2_Rot_1_sd0
(117/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd0
(118/456) train vimp2_824_05212013_JS_Aug4_Rot_7_sd0
(119/456) train vimp2_824_05212013_JS_Aug5_Rot_2_sd0
(120/456) train vimp2_845_05312013_VZ_Aug0_Rot_1_sd0
(121/456) train vimp2_845_05312013_VZ_Aug1_Rot_5_sd0
(122/456) train vimp2_845_05312013_VZ_Aug2_Rot_-6_sd0
(123/456) train vimp2_845_05312013_VZ_Aug3_Rot_3_sd0
(124/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd0
(125/456) train vimp2_845_05312013_VZ_Aug5_Rot_5_sd0
(126/456) train vimp2_869_06142013_BL_Aug0_Rot_-2_sd0
(127/456) train vimp2_869_06142013_BL_Aug1_Rot_-4_sd0
(128/456) train vimp2_869_06142013_BL_Aug2_Rot_-1_sd0
(129/456) train vimp2_869_06142013_BL_Aug3_Rot_3_sd0
(130/456) train vimp2_869_06142013_BL_Aug4_Rot_3_sd0
(131/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd0
(132/456) train vimp2_884_06272013_TS_Aug0_Rot_-7_sd0
(133/456) train vimp2_884_06272013_TS_Aug1_Rot_7_sd0
(134/456) train vimp2_884_06272013_TS_Aug2_Rot_-5_sd0
(135/456) train vimp2_884_06272013_TS_Aug3_Rot_-2_sd0
(136/456) train vimp2_884_06272013_TS_Aug4_Rot_6_sd0
(137/456) train vimp2_884_06272013_TS_Aug5_Rot_-1_sd0
(138/456) train vimp2_901_07052013_AS_Aug0_Rot_-4_sd0
(139/456) train vimp2_901_07052013_AS_Aug1_Rot_-2_sd0
(140/456) train vimp2_901_07052013_AS_Aug2_Rot_1_sd0
(141/456) train vimp2_901_07052013_AS_Aug3_Rot_2_sd0
(142/456) train vimp2_901_07052013_AS_Aug4_Rot_-7_sd0
(143/456) train vimp2_901_07052013_AS_Aug5_Rot_-5_sd0
(144/456) train vimp2_915_07112013_LC_Aug0_Rot_-2_sd0
(145/456) train vimp2_915_07112013_LC_Aug1_Rot_4_sd0
(146/456) train vimp2_915_07112013_LC_Aug2_Rot_-2_sd0
(147/456) train vimp2_915_07112013_LC_Aug3_Rot_-1_sd0
(148/456) train vimp2_915_07112013_LC_Aug4_Rot_1_sd0
(149/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd0
(150/456) train vimp2_943_07242013_PA_Aug0_Rot_-5_sd0
(151/456) train vimp2_943_07242013_PA_Aug1_Rot_-7_sd0
(152/456) train vimp2_943_07242013_PA_Aug2_Rot_-4_sd0
(153/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd0
(154/456) train vimp2_943_07242013_PA_Aug4_Rot_6_sd0
(155/456) train vimp2_943_07242013_PA_Aug5_Rot_5_sd0
(156/456) train vimp2_964_08092013_TG_Aug0_Rot_5_sd0
(157/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd0
(158/456) train vimp2_964_08092013_TG_Aug2_Rot_7_sd0
(159/456) train vimp2_964_08092013_TG_Aug3_Rot_-1_sd0
(160/456) train vimp2_964_08092013_TG_Aug4_Rot_-1_sd0
(161/456) train vimp2_964_08092013_TG_Aug5_Rot_2_sd0
(162/456) train vimp2_ANON606_20130110_Aug0_Rot_1_sd0
(163/456) train vimp2_ANON606_20130110_Aug1_Rot_2_sd0
(164/456) train vimp2_ANON606_20130110_Aug2_Rot_-5_sd0
(165/456) train vimp2_ANON606_20130110_Aug3_Rot_-5_sd0
(166/456) train vimp2_ANON606_20130110_Aug4_Rot_3_sd0
(167/456) train vimp2_ANON606_20130110_Aug5_Rot_-1_sd0
(168/456) train vimp2_ANON624_20130117_Aug0_Rot_1_sd0
(169/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd0
(170/456) train vimp2_ANON624_20130117_Aug2_Rot_5_sd0
(171/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd0
(172/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd0
(173/456) train vimp2_ANON624_20130117_Aug5_Rot_3_sd0
(174/456) train vimp2_ANON724_03272013_Aug0_Rot_-4_sd0
(175/456) train vimp2_ANON724_03272013_Aug1_Rot_-3_sd0
(176/456) train vimp2_ANON724_03272013_Aug2_Rot_-4_sd0
(177/456) train vimp2_ANON724_03272013_Aug3_Rot_5_sd0
(178/456) train vimp2_ANON724_03272013_Aug4_Rot_5_sd0
(179/456) train vimp2_ANON724_03272013_Aug5_Rot_-6_sd0
(180/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_5_sd0
(181/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd0
(182/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_4_sd0
(183/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd0
(184/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_3_sd0
(185/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-7_sd0
(186/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_6_sd0
(187/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_1_sd0
(188/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_4_sd0
(189/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-2_sd0
(190/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd0
(191/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_-2_sd0
(192/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_5_sd0
(193/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-3_sd0
(194/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_-4_sd0
(195/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_1_sd0
(196/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_3_sd0
(197/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_7_sd0
(198/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd0
(199/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-6_sd0
(200/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd0
(201/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_1_sd0
(202/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-2_sd0
(203/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_4_sd0
(204/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_-1_sd0
(205/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-3_sd0
(206/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd0
(207/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_3_sd0
(208/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_1_sd0
(209/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_-5_sd0
(210/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-5_sd0
(211/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_-2_sd0
(212/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_5_sd0
(213/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_3_sd0
(214/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_4_sd0
(215/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-7_sd0
(216/456) train vimp2_668_02282013_CD_Aug0_Rot_-5_sd1
(217/456) train vimp2_668_02282013_CD_Aug1_Rot_-6_sd1
(218/456) train vimp2_668_02282013_CD_Aug2_Rot_3_sd1
(219/456) train vimp2_668_02282013_CD_Aug3_Rot_4_sd1
(220/456) train vimp2_668_02282013_CD_Aug4_Rot_-6_sd1
(221/456) train vimp2_668_02282013_CD_Aug5_Rot_-2_sd1
(222/456) train vimp2_819_05172013_DS_Aug0_Rot_-1_sd1
(223/456) train vimp2_819_05172013_DS_Aug1_Rot_3_sd1
(224/456) train vimp2_819_05172013_DS_Aug2_Rot_6_sd1
(225/456) train vimp2_819_05172013_DS_Aug3_Rot_3_sd1
(226/456) train vimp2_819_05172013_DS_Aug4_Rot_-7_sd1
(227/456) train vimp2_819_05172013_DS_Aug5_Rot_-5_sd1
(228/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd1
(229/456) train vimp2_823_05202013_AJ_Aug1_Rot_3_sd1
(230/456) train vimp2_823_05202013_AJ_Aug2_Rot_6_sd1
(231/456) train vimp2_823_05202013_AJ_Aug3_Rot_-7_sd1
(232/456) train vimp2_823_05202013_AJ_Aug4_Rot_-4_sd1
(233/456) train vimp2_823_05202013_AJ_Aug5_Rot_-4_sd1
(234/456) train vimp2_824_05212013_JS_Aug0_Rot_7_sd1
(235/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd1
(236/456) train vimp2_824_05212013_JS_Aug2_Rot_3_sd1
(237/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd1
(238/456) train vimp2_824_05212013_JS_Aug4_Rot_4_sd1
(239/456) train vimp2_824_05212013_JS_Aug5_Rot_6_sd1
(240/456) train vimp2_845_05312013_VZ_Aug0_Rot_5_sd1
(241/456) train vimp2_845_05312013_VZ_Aug1_Rot_6_sd1
(242/456) train vimp2_845_05312013_VZ_Aug2_Rot_1_sd1
(243/456) train vimp2_845_05312013_VZ_Aug3_Rot_-6_sd1
(244/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd1
(245/456) train vimp2_845_05312013_VZ_Aug5_Rot_-6_sd1
(246/456) train vimp2_869_06142013_BL_Aug0_Rot_-4_sd1
(247/456) train vimp2_869_06142013_BL_Aug1_Rot_-3_sd1
(248/456) train vimp2_869_06142013_BL_Aug2_Rot_-4_sd1
(249/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd1
(250/456) train vimp2_869_06142013_BL_Aug4_Rot_7_sd1
(251/456) train vimp2_869_06142013_BL_Aug5_Rot_5_sd1
(252/456) train vimp2_884_06272013_TS_Aug0_Rot_5_sd1
(253/456) train vimp2_884_06272013_TS_Aug1_Rot_-1_sd1
(254/456) train vimp2_884_06272013_TS_Aug2_Rot_-1_sd1
(255/456) train vimp2_884_06272013_TS_Aug3_Rot_3_sd1
(256/456) train vimp2_884_06272013_TS_Aug4_Rot_4_sd1
(257/456) train vimp2_884_06272013_TS_Aug5_Rot_3_sd1
(258/456) train vimp2_901_07052013_AS_Aug0_Rot_3_sd1
(259/456) train vimp2_901_07052013_AS_Aug1_Rot_5_sd1
(260/456) train vimp2_901_07052013_AS_Aug2_Rot_-5_sd1
(261/456) train vimp2_901_07052013_AS_Aug3_Rot_7_sd1
(262/456) train vimp2_901_07052013_AS_Aug4_Rot_7_sd1
(263/456) train vimp2_901_07052013_AS_Aug5_Rot_6_sd1
(264/456) train vimp2_915_07112013_LC_Aug0_Rot_-6_sd1
(265/456) train vimp2_915_07112013_LC_Aug1_Rot_-5_sd1
(266/456) train vimp2_915_07112013_LC_Aug2_Rot_7_sd1
(267/456) train vimp2_915_07112013_LC_Aug3_Rot_1_sd1
(268/456) train vimp2_915_07112013_LC_Aug4_Rot_6_sd1
(269/456) train vimp2_915_07112013_LC_Aug5_Rot_-7_sd1
(270/456) train vimp2_943_07242013_PA_Aug0_Rot_1_sd1
(271/456) train vimp2_943_07242013_PA_Aug1_Rot_6_sd1
(272/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd1
(273/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd1
(274/456) train vimp2_943_07242013_PA_Aug4_Rot_3_sd1
(275/456) train vimp2_943_07242013_PA_Aug5_Rot_3_sd1
(276/456) train vimp2_964_08092013_TG_Aug0_Rot_6_sd1
(277/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd1
(278/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd1
(279/456) train vimp2_964_08092013_TG_Aug3_Rot_0_sd1
(280/456) train vimp2_964_08092013_TG_Aug4_Rot_3_sd1
(281/456) train vimp2_964_08092013_TG_Aug5_Rot_-1_sd1
(282/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd1
(283/456) train vimp2_ANON606_20130110_Aug1_Rot_7_sd1
(284/456) train vimp2_ANON606_20130110_Aug2_Rot_1_sd1
(285/456) train vimp2_ANON606_20130110_Aug3_Rot_1_sd1
(286/456) train vimp2_ANON606_20130110_Aug4_Rot_4_sd1
(287/456) train vimp2_ANON606_20130110_Aug5_Rot_4_sd1
(288/456) train vimp2_ANON624_20130117_Aug0_Rot_7_sd1
(289/456) train vimp2_ANON624_20130117_Aug1_Rot_6_sd1
(290/456) train vimp2_ANON624_20130117_Aug2_Rot_-1_sd1
(291/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd1
(292/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd1
(293/456) train vimp2_ANON624_20130117_Aug5_Rot_4_sd1
(294/456) train vimp2_ANON724_03272013_Aug0_Rot_-2_sd1
(295/456) train vimp2_ANON724_03272013_Aug1_Rot_4_sd1
(296/456) train vimp2_ANON724_03272013_Aug2_Rot_-1_sd1
(297/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd1
(298/456) train vimp2_ANON724_03272013_Aug4_Rot_-5_sd1
(299/456) train vimp2_ANON724_03272013_Aug5_Rot_-4_sd1
(300/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-2_sd1
(301/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd1
(302/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_2_sd1
(303/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd1
(304/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-1_sd1
(305/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-4_sd1
(306/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_7_sd1
(307/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_-1_sd1
(308/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_-5_sd1
(309/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-3_sd1
(310/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd1
(311/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_6_sd1
(312/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_1_sd1
(313/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-2_sd1
(314/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_3_sd1
(315/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_-1_sd1
(316/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_7_sd1
(317/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_6_sd1
(318/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_-3_sd1
(319/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_7_sd1
(320/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd1
(321/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-1_sd1
(322/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-5_sd1
(323/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_-5_sd1
(324/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_4_sd1
(325/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_3_sd1
(326/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_-1_sd1
(327/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd1
(328/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_2_sd1
(329/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_4_sd1
(330/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-2_sd1
(331/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_4_sd1
(332/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_-2_sd1
(333/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_2_sd1
(334/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_3_sd1
(335/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-4_sd1
(336/456) train vimp2_668_02282013_CD_Aug0_Rot_-3_sd2
(337/456) train vimp2_668_02282013_CD_Aug1_Rot_6_sd2
(338/456) train vimp2_668_02282013_CD_Aug2_Rot_-6_sd2
(339/456) train vimp2_668_02282013_CD_Aug3_Rot_7_sd2
(340/456) train vimp2_668_02282013_CD_Aug4_Rot_1_sd2
(341/456) train vimp2_668_02282013_CD_Aug5_Rot_7_sd2
(342/456) train vimp2_819_05172013_DS_Aug0_Rot_3_sd2
(343/456) train vimp2_819_05172013_DS_Aug1_Rot_-1_sd2
(344/456) train vimp2_819_05172013_DS_Aug2_Rot_-2_sd2
(345/456) train vimp2_819_05172013_DS_Aug3_Rot_-1_sd2
(346/456) train vimp2_819_05172013_DS_Aug4_Rot_7_sd2
(347/456) train vimp2_819_05172013_DS_Aug5_Rot_3_sd2
(348/456) train vimp2_823_05202013_AJ_Aug0_Rot_-6_sd2
(349/456) train vimp2_823_05202013_AJ_Aug1_Rot_-7_sd2
(350/456) train vimp2_823_05202013_AJ_Aug2_Rot_7_sd2
(351/456) train vimp2_823_05202013_AJ_Aug3_Rot_-2_sd2
(352/456) train vimp2_823_05202013_AJ_Aug4_Rot_1_sd2
(353/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd2
(354/456) train vimp2_824_05212013_JS_Aug0_Rot_-5_sd2
(355/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd2
(356/456) train vimp2_824_05212013_JS_Aug2_Rot_5_sd2
(357/456) train vimp2_824_05212013_JS_Aug3_Rot_2_sd2
(358/456) train vimp2_824_05212013_JS_Aug4_Rot_5_sd2
(359/456) train vimp2_824_05212013_JS_Aug5_Rot_-7_sd2
(360/456) train vimp2_845_05312013_VZ_Aug0_Rot_-5_sd2
(361/456) train vimp2_845_05312013_VZ_Aug1_Rot_3_sd2
(362/456) train vimp2_845_05312013_VZ_Aug2_Rot_-3_sd2
(363/456) train vimp2_845_05312013_VZ_Aug3_Rot_-1_sd2
(364/456) train vimp2_845_05312013_VZ_Aug4_Rot_4_sd2
(365/456) train vimp2_845_05312013_VZ_Aug5_Rot_-2_sd2
(366/456) train vimp2_869_06142013_BL_Aug0_Rot_3_sd2
(367/456) train vimp2_869_06142013_BL_Aug1_Rot_7_sd2
(368/456) train vimp2_869_06142013_BL_Aug2_Rot_4_sd2
(369/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd2
(370/456) train vimp2_869_06142013_BL_Aug4_Rot_1_sd2
(371/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd2
(372/456) train vimp2_884_06272013_TS_Aug0_Rot_3_sd2
(373/456) train vimp2_884_06272013_TS_Aug1_Rot_-5_sd2
(374/456) train vimp2_884_06272013_TS_Aug2_Rot_-7_sd2
(375/456) train vimp2_884_06272013_TS_Aug3_Rot_-6_sd2
(376/456) train vimp2_884_06272013_TS_Aug4_Rot_-2_sd2
(377/456) train vimp2_884_06272013_TS_Aug5_Rot_-6_sd2
(378/456) train vimp2_901_07052013_AS_Aug0_Rot_2_sd2
(379/456) train vimp2_901_07052013_AS_Aug1_Rot_-3_sd2
(380/456) train vimp2_901_07052013_AS_Aug2_Rot_-6_sd2
(381/456) train vimp2_901_07052013_AS_Aug3_Rot_6_sd2
(382/456) train vimp2_901_07052013_AS_Aug4_Rot_-5_sd2
(383/456) train vimp2_901_07052013_AS_Aug5_Rot_1_sd2
(384/456) train vimp2_915_07112013_LC_Aug0_Rot_1_sd2
(385/456) train vimp2_915_07112013_LC_Aug1_Rot_7_sd2
(386/456) train vimp2_915_07112013_LC_Aug2_Rot_3_sd2
(387/456) train vimp2_915_07112013_LC_Aug3_Rot_6_sd2
(388/456) train vimp2_915_07112013_LC_Aug4_Rot_4_sd2
(389/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd2
(390/456) train vimp2_943_07242013_PA_Aug0_Rot_3_sd2
(391/456) train vimp2_943_07242013_PA_Aug1_Rot_-4_sd2
(392/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd2
(393/456) train vimp2_943_07242013_PA_Aug3_Rot_2_sd2
(394/456) train vimp2_943_07242013_PA_Aug4_Rot_-1_sd2
(395/456) train vimp2_943_07242013_PA_Aug5_Rot_-3_sd2
(396/456) train vimp2_964_08092013_TG_Aug0_Rot_2_sd2
(397/456) train vimp2_964_08092013_TG_Aug1_Rot_-6_sd2
(398/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd2
(399/456) train vimp2_964_08092013_TG_Aug3_Rot_-7_sd2
(400/456) train vimp2_964_08092013_TG_Aug4_Rot_-6_sd2
(401/456) train vimp2_964_08092013_TG_Aug5_Rot_5_sd2
(402/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd2
(403/456) train vimp2_ANON606_20130110_Aug1_Rot_-5_sd2
(404/456) train vimp2_ANON606_20130110_Aug2_Rot_-3_sd2
(405/456) train vimp2_ANON606_20130110_Aug3_Rot_3_sd2
(406/456) train vimp2_ANON606_20130110_Aug4_Rot_-2_sd2
(407/456) train vimp2_ANON606_20130110_Aug5_Rot_-3_sd2
(408/456) train vimp2_ANON624_20130117_Aug0_Rot_2_sd2
(409/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd2
(410/456) train vimp2_ANON624_20130117_Aug2_Rot_-6_sd2
(411/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd2
(412/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd2
(413/456) train vimp2_ANON624_20130117_Aug5_Rot_-7_sd2
(414/456) train vimp2_ANON724_03272013_Aug0_Rot_-5_sd2
(415/456) train vimp2_ANON724_03272013_Aug1_Rot_6_sd2
(416/456) train vimp2_ANON724_03272013_Aug2_Rot_-3_sd2
(417/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd2
(418/456) train vimp2_ANON724_03272013_Aug4_Rot_-1_sd2
(419/456) train vimp2_ANON724_03272013_Aug5_Rot_-7_sd2
(420/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-4_sd2
(421/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_-3_sd2
(422/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_-5_sd2
(423/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_5_sd2
(424/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-2_sd2
(425/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_5_sd2
(426/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_-3_sd2
(427/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_7_sd2
(428/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_3_sd2
(429/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_5_sd2
(430/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_7_sd2
(431/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_2_sd2
(432/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_-7_sd2
(433/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_5_sd2
(434/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_5_sd2
(435/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_4_sd2
(436/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_5_sd2
(437/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_-4_sd2
(438/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd2
(439/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-2_sd2
(440/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_1_sd2
(441/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-7_sd2
(442/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-3_sd2
(443/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_7_sd2
(444/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_1_sd2
(445/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-7_sd2
(446/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd2
(447/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd2
(448/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_-5_sd2
(449/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_6_sd2
(450/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_2_sd2
(451/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_5_sd2
(452/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_2_sd2
(453/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_5_sd2
(454/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_-4_sd2
(455/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-3_sd22019-07-05 21:29:25.224212: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-05 21:29:25.567610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 14.65GiB
2019-07-05 21:29:25.567675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 21:29:25.970036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 21:29:25.970108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 21:29:25.970122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 21:29:25.970614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<12:36,  1.43s/it]Loading train:   0%|          | 2/532 [00:02<11:12,  1.27s/it]Loading train:   1%|          | 3/532 [00:03<10:07,  1.15s/it]Loading train:   1%|          | 4/532 [00:04<09:32,  1.08s/it]Loading train:   1%|          | 5/532 [00:05<09:36,  1.09s/it]Loading train:   1%|          | 6/532 [00:06<09:55,  1.13s/it]Loading train:   1%|▏         | 7/532 [00:07<10:14,  1.17s/it]Loading train:   2%|▏         | 8/532 [00:08<10:03,  1.15s/it]Loading train:   2%|▏         | 9/532 [00:09<10:03,  1.15s/it]Loading train:   2%|▏         | 10/532 [00:10<09:09,  1.05s/it]Loading train:   2%|▏         | 11/532 [00:11<08:49,  1.02s/it]Loading train:   2%|▏         | 12/532 [00:13<09:38,  1.11s/it]Loading train:   2%|▏         | 13/532 [00:13<08:32,  1.01it/s]Loading train:   3%|▎         | 14/532 [00:14<08:21,  1.03it/s]Loading train:   3%|▎         | 15/532 [00:15<08:09,  1.06it/s]Loading train:   3%|▎         | 16/532 [00:16<08:12,  1.05it/s]Loading train:   3%|▎         | 17/532 [00:17<08:05,  1.06it/s]Loading train:   3%|▎         | 18/532 [00:18<08:45,  1.02s/it]Loading train:   4%|▎         | 19/532 [00:19<07:48,  1.10it/s]Loading train:   4%|▍         | 20/532 [00:20<07:49,  1.09it/s]Loading train:   4%|▍         | 21/532 [00:21<08:08,  1.05it/s]Loading train:   4%|▍         | 22/532 [00:22<07:43,  1.10it/s]Loading train:   4%|▍         | 23/532 [00:23<07:51,  1.08it/s]Loading train:   5%|▍         | 24/532 [00:23<07:36,  1.11it/s]Loading train:   5%|▍         | 25/532 [00:25<08:31,  1.01s/it]Loading train:   5%|▍         | 26/532 [00:26<08:13,  1.03it/s]Loading train:   5%|▌         | 27/532 [00:27<09:05,  1.08s/it]Loading train:   5%|▌         | 28/532 [00:28<09:07,  1.09s/it]Loading train:   5%|▌         | 29/532 [00:29<08:56,  1.07s/it]Loading train:   6%|▌         | 30/532 [00:30<08:05,  1.03it/s]Loading train:   6%|▌         | 31/532 [00:31<07:41,  1.09it/s]Loading train:   6%|▌         | 32/532 [00:31<07:20,  1.13it/s]Loading train:   6%|▌         | 33/532 [00:32<07:19,  1.13it/s]Loading train:   6%|▋         | 34/532 [00:33<07:41,  1.08it/s]Loading train:   7%|▋         | 35/532 [00:34<07:53,  1.05it/s]Loading train:   7%|▋         | 36/532 [00:35<08:14,  1.00it/s]Loading train:   7%|▋         | 37/532 [00:36<08:14,  1.00it/s]Loading train:   7%|▋         | 38/532 [00:37<08:08,  1.01it/s]Loading train:   7%|▋         | 39/532 [00:38<07:27,  1.10it/s]Loading train:   8%|▊         | 40/532 [00:39<07:43,  1.06it/s]Loading train:   8%|▊         | 41/532 [00:40<07:45,  1.05it/s]Loading train:   8%|▊         | 42/532 [00:41<08:14,  1.01s/it]Loading train:   8%|▊         | 43/532 [00:42<07:37,  1.07it/s]Loading train:   8%|▊         | 44/532 [00:43<07:56,  1.02it/s]Loading train:   8%|▊         | 45/532 [00:44<08:06,  1.00it/s]Loading train:   9%|▊         | 46/532 [00:45<08:28,  1.05s/it]Loading train:   9%|▉         | 47/532 [00:46<08:42,  1.08s/it]Loading train:   9%|▉         | 48/532 [00:48<08:48,  1.09s/it]Loading train:   9%|▉         | 49/532 [00:48<08:07,  1.01s/it]Loading train:   9%|▉         | 50/532 [00:49<08:18,  1.03s/it]Loading train:  10%|▉         | 51/532 [00:50<07:49,  1.02it/s]Loading train:  10%|▉         | 52/532 [00:51<07:35,  1.05it/s]Loading train:  10%|▉         | 53/532 [00:52<07:46,  1.03it/s]Loading train:  10%|█         | 54/532 [00:53<08:13,  1.03s/it]Loading train:  10%|█         | 55/532 [00:55<08:34,  1.08s/it]Loading train:  11%|█         | 56/532 [00:56<08:46,  1.11s/it]Loading train:  11%|█         | 57/532 [00:57<09:08,  1.15s/it]Loading train:  11%|█         | 58/532 [00:58<08:23,  1.06s/it]Loading train:  11%|█         | 59/532 [00:59<09:16,  1.18s/it]Loading train:  11%|█▏        | 60/532 [01:00<08:09,  1.04s/it]Loading train:  11%|█▏        | 61/532 [01:01<07:47,  1.01it/s]Loading train:  12%|█▏        | 62/532 [01:02<07:39,  1.02it/s]Loading train:  12%|█▏        | 63/532 [01:03<08:26,  1.08s/it]Loading train:  12%|█▏        | 64/532 [01:04<07:53,  1.01s/it]Loading train:  12%|█▏        | 65/532 [01:05<08:34,  1.10s/it]Loading train:  12%|█▏        | 66/532 [01:06<08:42,  1.12s/it]Loading train:  13%|█▎        | 67/532 [01:08<09:19,  1.20s/it]Loading train:  13%|█▎        | 68/532 [01:09<08:37,  1.11s/it]Loading train:  13%|█▎        | 69/532 [01:10<08:01,  1.04s/it]Loading train:  13%|█▎        | 70/532 [01:11<07:40,  1.00it/s]Loading train:  13%|█▎        | 71/532 [01:11<07:31,  1.02it/s]Loading train:  14%|█▎        | 72/532 [01:12<06:46,  1.13it/s]Loading train:  14%|█▎        | 73/532 [01:13<07:05,  1.08it/s]Loading train:  14%|█▍        | 74/532 [01:14<07:40,  1.01s/it]Loading train:  14%|█▍        | 75/532 [01:16<09:26,  1.24s/it]Loading train:  14%|█▍        | 76/532 [01:17<08:57,  1.18s/it]Loading train:  14%|█▍        | 77/532 [01:18<08:54,  1.17s/it]Loading train:  15%|█▍        | 78/532 [01:19<08:32,  1.13s/it]Loading train:  15%|█▍        | 79/532 [01:20<07:56,  1.05s/it]Loading train:  15%|█▌        | 80/532 [01:21<08:04,  1.07s/it]Loading train:  15%|█▌        | 81/532 [01:22<07:33,  1.01s/it]Loading train:  15%|█▌        | 82/532 [01:23<08:02,  1.07s/it]Loading train:  16%|█▌        | 83/532 [01:24<07:57,  1.06s/it]Loading train:  16%|█▌        | 84/532 [01:25<07:37,  1.02s/it]Loading train:  16%|█▌        | 85/532 [01:26<07:02,  1.06it/s]Loading train:  16%|█▌        | 86/532 [01:27<07:02,  1.06it/s]Loading train:  16%|█▋        | 87/532 [01:28<06:47,  1.09it/s]Loading train:  17%|█▋        | 88/532 [01:29<06:56,  1.07it/s]Loading train:  17%|█▋        | 89/532 [01:30<06:53,  1.07it/s]Loading train:  17%|█▋        | 90/532 [01:31<06:44,  1.09it/s]Loading train:  17%|█▋        | 91/532 [01:32<07:30,  1.02s/it]Loading train:  17%|█▋        | 92/532 [01:33<07:49,  1.07s/it]Loading train:  17%|█▋        | 93/532 [01:34<07:12,  1.01it/s]Loading train:  18%|█▊        | 94/532 [01:35<06:56,  1.05it/s]Loading train:  18%|█▊        | 95/532 [01:36<07:11,  1.01it/s]Loading train:  18%|█▊        | 96/532 [01:37<07:26,  1.02s/it]Loading train:  18%|█▊        | 97/532 [01:38<07:28,  1.03s/it]Loading train:  18%|█▊        | 98/532 [01:39<07:46,  1.08s/it]Loading train:  19%|█▊        | 99/532 [01:40<07:33,  1.05s/it]Loading train:  19%|█▉        | 100/532 [01:41<07:53,  1.10s/it]Loading train:  19%|█▉        | 101/532 [01:42<07:09,  1.00it/s]Loading train:  19%|█▉        | 102/532 [01:43<07:13,  1.01s/it]Loading train:  19%|█▉        | 103/532 [01:44<06:38,  1.08it/s]Loading train:  20%|█▉        | 104/532 [01:45<06:33,  1.09it/s]Loading train:  20%|█▉        | 105/532 [01:46<06:11,  1.15it/s]Loading train:  20%|█▉        | 106/532 [01:47<06:20,  1.12it/s]Loading train:  20%|██        | 107/532 [01:47<06:01,  1.17it/s]Loading train:  20%|██        | 108/532 [01:48<06:02,  1.17it/s]Loading train:  20%|██        | 109/532 [01:49<05:56,  1.19it/s]Loading train:  21%|██        | 110/532 [01:50<06:04,  1.16it/s]Loading train:  21%|██        | 111/532 [01:51<05:43,  1.23it/s]Loading train:  21%|██        | 112/532 [01:51<05:45,  1.22it/s]Loading train:  21%|██        | 113/532 [01:52<06:06,  1.14it/s]Loading train:  21%|██▏       | 114/532 [01:54<06:28,  1.08it/s]Loading train:  22%|██▏       | 115/532 [01:54<06:25,  1.08it/s]Loading train:  22%|██▏       | 116/532 [01:56<06:42,  1.03it/s]Loading train:  22%|██▏       | 117/532 [01:57<06:59,  1.01s/it]Loading train:  22%|██▏       | 118/532 [01:58<06:42,  1.03it/s]Loading train:  22%|██▏       | 119/532 [01:59<07:11,  1.04s/it]Loading train:  23%|██▎       | 120/532 [02:00<06:49,  1.01it/s]Loading train:  23%|██▎       | 121/532 [02:01<06:56,  1.01s/it]Loading train:  23%|██▎       | 122/532 [02:02<06:37,  1.03it/s]Loading train:  23%|██▎       | 123/532 [02:03<07:04,  1.04s/it]Loading train:  23%|██▎       | 124/532 [02:04<07:20,  1.08s/it]Loading train:  23%|██▎       | 125/532 [02:05<07:37,  1.12s/it]Loading train:  24%|██▎       | 126/532 [02:06<07:25,  1.10s/it]Loading train:  24%|██▍       | 127/532 [02:07<07:28,  1.11s/it]Loading train:  24%|██▍       | 128/532 [02:09<07:40,  1.14s/it]Loading train:  24%|██▍       | 129/532 [02:10<07:41,  1.14s/it]Loading train:  24%|██▍       | 130/532 [02:11<07:18,  1.09s/it]Loading train:  25%|██▍       | 131/532 [02:12<07:31,  1.13s/it]Loading train:  25%|██▍       | 132/532 [02:14<08:50,  1.33s/it]Loading train:  25%|██▌       | 133/532 [02:15<09:37,  1.45s/it]Loading train:  25%|██▌       | 134/532 [02:17<09:43,  1.47s/it]Loading train:  25%|██▌       | 135/532 [02:19<10:03,  1.52s/it]Loading train:  26%|██▌       | 136/532 [02:20<10:26,  1.58s/it]Loading train:  26%|██▌       | 137/532 [02:22<10:54,  1.66s/it]Loading train:  26%|██▌       | 138/532 [02:24<11:10,  1.70s/it]Loading train:  26%|██▌       | 139/532 [02:26<11:28,  1.75s/it]Loading train:  26%|██▋       | 140/532 [02:27<11:08,  1.71s/it]Loading train:  27%|██▋       | 141/532 [02:29<11:16,  1.73s/it]Loading train:  27%|██▋       | 142/532 [02:31<11:10,  1.72s/it]Loading train:  27%|██▋       | 143/532 [02:32<10:34,  1.63s/it]Loading train:  27%|██▋       | 144/532 [02:34<10:16,  1.59s/it]Loading train:  27%|██▋       | 145/532 [02:35<10:01,  1.55s/it]Loading train:  27%|██▋       | 146/532 [02:37<09:51,  1.53s/it]Loading train:  28%|██▊       | 147/532 [02:38<09:25,  1.47s/it]Loading train:  28%|██▊       | 148/532 [02:39<09:01,  1.41s/it]Loading train:  28%|██▊       | 149/532 [02:41<08:58,  1.41s/it]Loading train:  28%|██▊       | 150/532 [02:42<08:39,  1.36s/it]Loading train:  28%|██▊       | 151/532 [02:43<08:26,  1.33s/it]Loading train:  29%|██▊       | 152/532 [02:45<08:36,  1.36s/it]Loading train:  29%|██▉       | 153/532 [02:46<08:36,  1.36s/it]Loading train:  29%|██▉       | 154/532 [02:47<08:16,  1.31s/it]Loading train:  29%|██▉       | 155/532 [02:49<09:30,  1.51s/it]Loading train:  29%|██▉       | 156/532 [02:51<10:19,  1.65s/it]Loading train:  30%|██▉       | 157/532 [02:53<10:26,  1.67s/it]Loading train:  30%|██▉       | 158/532 [02:55<10:41,  1.71s/it]Loading train:  30%|██▉       | 159/532 [02:56<10:34,  1.70s/it]Loading train:  30%|███       | 160/532 [02:58<11:02,  1.78s/it]Loading train:  30%|███       | 161/532 [03:00<10:47,  1.75s/it]Loading train:  30%|███       | 162/532 [03:02<10:25,  1.69s/it]Loading train:  31%|███       | 163/532 [03:03<10:04,  1.64s/it]Loading train:  31%|███       | 164/532 [03:05<09:54,  1.62s/it]Loading train:  31%|███       | 165/532 [03:06<09:39,  1.58s/it]Loading train:  31%|███       | 166/532 [03:07<08:57,  1.47s/it]Loading train:  31%|███▏      | 167/532 [03:09<08:49,  1.45s/it]Loading train:  32%|███▏      | 168/532 [03:10<08:34,  1.41s/it]Loading train:  32%|███▏      | 169/532 [03:12<08:37,  1.42s/it]Loading train:  32%|███▏      | 170/532 [03:13<08:41,  1.44s/it]Loading train:  32%|███▏      | 171/532 [03:14<08:21,  1.39s/it]Loading train:  32%|███▏      | 172/532 [03:16<08:37,  1.44s/it]Loading train:  33%|███▎      | 173/532 [03:17<08:25,  1.41s/it]Loading train:  33%|███▎      | 174/532 [03:18<08:12,  1.37s/it]Loading train:  33%|███▎      | 175/532 [03:20<08:41,  1.46s/it]Loading train:  33%|███▎      | 176/532 [03:21<08:15,  1.39s/it]Loading train:  33%|███▎      | 177/532 [03:23<08:05,  1.37s/it]Loading train:  33%|███▎      | 178/532 [03:24<08:04,  1.37s/it]Loading train:  34%|███▎      | 179/532 [03:25<07:39,  1.30s/it]Loading train:  34%|███▍      | 180/532 [03:26<07:11,  1.23s/it]Loading train:  34%|███▍      | 181/532 [03:28<07:29,  1.28s/it]Loading train:  34%|███▍      | 182/532 [03:29<07:29,  1.28s/it]Loading train:  34%|███▍      | 183/532 [03:30<07:10,  1.23s/it]Loading train:  35%|███▍      | 184/532 [03:31<07:08,  1.23s/it]Loading train:  35%|███▍      | 185/532 [03:32<06:52,  1.19s/it]Loading train:  35%|███▍      | 186/532 [03:33<06:43,  1.17s/it]Loading train:  35%|███▌      | 187/532 [03:35<06:43,  1.17s/it]Loading train:  35%|███▌      | 188/532 [03:36<06:59,  1.22s/it]Loading train:  36%|███▌      | 189/532 [03:37<07:01,  1.23s/it]Loading train:  36%|███▌      | 190/532 [03:39<07:18,  1.28s/it]Loading train:  36%|███▌      | 191/532 [03:40<08:04,  1.42s/it]Loading train:  36%|███▌      | 192/532 [03:42<08:47,  1.55s/it]Loading train:  36%|███▋      | 193/532 [03:44<09:24,  1.67s/it]Loading train:  36%|███▋      | 194/532 [03:46<09:38,  1.71s/it]Loading train:  37%|███▋      | 195/532 [03:48<10:06,  1.80s/it]Loading train:  37%|███▋      | 196/532 [03:50<09:35,  1.71s/it]Loading train:  37%|███▋      | 197/532 [03:51<08:57,  1.61s/it]Loading train:  37%|███▋      | 198/532 [03:53<09:00,  1.62s/it]Loading train:  37%|███▋      | 199/532 [03:54<09:17,  1.67s/it]Loading train:  38%|███▊      | 200/532 [03:56<08:54,  1.61s/it]Loading train:  38%|███▊      | 201/532 [03:58<09:06,  1.65s/it]Loading train:  38%|███▊      | 202/532 [03:59<08:43,  1.59s/it]Loading train:  38%|███▊      | 203/532 [04:00<08:18,  1.51s/it]Loading train:  38%|███▊      | 204/532 [04:02<08:22,  1.53s/it]Loading train:  39%|███▊      | 205/532 [04:03<08:29,  1.56s/it]Loading train:  39%|███▊      | 206/532 [04:05<08:07,  1.49s/it]Loading train:  39%|███▉      | 207/532 [04:06<08:12,  1.52s/it]Loading train:  39%|███▉      | 208/532 [04:08<07:41,  1.42s/it]Loading train:  39%|███▉      | 209/532 [04:09<07:42,  1.43s/it]Loading train:  39%|███▉      | 210/532 [04:10<07:37,  1.42s/it]Loading train:  40%|███▉      | 211/532 [04:12<07:12,  1.35s/it]Loading train:  40%|███▉      | 212/532 [04:13<07:04,  1.33s/it]Loading train:  40%|████      | 213/532 [04:14<06:46,  1.28s/it]Loading train:  40%|████      | 214/532 [04:15<06:53,  1.30s/it]Loading train:  40%|████      | 215/532 [04:17<07:53,  1.49s/it]Loading train:  41%|████      | 216/532 [04:19<08:19,  1.58s/it]Loading train:  41%|████      | 217/532 [04:21<08:46,  1.67s/it]Loading train:  41%|████      | 218/532 [04:23<08:47,  1.68s/it]Loading train:  41%|████      | 219/532 [04:25<09:29,  1.82s/it]Loading train:  41%|████▏     | 220/532 [04:27<09:35,  1.85s/it]Loading train:  42%|████▏     | 221/532 [04:28<09:02,  1.74s/it]Loading train:  42%|████▏     | 222/532 [04:30<08:48,  1.71s/it]Loading train:  42%|████▏     | 223/532 [04:31<08:21,  1.62s/it]Loading train:  42%|████▏     | 224/532 [04:32<07:33,  1.47s/it]Loading train:  42%|████▏     | 225/532 [04:34<07:31,  1.47s/it]Loading train:  42%|████▏     | 226/532 [04:35<07:24,  1.45s/it]Loading train:  43%|████▎     | 227/532 [04:37<07:17,  1.44s/it]Loading train:  43%|████▎     | 228/532 [04:38<07:25,  1.47s/it]Loading train:  43%|████▎     | 229/532 [04:40<07:12,  1.43s/it]Loading train:  43%|████▎     | 230/532 [04:41<06:53,  1.37s/it]Loading train:  43%|████▎     | 231/532 [04:42<07:03,  1.41s/it]Loading train:  44%|████▎     | 232/532 [04:44<06:41,  1.34s/it]Loading train:  44%|████▍     | 233/532 [04:45<06:58,  1.40s/it]Loading train:  44%|████▍     | 234/532 [04:47<07:12,  1.45s/it]Loading train:  44%|████▍     | 235/532 [04:48<07:17,  1.47s/it]Loading train:  44%|████▍     | 236/532 [04:50<07:04,  1.43s/it]Loading train:  45%|████▍     | 237/532 [04:51<06:55,  1.41s/it]Loading train:  45%|████▍     | 238/532 [04:52<07:07,  1.45s/it]Loading train:  45%|████▍     | 239/532 [04:54<07:18,  1.50s/it]Loading train:  45%|████▌     | 240/532 [04:55<07:08,  1.47s/it]Loading train:  45%|████▌     | 241/532 [04:57<07:03,  1.45s/it]Loading train:  45%|████▌     | 242/532 [04:58<06:41,  1.39s/it]Loading train:  46%|████▌     | 243/532 [04:59<06:43,  1.40s/it]Loading train:  46%|████▌     | 244/532 [05:01<06:54,  1.44s/it]Loading train:  46%|████▌     | 245/532 [05:02<06:46,  1.42s/it]Loading train:  46%|████▌     | 246/532 [05:04<06:39,  1.40s/it]Loading train:  46%|████▋     | 247/532 [05:05<06:17,  1.33s/it]Loading train:  47%|████▋     | 248/532 [05:06<06:24,  1.35s/it]Loading train:  47%|████▋     | 249/532 [05:08<06:18,  1.34s/it]Loading train:  47%|████▋     | 250/532 [05:09<05:57,  1.27s/it]Loading train:  47%|████▋     | 251/532 [05:10<06:06,  1.30s/it]Loading train:  47%|████▋     | 252/532 [05:12<06:16,  1.34s/it]Loading train:  48%|████▊     | 253/532 [05:13<06:18,  1.36s/it]Loading train:  48%|████▊     | 254/532 [05:14<06:16,  1.35s/it]Loading train:  48%|████▊     | 255/532 [05:16<06:22,  1.38s/it]Loading train:  48%|████▊     | 256/532 [05:17<06:20,  1.38s/it]Loading train:  48%|████▊     | 257/532 [05:19<06:58,  1.52s/it]Loading train:  48%|████▊     | 258/532 [05:21<07:26,  1.63s/it]Loading train:  49%|████▊     | 259/532 [05:23<07:37,  1.68s/it]Loading train:  49%|████▉     | 260/532 [05:24<07:38,  1.68s/it]Loading train:  49%|████▉     | 261/532 [05:26<07:23,  1.64s/it]Loading train:  49%|████▉     | 262/532 [05:28<07:31,  1.67s/it]Loading train:  49%|████▉     | 263/532 [05:29<07:00,  1.56s/it]Loading train:  50%|████▉     | 264/532 [05:30<06:31,  1.46s/it]Loading train:  50%|████▉     | 265/532 [05:31<06:09,  1.38s/it]Loading train:  50%|█████     | 266/532 [05:32<05:39,  1.28s/it]Loading train:  50%|█████     | 267/532 [05:33<05:09,  1.17s/it]Loading train:  50%|█████     | 268/532 [05:34<04:50,  1.10s/it]Loading train:  51%|█████     | 269/532 [05:36<05:10,  1.18s/it]Loading train:  51%|█████     | 270/532 [05:37<05:31,  1.26s/it]Loading train:  51%|█████     | 271/532 [05:39<05:46,  1.33s/it]Loading train:  51%|█████     | 272/532 [05:40<05:44,  1.32s/it]Loading train:  51%|█████▏    | 273/532 [05:41<05:41,  1.32s/it]Loading train:  52%|█████▏    | 274/532 [05:43<05:52,  1.37s/it]Loading train:  52%|█████▏    | 275/532 [05:44<06:13,  1.45s/it]Loading train:  52%|█████▏    | 276/532 [05:46<06:19,  1.48s/it]Loading train:  52%|█████▏    | 277/532 [05:47<06:24,  1.51s/it]Loading train:  52%|█████▏    | 278/532 [05:49<06:30,  1.54s/it]Loading train:  52%|█████▏    | 279/532 [05:51<06:39,  1.58s/it]Loading train:  53%|█████▎    | 280/532 [05:52<06:19,  1.51s/it]Loading train:  53%|█████▎    | 281/532 [05:54<06:42,  1.61s/it]Loading train:  53%|█████▎    | 282/532 [05:56<07:04,  1.70s/it]Loading train:  53%|█████▎    | 283/532 [05:57<06:59,  1.69s/it]Loading train:  53%|█████▎    | 284/532 [05:59<06:49,  1.65s/it]Loading train:  54%|█████▎    | 285/532 [06:01<06:49,  1.66s/it]Loading train:  54%|█████▍    | 286/532 [06:02<06:46,  1.65s/it]Loading train:  54%|█████▍    | 287/532 [06:04<06:26,  1.58s/it]Loading train:  54%|█████▍    | 288/532 [06:05<05:57,  1.47s/it]Loading train:  54%|█████▍    | 289/532 [06:06<05:41,  1.40s/it]Loading train:  55%|█████▍    | 290/532 [06:07<05:24,  1.34s/it]Loading train:  55%|█████▍    | 291/532 [06:09<05:11,  1.29s/it]Loading train:  55%|█████▍    | 292/532 [06:10<05:04,  1.27s/it]Loading train:  55%|█████▌    | 293/532 [06:11<05:21,  1.34s/it]Loading train:  55%|█████▌    | 294/532 [06:13<05:29,  1.38s/it]Loading train:  55%|█████▌    | 295/532 [06:15<05:54,  1.49s/it]Loading train:  56%|█████▌    | 296/532 [06:16<05:56,  1.51s/it]Loading train:  56%|█████▌    | 297/532 [06:18<06:05,  1.55s/it]Loading train:  56%|█████▌    | 298/532 [06:19<05:48,  1.49s/it]Loading train:  56%|█████▌    | 299/532 [06:20<05:32,  1.43s/it]Loading train:  56%|█████▋    | 300/532 [06:22<05:20,  1.38s/it]Loading train:  57%|█████▋    | 301/532 [06:23<05:17,  1.37s/it]Loading train:  57%|█████▋    | 302/532 [06:24<05:12,  1.36s/it]Loading train:  57%|█████▋    | 303/532 [06:25<04:57,  1.30s/it]Loading train:  57%|█████▋    | 304/532 [06:27<05:16,  1.39s/it]Loading train:  57%|█████▋    | 305/532 [06:29<05:45,  1.52s/it]Loading train:  58%|█████▊    | 306/532 [06:31<06:15,  1.66s/it]Loading train:  58%|█████▊    | 307/532 [06:33<06:52,  1.83s/it]Loading train:  58%|█████▊    | 308/532 [06:35<06:40,  1.79s/it]Loading train:  58%|█████▊    | 309/532 [06:37<06:36,  1.78s/it]Loading train:  58%|█████▊    | 310/532 [06:38<06:18,  1.70s/it]Loading train:  58%|█████▊    | 311/532 [06:40<06:37,  1.80s/it]Loading train:  59%|█████▊    | 312/532 [06:42<06:58,  1.90s/it]Loading train:  59%|█████▉    | 313/532 [06:44<06:59,  1.91s/it]Loading train:  59%|█████▉    | 314/532 [06:46<07:00,  1.93s/it]Loading train:  59%|█████▉    | 315/532 [06:48<07:07,  1.97s/it]Loading train:  59%|█████▉    | 316/532 [06:51<07:29,  2.08s/it]Loading train:  60%|█████▉    | 317/532 [06:52<06:57,  1.94s/it]Loading train:  60%|█████▉    | 318/532 [06:54<06:22,  1.79s/it]Loading train:  60%|█████▉    | 319/532 [06:55<05:58,  1.68s/it]Loading train:  60%|██████    | 320/532 [06:56<05:36,  1.59s/it]Loading train:  60%|██████    | 321/532 [06:58<05:35,  1.59s/it]Loading train:  61%|██████    | 322/532 [06:59<05:26,  1.55s/it]Loading train:  61%|██████    | 323/532 [07:01<05:39,  1.63s/it]Loading train:  61%|██████    | 324/532 [07:03<05:34,  1.61s/it]Loading train:  61%|██████    | 325/532 [07:05<05:47,  1.68s/it]Loading train:  61%|██████▏   | 326/532 [07:07<05:57,  1.74s/it]Loading train:  61%|██████▏   | 327/532 [07:08<05:34,  1.63s/it]Loading train:  62%|██████▏   | 328/532 [07:09<05:12,  1.53s/it]Loading train:  62%|██████▏   | 329/532 [07:10<04:34,  1.35s/it]Loading train:  62%|██████▏   | 330/532 [07:11<04:32,  1.35s/it]Loading train:  62%|██████▏   | 331/532 [07:13<04:16,  1.28s/it]Loading train:  62%|██████▏   | 332/532 [07:13<03:49,  1.15s/it]Loading train:  63%|██████▎   | 333/532 [07:14<03:31,  1.06s/it]Loading train:  63%|██████▎   | 334/532 [07:15<03:17,  1.00it/s]Loading train:  63%|██████▎   | 335/532 [07:16<03:36,  1.10s/it]Loading train:  63%|██████▎   | 336/532 [07:18<03:39,  1.12s/it]Loading train:  63%|██████▎   | 337/532 [07:19<03:35,  1.11s/it]Loading train:  64%|██████▎   | 338/532 [07:20<04:10,  1.29s/it]Loading train:  64%|██████▎   | 339/532 [07:22<04:22,  1.36s/it]Loading train:  64%|██████▍   | 340/532 [07:24<04:38,  1.45s/it]Loading train:  64%|██████▍   | 341/532 [07:25<04:33,  1.43s/it]Loading train:  64%|██████▍   | 342/532 [07:26<04:23,  1.39s/it]Loading train:  64%|██████▍   | 343/532 [07:27<04:05,  1.30s/it]Loading train:  65%|██████▍   | 344/532 [07:29<04:00,  1.28s/it]Loading train:  65%|██████▍   | 345/532 [07:30<03:54,  1.25s/it]Loading train:  65%|██████▌   | 346/532 [07:31<03:50,  1.24s/it]Loading train:  65%|██████▌   | 347/532 [07:32<03:42,  1.20s/it]Loading train:  65%|██████▌   | 348/532 [07:33<03:37,  1.18s/it]Loading train:  66%|██████▌   | 349/532 [07:34<03:34,  1.17s/it]Loading train:  66%|██████▌   | 350/532 [07:36<03:33,  1.17s/it]Loading train:  66%|██████▌   | 351/532 [07:37<03:45,  1.25s/it]Loading train:  66%|██████▌   | 352/532 [07:38<03:43,  1.24s/it]Loading train:  66%|██████▋   | 353/532 [07:40<03:42,  1.24s/it]Loading train:  67%|██████▋   | 354/532 [07:41<03:42,  1.25s/it]Loading train:  67%|██████▋   | 355/532 [07:42<03:36,  1.22s/it]Loading train:  67%|██████▋   | 356/532 [07:43<03:40,  1.26s/it]Loading train:  67%|██████▋   | 357/532 [07:44<03:31,  1.21s/it]Loading train:  67%|██████▋   | 358/532 [07:46<03:32,  1.22s/it]Loading train:  67%|██████▋   | 359/532 [07:47<03:37,  1.26s/it]Loading train:  68%|██████▊   | 360/532 [07:48<03:47,  1.32s/it]Loading train:  68%|██████▊   | 361/532 [07:50<03:40,  1.29s/it]Loading train:  68%|██████▊   | 362/532 [07:51<03:32,  1.25s/it]Loading train:  68%|██████▊   | 363/532 [07:52<03:39,  1.30s/it]Loading train:  68%|██████▊   | 364/532 [07:53<03:30,  1.26s/it]Loading train:  69%|██████▊   | 365/532 [07:55<03:37,  1.30s/it]Loading train:  69%|██████▉   | 366/532 [07:56<03:16,  1.19s/it]Loading train:  69%|██████▉   | 367/532 [07:57<03:13,  1.17s/it]Loading train:  69%|██████▉   | 368/532 [07:58<03:07,  1.14s/it]Loading train:  69%|██████▉   | 369/532 [07:59<03:09,  1.16s/it]Loading train:  70%|██████▉   | 370/532 [08:00<03:00,  1.12s/it]Loading train:  70%|██████▉   | 371/532 [08:02<03:19,  1.24s/it]Loading train:  70%|██████▉   | 372/532 [08:03<03:38,  1.36s/it]Loading train:  70%|███████   | 373/532 [08:05<03:48,  1.44s/it]Loading train:  70%|███████   | 374/532 [08:06<03:41,  1.40s/it]Loading train:  70%|███████   | 375/532 [08:08<03:59,  1.53s/it]Loading train:  71%|███████   | 376/532 [08:10<04:04,  1.57s/it]Loading train:  71%|███████   | 377/532 [08:11<03:56,  1.53s/it]Loading train:  71%|███████   | 378/532 [08:13<03:47,  1.48s/it]Loading train:  71%|███████   | 379/532 [08:14<03:44,  1.47s/it]Loading train:  71%|███████▏  | 380/532 [08:15<03:26,  1.36s/it]Loading train:  72%|███████▏  | 381/532 [08:16<03:22,  1.34s/it]Loading train:  72%|███████▏  | 382/532 [08:18<03:19,  1.33s/it]Loading train:  72%|███████▏  | 383/532 [08:19<03:09,  1.28s/it]Loading train:  72%|███████▏  | 384/532 [08:20<03:10,  1.28s/it]Loading train:  72%|███████▏  | 385/532 [08:21<02:55,  1.20s/it]Loading train:  73%|███████▎  | 386/532 [08:22<03:01,  1.25s/it]Loading train:  73%|███████▎  | 387/532 [08:24<02:56,  1.21s/it]Loading train:  73%|███████▎  | 388/532 [08:25<02:59,  1.25s/it]Loading train:  73%|███████▎  | 389/532 [08:26<03:06,  1.30s/it]Loading train:  73%|███████▎  | 390/532 [08:28<03:13,  1.36s/it]Loading train:  73%|███████▎  | 391/532 [08:29<03:14,  1.38s/it]Loading train:  74%|███████▎  | 392/532 [08:30<03:02,  1.30s/it]Loading train:  74%|███████▍  | 393/532 [08:32<03:12,  1.38s/it]Loading train:  74%|███████▍  | 394/532 [08:33<03:09,  1.37s/it]Loading train:  74%|███████▍  | 395/532 [08:35<03:06,  1.36s/it]Loading train:  74%|███████▍  | 396/532 [08:36<03:08,  1.39s/it]Loading train:  75%|███████▍  | 397/532 [08:38<03:10,  1.41s/it]Loading train:  75%|███████▍  | 398/532 [08:39<02:58,  1.34s/it]Loading train:  75%|███████▌  | 399/532 [08:40<02:53,  1.30s/it]Loading train:  75%|███████▌  | 400/532 [08:41<02:59,  1.36s/it]Loading train:  75%|███████▌  | 401/532 [08:43<03:04,  1.41s/it]Loading train:  76%|███████▌  | 402/532 [08:44<03:03,  1.41s/it]Loading train:  76%|███████▌  | 403/532 [08:46<03:04,  1.43s/it]Loading train:  76%|███████▌  | 404/532 [08:47<02:55,  1.37s/it]Loading train:  76%|███████▌  | 405/532 [08:49<03:00,  1.42s/it]Loading train:  76%|███████▋  | 406/532 [08:50<02:57,  1.41s/it]Loading train:  77%|███████▋  | 407/532 [08:51<02:49,  1.36s/it]Loading train:  77%|███████▋  | 408/532 [08:52<02:33,  1.24s/it]Loading train:  77%|███████▋  | 409/532 [08:53<02:30,  1.22s/it]Loading train:  77%|███████▋  | 410/532 [08:54<02:24,  1.18s/it]Loading train:  77%|███████▋  | 411/532 [08:55<02:15,  1.12s/it]Loading train:  77%|███████▋  | 412/532 [08:57<02:11,  1.10s/it]Loading train:  78%|███████▊  | 413/532 [08:58<02:09,  1.09s/it]Loading train:  78%|███████▊  | 414/532 [08:59<02:21,  1.20s/it]Loading train:  78%|███████▊  | 415/532 [09:00<02:17,  1.17s/it]Loading train:  78%|███████▊  | 416/532 [09:01<02:14,  1.16s/it]Loading train:  78%|███████▊  | 417/532 [09:03<02:19,  1.21s/it]Loading train:  79%|███████▊  | 418/532 [09:04<02:11,  1.15s/it]Loading train:  79%|███████▉  | 419/532 [09:05<02:21,  1.25s/it]Loading train:  79%|███████▉  | 420/532 [09:06<02:19,  1.25s/it]Loading train:  79%|███████▉  | 421/532 [09:08<02:24,  1.30s/it]Loading train:  79%|███████▉  | 422/532 [09:09<02:20,  1.28s/it]Loading train:  80%|███████▉  | 423/532 [09:10<02:11,  1.20s/it]Loading train:  80%|███████▉  | 424/532 [09:11<02:07,  1.18s/it]Loading train:  80%|███████▉  | 425/532 [09:12<02:03,  1.16s/it]Loading train:  80%|████████  | 426/532 [09:14<02:19,  1.32s/it]Loading train:  80%|████████  | 427/532 [09:16<02:26,  1.40s/it]Loading train:  80%|████████  | 428/532 [09:17<02:19,  1.34s/it]Loading train:  81%|████████  | 429/532 [09:18<02:12,  1.29s/it]Loading train:  81%|████████  | 430/532 [09:19<02:13,  1.31s/it]Loading train:  81%|████████  | 431/532 [09:21<02:16,  1.35s/it]Loading train:  81%|████████  | 432/532 [09:22<02:20,  1.41s/it]Loading train:  81%|████████▏ | 433/532 [09:24<02:19,  1.41s/it]Loading train:  82%|████████▏ | 434/532 [09:25<02:15,  1.39s/it]Loading train:  82%|████████▏ | 435/532 [09:26<02:12,  1.37s/it]Loading train:  82%|████████▏ | 436/532 [09:28<02:11,  1.37s/it]Loading train:  82%|████████▏ | 437/532 [09:29<02:12,  1.39s/it]Loading train:  82%|████████▏ | 438/532 [09:30<01:59,  1.27s/it]Loading train:  83%|████████▎ | 439/532 [09:31<01:57,  1.27s/it]Loading train:  83%|████████▎ | 440/532 [09:33<01:54,  1.24s/it]Loading train:  83%|████████▎ | 441/532 [09:34<01:44,  1.15s/it]Loading train:  83%|████████▎ | 442/532 [09:34<01:38,  1.09s/it]Loading train:  83%|████████▎ | 443/532 [09:37<02:19,  1.56s/it]Loading train:  83%|████████▎ | 444/532 [09:38<02:08,  1.46s/it]Loading train:  84%|████████▎ | 445/532 [09:40<02:24,  1.66s/it]Loading train:  84%|████████▍ | 446/532 [09:43<02:40,  1.87s/it]Loading train:  84%|████████▍ | 447/532 [09:45<02:43,  1.92s/it]Loading train:  84%|████████▍ | 448/532 [09:47<02:58,  2.12s/it]Loading train:  84%|████████▍ | 449/532 [09:50<03:13,  2.33s/it]Loading train:  85%|████████▍ | 450/532 [09:53<03:32,  2.59s/it]Loading train:  85%|████████▍ | 451/532 [09:55<03:13,  2.39s/it]Loading train:  85%|████████▍ | 452/532 [09:58<03:26,  2.58s/it]Loading train:  85%|████████▌ | 453/532 [10:01<03:13,  2.44s/it]Loading train:  85%|████████▌ | 454/532 [10:04<03:24,  2.62s/it]Loading train:  86%|████████▌ | 455/532 [10:07<03:36,  2.81s/it]Loading train:  86%|████████▌ | 456/532 [10:09<03:28,  2.75s/it]Loading train:  86%|████████▌ | 457/532 [10:11<03:07,  2.50s/it]Loading train:  86%|████████▌ | 458/532 [10:14<03:14,  2.63s/it]Loading train:  86%|████████▋ | 459/532 [10:17<03:04,  2.52s/it]Loading train:  86%|████████▋ | 460/532 [10:19<02:58,  2.48s/it]Loading train:  87%|████████▋ | 461/532 [10:22<03:03,  2.59s/it]Loading train:  87%|████████▋ | 462/532 [10:25<03:03,  2.63s/it]Loading train:  87%|████████▋ | 463/532 [10:27<03:02,  2.64s/it]Loading train:  87%|████████▋ | 464/532 [10:30<02:58,  2.63s/it]Loading train:  87%|████████▋ | 465/532 [10:33<03:12,  2.88s/it]Loading train:  88%|████████▊ | 466/532 [10:37<03:19,  3.02s/it]Loading train:  88%|████████▊ | 467/532 [10:40<03:22,  3.11s/it]Loading train:  88%|████████▊ | 468/532 [10:43<03:21,  3.15s/it]Loading train:  88%|████████▊ | 469/532 [10:45<02:55,  2.79s/it]Loading train:  88%|████████▊ | 470/532 [10:48<02:47,  2.71s/it]Loading train:  89%|████████▊ | 471/532 [10:50<02:42,  2.66s/it]Loading train:  89%|████████▊ | 472/532 [10:53<02:48,  2.80s/it]Loading train:  89%|████████▉ | 473/532 [10:56<02:35,  2.63s/it]Loading train:  89%|████████▉ | 474/532 [10:59<02:41,  2.78s/it]Loading train:  89%|████████▉ | 475/532 [11:02<02:52,  3.03s/it]Loading train:  89%|████████▉ | 476/532 [11:04<02:31,  2.70s/it]Loading train:  90%|████████▉ | 477/532 [11:07<02:23,  2.60s/it]Loading train:  90%|████████▉ | 478/532 [11:08<02:08,  2.37s/it]Loading train:  90%|█████████ | 479/532 [11:11<02:05,  2.37s/it]Loading train:  90%|█████████ | 480/532 [11:13<02:06,  2.44s/it]Loading train:  90%|█████████ | 481/532 [11:15<01:53,  2.23s/it]Loading train:  91%|█████████ | 482/532 [11:18<01:54,  2.29s/it]Loading train:  91%|█████████ | 483/532 [11:20<01:50,  2.25s/it]Loading train:  91%|█████████ | 484/532 [11:22<01:54,  2.38s/it]Loading train:  91%|█████████ | 485/532 [11:25<01:56,  2.47s/it]Loading train:  91%|█████████▏| 486/532 [11:28<01:57,  2.55s/it]Loading train:  92%|█████████▏| 487/532 [11:30<01:51,  2.47s/it]Loading train:  92%|█████████▏| 488/532 [11:33<01:55,  2.63s/it]Loading train:  92%|█████████▏| 489/532 [11:35<01:45,  2.45s/it]Loading train:  92%|█████████▏| 490/532 [11:38<01:44,  2.49s/it]Loading train:  92%|█████████▏| 491/532 [11:40<01:39,  2.42s/it]Loading train:  92%|█████████▏| 492/532 [11:42<01:32,  2.31s/it]Loading train:  93%|█████████▎| 493/532 [11:44<01:30,  2.31s/it]Loading train:  93%|█████████▎| 494/532 [11:47<01:35,  2.52s/it]Loading train:  93%|█████████▎| 495/532 [11:50<01:30,  2.45s/it]Loading train:  93%|█████████▎| 496/532 [11:52<01:27,  2.42s/it]Loading train:  93%|█████████▎| 497/532 [11:55<01:26,  2.48s/it]Loading train:  94%|█████████▎| 498/532 [11:57<01:22,  2.44s/it]Loading train:  94%|█████████▍| 499/532 [11:59<01:19,  2.41s/it]Loading train:  94%|█████████▍| 500/532 [12:03<01:24,  2.65s/it]Loading train:  94%|█████████▍| 501/532 [12:06<01:26,  2.80s/it]Loading train:  94%|█████████▍| 502/532 [12:07<01:14,  2.47s/it]Loading train:  95%|█████████▍| 503/532 [12:10<01:10,  2.43s/it]Loading train:  95%|█████████▍| 504/532 [12:12<01:10,  2.52s/it]Loading train:  95%|█████████▍| 505/532 [12:15<01:05,  2.41s/it]Loading train:  95%|█████████▌| 506/532 [12:18<01:10,  2.71s/it]Loading train:  95%|█████████▌| 507/532 [12:20<01:04,  2.56s/it]Loading train:  95%|█████████▌| 508/532 [12:23<01:06,  2.77s/it]Loading train:  96%|█████████▌| 509/532 [12:26<01:02,  2.70s/it]Loading train:  96%|█████████▌| 510/532 [12:29<00:58,  2.64s/it]Loading train:  96%|█████████▌| 511/532 [12:31<00:57,  2.73s/it]Loading train:  96%|█████████▌| 512/532 [12:34<00:56,  2.80s/it]Loading train:  96%|█████████▋| 513/532 [12:37<00:54,  2.87s/it]Loading train:  97%|█████████▋| 514/532 [12:41<00:53,  2.98s/it]Loading train:  97%|█████████▋| 515/532 [12:43<00:46,  2.72s/it]Loading train:  97%|█████████▋| 516/532 [12:45<00:43,  2.71s/it]Loading train:  97%|█████████▋| 517/532 [12:48<00:40,  2.68s/it]Loading train:  97%|█████████▋| 518/532 [12:50<00:36,  2.57s/it]Loading train:  98%|█████████▊| 519/532 [12:53<00:34,  2.66s/it]Loading train:  98%|█████████▊| 520/532 [12:56<00:33,  2.75s/it]Loading train:  98%|█████████▊| 521/532 [12:59<00:31,  2.86s/it]Loading train:  98%|█████████▊| 522/532 [13:02<00:27,  2.79s/it]Loading train:  98%|█████████▊| 523/532 [13:04<00:22,  2.48s/it]Loading train:  98%|█████████▊| 524/532 [13:05<00:17,  2.17s/it]Loading train:  99%|█████████▊| 525/532 [13:07<00:14,  2.07s/it]Loading train:  99%|█████████▉| 526/532 [13:10<00:14,  2.35s/it]Loading train:  99%|█████████▉| 527/532 [13:13<00:12,  2.51s/it]Loading train:  99%|█████████▉| 528/532 [13:15<00:09,  2.32s/it]Loading train:  99%|█████████▉| 529/532 [13:17<00:06,  2.21s/it]Loading train: 100%|█████████▉| 530/532 [13:18<00:04,  2.07s/it]Loading train: 100%|█████████▉| 531/532 [13:20<00:01,  1.83s/it]Loading train: 100%|██████████| 532/532 [13:22<00:00,  1.81s/it]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   0%|          | 2/532 [00:00<00:42, 12.44it/s]concatenating: train:   1%|          | 4/532 [00:00<00:37, 14.03it/s]concatenating: train:   1%|          | 6/532 [00:00<00:35, 15.00it/s]concatenating: train:   2%|▏         | 8/532 [00:00<00:33, 15.75it/s]concatenating: train:   2%|▏         | 11/532 [00:00<00:30, 17.11it/s]concatenating: train:   3%|▎         | 14/532 [00:00<00:27, 18.76it/s]concatenating: train:   3%|▎         | 18/532 [00:00<00:23, 21.88it/s]concatenating: train:   5%|▌         | 28/532 [00:00<00:17, 28.56it/s]concatenating: train:   7%|▋         | 37/532 [00:01<00:13, 35.43it/s]concatenating: train:   8%|▊         | 43/532 [00:01<00:17, 27.66it/s]concatenating: train:   9%|▉         | 48/532 [00:01<00:21, 22.35it/s]concatenating: train:  10%|▉         | 52/532 [00:01<00:20, 22.92it/s]concatenating: train:  11%|█▏        | 61/532 [00:01<00:16, 29.39it/s]concatenating: train:  14%|█▎        | 73/532 [00:02<00:12, 36.88it/s]concatenating: train:  15%|█▌        | 80/532 [00:02<00:16, 27.04it/s]concatenating: train:  16%|█▌        | 85/532 [00:02<00:21, 20.35it/s]concatenating: train:  17%|█▋        | 89/532 [00:03<00:24, 17.86it/s]concatenating: train:  18%|█▊        | 94/532 [00:03<00:20, 21.44it/s]concatenating: train:  19%|█▊        | 99/532 [00:03<00:16, 25.54it/s]concatenating: train:  21%|██        | 113/532 [00:03<00:12, 33.76it/s]concatenating: train:  23%|██▎       | 120/532 [00:03<00:12, 31.85it/s]concatenating: train:  24%|██▎       | 126/532 [00:04<00:20, 19.86it/s]concatenating: train:  25%|██▍       | 131/532 [00:04<00:20, 19.51it/s]concatenating: train:  26%|██▌       | 139/532 [00:04<00:15, 25.10it/s]concatenating: train:  29%|██▉       | 153/532 [00:04<00:11, 33.21it/s]concatenating: train:  32%|███▏      | 169/532 [00:04<00:08, 43.45it/s]concatenating: train:  34%|███▍      | 180/532 [00:05<00:07, 46.32it/s]concatenating: train:  36%|███▌      | 189/532 [00:05<00:14, 24.24it/s]concatenating: train:  37%|███▋      | 196/532 [00:06<00:15, 22.33it/s]concatenating: train:  38%|███▊      | 204/532 [00:06<00:11, 28.35it/s]concatenating: train:  39%|███▉      | 210/532 [00:06<00:09, 33.04it/s]concatenating: train:  41%|████      | 216/532 [00:06<00:14, 22.33it/s]concatenating: train:  42%|████▏     | 221/532 [00:07<00:16, 18.31it/s]concatenating: train:  42%|████▏     | 225/532 [00:07<00:15, 19.55it/s]concatenating: train:  44%|████▎     | 232/532 [00:07<00:12, 24.94it/s]concatenating: train:  48%|████▊     | 254/532 [00:07<00:08, 33.85it/s]concatenating: train:  50%|████▉     | 264/532 [00:08<00:08, 31.42it/s]concatenating: train:  51%|█████     | 272/532 [00:08<00:12, 21.54it/s]concatenating: train:  55%|█████▍    | 291/532 [00:08<00:08, 29.07it/s]concatenating: train:  59%|█████▊    | 312/532 [00:09<00:05, 39.12it/s]concatenating: train:  61%|██████    | 325/532 [00:10<00:08, 23.21it/s]concatenating: train:  63%|██████▎   | 334/532 [00:10<00:07, 27.34it/s]concatenating: train:  66%|██████▌   | 350/532 [00:10<00:05, 36.35it/s]concatenating: train:  69%|██████▊   | 365/532 [00:10<00:03, 45.59it/s]concatenating: train:  71%|███████   | 376/532 [00:11<00:05, 26.51it/s]concatenating: train:  72%|███████▏  | 384/532 [00:11<00:04, 29.64it/s]concatenating: train:  75%|███████▌  | 399/532 [00:11<00:03, 38.99it/s]concatenating: train:  77%|███████▋  | 409/532 [00:12<00:03, 33.69it/s]concatenating: train:  78%|███████▊  | 417/532 [00:12<00:05, 22.71it/s]concatenating: train:  80%|████████  | 427/532 [00:12<00:03, 29.45it/s]concatenating: train:  83%|████████▎ | 439/532 [00:12<00:02, 37.92it/s]concatenating: train:  84%|████████▍ | 448/532 [00:13<00:02, 28.06it/s]concatenating: train:  86%|████████▌ | 455/532 [00:14<00:04, 18.43it/s]concatenating: train:  86%|████████▋ | 460/532 [00:14<00:04, 16.78it/s]concatenating: train:  89%|████████▊ | 472/532 [00:14<00:02, 22.60it/s]concatenating: train:  91%|█████████▏| 486/532 [00:14<00:01, 30.13it/s]concatenating: train:  94%|█████████▍| 500/532 [00:14<00:00, 39.25it/s]concatenating: train:  97%|█████████▋| 514/532 [00:14<00:00, 49.29it/s]concatenating: train:  99%|█████████▊| 525/532 [00:15<00:00, 23.85it/s]concatenating: train: 100%|██████████| 532/532 [00:16<00:00, 32.93it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:02<00:33,  2.37s/it]Loading test:  13%|█▎        | 2/15 [00:04<00:27,  2.15s/it]Loading test:  20%|██        | 3/15 [00:06<00:26,  2.22s/it]Loading test:  27%|██▋       | 4/15 [00:08<00:23,  2.15s/it]Loading test:  33%|███▎      | 5/15 [00:10<00:20,  2.08s/it]Loading test:  40%|████      | 6/15 [00:12<00:19,  2.16s/it]Loading test:  47%|████▋     | 7/15 [00:14<00:16,  2.04s/it]Loading test:  53%|█████▎    | 8/15 [00:17<00:15,  2.23s/it]Loading test:  60%|██████    | 9/15 [00:19<00:13,  2.27s/it]Loading test:  67%|██████▋   | 10/15 [00:20<00:10,  2.04s/it]Loading test:  73%|███████▎  | 11/15 [00:22<00:08,  2.02s/it]Loading test:  80%|████████  | 12/15 [00:25<00:06,  2.06s/it]Loading test:  87%|████████▋ | 13/15 [00:27<00:04,  2.18s/it]Loading test:  93%|█████████▎| 14/15 [00:29<00:02,  2.08s/it]Loading test: 100%|██████████| 15/15 [00:31<00:00,  2.16s/it]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation:  13%|█▎        | 2/15 [00:00<00:01,  7.71it/s]concatenating: validation:  27%|██▋       | 4/15 [00:00<00:01,  9.24it/s]concatenating: validation:  40%|████      | 6/15 [00:00<00:00, 10.21it/s]concatenating: validation:  67%|██████▋   | 10/15 [00:00<00:00, 13.14it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 21.31it/s]
(0/4) test vimp2_ctrl_991_08302013_JF
(1/4) test vimp2_967_08132013_KW
(2/4) test vimp2_765_04162013_AW
(3/4) test vimp2_ANON695_03132013
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 84, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 84, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 84, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 84, 10)   910         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 84, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 84, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 84, 11)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 42, 11)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 42, 11)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 42, 20)   2000        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 42, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 42, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 42, 20)   3620        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 42, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 42, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 42, 31)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 21, 31)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 21, 31)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 21, 40)   11200       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 21, 40)   160         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 21, 40)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 21, 40)   14440       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 21, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 21, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 21, 71)   0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 21, 71)   0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 42, 20)   5700        dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 42, 51)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 42, 20)   9200        concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 42, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 42, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 42, 20)   3620        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 42, 20)   80          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 42, 20)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 42, 71)   0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 28, 42, 71)   0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 84, 10)   2850        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 84, 21)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 84, 10)   1900        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 84, 10)   40          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 84, 10)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 84, 10)   910         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 84, 10)   40          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 84, 10)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 84, 31)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 84, 31)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 84, 13)   416         dropout_5[0][0]                  
==================================================================================================
Total params: 57,666
Trainable params: 57,266
Non-trainable params: 400
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53383120e-02 2.88786827e-02 1.16652967e-01 1.00133292e-02
 3.03165963e-02 5.79538965e-03 6.85058777e-02 1.28145429e-01
 7.55135052e-02 1.22435092e-02 2.73464902e-01 1.84941443e-01
 1.90056842e-04]
Train on 20749 samples, validate on 584 samples
Epoch 1/300
 - 29s - loss: 96.3094 - acc: 0.6888 - mDice: 0.0158 - val_loss: 5.3281 - val_acc: 0.9134 - val_mDice: 0.0122

Epoch 00001: val_mDice improved from -inf to 0.01215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 18s - loss: 9.0107 - acc: 0.8891 - mDice: 0.0277 - val_loss: 4.2482 - val_acc: 0.9133 - val_mDice: 0.0305

Epoch 00002: val_mDice improved from 0.01215 to 0.03046, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 17s - loss: 6.0210 - acc: 0.8952 - mDice: 0.0421 - val_loss: 3.8304 - val_acc: 0.9120 - val_mDice: 0.0465

Epoch 00003: val_mDice improved from 0.03046 to 0.04648, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 19s - loss: 5.0955 - acc: 0.8974 - mDice: 0.0557 - val_loss: 3.5009 - val_acc: 0.9084 - val_mDice: 0.0705

Epoch 00004: val_mDice improved from 0.04648 to 0.07045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 18s - loss: 4.5328 - acc: 0.9011 - mDice: 0.0723 - val_loss: 3.2234 - val_acc: 0.9156 - val_mDice: 0.0962

Epoch 00005: val_mDice improved from 0.07045 to 0.09618, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 18s - loss: 4.0539 - acc: 0.9086 - mDice: 0.1007 - val_loss: 2.8420 - val_acc: 0.9318 - val_mDice: 0.1489

Epoch 00006: val_mDice improved from 0.09618 to 0.14885, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 17s - loss: 3.6910 - acc: 0.9143 - mDice: 0.1329 - val_loss: 2.4868 - val_acc: 0.9368 - val_mDice: 0.1989

Epoch 00007: val_mDice improved from 0.14885 to 0.19891, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300
 - 19s - loss: 3.3815 - acc: 0.9186 - mDice: 0.1669 - val_loss: 2.1941 - val_acc: 0.9422 - val_mDice: 0.2522

Epoch 00008: val_mDice improved from 0.19891 to 0.25221, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300
 - 17s - loss: 3.1257 - acc: 0.9223 - mDice: 0.2004 - val_loss: 2.0389 - val_acc: 0.9465 - val_mDice: 0.2910

Epoch 00009: val_mDice improved from 0.25221 to 0.29104, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300
 - 19s - loss: 2.9256 - acc: 0.9254 - mDice: 0.2283 - val_loss: 1.8320 - val_acc: 0.9532 - val_mDice: 0.3408

Epoch 00010: val_mDice improved from 0.29104 to 0.34076, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 17s - loss: 2.7445 - acc: 0.9276 - mDice: 0.2553 - val_loss: 1.7305 - val_acc: 0.9525 - val_mDice: 0.3650

Epoch 00011: val_mDice improved from 0.34076 to 0.36500, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 19s - loss: 2.5861 - acc: 0.9294 - mDice: 0.2804 - val_loss: 1.5919 - val_acc: 0.9565 - val_mDice: 0.4058

Epoch 00012: val_mDice improved from 0.36500 to 0.40581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300
 - 18s - loss: 2.4376 - acc: 0.9310 - mDice: 0.3055 - val_loss: 1.4906 - val_acc: 0.9572 - val_mDice: 0.4390

Epoch 00013: val_mDice improved from 0.40581 to 0.43903, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 18s - loss: 2.3096 - acc: 0.9327 - mDice: 0.3291 - val_loss: 1.4406 - val_acc: 0.9574 - val_mDice: 0.4587

Epoch 00014: val_mDice improved from 0.43903 to 0.45873, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 19s - loss: 2.2061 - acc: 0.9341 - mDice: 0.3493 - val_loss: 1.3589 - val_acc: 0.9604 - val_mDice: 0.4852

Epoch 00015: val_mDice improved from 0.45873 to 0.48519, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300
 - 18s - loss: 2.1058 - acc: 0.9357 - mDice: 0.3704 - val_loss: 1.2929 - val_acc: 0.9624 - val_mDice: 0.5077

Epoch 00016: val_mDice improved from 0.48519 to 0.50767, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 17/300
 - 19s - loss: 2.0231 - acc: 0.9369 - mDice: 0.3868 - val_loss: 1.2421 - val_acc: 0.9628 - val_mDice: 0.5232

Epoch 00017: val_mDice improved from 0.50767 to 0.52321, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 18/300
 - 17s - loss: 1.9488 - acc: 0.9382 - mDice: 0.4025 - val_loss: 1.2336 - val_acc: 0.9642 - val_mDice: 0.5302

Epoch 00018: val_mDice improved from 0.52321 to 0.53019, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 19/300
 - 17s - loss: 1.8852 - acc: 0.9393 - mDice: 0.4171 - val_loss: 1.1766 - val_acc: 0.9657 - val_mDice: 0.5460

Epoch 00019: val_mDice improved from 0.53019 to 0.54604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300
 - 16s - loss: 1.8198 - acc: 0.9406 - mDice: 0.4318 - val_loss: 1.1967 - val_acc: 0.9651 - val_mDice: 0.5495

Epoch 00020: val_mDice improved from 0.54604 to 0.54952, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300
 - 16s - loss: 1.7729 - acc: 0.9417 - mDice: 0.4439 - val_loss: 1.1538 - val_acc: 0.9668 - val_mDice: 0.5687

Epoch 00021: val_mDice improved from 0.54952 to 0.56868, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300
 - 17s - loss: 1.7282 - acc: 0.9425 - mDice: 0.4547 - val_loss: 1.1092 - val_acc: 0.9675 - val_mDice: 0.5820

Epoch 00022: val_mDice improved from 0.56868 to 0.58202, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300
 - 16s - loss: 1.6772 - acc: 0.9436 - mDice: 0.4676 - val_loss: 1.0865 - val_acc: 0.9680 - val_mDice: 0.5932

Epoch 00023: val_mDice improved from 0.58202 to 0.59323, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 24/300
 - 16s - loss: 1.6341 - acc: 0.9446 - mDice: 0.4793 - val_loss: 1.0579 - val_acc: 0.9687 - val_mDice: 0.6006

Epoch 00024: val_mDice improved from 0.59323 to 0.60056, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 25/300
 - 16s - loss: 1.5986 - acc: 0.9454 - mDice: 0.4893 - val_loss: 1.0512 - val_acc: 0.9692 - val_mDice: 0.6082

Epoch 00025: val_mDice improved from 0.60056 to 0.60818, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300
 - 16s - loss: 1.5592 - acc: 0.9462 - mDice: 0.4995 - val_loss: 1.0130 - val_acc: 0.9697 - val_mDice: 0.6198

Epoch 00026: val_mDice improved from 0.60818 to 0.61981, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 27/300
 - 17s - loss: 1.5299 - acc: 0.9469 - mDice: 0.5073 - val_loss: 1.0258 - val_acc: 0.9700 - val_mDice: 0.6208

Epoch 00027: val_mDice improved from 0.61981 to 0.62079, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300
 - 16s - loss: 1.4968 - acc: 0.9475 - mDice: 0.5164 - val_loss: 1.0002 - val_acc: 0.9700 - val_mDice: 0.6332

Epoch 00028: val_mDice improved from 0.62079 to 0.63319, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 29/300
 - 16s - loss: 1.4728 - acc: 0.9480 - mDice: 0.5237 - val_loss: 0.9902 - val_acc: 0.9705 - val_mDice: 0.6363

Epoch 00029: val_mDice improved from 0.63319 to 0.63632, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 16s - loss: 1.4408 - acc: 0.9488 - mDice: 0.5327 - val_loss: 0.9727 - val_acc: 0.9715 - val_mDice: 0.6412

Epoch 00030: val_mDice improved from 0.63632 to 0.64116, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 31/300
 - 16s - loss: 1.4210 - acc: 0.9493 - mDice: 0.5386 - val_loss: 0.9948 - val_acc: 0.9714 - val_mDice: 0.6380

Epoch 00031: val_mDice did not improve from 0.64116
Epoch 32/300
 - 16s - loss: 1.3943 - acc: 0.9499 - mDice: 0.5452 - val_loss: 0.9454 - val_acc: 0.9718 - val_mDice: 0.6521

Epoch 00032: val_mDice improved from 0.64116 to 0.65214, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 33/300
 - 17s - loss: 1.3759 - acc: 0.9505 - mDice: 0.5516 - val_loss: 0.9448 - val_acc: 0.9717 - val_mDice: 0.6544

Epoch 00033: val_mDice improved from 0.65214 to 0.65443, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 34/300
 - 16s - loss: 1.3575 - acc: 0.9510 - mDice: 0.5565 - val_loss: 0.9144 - val_acc: 0.9724 - val_mDice: 0.6657

Epoch 00034: val_mDice improved from 0.65443 to 0.66569, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 35/300
 - 16s - loss: 1.3358 - acc: 0.9514 - mDice: 0.5629 - val_loss: 0.9126 - val_acc: 0.9720 - val_mDice: 0.6634

Epoch 00035: val_mDice did not improve from 0.66569
Epoch 36/300
 - 16s - loss: 1.3194 - acc: 0.9517 - mDice: 0.5676 - val_loss: 0.9194 - val_acc: 0.9722 - val_mDice: 0.6655

Epoch 00036: val_mDice did not improve from 0.66569
Epoch 37/300
 - 16s - loss: 1.3028 - acc: 0.9522 - mDice: 0.5719 - val_loss: 0.9197 - val_acc: 0.9729 - val_mDice: 0.6700

Epoch 00037: val_mDice improved from 0.66569 to 0.67001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 38/300
 - 16s - loss: 1.2866 - acc: 0.9526 - mDice: 0.5772 - val_loss: 0.8961 - val_acc: 0.9726 - val_mDice: 0.6765

Epoch 00038: val_mDice improved from 0.67001 to 0.67653, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 39/300
 - 17s - loss: 1.2757 - acc: 0.9528 - mDice: 0.5810 - val_loss: 0.9119 - val_acc: 0.9733 - val_mDice: 0.6723

Epoch 00039: val_mDice did not improve from 0.67653
Epoch 40/300
 - 16s - loss: 1.2654 - acc: 0.9532 - mDice: 0.5844 - val_loss: 0.8829 - val_acc: 0.9730 - val_mDice: 0.6775

Epoch 00040: val_mDice improved from 0.67653 to 0.67754, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 41/300
 - 16s - loss: 1.2447 - acc: 0.9537 - mDice: 0.5902 - val_loss: 0.8778 - val_acc: 0.9732 - val_mDice: 0.6798

Epoch 00041: val_mDice improved from 0.67754 to 0.67983, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 42/300
 - 16s - loss: 1.2355 - acc: 0.9538 - mDice: 0.5937 - val_loss: 0.8751 - val_acc: 0.9732 - val_mDice: 0.6844

Epoch 00042: val_mDice improved from 0.67983 to 0.68444, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 43/300
 - 16s - loss: 1.2242 - acc: 0.9540 - mDice: 0.5973 - val_loss: 0.8850 - val_acc: 0.9726 - val_mDice: 0.6853

Epoch 00043: val_mDice improved from 0.68444 to 0.68528, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 44/300
 - 16s - loss: 1.2163 - acc: 0.9542 - mDice: 0.5998 - val_loss: 0.8657 - val_acc: 0.9737 - val_mDice: 0.6858

Epoch 00044: val_mDice improved from 0.68528 to 0.68577, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 45/300
 - 16s - loss: 1.2028 - acc: 0.9544 - mDice: 0.6031 - val_loss: 0.8609 - val_acc: 0.9732 - val_mDice: 0.6897

Epoch 00045: val_mDice improved from 0.68577 to 0.68969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 46/300
 - 17s - loss: 1.1964 - acc: 0.9545 - mDice: 0.6055 - val_loss: 0.8687 - val_acc: 0.9732 - val_mDice: 0.6904

Epoch 00046: val_mDice improved from 0.68969 to 0.69045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 47/300
 - 16s - loss: 1.1873 - acc: 0.9548 - mDice: 0.6083 - val_loss: 0.8673 - val_acc: 0.9730 - val_mDice: 0.6906

Epoch 00047: val_mDice improved from 0.69045 to 0.69061, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 48/300
 - 16s - loss: 1.1800 - acc: 0.9549 - mDice: 0.6102 - val_loss: 0.8750 - val_acc: 0.9743 - val_mDice: 0.6911

Epoch 00048: val_mDice improved from 0.69061 to 0.69113, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 49/300
 - 16s - loss: 1.1732 - acc: 0.9551 - mDice: 0.6126 - val_loss: 0.8671 - val_acc: 0.9737 - val_mDice: 0.6907

Epoch 00049: val_mDice did not improve from 0.69113
Epoch 50/300
 - 16s - loss: 1.1648 - acc: 0.9552 - mDice: 0.6149 - val_loss: 0.8517 - val_acc: 0.9742 - val_mDice: 0.6905

Epoch 00050: val_mDice did not improve from 0.69113
Epoch 51/300
 - 16s - loss: 1.1586 - acc: 0.9553 - mDice: 0.6166 - val_loss: 0.8711 - val_acc: 0.9744 - val_mDice: 0.6930

Epoch 00051: val_mDice improved from 0.69113 to 0.69296, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 52/300
 - 16s - loss: 1.1524 - acc: 0.9554 - mDice: 0.6189 - val_loss: 0.8559 - val_acc: 0.9748 - val_mDice: 0.6926

Epoch 00052: val_mDice did not improve from 0.69296
Epoch 53/300
 - 16s - loss: 1.1463 - acc: 0.9555 - mDice: 0.6205 - val_loss: 0.8340 - val_acc: 0.9741 - val_mDice: 0.6985

Epoch 00053: val_mDice improved from 0.69296 to 0.69847, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 54/300
 - 17s - loss: 1.1435 - acc: 0.9556 - mDice: 0.6217 - val_loss: 0.8321 - val_acc: 0.9750 - val_mDice: 0.6993

Epoch 00054: val_mDice improved from 0.69847 to 0.69928, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 55/300
 - 16s - loss: 1.1364 - acc: 0.9556 - mDice: 0.6235 - val_loss: 0.8487 - val_acc: 0.9743 - val_mDice: 0.6991

Epoch 00055: val_mDice did not improve from 0.69928
Epoch 56/300
 - 16s - loss: 1.1274 - acc: 0.9558 - mDice: 0.6259 - val_loss: 0.8295 - val_acc: 0.9746 - val_mDice: 0.7032

Epoch 00056: val_mDice improved from 0.69928 to 0.70323, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 57/300
 - 16s - loss: 1.1251 - acc: 0.9559 - mDice: 0.6267 - val_loss: 0.8336 - val_acc: 0.9746 - val_mDice: 0.6992

Epoch 00057: val_mDice did not improve from 0.70323
Epoch 58/300
 - 16s - loss: 1.1183 - acc: 0.9561 - mDice: 0.6289 - val_loss: 0.8481 - val_acc: 0.9747 - val_mDice: 0.6946

Epoch 00058: val_mDice did not improve from 0.70323
Epoch 59/300
 - 16s - loss: 1.1181 - acc: 0.9561 - mDice: 0.6293 - val_loss: 0.8365 - val_acc: 0.9752 - val_mDice: 0.7021

Epoch 00059: val_mDice did not improve from 0.70323
Epoch 60/300
 - 17s - loss: 1.1096 - acc: 0.9562 - mDice: 0.6316 - val_loss: 0.8338 - val_acc: 0.9754 - val_mDice: 0.7037

Epoch 00060: val_mDice improved from 0.70323 to 0.70367, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 61/300
 - 17s - loss: 1.1033 - acc: 0.9564 - mDice: 0.6334 - val_loss: 0.8269 - val_acc: 0.9750 - val_mDice: 0.7060

Epoch 00061: val_mDice improved from 0.70367 to 0.70603, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 62/300
 - 16s - loss: 1.1037 - acc: 0.9563 - mDice: 0.6332 - val_loss: 0.8257 - val_acc: 0.9753 - val_mDice: 0.7029

Epoch 00062: val_mDice did not improve from 0.70603
Epoch 63/300
 - 16s - loss: 1.0981 - acc: 0.9564 - mDice: 0.6349 - val_loss: 0.8364 - val_acc: 0.9754 - val_mDice: 0.7027

Epoch 00063: val_mDice did not improve from 0.70603
Epoch 64/300
 - 16s - loss: 1.0943 - acc: 0.9566 - mDice: 0.6364 - val_loss: 0.8278 - val_acc: 0.9753 - val_mDice: 0.7068

Epoch 00064: val_mDice improved from 0.70603 to 0.70677, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 65/300
 - 17s - loss: 1.0874 - acc: 0.9567 - mDice: 0.6383 - val_loss: 0.8240 - val_acc: 0.9757 - val_mDice: 0.7054

Epoch 00065: val_mDice did not improve from 0.70677
Epoch 66/300
 - 17s - loss: 1.0861 - acc: 0.9567 - mDice: 0.6384 - val_loss: 0.8257 - val_acc: 0.9751 - val_mDice: 0.7065

Epoch 00066: val_mDice did not improve from 0.70677
Epoch 67/300
 - 16s - loss: 1.0824 - acc: 0.9568 - mDice: 0.6397 - val_loss: 0.8228 - val_acc: 0.9755 - val_mDice: 0.7064

Epoch 00067: val_mDice did not improve from 0.70677
Epoch 68/300
 - 16s - loss: 1.0771 - acc: 0.9570 - mDice: 0.6412 - val_loss: 0.8106 - val_acc: 0.9757 - val_mDice: 0.7114

Epoch 00068: val_mDice improved from 0.70677 to 0.71138, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 69/300
 - 16s - loss: 1.0731 - acc: 0.9570 - mDice: 0.6427 - val_loss: 0.8073 - val_acc: 0.9757 - val_mDice: 0.7102

Epoch 00069: val_mDice did not improve from 0.71138
Epoch 70/300
 - 16s - loss: 1.0692 - acc: 0.9571 - mDice: 0.6436 - val_loss: 0.8160 - val_acc: 0.9754 - val_mDice: 0.7074

Epoch 00070: val_mDice did not improve from 0.71138
Epoch 71/300
 - 17s - loss: 1.0674 - acc: 0.9571 - mDice: 0.6443 - val_loss: 0.7991 - val_acc: 0.9758 - val_mDice: 0.7144

Epoch 00071: val_mDice improved from 0.71138 to 0.71444, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 72/300
 - 16s - loss: 1.0611 - acc: 0.9573 - mDice: 0.6461 - val_loss: 0.8221 - val_acc: 0.9756 - val_mDice: 0.7091

Epoch 00072: val_mDice did not improve from 0.71444
Epoch 73/300
 - 18s - loss: 1.0579 - acc: 0.9574 - mDice: 0.6475 - val_loss: 0.8015 - val_acc: 0.9758 - val_mDice: 0.7112

Epoch 00073: val_mDice did not improve from 0.71444
Epoch 74/300
 - 16s - loss: 1.0550 - acc: 0.9574 - mDice: 0.6477 - val_loss: 0.8037 - val_acc: 0.9759 - val_mDice: 0.7128

Epoch 00074: val_mDice did not improve from 0.71444
Epoch 75/300
 - 16s - loss: 1.0552 - acc: 0.9574 - mDice: 0.6483 - val_loss: 0.8010 - val_acc: 0.9755 - val_mDice: 0.7114

Epoch 00075: val_mDice did not improve from 0.71444
Epoch 76/300
 - 16s - loss: 1.0524 - acc: 0.9575 - mDice: 0.6486 - val_loss: 0.7975 - val_acc: 0.9754 - val_mDice: 0.7173

Epoch 00076: val_mDice improved from 0.71444 to 0.71728, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 77/300
 - 16s - loss: 1.0524 - acc: 0.9575 - mDice: 0.6491 - val_loss: 0.7958 - val_acc: 0.9757 - val_mDice: 0.7158

Epoch 00077: val_mDice did not improve from 0.71728
Epoch 78/300
 - 16s - loss: 1.0481 - acc: 0.9577 - mDice: 0.6503 - val_loss: 0.8008 - val_acc: 0.9758 - val_mDice: 0.7124

Epoch 00078: val_mDice did not improve from 0.71728
Epoch 79/300
 - 17s - loss: 1.0429 - acc: 0.9579 - mDice: 0.6513 - val_loss: 0.7906 - val_acc: 0.9761 - val_mDice: 0.7168

Epoch 00079: val_mDice did not improve from 0.71728
Epoch 80/300
 - 16s - loss: 1.0407 - acc: 0.9580 - mDice: 0.6527 - val_loss: 0.7844 - val_acc: 0.9761 - val_mDice: 0.7179

Epoch 00080: val_mDice improved from 0.71728 to 0.71795, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 81/300
 - 16s - loss: 1.0376 - acc: 0.9581 - mDice: 0.6532 - val_loss: 0.7994 - val_acc: 0.9752 - val_mDice: 0.7143

Epoch 00081: val_mDice did not improve from 0.71795
Epoch 82/300
 - 16s - loss: 1.0356 - acc: 0.9582 - mDice: 0.6542 - val_loss: 0.7919 - val_acc: 0.9763 - val_mDice: 0.7172

Epoch 00082: val_mDice did not improve from 0.71795
Epoch 83/300
 - 16s - loss: 1.0340 - acc: 0.9582 - mDice: 0.6543 - val_loss: 0.7883 - val_acc: 0.9763 - val_mDice: 0.7167

Epoch 00083: val_mDice did not improve from 0.71795
Epoch 84/300
 - 17s - loss: 1.0312 - acc: 0.9583 - mDice: 0.6552 - val_loss: 0.7875 - val_acc: 0.9759 - val_mDice: 0.7218

Epoch 00084: val_mDice improved from 0.71795 to 0.72177, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 85/300
 - 16s - loss: 1.0276 - acc: 0.9584 - mDice: 0.6563 - val_loss: 0.7943 - val_acc: 0.9759 - val_mDice: 0.7146

Epoch 00085: val_mDice did not improve from 0.72177
Epoch 86/300
 - 16s - loss: 1.0262 - acc: 0.9585 - mDice: 0.6569 - val_loss: 0.7872 - val_acc: 0.9760 - val_mDice: 0.7183

Epoch 00086: val_mDice did not improve from 0.72177
Epoch 87/300
 - 16s - loss: 1.0253 - acc: 0.9585 - mDice: 0.6571 - val_loss: 0.7898 - val_acc: 0.9765 - val_mDice: 0.7196

Epoch 00087: val_mDice did not improve from 0.72177
Epoch 88/300
 - 17s - loss: 1.0247 - acc: 0.9585 - mDice: 0.6578 - val_loss: 0.7898 - val_acc: 0.9764 - val_mDice: 0.7180

Epoch 00088: val_mDice did not improve from 0.72177
Epoch 89/300
 - 16s - loss: 1.0195 - acc: 0.9587 - mDice: 0.6591 - val_loss: 0.7975 - val_acc: 0.9763 - val_mDice: 0.7187

Epoch 00089: val_mDice did not improve from 0.72177
Epoch 90/300
 - 16s - loss: 1.0174 - acc: 0.9587 - mDice: 0.6596 - val_loss: 0.7801 - val_acc: 0.9765 - val_mDice: 0.7217

Epoch 00090: val_mDice did not improve from 0.72177
Epoch 91/300
 - 16s - loss: 1.0154 - acc: 0.9588 - mDice: 0.6599 - val_loss: 0.7833 - val_acc: 0.9760 - val_mDice: 0.7205

Epoch 00091: val_mDice did not improve from 0.72177
Epoch 92/300
 - 17s - loss: 1.0156 - acc: 0.9588 - mDice: 0.6607 - val_loss: 0.7802 - val_acc: 0.9763 - val_mDice: 0.7231

Epoch 00092: val_mDice improved from 0.72177 to 0.72310, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 93/300
 - 16s - loss: 1.0129 - acc: 0.9588 - mDice: 0.6609 - val_loss: 0.7843 - val_acc: 0.9767 - val_mDice: 0.7239

Epoch 00093: val_mDice improved from 0.72310 to 0.72392, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 94/300
 - 16s - loss: 1.0118 - acc: 0.9589 - mDice: 0.6617 - val_loss: 0.7840 - val_acc: 0.9755 - val_mDice: 0.7218

Epoch 00094: val_mDice did not improve from 0.72392
Epoch 95/300
 - 16s - loss: 1.0083 - acc: 0.9590 - mDice: 0.6624 - val_loss: 0.7930 - val_acc: 0.9758 - val_mDice: 0.7200

Epoch 00095: val_mDice did not improve from 0.72392
Epoch 96/300
 - 16s - loss: 1.0102 - acc: 0.9589 - mDice: 0.6621 - val_loss: 0.7738 - val_acc: 0.9764 - val_mDice: 0.7238

Epoch 00096: val_mDice did not improve from 0.72392
Epoch 97/300
 - 17s - loss: 1.0059 - acc: 0.9592 - mDice: 0.6636 - val_loss: 0.7781 - val_acc: 0.9765 - val_mDice: 0.7259

Epoch 00097: val_mDice improved from 0.72392 to 0.72593, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 98/300
 - 16s - loss: 1.0047 - acc: 0.9591 - mDice: 0.6642 - val_loss: 0.7829 - val_acc: 0.9764 - val_mDice: 0.7212

Epoch 00098: val_mDice did not improve from 0.72593
Epoch 99/300
 - 16s - loss: 1.0007 - acc: 0.9593 - mDice: 0.6650 - val_loss: 0.7777 - val_acc: 0.9762 - val_mDice: 0.7211

Epoch 00099: val_mDice did not improve from 0.72593
Epoch 100/300
 - 16s - loss: 1.0019 - acc: 0.9593 - mDice: 0.6647 - val_loss: 0.7752 - val_acc: 0.9764 - val_mDice: 0.7242

Epoch 00100: val_mDice did not improve from 0.72593
Epoch 101/300
 - 17s - loss: 0.9972 - acc: 0.9594 - mDice: 0.6659 - val_loss: 0.7758 - val_acc: 0.9762 - val_mDice: 0.7236

Epoch 00101: val_mDice did not improve from 0.72593
Epoch 102/300
 - 16s - loss: 1.0012 - acc: 0.9592 - mDice: 0.6651 - val_loss: 0.7775 - val_acc: 0.9763 - val_mDice: 0.7255

Epoch 00102: val_mDice did not improve from 0.72593
Epoch 103/300
 - 16s - loss: 0.9975 - acc: 0.9594 - mDice: 0.6664 - val_loss: 0.7829 - val_acc: 0.9759 - val_mDice: 0.7204

Epoch 00103: val_mDice did not improve from 0.72593
Epoch 104/300
 - 16s - loss: 0.9960 - acc: 0.9594 - mDice: 0.6667 - val_loss: 0.7709 - val_acc: 0.9762 - val_mDice: 0.7223

Epoch 00104: val_mDice did not improve from 0.72593
Epoch 105/300
 - 16s - loss: 0.9953 - acc: 0.9594 - mDice: 0.6666 - val_loss: 0.7756 - val_acc: 0.9764 - val_mDice: 0.7252

Epoch 00105: val_mDice did not improve from 0.72593
Epoch 106/300
 - 17s - loss: 0.9932 - acc: 0.9595 - mDice: 0.6674 - val_loss: 0.7673 - val_acc: 0.9765 - val_mDice: 0.7241

Epoch 00106: val_mDice did not improve from 0.72593
Epoch 107/300
 - 16s - loss: 0.9907 - acc: 0.9596 - mDice: 0.6682 - val_loss: 0.7700 - val_acc: 0.9766 - val_mDice: 0.7270

Epoch 00107: val_mDice improved from 0.72593 to 0.72697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 108/300
 - 16s - loss: 0.9877 - acc: 0.9597 - mDice: 0.6696 - val_loss: 0.7793 - val_acc: 0.9764 - val_mDice: 0.7227

Epoch 00108: val_mDice did not improve from 0.72697
Epoch 109/300
 - 16s - loss: 0.9876 - acc: 0.9597 - mDice: 0.6693 - val_loss: 0.7640 - val_acc: 0.9763 - val_mDice: 0.7262

Epoch 00109: val_mDice did not improve from 0.72697
Epoch 110/300
 - 16s - loss: 0.9840 - acc: 0.9598 - mDice: 0.6707 - val_loss: 0.7788 - val_acc: 0.9762 - val_mDice: 0.7254

Epoch 00110: val_mDice did not improve from 0.72697
Epoch 111/300
 - 16s - loss: 0.9856 - acc: 0.9598 - mDice: 0.6702 - val_loss: 0.7712 - val_acc: 0.9761 - val_mDice: 0.7253

Epoch 00111: val_mDice did not improve from 0.72697
Epoch 112/300
 - 17s - loss: 0.9856 - acc: 0.9597 - mDice: 0.6700 - val_loss: 0.7694 - val_acc: 0.9761 - val_mDice: 0.7261

Epoch 00112: val_mDice did not improve from 0.72697
Epoch 113/300
 - 16s - loss: 0.9855 - acc: 0.9598 - mDice: 0.6700 - val_loss: 0.7722 - val_acc: 0.9765 - val_mDice: 0.7252

Epoch 00113: val_mDice did not improve from 0.72697
Epoch 114/300
 - 16s - loss: 0.9817 - acc: 0.9599 - mDice: 0.6709 - val_loss: 0.7734 - val_acc: 0.9767 - val_mDice: 0.7258

Epoch 00114: val_mDice did not improve from 0.72697
Epoch 115/300
 - 16s - loss: 0.9799 - acc: 0.9598 - mDice: 0.6717 - val_loss: 0.7711 - val_acc: 0.9763 - val_mDice: 0.7276

Epoch 00115: val_mDice improved from 0.72697 to 0.72759, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 116/300
 - 16s - loss: 0.9806 - acc: 0.9598 - mDice: 0.6716 - val_loss: 0.7687 - val_acc: 0.9764 - val_mDice: 0.7251

Epoch 00116: val_mDice did not improve from 0.72759
Epoch 117/300
 - 17s - loss: 0.9809 - acc: 0.9599 - mDice: 0.6715 - val_loss: 0.7619 - val_acc: 0.9765 - val_mDice: 0.7284

Epoch 00117: val_mDice improved from 0.72759 to 0.72839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 118/300
 - 16s - loss: 0.9792 - acc: 0.9599 - mDice: 0.6719 - val_loss: 0.7633 - val_acc: 0.9765 - val_mDice: 0.7266

Epoch 00118: val_mDice did not improve from 0.72839
Epoch 119/300
 - 16s - loss: 0.9753 - acc: 0.9600 - mDice: 0.6731 - val_loss: 0.7668 - val_acc: 0.9765 - val_mDice: 0.7264

Epoch 00119: val_mDice did not improve from 0.72839
Epoch 120/300
 - 16s - loss: 0.9733 - acc: 0.9600 - mDice: 0.6738 - val_loss: 0.7717 - val_acc: 0.9767 - val_mDice: 0.7272

Epoch 00120: val_mDice did not improve from 0.72839
Epoch 121/300
 - 16s - loss: 0.9728 - acc: 0.9600 - mDice: 0.6740 - val_loss: 0.7811 - val_acc: 0.9764 - val_mDice: 0.7247

Epoch 00121: val_mDice did not improve from 0.72839
Epoch 122/300
 - 16s - loss: 0.9730 - acc: 0.9600 - mDice: 0.6739 - val_loss: 0.7634 - val_acc: 0.9767 - val_mDice: 0.7315

Epoch 00122: val_mDice improved from 0.72839 to 0.73149, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 123/300
 - 17s - loss: 0.9724 - acc: 0.9600 - mDice: 0.6747 - val_loss: 0.7623 - val_acc: 0.9766 - val_mDice: 0.7290

Epoch 00123: val_mDice did not improve from 0.73149
Epoch 124/300
 - 16s - loss: 0.9687 - acc: 0.9599 - mDice: 0.6759 - val_loss: 0.7686 - val_acc: 0.9764 - val_mDice: 0.7292

Epoch 00124: val_mDice did not improve from 0.73149
Epoch 125/300
 - 16s - loss: 0.9666 - acc: 0.9597 - mDice: 0.6761 - val_loss: 0.7542 - val_acc: 0.9765 - val_mDice: 0.7302

Epoch 00125: val_mDice did not improve from 0.73149
Epoch 126/300
 - 16s - loss: 0.9635 - acc: 0.9596 - mDice: 0.6772 - val_loss: 0.7753 - val_acc: 0.9765 - val_mDice: 0.7275

Epoch 00126: val_mDice did not improve from 0.73149
Epoch 127/300
 - 16s - loss: 0.9641 - acc: 0.9596 - mDice: 0.6776 - val_loss: 0.7619 - val_acc: 0.9767 - val_mDice: 0.7320

Epoch 00127: val_mDice improved from 0.73149 to 0.73204, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 128/300
 - 16s - loss: 0.9627 - acc: 0.9595 - mDice: 0.6778 - val_loss: 0.7640 - val_acc: 0.9764 - val_mDice: 0.7298

Epoch 00128: val_mDice did not improve from 0.73204
Epoch 129/300
 - 16s - loss: 0.9618 - acc: 0.9595 - mDice: 0.6782 - val_loss: 0.7620 - val_acc: 0.9767 - val_mDice: 0.7290

Epoch 00129: val_mDice did not improve from 0.73204
Epoch 130/300
 - 17s - loss: 0.9594 - acc: 0.9595 - mDice: 0.6788 - val_loss: 0.7645 - val_acc: 0.9764 - val_mDice: 0.7308

Epoch 00130: val_mDice did not improve from 0.73204
Epoch 131/300
 - 16s - loss: 0.9598 - acc: 0.9595 - mDice: 0.6789 - val_loss: 0.7651 - val_acc: 0.9768 - val_mDice: 0.7271

Epoch 00131: val_mDice did not improve from 0.73204
Epoch 132/300
 - 16s - loss: 0.9596 - acc: 0.9595 - mDice: 0.6791 - val_loss: 0.7546 - val_acc: 0.9764 - val_mDice: 0.7317

Epoch 00132: val_mDice did not improve from 0.73204
Epoch 133/300
 - 16s - loss: 0.9575 - acc: 0.9595 - mDice: 0.6801 - val_loss: 0.7601 - val_acc: 0.9769 - val_mDice: 0.7291

Epoch 00133: val_mDice did not improve from 0.73204
Epoch 134/300
 - 17s - loss: 0.9604 - acc: 0.9594 - mDice: 0.6789 - val_loss: 0.7686 - val_acc: 0.9762 - val_mDice: 0.7268

Epoch 00134: val_mDice did not improve from 0.73204
Epoch 135/300
 - 16s - loss: 0.9565 - acc: 0.9595 - mDice: 0.6798 - val_loss: 0.7617 - val_acc: 0.9766 - val_mDice: 0.7301

Epoch 00135: val_mDice did not improve from 0.73204
Epoch 136/300
 - 16s - loss: 0.9553 - acc: 0.9595 - mDice: 0.6804 - val_loss: 0.7615 - val_acc: 0.9767 - val_mDice: 0.7306

Epoch 00136: val_mDice did not improve from 0.73204
Epoch 137/300
 - 16s - loss: 0.9537 - acc: 0.9595 - mDice: 0.6811 - val_loss: 0.7639 - val_acc: 0.9767 - val_mDice: 0.7289

Epoch 00137: val_mDice did not improve from 0.73204
Epoch 138/300
 - 16s - loss: 0.9540 - acc: 0.9595 - mDice: 0.6808 - val_loss: 0.7671 - val_acc: 0.9765 - val_mDice: 0.7293

Epoch 00138: val_mDice did not improve from 0.73204
Epoch 139/300
 - 17s - loss: 0.9530 - acc: 0.9595 - mDice: 0.6811 - val_loss: 0.7689 - val_acc: 0.9764 - val_mDice: 0.7271

Epoch 00139: val_mDice did not improve from 0.73204
Epoch 140/300
 - 18s - loss: 0.9530 - acc: 0.9595 - mDice: 0.6811 - val_loss: 0.7549 - val_acc: 0.9763 - val_mDice: 0.7315

Epoch 00140: val_mDice did not improve from 0.73204
Epoch 141/300
 - 17s - loss: 0.9508 - acc: 0.9595 - mDice: 0.6820 - val_loss: 0.7633 - val_acc: 0.9766 - val_mDice: 0.7311

Epoch 00141: val_mDice did not improve from 0.73204
Epoch 142/300
 - 18s - loss: 0.9490 - acc: 0.9596 - mDice: 0.6825 - val_loss: 0.7725 - val_acc: 0.9765 - val_mDice: 0.7272

Epoch 00142: val_mDice did not improve from 0.73204
Epoch 143/300
 - 17s - loss: 0.9506 - acc: 0.9595 - mDice: 0.6821 - val_loss: 0.7660 - val_acc: 0.9765 - val_mDice: 0.7311

Epoch 00143: val_mDice did not improve from 0.73204
Epoch 144/300
 - 17s - loss: 0.9477 - acc: 0.9596 - mDice: 0.6830 - val_loss: 0.7600 - val_acc: 0.9769 - val_mDice: 0.7329

Epoch 00144: val_mDice improved from 0.73204 to 0.73289, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 145/300
 - 17s - loss: 0.9484 - acc: 0.9596 - mDice: 0.6825 - val_loss: 0.7534 - val_acc: 0.9765 - val_mDice: 0.7336

Epoch 00145: val_mDice improved from 0.73289 to 0.73356, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 146/300
 - 18s - loss: 0.9477 - acc: 0.9596 - mDice: 0.6832 - val_loss: 0.7681 - val_acc: 0.9767 - val_mDice: 0.7287

Epoch 00146: val_mDice did not improve from 0.73356
Epoch 147/300
 - 17s - loss: 0.9452 - acc: 0.9595 - mDice: 0.6838 - val_loss: 0.7742 - val_acc: 0.9760 - val_mDice: 0.7267

Epoch 00147: val_mDice did not improve from 0.73356
Epoch 148/300
 - 16s - loss: 0.9451 - acc: 0.9596 - mDice: 0.6836 - val_loss: 0.7740 - val_acc: 0.9765 - val_mDice: 0.7287

Epoch 00148: val_mDice did not improve from 0.73356
Epoch 149/300
 - 16s - loss: 0.9443 - acc: 0.9596 - mDice: 0.6841 - val_loss: 0.7656 - val_acc: 0.9765 - val_mDice: 0.7298

Epoch 00149: val_mDice did not improve from 0.73356
Epoch 150/300
 - 16s - loss: 0.9435 - acc: 0.9596 - mDice: 0.6842 - val_loss: 0.7602 - val_acc: 0.9768 - val_mDice: 0.7307

Epoch 00150: val_mDice did not improve from 0.73356
Epoch 151/300
 - 17s - loss: 0.9442 - acc: 0.9596 - mDice: 0.6843 - val_loss: 0.7596 - val_acc: 0.9769 - val_mDice: 0.7305

Epoch 00151: val_mDice did not improve from 0.73356
Epoch 152/300
 - 18s - loss: 0.9412 - acc: 0.9596 - mDice: 0.6849 - val_loss: 0.7557 - val_acc: 0.9764 - val_mDice: 0.7304

Epoch 00152: val_mDice did not improve from 0.73356
Epoch 153/300
 - 16s - loss: 0.9417 - acc: 0.9597 - mDice: 0.6850 - val_loss: 0.7524 - val_acc: 0.9767 - val_mDice: 0.7321

Epoch 00153: val_mDice did not improve from 0.73356
Epoch 154/300
 - 16s - loss: 0.9392 - acc: 0.9597 - mDice: 0.6858 - val_loss: 0.7609 - val_acc: 0.9766 - val_mDice: 0.7275

Epoch 00154: val_mDice did not improve from 0.73356
Epoch 155/300
 - 16s - loss: 0.9397 - acc: 0.9597 - mDice: 0.6852 - val_loss: 0.7481 - val_acc: 0.9768 - val_mDice: 0.7353

Epoch 00155: val_mDice improved from 0.73356 to 0.73530, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 156/300
 - 18s - loss: 0.9410 - acc: 0.9597 - mDice: 0.6851 - val_loss: 0.7536 - val_acc: 0.9769 - val_mDice: 0.7345

Epoch 00156: val_mDice did not improve from 0.73530
Epoch 157/300
 - 17s - loss: 0.9361 - acc: 0.9598 - mDice: 0.6862 - val_loss: 0.7605 - val_acc: 0.9770 - val_mDice: 0.7346

Epoch 00157: val_mDice did not improve from 0.73530
Epoch 158/300
 - 17s - loss: 0.9373 - acc: 0.9598 - mDice: 0.6864 - val_loss: 0.7549 - val_acc: 0.9772 - val_mDice: 0.7339

Epoch 00158: val_mDice did not improve from 0.73530
Epoch 159/300
 - 17s - loss: 0.9359 - acc: 0.9599 - mDice: 0.6868 - val_loss: 0.7580 - val_acc: 0.9769 - val_mDice: 0.7318

Epoch 00159: val_mDice did not improve from 0.73530
Epoch 160/300
 - 18s - loss: 0.9379 - acc: 0.9598 - mDice: 0.6860 - val_loss: 0.7551 - val_acc: 0.9771 - val_mDice: 0.7308

Epoch 00160: val_mDice did not improve from 0.73530
Epoch 161/300
 - 17s - loss: 0.9357 - acc: 0.9599 - mDice: 0.6866 - val_loss: 0.7632 - val_acc: 0.9768 - val_mDice: 0.7320

Epoch 00161: val_mDice did not improve from 0.73530
Epoch 162/300
 - 17s - loss: 0.9356 - acc: 0.9599 - mDice: 0.6866 - val_loss: 0.7483 - val_acc: 0.9769 - val_mDice: 0.7350

Epoch 00162: val_mDice did not improve from 0.73530
Epoch 163/300
 - 17s - loss: 0.9326 - acc: 0.9600 - mDice: 0.6875 - val_loss: 0.7545 - val_acc: 0.9770 - val_mDice: 0.7342

Epoch 00163: val_mDice did not improve from 0.73530
Epoch 164/300
 - 18s - loss: 0.9333 - acc: 0.9600 - mDice: 0.6875 - val_loss: 0.7468 - val_acc: 0.9773 - val_mDice: 0.7329

Epoch 00164: val_mDice did not improve from 0.73530
Epoch 165/300
 - 17s - loss: 0.9328 - acc: 0.9600 - mDice: 0.6874 - val_loss: 0.7496 - val_acc: 0.9771 - val_mDice: 0.7361

Epoch 00165: val_mDice improved from 0.73530 to 0.73613, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 166/300
 - 17s - loss: 0.9338 - acc: 0.9600 - mDice: 0.6876 - val_loss: 0.7887 - val_acc: 0.9763 - val_mDice: 0.7299

Epoch 00166: val_mDice did not improve from 0.73613
Epoch 167/300
 - 17s - loss: 0.9325 - acc: 0.9600 - mDice: 0.6880 - val_loss: 0.7512 - val_acc: 0.9772 - val_mDice: 0.7331

Epoch 00167: val_mDice did not improve from 0.73613
Epoch 168/300
 - 27s - loss: 0.9292 - acc: 0.9601 - mDice: 0.6885 - val_loss: 0.7570 - val_acc: 0.9770 - val_mDice: 0.7335

Epoch 00168: val_mDice did not improve from 0.73613
Epoch 169/300
 - 27s - loss: 0.9308 - acc: 0.9601 - mDice: 0.6882 - val_loss: 0.7550 - val_acc: 0.9769 - val_mDice: 0.7369

Epoch 00169: val_mDice improved from 0.73613 to 0.73687, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 170/300
 - 27s - loss: 0.9302 - acc: 0.9601 - mDice: 0.6885 - val_loss: 0.7566 - val_acc: 0.9770 - val_mDice: 0.7342

Epoch 00170: val_mDice did not improve from 0.73687
Epoch 171/300
 - 24s - loss: 0.9305 - acc: 0.9602 - mDice: 0.6886 - val_loss: 0.7527 - val_acc: 0.9773 - val_mDice: 0.7338

Epoch 00171: val_mDice did not improve from 0.73687
Epoch 172/300
 - 29s - loss: 0.9291 - acc: 0.9601 - mDice: 0.6890 - val_loss: 0.7638 - val_acc: 0.9769 - val_mDice: 0.7289

Epoch 00172: val_mDice did not improve from 0.73687
Epoch 173/300
 - 29s - loss: 0.9305 - acc: 0.9601 - mDice: 0.6881 - val_loss: 0.7506 - val_acc: 0.9772 - val_mDice: 0.7383

Epoch 00173: val_mDice improved from 0.73687 to 0.73827, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 174/300
 - 22s - loss: 0.9264 - acc: 0.9602 - mDice: 0.6896 - val_loss: 0.7498 - val_acc: 0.9771 - val_mDice: 0.7357

Epoch 00174: val_mDice did not improve from 0.73827
Epoch 175/300
 - 18s - loss: 0.9280 - acc: 0.9601 - mDice: 0.6891 - val_loss: 0.7553 - val_acc: 0.9769 - val_mDice: 0.7369

Epoch 00175: val_mDice did not improve from 0.73827
Epoch 176/300
 - 19s - loss: 0.9289 - acc: 0.9602 - mDice: 0.6888 - val_loss: 0.7521 - val_acc: 0.9771 - val_mDice: 0.7344

Epoch 00176: val_mDice did not improve from 0.73827
Epoch 177/300
 - 18s - loss: 0.9272 - acc: 0.9601 - mDice: 0.6890 - val_loss: 0.7577 - val_acc: 0.9771 - val_mDice: 0.7374

Epoch 00177: val_mDice did not improve from 0.73827
Epoch 178/300
 - 21s - loss: 0.9265 - acc: 0.9602 - mDice: 0.6891 - val_loss: 0.7486 - val_acc: 0.9769 - val_mDice: 0.7386

Epoch 00178: val_mDice improved from 0.73827 to 0.73863, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 179/300
 - 18s - loss: 0.9245 - acc: 0.9603 - mDice: 0.6902 - val_loss: 0.7466 - val_acc: 0.9773 - val_mDice: 0.7380

Epoch 00179: val_mDice did not improve from 0.73863
Epoch 180/300
 - 18s - loss: 0.9251 - acc: 0.9602 - mDice: 0.6897 - val_loss: 0.7432 - val_acc: 0.9768 - val_mDice: 0.7400

Epoch 00180: val_mDice improved from 0.73863 to 0.74005, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 181/300
 - 19s - loss: 0.9257 - acc: 0.9603 - mDice: 0.6899 - val_loss: 0.7560 - val_acc: 0.9770 - val_mDice: 0.7322

Epoch 00181: val_mDice did not improve from 0.74005
Epoch 182/300
 - 18s - loss: 0.9257 - acc: 0.9603 - mDice: 0.6898 - val_loss: 0.7513 - val_acc: 0.9774 - val_mDice: 0.7367

Epoch 00182: val_mDice did not improve from 0.74005
Epoch 183/300
 - 19s - loss: 0.9255 - acc: 0.9603 - mDice: 0.6898 - val_loss: 0.7525 - val_acc: 0.9769 - val_mDice: 0.7385

Epoch 00183: val_mDice did not improve from 0.74005
Epoch 184/300
 - 18s - loss: 0.9231 - acc: 0.9604 - mDice: 0.6907 - val_loss: 0.7528 - val_acc: 0.9773 - val_mDice: 0.7341

Epoch 00184: val_mDice did not improve from 0.74005
Epoch 185/300
 - 17s - loss: 0.9240 - acc: 0.9603 - mDice: 0.6903 - val_loss: 0.7505 - val_acc: 0.9770 - val_mDice: 0.7391

Epoch 00185: val_mDice did not improve from 0.74005
Epoch 186/300
 - 16s - loss: 0.9250 - acc: 0.9604 - mDice: 0.6899 - val_loss: 0.7576 - val_acc: 0.9770 - val_mDice: 0.7345

Epoch 00186: val_mDice did not improve from 0.74005
Epoch 187/300
 - 18s - loss: 0.9216 - acc: 0.9604 - mDice: 0.6911 - val_loss: 0.7501 - val_acc: 0.9772 - val_mDice: 0.7408

Epoch 00187: val_mDice improved from 0.74005 to 0.74084, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 188/300
 - 16s - loss: 0.9201 - acc: 0.9604 - mDice: 0.6918 - val_loss: 0.7474 - val_acc: 0.9774 - val_mDice: 0.7398

Epoch 00188: val_mDice did not improve from 0.74084
Epoch 189/300
 - 16s - loss: 0.9215 - acc: 0.9604 - mDice: 0.6910 - val_loss: 0.7630 - val_acc: 0.9767 - val_mDice: 0.7369

Epoch 00189: val_mDice did not improve from 0.74084
Epoch 190/300
 - 17s - loss: 0.9221 - acc: 0.9604 - mDice: 0.6912 - val_loss: 0.7570 - val_acc: 0.9771 - val_mDice: 0.7341

Epoch 00190: val_mDice did not improve from 0.74084
Epoch 191/300
 - 17s - loss: 0.9198 - acc: 0.9605 - mDice: 0.6914 - val_loss: 0.7455 - val_acc: 0.9773 - val_mDice: 0.7378

Epoch 00191: val_mDice did not improve from 0.74084
Epoch 192/300
 - 16s - loss: 0.9197 - acc: 0.9605 - mDice: 0.6918 - val_loss: 0.7461 - val_acc: 0.9765 - val_mDice: 0.7383

Epoch 00192: val_mDice did not improve from 0.74084
Epoch 193/300
 - 16s - loss: 0.9190 - acc: 0.9605 - mDice: 0.6918 - val_loss: 0.7557 - val_acc: 0.9773 - val_mDice: 0.7368

Epoch 00193: val_mDice did not improve from 0.74084
Epoch 194/300
 - 16s - loss: 0.9192 - acc: 0.9605 - mDice: 0.6919 - val_loss: 0.7475 - val_acc: 0.9772 - val_mDice: 0.7385

Epoch 00194: val_mDice did not improve from 0.74084
Epoch 195/300
 - 17s - loss: 0.9208 - acc: 0.9605 - mDice: 0.6912 - val_loss: 0.7547 - val_acc: 0.9774 - val_mDice: 0.7372

Epoch 00195: val_mDice did not improve from 0.74084
Epoch 196/300
 - 17s - loss: 0.9171 - acc: 0.9605 - mDice: 0.6924 - val_loss: 0.7527 - val_acc: 0.9772 - val_mDice: 0.7358

Epoch 00196: val_mDice did not improve from 0.74084
Epoch 197/300
 - 16s - loss: 0.9184 - acc: 0.9605 - mDice: 0.6921 - val_loss: 0.7439 - val_acc: 0.9770 - val_mDice: 0.7385

Epoch 00197: val_mDice did not improve from 0.74084
Epoch 198/300
 - 16s - loss: 0.9198 - acc: 0.9605 - mDice: 0.6916 - val_loss: 0.7441 - val_acc: 0.9772 - val_mDice: 0.7388

Epoch 00198: val_mDice did not improve from 0.74084
Epoch 199/300
 - 16s - loss: 0.9151 - acc: 0.9606 - mDice: 0.6928 - val_loss: 0.7422 - val_acc: 0.9774 - val_mDice: 0.7407

Epoch 00199: val_mDice did not improve from 0.74084
Epoch 200/300
 - 16s - loss: 0.9170 - acc: 0.9606 - mDice: 0.6928 - val_loss: 0.7488 - val_acc: 0.9775 - val_mDice: 0.7371

Epoch 00200: val_mDice did not improve from 0.74084
Epoch 201/300
 - 17s - loss: 0.9176 - acc: 0.9606 - mDice: 0.6923 - val_loss: 0.7472 - val_acc: 0.9772 - val_mDice: 0.7379

Epoch 00201: val_mDice did not improve from 0.74084
Epoch 202/300
 - 16s - loss: 0.9168 - acc: 0.9607 - mDice: 0.6927 - val_loss: 0.7444 - val_acc: 0.9774 - val_mDice: 0.7397

Epoch 00202: val_mDice did not improve from 0.74084
Epoch 203/300
 - 16s - loss: 0.9170 - acc: 0.9606 - mDice: 0.6926 - val_loss: 0.7439 - val_acc: 0.9776 - val_mDice: 0.7398

Epoch 00203: val_mDice did not improve from 0.74084
Epoch 204/300
 - 16s - loss: 0.9166 - acc: 0.9606 - mDice: 0.6927 - val_loss: 0.7426 - val_acc: 0.9774 - val_mDice: 0.7383

Epoch 00204: val_mDice did not improve from 0.74084
Epoch 205/300
 - 16s - loss: 0.9135 - acc: 0.9607 - mDice: 0.6936 - val_loss: 0.7518 - val_acc: 0.9772 - val_mDice: 0.7383

Epoch 00205: val_mDice did not improve from 0.74084
Epoch 206/300
 - 16s - loss: 0.9166 - acc: 0.9606 - mDice: 0.6925 - val_loss: 0.7591 - val_acc: 0.9772 - val_mDice: 0.7391

Epoch 00206: val_mDice did not improve from 0.74084
Epoch 207/300
 - 17s - loss: 0.9156 - acc: 0.9607 - mDice: 0.6929 - val_loss: 0.7492 - val_acc: 0.9771 - val_mDice: 0.7372

Epoch 00207: val_mDice did not improve from 0.74084
Epoch 208/300
 - 16s - loss: 0.9151 - acc: 0.9607 - mDice: 0.6930 - val_loss: 0.7526 - val_acc: 0.9775 - val_mDice: 0.7374

Epoch 00208: val_mDice did not improve from 0.74084
Epoch 209/300
 - 16s - loss: 0.9126 - acc: 0.9607 - mDice: 0.6941 - val_loss: 0.7445 - val_acc: 0.9775 - val_mDice: 0.7400

Epoch 00209: val_mDice did not improve from 0.74084
Epoch 210/300
 - 16s - loss: 0.9140 - acc: 0.9607 - mDice: 0.6934 - val_loss: 0.7546 - val_acc: 0.9774 - val_mDice: 0.7367

Epoch 00210: val_mDice did not improve from 0.74084
Epoch 211/300
 - 16s - loss: 0.9119 - acc: 0.9607 - mDice: 0.6938 - val_loss: 0.7537 - val_acc: 0.9775 - val_mDice: 0.7393

Epoch 00211: val_mDice did not improve from 0.74084
Epoch 212/300
 - 16s - loss: 0.9134 - acc: 0.9607 - mDice: 0.6935 - val_loss: 0.7486 - val_acc: 0.9773 - val_mDice: 0.7401

Epoch 00212: val_mDice did not improve from 0.74084
Epoch 213/300
 - 16s - loss: 0.9113 - acc: 0.9608 - mDice: 0.6943 - val_loss: 0.7482 - val_acc: 0.9774 - val_mDice: 0.7409

Epoch 00213: val_mDice improved from 0.74084 to 0.74088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 214/300
 - 17s - loss: 0.9108 - acc: 0.9608 - mDice: 0.6941 - val_loss: 0.7437 - val_acc: 0.9774 - val_mDice: 0.7402

Epoch 00214: val_mDice did not improve from 0.74088
Epoch 215/300
 - 17s - loss: 0.9103 - acc: 0.9609 - mDice: 0.6948 - val_loss: 0.7392 - val_acc: 0.9776 - val_mDice: 0.7413

Epoch 00215: val_mDice improved from 0.74088 to 0.74134, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 216/300
 - 16s - loss: 0.9092 - acc: 0.9609 - mDice: 0.6950 - val_loss: 0.7417 - val_acc: 0.9775 - val_mDice: 0.7395

Epoch 00216: val_mDice did not improve from 0.74134
Epoch 217/300
 - 16s - loss: 0.9098 - acc: 0.9609 - mDice: 0.6944 - val_loss: 0.7542 - val_acc: 0.9771 - val_mDice: 0.7367

Epoch 00217: val_mDice did not improve from 0.74134
Epoch 218/300
 - 16s - loss: 0.9069 - acc: 0.9609 - mDice: 0.6956 - val_loss: 0.7546 - val_acc: 0.9772 - val_mDice: 0.7389

Epoch 00218: val_mDice did not improve from 0.74134
Epoch 219/300
 - 16s - loss: 0.9117 - acc: 0.9609 - mDice: 0.6943 - val_loss: 0.7499 - val_acc: 0.9776 - val_mDice: 0.7388

Epoch 00219: val_mDice did not improve from 0.74134
Epoch 220/300
 - 16s - loss: 0.9111 - acc: 0.9609 - mDice: 0.6944 - val_loss: 0.7400 - val_acc: 0.9777 - val_mDice: 0.7393

Epoch 00220: val_mDice did not improve from 0.74134
Epoch 221/300
 - 17s - loss: 0.9079 - acc: 0.9610 - mDice: 0.6953 - val_loss: 0.7478 - val_acc: 0.9776 - val_mDice: 0.7396

Epoch 00221: val_mDice did not improve from 0.74134
Epoch 222/300
 - 16s - loss: 0.9081 - acc: 0.9610 - mDice: 0.6950 - val_loss: 0.7662 - val_acc: 0.9773 - val_mDice: 0.7348

Epoch 00222: val_mDice did not improve from 0.74134
Epoch 223/300
 - 16s - loss: 0.9102 - acc: 0.9610 - mDice: 0.6949 - val_loss: 0.7513 - val_acc: 0.9773 - val_mDice: 0.7378

Epoch 00223: val_mDice did not improve from 0.74134
Epoch 224/300
 - 16s - loss: 0.9082 - acc: 0.9611 - mDice: 0.6953 - val_loss: 0.7498 - val_acc: 0.9770 - val_mDice: 0.7393

Epoch 00224: val_mDice did not improve from 0.74134
Epoch 225/300
 - 16s - loss: 0.9083 - acc: 0.9611 - mDice: 0.6955 - val_loss: 0.7520 - val_acc: 0.9776 - val_mDice: 0.7401

Epoch 00225: val_mDice did not improve from 0.74134
Epoch 226/300
 - 16s - loss: 0.9073 - acc: 0.9612 - mDice: 0.6959 - val_loss: 0.7410 - val_acc: 0.9776 - val_mDice: 0.7402

Epoch 00226: val_mDice did not improve from 0.74134
Epoch 227/300
 - 17s - loss: 0.9064 - acc: 0.9611 - mDice: 0.6959 - val_loss: 0.7542 - val_acc: 0.9776 - val_mDice: 0.7402

Epoch 00227: val_mDice did not improve from 0.74134
Epoch 228/300
 - 17s - loss: 0.9047 - acc: 0.9611 - mDice: 0.6959 - val_loss: 0.7493 - val_acc: 0.9775 - val_mDice: 0.7404

Epoch 00228: val_mDice did not improve from 0.74134
Epoch 229/300
 - 16s - loss: 0.9070 - acc: 0.9612 - mDice: 0.6955 - val_loss: 0.7758 - val_acc: 0.9771 - val_mDice: 0.7327

Epoch 00229: val_mDice did not improve from 0.74134
Epoch 230/300
 - 16s - loss: 0.9082 - acc: 0.9612 - mDice: 0.6955 - val_loss: 0.7502 - val_acc: 0.9777 - val_mDice: 0.7421

Epoch 00230: val_mDice improved from 0.74134 to 0.74210, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 231/300
 - 16s - loss: 0.9046 - acc: 0.9611 - mDice: 0.6965 - val_loss: 0.7463 - val_acc: 0.9774 - val_mDice: 0.7399

Epoch 00231: val_mDice did not improve from 0.74210
Epoch 232/300
 - 16s - loss: 0.9041 - acc: 0.9612 - mDice: 0.6965 - val_loss: 0.7544 - val_acc: 0.9774 - val_mDice: 0.7418

Epoch 00232: val_mDice did not improve from 0.74210
Epoch 233/300
 - 16s - loss: 0.9049 - acc: 0.9612 - mDice: 0.6965 - val_loss: 0.7525 - val_acc: 0.9773 - val_mDice: 0.7398

Epoch 00233: val_mDice did not improve from 0.74210
Epoch 234/300
 - 17s - loss: 0.9066 - acc: 0.9612 - mDice: 0.6960 - val_loss: 0.7531 - val_acc: 0.9775 - val_mDice: 0.7398

Epoch 00234: val_mDice did not improve from 0.74210
Epoch 235/300
 - 16s - loss: 0.9035 - acc: 0.9612 - mDice: 0.6967 - val_loss: 0.7541 - val_acc: 0.9776 - val_mDice: 0.7390

Epoch 00235: val_mDice did not improve from 0.74210
Epoch 236/300
 - 16s - loss: 0.9041 - acc: 0.9612 - mDice: 0.6966 - val_loss: 0.7484 - val_acc: 0.9775 - val_mDice: 0.7391

Epoch 00236: val_mDice did not improve from 0.74210
Epoch 237/300
 - 16s - loss: 0.9013 - acc: 0.9613 - mDice: 0.6977 - val_loss: 0.7522 - val_acc: 0.9775 - val_mDice: 0.7406

Epoch 00237: val_mDice did not improve from 0.74210
Epoch 238/300
 - 16s - loss: 0.9030 - acc: 0.9612 - mDice: 0.6970 - val_loss: 0.7408 - val_acc: 0.9775 - val_mDice: 0.7399

Epoch 00238: val_mDice did not improve from 0.74210
Epoch 239/300
 - 16s - loss: 0.9037 - acc: 0.9613 - mDice: 0.6967 - val_loss: 0.7509 - val_acc: 0.9774 - val_mDice: 0.7401

Epoch 00239: val_mDice did not improve from 0.74210
Epoch 240/300
 - 17s - loss: 0.9031 - acc: 0.9612 - mDice: 0.6969 - val_loss: 0.7501 - val_acc: 0.9777 - val_mDice: 0.7393

Epoch 00240: val_mDice did not improve from 0.74210
Epoch 241/300
 - 16s - loss: 0.9051 - acc: 0.9613 - mDice: 0.6962 - val_loss: 0.7514 - val_acc: 0.9775 - val_mDice: 0.7400

Epoch 00241: val_mDice did not improve from 0.74210
Epoch 242/300
 - 16s - loss: 0.9005 - acc: 0.9613 - mDice: 0.6979 - val_loss: 0.7515 - val_acc: 0.9775 - val_mDice: 0.7377

Epoch 00242: val_mDice did not improve from 0.74210
Epoch 243/300
 - 16s - loss: 0.9033 - acc: 0.9613 - mDice: 0.6966 - val_loss: 0.7532 - val_acc: 0.9775 - val_mDice: 0.7400

Epoch 00243: val_mDice did not improve from 0.74210
Epoch 244/300
 - 16s - loss: 0.9022 - acc: 0.9613 - mDice: 0.6973 - val_loss: 0.7487 - val_acc: 0.9775 - val_mDice: 0.7390

Epoch 00244: val_mDice did not improve from 0.74210
Epoch 245/300
 - 16s - loss: 0.8998 - acc: 0.9614 - mDice: 0.6981 - val_loss: 0.7547 - val_acc: 0.9771 - val_mDice: 0.7397

Epoch 00245: val_mDice did not improve from 0.74210
Epoch 246/300
 - 16s - loss: 0.9002 - acc: 0.9613 - mDice: 0.6981 - val_loss: 0.7492 - val_acc: 0.9775 - val_mDice: 0.7415

Epoch 00246: val_mDice did not improve from 0.74210
Epoch 247/300
 - 17s - loss: 0.8996 - acc: 0.9613 - mDice: 0.6981 - val_loss: 0.7519 - val_acc: 0.9773 - val_mDice: 0.7408

Epoch 00247: val_mDice did not improve from 0.74210
Epoch 248/300
 - 16s - loss: 0.9008 - acc: 0.9613 - mDice: 0.6980 - val_loss: 0.7463 - val_acc: 0.9774 - val_mDice: 0.7408

Epoch 00248: val_mDice did not improve from 0.74210
Epoch 249/300
 - 16s - loss: 0.8997 - acc: 0.9613 - mDice: 0.6980 - val_loss: 0.7525 - val_acc: 0.9775 - val_mDice: 0.7438

Epoch 00249: val_mDice improved from 0.74210 to 0.74383, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 250/300
 - 16s - loss: 0.8991 - acc: 0.9614 - mDice: 0.6982 - val_loss: 0.7474 - val_acc: 0.9777 - val_mDice: 0.7398

Epoch 00250: val_mDice did not improve from 0.74383
Epoch 251/300
 - 16s - loss: 0.8984 - acc: 0.9614 - mDice: 0.6986 - val_loss: 0.7528 - val_acc: 0.9776 - val_mDice: 0.7402

Epoch 00251: val_mDice did not improve from 0.74383
Epoch 252/300
 - 16s - loss: 0.8991 - acc: 0.9614 - mDice: 0.6983 - val_loss: 0.7583 - val_acc: 0.9775 - val_mDice: 0.7365

Epoch 00252: val_mDice did not improve from 0.74383
Epoch 253/300
 - 17s - loss: 0.8980 - acc: 0.9614 - mDice: 0.6983 - val_loss: 0.7517 - val_acc: 0.9777 - val_mDice: 0.7370

Epoch 00253: val_mDice did not improve from 0.74383
Epoch 254/300
 - 16s - loss: 0.8980 - acc: 0.9614 - mDice: 0.6985 - val_loss: 0.7464 - val_acc: 0.9776 - val_mDice: 0.7436

Epoch 00254: val_mDice did not improve from 0.74383
Epoch 255/300
 - 16s - loss: 0.8981 - acc: 0.9615 - mDice: 0.6982 - val_loss: 0.7457 - val_acc: 0.9777 - val_mDice: 0.7417

Epoch 00255: val_mDice did not improve from 0.74383
Epoch 256/300
 - 16s - loss: 0.8960 - acc: 0.9615 - mDice: 0.6990 - val_loss: 0.7579 - val_acc: 0.9769 - val_mDice: 0.7392

Epoch 00256: val_mDice did not improve from 0.74383
Epoch 257/300
 - 16s - loss: 0.8949 - acc: 0.9615 - mDice: 0.6994 - val_loss: 0.7510 - val_acc: 0.9777 - val_mDice: 0.7421

Epoch 00257: val_mDice did not improve from 0.74383
Epoch 258/300
 - 17s - loss: 0.8976 - acc: 0.9615 - mDice: 0.6986 - val_loss: 0.7569 - val_acc: 0.9770 - val_mDice: 0.7409

Epoch 00258: val_mDice did not improve from 0.74383
Epoch 259/300
 - 16s - loss: 0.8974 - acc: 0.9615 - mDice: 0.6989 - val_loss: 0.7477 - val_acc: 0.9775 - val_mDice: 0.7393

Epoch 00259: val_mDice did not improve from 0.74383
Epoch 260/300
 - 16s - loss: 0.8953 - acc: 0.9616 - mDice: 0.6993 - val_loss: 0.7516 - val_acc: 0.9776 - val_mDice: 0.7394

Epoch 00260: val_mDice did not improve from 0.74383
Epoch 261/300
 - 16s - loss: 0.8961 - acc: 0.9615 - mDice: 0.6992 - val_loss: 0.7607 - val_acc: 0.9775 - val_mDice: 0.7382

Epoch 00261: val_mDice did not improve from 0.74383
Epoch 262/300
 - 16s - loss: 0.8960 - acc: 0.9616 - mDice: 0.6993 - val_loss: 0.7396 - val_acc: 0.9776 - val_mDice: 0.7453

Epoch 00262: val_mDice improved from 0.74383 to 0.74526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 263/300
 - 16s - loss: 0.8952 - acc: 0.9616 - mDice: 0.6994 - val_loss: 0.7448 - val_acc: 0.9775 - val_mDice: 0.7415

Epoch 00263: val_mDice did not improve from 0.74526
Epoch 264/300
 - 17s - loss: 0.8928 - acc: 0.9616 - mDice: 0.7003 - val_loss: 0.7574 - val_acc: 0.9773 - val_mDice: 0.7398

Epoch 00264: val_mDice did not improve from 0.74526
Epoch 265/300
 - 16s - loss: 0.8946 - acc: 0.9616 - mDice: 0.6996 - val_loss: 0.7455 - val_acc: 0.9778 - val_mDice: 0.7415

Epoch 00265: val_mDice did not improve from 0.74526
Epoch 266/300
 - 16s - loss: 0.8949 - acc: 0.9616 - mDice: 0.6997 - val_loss: 0.7409 - val_acc: 0.9777 - val_mDice: 0.7456

Epoch 00266: val_mDice improved from 0.74526 to 0.74565, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 267/300
 - 16s - loss: 0.8946 - acc: 0.9616 - mDice: 0.6995 - val_loss: 0.7408 - val_acc: 0.9778 - val_mDice: 0.7428

Epoch 00267: val_mDice did not improve from 0.74565
Epoch 268/300
 - 16s - loss: 0.8935 - acc: 0.9616 - mDice: 0.6997 - val_loss: 0.7504 - val_acc: 0.9770 - val_mDice: 0.7399

Epoch 00268: val_mDice did not improve from 0.74565
Epoch 269/300
 - 16s - loss: 0.8927 - acc: 0.9616 - mDice: 0.7002 - val_loss: 0.7454 - val_acc: 0.9777 - val_mDice: 0.7428

Epoch 00269: val_mDice did not improve from 0.74565
Epoch 270/300
 - 16s - loss: 0.8931 - acc: 0.9616 - mDice: 0.7002 - val_loss: 0.7480 - val_acc: 0.9774 - val_mDice: 0.7436

Epoch 00270: val_mDice did not improve from 0.74565
Epoch 271/300
 - 17s - loss: 0.8925 - acc: 0.9617 - mDice: 0.7005 - val_loss: 0.7571 - val_acc: 0.9775 - val_mDice: 0.7410

Epoch 00271: val_mDice did not improve from 0.74565
Epoch 272/300
 - 17s - loss: 0.8941 - acc: 0.9616 - mDice: 0.6995 - val_loss: 0.7400 - val_acc: 0.9778 - val_mDice: 0.7426

Epoch 00272: val_mDice did not improve from 0.74565
Epoch 273/300
 - 16s - loss: 0.8923 - acc: 0.9616 - mDice: 0.7002 - val_loss: 0.7452 - val_acc: 0.9775 - val_mDice: 0.7444

Epoch 00273: val_mDice did not improve from 0.74565
Epoch 274/300
 - 16s - loss: 0.8914 - acc: 0.9617 - mDice: 0.7006 - val_loss: 0.7411 - val_acc: 0.9778 - val_mDice: 0.7422

Epoch 00274: val_mDice did not improve from 0.74565
Epoch 275/300
 - 16s - loss: 0.8920 - acc: 0.9616 - mDice: 0.7005 - val_loss: 0.7583 - val_acc: 0.9775 - val_mDice: 0.7393

Epoch 00275: val_mDice did not improve from 0.74565
Epoch 276/300
 - 16s - loss: 0.8906 - acc: 0.9617 - mDice: 0.7009 - val_loss: 0.7409 - val_acc: 0.9777 - val_mDice: 0.7443

Epoch 00276: val_mDice did not improve from 0.74565
Epoch 277/300
 - 17s - loss: 0.8933 - acc: 0.9616 - mDice: 0.7001 - val_loss: 0.7447 - val_acc: 0.9775 - val_mDice: 0.7438

Epoch 00277: val_mDice did not improve from 0.74565
Epoch 278/300
 - 16s - loss: 0.8894 - acc: 0.9617 - mDice: 0.7007 - val_loss: 0.7392 - val_acc: 0.9775 - val_mDice: 0.7462

Epoch 00278: val_mDice improved from 0.74565 to 0.74620, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 279/300
 - 16s - loss: 0.8914 - acc: 0.9616 - mDice: 0.7007 - val_loss: 0.7434 - val_acc: 0.9774 - val_mDice: 0.7418

Epoch 00279: val_mDice did not improve from 0.74620
Epoch 280/300
 - 16s - loss: 0.8903 - acc: 0.9617 - mDice: 0.7009 - val_loss: 0.7615 - val_acc: 0.9775 - val_mDice: 0.7398

Epoch 00280: val_mDice did not improve from 0.74620
Epoch 281/300
 - 16s - loss: 0.8901 - acc: 0.9616 - mDice: 0.7008 - val_loss: 0.7405 - val_acc: 0.9778 - val_mDice: 0.7423

Epoch 00281: val_mDice did not improve from 0.74620
Epoch 282/300
 - 17s - loss: 0.8898 - acc: 0.9617 - mDice: 0.7010 - val_loss: 0.7507 - val_acc: 0.9775 - val_mDice: 0.7402

Epoch 00282: val_mDice did not improve from 0.74620
Epoch 283/300
 - 17s - loss: 0.8903 - acc: 0.9617 - mDice: 0.7008 - val_loss: 0.7456 - val_acc: 0.9777 - val_mDice: 0.7412

Epoch 00283: val_mDice did not improve from 0.74620
Epoch 284/300
 - 16s - loss: 0.8907 - acc: 0.9617 - mDice: 0.7010 - val_loss: 0.7472 - val_acc: 0.9776 - val_mDice: 0.7422

Epoch 00284: val_mDice did not improve from 0.74620
Epoch 285/300
 - 16s - loss: 0.8888 - acc: 0.9618 - mDice: 0.7012 - val_loss: 0.7476 - val_acc: 0.9777 - val_mDice: 0.7435

Epoch 00285: val_mDice did not improve from 0.74620
Epoch 286/300
 - 16s - loss: 0.8887 - acc: 0.9618 - mDice: 0.7016 - val_loss: 0.7518 - val_acc: 0.9774 - val_mDice: 0.7419

Epoch 00286: val_mDice did not improve from 0.74620
Epoch 287/300
 - 16s - loss: 0.8893 - acc: 0.9618 - mDice: 0.7016 - val_loss: 0.7429 - val_acc: 0.9778 - val_mDice: 0.7441

Epoch 00287: val_mDice did not improve from 0.74620
Epoch 288/300
 - 16s - loss: 0.8895 - acc: 0.9617 - mDice: 0.7007 - val_loss: 0.7495 - val_acc: 0.9779 - val_mDice: 0.7422

Epoch 00288: val_mDice did not improve from 0.74620
Epoch 289/300
 - 17s - loss: 0.8901 - acc: 0.9618 - mDice: 0.7008 - val_loss: 0.7453 - val_acc: 0.9778 - val_mDice: 0.7417

Epoch 00289: val_mDice did not improve from 0.74620
Epoch 290/300
 - 16s - loss: 0.8886 - acc: 0.9618 - mDice: 0.7015 - val_loss: 0.7525 - val_acc: 0.9776 - val_mDice: 0.7409

Epoch 00290: val_mDice did not improve from 0.74620
Epoch 291/300
 - 16s - loss: 0.8893 - acc: 0.9617 - mDice: 0.7014 - val_loss: 0.7482 - val_acc: 0.9778 - val_mDice: 0.7426

Epoch 00291: val_mDice did not improve from 0.74620
Epoch 292/300
 - 16s - loss: 0.8874 - acc: 0.9617 - mDice: 0.7017 - val_loss: 0.7452 - val_acc: 0.9770 - val_mDice: 0.7448

Epoch 00292: val_mDice did not improve from 0.74620
Epoch 293/300
 - 16s - loss: 0.8868 - acc: 0.9618 - mDice: 0.7023 - val_loss: 0.7447 - val_acc: 0.9775 - val_mDice: 0.7426

Epoch 00293: val_mDice did not improve from 0.74620
Epoch 294/300
 - 16s - loss: 0.8873 - acc: 0.9617 - mDice: 0.7015 - val_loss: 0.7543 - val_acc: 0.9776 - val_mDice: 0.7419

Epoch 00294: val_mDice did not improve from 0.74620
Epoch 295/300
 - 16s - loss: 0.8877 - acc: 0.9618 - mDice: 0.7018 - val_loss: 0.7489 - val_acc: 0.9777 - val_mDice: 0.7410

Epoch 00295: val_mDice did not improve from 0.74620
Epoch 296/300
 - 17s - loss: 0.8866 - acc: 0.9618 - mDice: 0.7017 - val_loss: 0.7480 - val_acc: 0.9778 - val_mDice: 0.7439

Epoch 00296: val_mDice did not improve from 0.74620
Epoch 297/300
 - 16s - loss: 0.8864 - acc: 0.9618 - mDice: 0.7023 - val_loss: 0.7487 - val_acc: 0.9780 - val_mDice: 0.7451

Epoch 00297: val_mDice did not improve from 0.74620
Epoch 298/300
 - 16s - loss: 0.8856 - acc: 0.9618 - mDice: 0.7025 - val_loss: 0.7476 - val_acc: 0.9776 - val_mDice: 0.7424

Epoch 00298: val_mDice did not improve from 0.74620
Epoch 299/300
 - 16s - loss: 0.8873 - acc: 0.9618 - mDice: 0.7019 - val_loss: 0.7479 - val_acc: 0.9777 - val_mDice: 0.7435

Epoch 00299: val_mDice did not improve from 0.74620
Epoch 300/300
 - 16s - loss: 0.8864 - acc: 0.9618 - mDice: 0.7019 - val_loss: 0.7481 - val_acc: 0.9776 - val_mDice: 0.7435

Epoch 00300: val_mDice did not improve from 0.74620
{'val_loss': [5.328091026985482, 4.248170633838601, 3.830350457805477, 3.5009070749152196, 3.22340054544684, 2.842013230062511, 2.4867906374474096, 2.1940614099371922, 2.038930647993741, 1.831980479906683, 1.7304970573072564, 1.5918649410548276, 1.490614840429123, 1.4406370038855565, 1.358879026484816, 1.2929383744932201, 1.242068390323691, 1.2335601073421845, 1.1765803119091138, 1.1967285487749806, 1.1537634999784705, 1.109234145242874, 1.0864748093363357, 1.0578690448852435, 1.0511942473176408, 1.013045904979314, 1.0257747083494109, 1.0002387080290547, 0.9901977144692042, 0.9727420296571027, 0.9947944916274449, 0.9454473991916604, 0.9448355489397702, 0.91441484061006, 0.912579112265208, 0.919384432982092, 0.9196970666924568, 0.8961068738813269, 0.9119132181552991, 0.882861565645427, 0.8777712318178725, 0.8750546954266013, 0.884968779674948, 0.8656848134243325, 0.8609436770824537, 0.8686775205886528, 0.867300177681936, 0.8750409279784112, 0.8671360999754031, 0.8516891488473709, 0.8711123188881025, 0.85586912827949, 0.8340323518400323, 0.8320916927840611, 0.8487008801878315, 0.8294651034760149, 0.8335573207842161, 0.8481447696685791, 0.8364851458431923, 0.8338228496786666, 0.8269174278598942, 0.8256828842097765, 0.8363705714271493, 0.8277542411464535, 0.8240309978184635, 0.8256526332195491, 0.822809541878635, 0.810610930805337, 0.8072917881893785, 0.8159829982339519, 0.7991359299176359, 0.822108364676776, 0.8014534434227094, 0.8036675481763604, 0.8010168896146017, 0.7974924767670566, 0.7958292687592441, 0.8007950480670145, 0.7905923087302953, 0.7844181260834001, 0.7994460332883547, 0.791915822110764, 0.7883109972901541, 0.787545964734195, 0.7943041769609059, 0.7871571995624124, 0.7897589145457908, 0.7897937232500887, 0.7975345147799139, 0.780076701347142, 0.7833159859866312, 0.7801540624605466, 0.7843422803976764, 0.7839687303321002, 0.7930108525165139, 0.7738017888918315, 0.7781163927626936, 0.7828940275597246, 0.7777424724134681, 0.7751670018450855, 0.7758007098550666, 0.7775149974104476, 0.7828586571020623, 0.7708646406049597, 0.7755812484107606, 0.7673135032392528, 0.769966821964473, 0.7792633771896362, 0.7640095814450146, 0.7788067945878799, 0.7711715216506018, 0.7694457606093524, 0.7721753402115548, 0.7733823804006185, 0.7711433074245714, 0.7687293401319687, 0.7619330429867522, 0.7633146145572401, 0.7668460342165542, 0.7716592770733245, 0.7810854017734528, 0.7634214705800357, 0.7622623586491363, 0.7685938277473189, 0.7541562604577574, 0.7753128699243885, 0.7619246864972049, 0.7640486445328961, 0.7619530921929503, 0.7645456060154797, 0.765054074868764, 0.7545623011785011, 0.7600567716441743, 0.7685677841918109, 0.7617227537174748, 0.761482541691767, 0.7638875143985225, 0.7671184400989585, 0.7688675481979161, 0.7549487205400859, 0.7632782197978398, 0.7724572726308483, 0.7660339066427048, 0.7600364885101579, 0.7533617264603916, 0.7681459637537394, 0.7742307214704278, 0.7739619882139441, 0.7655570956125651, 0.7602259618778752, 0.7595676812407088, 0.7556770513318989, 0.7524283573235551, 0.7608699218867576, 0.7480930445128924, 0.7535544768588184, 0.7604589682735808, 0.7549338993960863, 0.757983487762817, 0.7550587756176518, 0.7632312231684384, 0.7483291985237435, 0.7545079131648965, 0.7467881841202305, 0.7496219032431302, 0.7886571496316831, 0.7512000611383621, 0.7570108199772769, 0.7549602491398381, 0.7566034099827074, 0.7527361279481077, 0.7638347520403665, 0.7505765998200195, 0.7497921953462574, 0.7553394869582294, 0.7520881050253567, 0.7576881973710778, 0.7485634978503397, 0.7466296953697728, 0.7432459874512398, 0.7559691030685216, 0.7513130882831469, 0.7525034228416339, 0.7528463046844691, 0.750451726456211, 0.7576474755594175, 0.7501274535100754, 0.7473817900435565, 0.7629979432445683, 0.756969496811906, 0.7455366074222408, 0.7461214971868959, 0.7557352164836779, 0.7475494693403375, 0.7547054392834233, 0.7527444983181888, 0.7438828618559119, 0.744101245109349, 0.7421713593887956, 0.7487974779246604, 0.7472427818056655, 0.7444279638871755, 0.7438722256928274, 0.7425670497221489, 0.7517963900958022, 0.7590746904072696, 0.7491540198456751, 0.7526357737306046, 0.7445495022486334, 0.7546069707772504, 0.7537474636345693, 0.7485690423070568, 0.7481762138131547, 0.743737037459465, 0.7392351974363196, 0.7417070628845528, 0.75421765487488, 0.7546481371742405, 0.749943978165927, 0.7400374943262911, 0.7478410940464228, 0.766163363440396, 0.7513154520563883, 0.7497596924435602, 0.7519846444260584, 0.7409776649246477, 0.7541790984264792, 0.7492747151688354, 0.7757651989590632, 0.7502201424069601, 0.7463163692657262, 0.7544455009780519, 0.7525415702225411, 0.7530710893134548, 0.754058741951642, 0.7484492169667597, 0.7522396869855384, 0.7407672225612484, 0.7509316046760507, 0.7501446343448064, 0.7514263077141488, 0.7515403901877469, 0.7532059195106977, 0.7486885847294167, 0.7546630028175981, 0.7492076264668818, 0.7518544229742599, 0.7462991345418643, 0.7525083104225054, 0.747398750831003, 0.7527787513928871, 0.7582931273604092, 0.7516756261864753, 0.7464023681536113, 0.7457088244287935, 0.7578648708454551, 0.75102367793044, 0.7569126213250095, 0.7477021503121886, 0.7515689408125943, 0.7606952153656581, 0.7396320663902858, 0.7448499488503966, 0.7574036880715252, 0.7454545914310299, 0.7408971496640819, 0.7407855101644176, 0.7503690470571387, 0.745391893468491, 0.7480462683390264, 0.7570623453349283, 0.7400325181549543, 0.745234582930395, 0.741120792006793, 0.7583274963783891, 0.7409315725711927, 0.7447289412152277, 0.7392097760553229, 0.7434352730235009, 0.7614734287131323, 0.740491511478816, 0.7506688710761397, 0.7455892734331627, 0.7471926828769788, 0.7475541858640435, 0.7518374911726338, 0.742935495017326, 0.7495079060939893, 0.7452556552135781, 0.7524749297801763, 0.7481866119659111, 0.7451688859560718, 0.7446865116896695, 0.7543165875624304, 0.7488522864367864, 0.7479981845372343, 0.7486859057864098, 0.7475597552240711, 0.7479161318034342, 0.7481245953742772], 'val_acc': [0.9134473159705123, 0.9133301030283105, 0.9119923482202503, 0.9083885889347285, 0.9156128290581377, 0.9318242746673219, 0.9368283862936987, 0.9421735989720854, 0.9465486925758727, 0.9531938004983614, 0.9525276463325709, 0.9564925158677036, 0.9571997964218871, 0.9573988963479865, 0.9603514242662142, 0.9624073987137781, 0.9628220151548517, 0.9641517486474286, 0.9657184783726522, 0.9651029207935072, 0.9667533889208755, 0.9674555663376638, 0.9679884653385371, 0.9687383595394762, 0.9691777200731513, 0.9696596779235421, 0.9699858284159882, 0.9700429713072842, 0.9704514068283446, 0.9714924864573021, 0.9713814491278505, 0.9717938977561585, 0.9716897998770623, 0.9724076271873631, 0.9719748158160955, 0.9722059740595621, 0.9728742984876241, 0.9725947371900898, 0.9732783457187757, 0.9730053384826608, 0.9731840756658006, 0.9731607759652072, 0.9725976423041461, 0.9737100837981865, 0.9732070144725172, 0.9731727969156553, 0.9730450212955475, 0.9742717489804307, 0.9736940554560047, 0.9742273508685909, 0.9743998879439211, 0.9747868257842652, 0.9741035736586949, 0.9749866777086911, 0.9742710357659483, 0.9745978982481238, 0.9745665984610988, 0.9747154978040147, 0.9751803278923035, 0.9753805484673749, 0.9750074327808537, 0.975331038644869, 0.9754387781228104, 0.9752505748239282, 0.9756906873559299, 0.9750933459360306, 0.9754569881582913, 0.9757059600255261, 0.9757198062661576, 0.9754449806801261, 0.9757918817539738, 0.9756298951090199, 0.9758355568533075, 0.9758952651938347, 0.9755090442422318, 0.9754092864794274, 0.975697249582369, 0.9758475765790025, 0.9761355062053628, 0.9760991000149348, 0.9752440281110267, 0.9762873029055661, 0.9763022355020863, 0.9759258440096085, 0.9759305629828204, 0.9759968200775042, 0.9764929679158616, 0.9763972224438027, 0.9762680061059456, 0.9765250103114402, 0.9760365024821399, 0.9763106079134223, 0.976743414385678, 0.9754533457429442, 0.9758180788118546, 0.9763775847546042, 0.976502435256357, 0.9764107057493027, 0.9762257807058831, 0.9763612000093068, 0.9761711804017629, 0.9763120686354703, 0.9759258358445886, 0.9761799292205131, 0.9763826842177404, 0.9764969797983561, 0.9766069042356047, 0.9763855881070438, 0.9763022261123134, 0.9762494515066278, 0.9761402513066383, 0.9760586994968049, 0.9765122675732391, 0.9766611416045934, 0.9762742090715121, 0.9763539155868635, 0.9765071762751226, 0.9765399359676936, 0.9765391929508889, 0.976696104627766, 0.9764307218871705, 0.9766655111149566, 0.9765625053072629, 0.9764390922572515, 0.9765322857523617, 0.9765428378157419, 0.9766775361479145, 0.9763531917578554, 0.9766553317847317, 0.9764401969844347, 0.9768486169919576, 0.9764358197173028, 0.9768733733320889, 0.9761730146734682, 0.9766047323403293, 0.976716111784112, 0.9766746322586112, 0.9764849588479081, 0.9763823029113142, 0.9762738498106395, 0.9766367686121431, 0.9765159054978253, 0.9765304678106961, 0.9768824614074132, 0.9764798720405526, 0.9766960797244555, 0.9759735228264168, 0.976491148749443, 0.9765461152547026, 0.9768129464698164, 0.9768901181547609, 0.9764489213081256, 0.9767353959279518, 0.9765519320148311, 0.9767790971553489, 0.9768824744714449, 0.9769629137973262, 0.9772268362241249, 0.9768857519104056, 0.9770815817460622, 0.9768478943877024, 0.9768577107827957, 0.9769738373691088, 0.9773196500458129, 0.977094688235897, 0.9763440881689934, 0.977189689874649, 0.9770331656279629, 0.9769239780837542, 0.977023329636822, 0.9772981784931601, 0.9769337969283535, 0.9772242797564153, 0.9770706740960683, 0.9769181449935861, 0.9770626552300911, 0.9770906739038964, 0.9769239699187344, 0.9773462141213352, 0.9767776331672929, 0.9769578098434292, 0.9773768002856268, 0.9769061383319227, 0.9772847066186878, 0.977016419172287, 0.9770058801729385, 0.9772093524671581, 0.9773684388970676, 0.9766928267805544, 0.9770834033619867, 0.9773251161183396, 0.9765402964533192, 0.9773269275279894, 0.9772079039926398, 0.9773509551401007, 0.9772483098180327, 0.976967647875825, 0.9772446751594543, 0.9773982722465306, 0.9774954678261116, 0.9771503924506985, 0.9773808040031015, 0.9776119651043251, 0.9773596982433371, 0.9771784205142766, 0.9772326627822772, 0.9770575680144845, 0.9774918356170393, 0.97745397972734, 0.9773800834401013, 0.977487465698425, 0.9773269320187503, 0.9773738927220645, 0.977443421131944, 0.9776112376010582, 0.9774612604755245, 0.9770768444015555, 0.9771966101372078, 0.9775679033913024, 0.9777240716431239, 0.9775911924773699, 0.9772781574562804, 0.9772712425009845, 0.9770167943549483, 0.9775824840754679, 0.9775715400911358, 0.9775759161335148, 0.9775180543122226, 0.9770830465506201, 0.9776621914073212, 0.9773724336330205, 0.9774099314866, 0.977316383221378, 0.977489656373246, 0.977609783411026, 0.9774940226176013, 0.9774787262694477, 0.9774976678907055, 0.9774437824340716, 0.977661473293827, 0.9774925647533104, 0.9774761828657699, 0.9775315061823963, 0.9774878310830626, 0.9771154449410635, 0.9775052976934877, 0.9772708709925821, 0.9773524113713878, 0.9774881870779273, 0.97768147514291, 0.9775609859865005, 0.9775002063953713, 0.9776629164610824, 0.9775791878569616, 0.9776982138418171, 0.9768828312828116, 0.9776803912365273, 0.9770146093956412, 0.9775020304608019, 0.977623607606104, 0.9774936551917089, 0.9776432563180792, 0.9774932902153224, 0.9773116426108635, 0.9777553738796547, 0.9777440988037684, 0.9778376405369745, 0.976956355653397, 0.9777098665498707, 0.977446679791359, 0.9774896498412302, 0.9778354600684284, 0.9775285859630533, 0.9778362034934841, 0.9775245895940964, 0.9776985963729963, 0.9775471683234385, 0.9775438884349719, 0.9773909878240873, 0.97750129683377, 0.9777575653709777, 0.9775479043999763, 0.9777437354603858, 0.9776101438966516, 0.9777324318069301, 0.9774386788884254, 0.9777652025222778, 0.9778678592753737, 0.9777579278978583, 0.977606133647161, 0.9778289019244991, 0.9770371722031946, 0.9775336850179385, 0.9775537207518539, 0.9777229742644584, 0.9778321830377187, 0.9780123662458707, 0.9775679111480713, 0.9777091517023844, 0.9776476290944505], 'val_mDice': [0.012154050649794405, 0.030461068099288093, 0.046482853387316614, 0.07045264501277715, 0.09618262844542934, 0.1488528006697354, 0.19891103806152735, 0.2522111307472399, 0.291038963280312, 0.34075774823966093, 0.3649979911438406, 0.4058064430543821, 0.4390257492865602, 0.4587282811942166, 0.4851874051845237, 0.5076676607131958, 0.5232056826353073, 0.5301859088956493, 0.5460423063741972, 0.549516450460643, 0.5686809698196307, 0.5820249359901637, 0.593231007660905, 0.6005585573307456, 0.6081823865844779, 0.619808750609829, 0.6207945261099567, 0.6331930091119793, 0.6363170522533051, 0.6411639839819033, 0.638049548619414, 0.6521440732152495, 0.6544278153817947, 0.6656935684484978, 0.6633856565984961, 0.6655290792249653, 0.6700100090405713, 0.676534796822561, 0.6722650883132464, 0.6775361405660029, 0.6798275235581072, 0.6844408246752334, 0.6852811664751132, 0.6857703005614346, 0.6896938497889532, 0.6904463388331948, 0.6906073816018562, 0.6911299081697856, 0.6907139208218823, 0.6905323442530958, 0.6929580263079029, 0.6926104373311344, 0.6984743369768743, 0.699280415495781, 0.6990837887541889, 0.7032276738996375, 0.6991656839031063, 0.694615218737354, 0.7021223825134643, 0.7036713594443178, 0.7060348497678156, 0.7029270664469837, 0.7027453016744901, 0.7067686315268686, 0.7053863569481732, 0.7064674406835477, 0.7063703390016948, 0.7113832496616939, 0.7102461391932344, 0.70737872017573, 0.7144391614280335, 0.7091221254165858, 0.7112029109099139, 0.7128149552704537, 0.7113582451049596, 0.7172779963441092, 0.7158469680237444, 0.7123648761069938, 0.7168381536660129, 0.7179456463415329, 0.7142808180149287, 0.7172363547429647, 0.7166589180084124, 0.7217691891813931, 0.7146006185714513, 0.7182684961247118, 0.7195664448280857, 0.7179865518661395, 0.7187176559069385, 0.7217057498350535, 0.7205105121821573, 0.7231021413247879, 0.7239161472614497, 0.7218203765072234, 0.7199943984208041, 0.7237934625312074, 0.7259303490593009, 0.7211904427776598, 0.7211370284426702, 0.7241618110708994, 0.7236233814938428, 0.7255199882265639, 0.7204477586974837, 0.7223476522589383, 0.7251650315441497, 0.7241048694473423, 0.726965166934549, 0.722665305823496, 0.7262389529241274, 0.725359221843824, 0.7252733593117701, 0.7260878874014501, 0.7252486103201565, 0.7257802792607921, 0.7275856798642302, 0.7250696481907204, 0.7283914501536383, 0.7265846329192592, 0.7264291341990641, 0.7272452365045678, 0.7246507067386418, 0.7314875787251616, 0.7290406361834644, 0.7292273277289247, 0.7302161091811037, 0.7274897465967152, 0.732035369089205, 0.7297791687593068, 0.7290018988798742, 0.730757522256407, 0.7270573251051445, 0.7317101788030912, 0.7290888106169766, 0.7267847583718496, 0.7301156124840044, 0.7305801845576665, 0.7288905557704298, 0.7292651595318154, 0.7270654982899967, 0.7315271427370098, 0.7310724589106155, 0.727232718712663, 0.731064015055356, 0.7328853541857576, 0.7335606699120508, 0.7286651853012712, 0.7267336184031343, 0.728704764418406, 0.7298044031613493, 0.7306867699100547, 0.7304787680710831, 0.7304278199803339, 0.7321128028712861, 0.7274825781175535, 0.7352984890545884, 0.7345420375262222, 0.7346356482538459, 0.7338845721662861, 0.7318392338818067, 0.730774999073107, 0.7320200947866048, 0.7349586850159788, 0.7342258557064892, 0.7328864699357176, 0.7361263183698262, 0.7298810004371487, 0.7331027727421016, 0.7335115487444891, 0.7368727261072969, 0.7341758408775069, 0.7337674870066446, 0.7288835681464574, 0.7382734197459809, 0.7357022015199269, 0.7369431734901585, 0.734436425444198, 0.7373572187880947, 0.7386267707772451, 0.7380010808167392, 0.740045476041428, 0.7321509028950782, 0.7366972855509144, 0.7384673961221355, 0.7341419148118529, 0.739109658215144, 0.7345340456048103, 0.7408377940524115, 0.7398376170903036, 0.7368865396878491, 0.7340562976386449, 0.7378138777327864, 0.7382670591955316, 0.7368391156196594, 0.7385103951578271, 0.7372396751625897, 0.7358041818011297, 0.7385487670767797, 0.7388233709825228, 0.7406793689891084, 0.7370956442127489, 0.737880240972728, 0.7397318457903928, 0.7398418224837682, 0.7383463333730829, 0.7382527581632954, 0.7391054111800782, 0.7372368035251147, 0.7373802425110176, 0.7400022410366633, 0.7366887804580061, 0.7393080931003779, 0.7400667740874094, 0.7408821599124229, 0.7402004897594452, 0.7413440759051336, 0.739454155915404, 0.7366569107525969, 0.7388691693952639, 0.7387621749753821, 0.7393157682190202, 0.739645001006453, 0.7348338928941178, 0.737805817633459, 0.7392729296259684, 0.7400552773312347, 0.7401822331833513, 0.7402215448960866, 0.7403920370421998, 0.7326715335454026, 0.7420985514987005, 0.7399173383026907, 0.7417872299070227, 0.7398113465472443, 0.7397665303863891, 0.7390262713171032, 0.7391001798518716, 0.7405556223980369, 0.7399130341124861, 0.7401443923989387, 0.7392552115329324, 0.7400028558626567, 0.7377056189595836, 0.7399852990287624, 0.7389538753522585, 0.7397396001097274, 0.7414537500845243, 0.7408413041944373, 0.7407574735275687, 0.7438330050200632, 0.7397686708463381, 0.7401647645316712, 0.7365192387202014, 0.736972591240112, 0.7435598855149256, 0.7416597237325695, 0.7392319449006695, 0.7421443968603055, 0.7408846604497465, 0.739253429925605, 0.7393592387846072, 0.738152904053257, 0.745258481943444, 0.7415048435126266, 0.7398274859336957, 0.7415368430418511, 0.7456458129294931, 0.742835426575517, 0.7398817102386527, 0.7428389203875032, 0.7436368971654813, 0.7410264431613766, 0.7425939518294923, 0.7444056868553162, 0.7421914102279976, 0.7392520790230738, 0.7443184595401973, 0.7438075138281469, 0.7461986974494098, 0.7417680490506838, 0.7397606311595604, 0.7423433285869964, 0.7401574278531009, 0.7411660009051022, 0.7421774443698256, 0.7435048822670767, 0.741873857909686, 0.7440921913271081, 0.7421873677266787, 0.7417255578792259, 0.7409217561760993, 0.7425532500221305, 0.7448152107735203, 0.7425911965435499, 0.7418585158374211, 0.7410035553860338, 0.7438572457391922, 0.7450631432337304, 0.7424384763796036, 0.7435269412929064, 0.74350569468655], 'loss': [96.30939423144538, 9.010665557236146, 6.020977223049883, 5.095483579235287, 4.532758358111008, 4.053913817606097, 3.6910372562216325, 3.381541121212268, 3.1256551477235623, 2.9256300188454154, 2.7445039482735814, 2.586119198604942, 2.4375737882291295, 2.3095978721721333, 2.2060548285069688, 2.105821818086037, 2.02313724349933, 1.9488424932280473, 1.8852004338686217, 1.8198381982037426, 1.7729464508497925, 1.7281905144483212, 1.6771835693279356, 1.6340751983072737, 1.5986485302439655, 1.5591577658556048, 1.5299111421271758, 1.4968184170547614, 1.4727870373589382, 1.4407967514311286, 1.4210230573050182, 1.3942951288238894, 1.375924093630568, 1.3575140732076634, 1.3358389488202564, 1.319437602435694, 1.3027585772693782, 1.286595559811338, 1.275661441014585, 1.2654027690272245, 1.2446743876906818, 1.2355230505296746, 1.2241562539809894, 1.2162532091996798, 1.202817210373094, 1.196429427614947, 1.187296320087416, 1.1799614635701523, 1.1731774596324789, 1.1648094400456857, 1.1585647993716535, 1.1523864261040808, 1.146289340624942, 1.1434565103854368, 1.1364178302299661, 1.1274377347974986, 1.1251499614627558, 1.1183223441753785, 1.1180817243581658, 1.1095638056652615, 1.1032666319841087, 1.1036642101653955, 1.0980979851098962, 1.0942532380659429, 1.0873889228186957, 1.086132156535214, 1.082374762880342, 1.077051690578047, 1.0731089636500333, 1.0691566107744102, 1.0673550808766967, 1.0610970946396798, 1.057949742160192, 1.0550379862761898, 1.0551691483739452, 1.0523972450477472, 1.0524441590222504, 1.0480681320886576, 1.0429036447505353, 1.0406897106143767, 1.0375959980378089, 1.0355563074820036, 1.0340489776164712, 1.0312204972721766, 1.0275990446062915, 1.026186979962025, 1.025337247772328, 1.0247458065295543, 1.0194727401686516, 1.0174196047753368, 1.0154384730267143, 1.0155788551089768, 1.0129399796992453, 1.011844357827628, 1.008337702807004, 1.010151264362895, 1.0058942456131434, 1.0046530078405862, 1.0006850621253187, 1.0018679274597009, 0.9971503906781096, 1.0012464929939242, 0.9974904217627073, 0.9959991928427885, 0.9952955088665286, 0.9931719900685694, 0.9907039058407253, 0.9877346339607946, 0.9875569262414491, 0.9840136925061529, 0.9856183271682937, 0.9856410328753225, 0.9854898305147595, 0.9817187866637078, 0.9799103043634833, 0.980627198566373, 0.9809389857462995, 0.979231341333928, 0.9753035121213626, 0.9733269646030557, 0.9727988270364489, 0.9729612698795835, 0.972374489545374, 0.9686717793084758, 0.9666053399869498, 0.9634937549695675, 0.9641323676568478, 0.9627096512483238, 0.9617975893147142, 0.9594293875078723, 0.9597899464189165, 0.9595720656846333, 0.9575014447621814, 0.9604110928999499, 0.9565461117926319, 0.955336174339701, 0.9536708648351079, 0.9539832519366889, 0.9530034711248438, 0.9529546398652066, 0.9508134980716845, 0.9490014592695423, 0.9505562078775259, 0.947739374451456, 0.9484232471669356, 0.9476827408133992, 0.9452494639012605, 0.9451017806349481, 0.9442582029228619, 0.9435439398934614, 0.9441785539361136, 0.9411781661033263, 0.9417064366995487, 0.9391504424704983, 0.9396579842250533, 0.9410024204652415, 0.9361491482041475, 0.9372954872985813, 0.9359324973323809, 0.9379240775477186, 0.9356671465946178, 0.9355844947477554, 0.9325573213333462, 0.9332889870288166, 0.93280496133759, 0.9338107697187754, 0.9325191861012164, 0.9292050257516922, 0.9308002008703038, 0.9301953461608103, 0.9304620190249092, 0.9290792378260547, 0.9305415368751191, 0.92641427196177, 0.928019886610278, 0.9288599657704442, 0.9272492180801805, 0.9264926398321545, 0.924542987889293, 0.9250639371498158, 0.9257124252299572, 0.9257091163721387, 0.9255066611326266, 0.9231076115263221, 0.9239525478732713, 0.9249573545350449, 0.9215634256232198, 0.9200942414922308, 0.9214887723769789, 0.9221207295962371, 0.9197726754778764, 0.9197128337316326, 0.9189694428568582, 0.9191777679085864, 0.9208080381021436, 0.9170695203023621, 0.9183854412168438, 0.919840768059338, 0.9150961155853844, 0.9169716773331783, 0.9175717255021699, 0.9167995351476264, 0.9169786245376267, 0.9165664317449999, 0.9135468817069172, 0.916634250641777, 0.91563727198391, 0.9150864788854408, 0.9125850429229951, 0.9140419165791301, 0.9118538780139782, 0.9134079761071702, 0.9113377970846895, 0.9108489972679602, 0.9103147501405208, 0.9091847325337697, 0.9097849754857652, 0.9068956737765589, 0.9117000628534044, 0.9110776119114055, 0.9078685997066639, 0.9080711897295454, 0.9102197231898923, 0.908186956888923, 0.9082855232980281, 0.9073401941259864, 0.9063548424731244, 0.9046820122406588, 0.9070004483818566, 0.9081514892971564, 0.9046375543975022, 0.904099069790321, 0.9049291887727552, 0.906602707224999, 0.9034519076700687, 0.9040816517828563, 0.9013048537157031, 0.9029528882651279, 0.9037072245480268, 0.9030677784744001, 0.905091723272143, 0.9004620373381196, 0.903345110843116, 0.9022333039864672, 0.8998276252414273, 0.9002117719430338, 0.8996441722235063, 0.9008012058645957, 0.899671736589552, 0.8990806553704381, 0.8983597110062256, 0.8990840660350238, 0.8979562529684969, 0.8979686871497762, 0.8981174286462995, 0.8959817290464087, 0.8949050061274358, 0.8975850531905983, 0.897399068213387, 0.8952874183077382, 0.896119468401378, 0.8960383319083142, 0.8952441942798666, 0.892796897388055, 0.8946331431362519, 0.8948874622877836, 0.8945675160509804, 0.8935357127179754, 0.8927045456888797, 0.8931351195496762, 0.8924799928223233, 0.8941088488842643, 0.892312538194923, 0.8914379860511532, 0.8919671498351432, 0.8905798965557218, 0.8932957525462769, 0.8894079626482696, 0.8913807217813617, 0.8903228049881746, 0.8901254625856242, 0.8898259444167007, 0.8903369283057379, 0.890725176612109, 0.888755939684349, 0.8886791970797163, 0.8893389300648761, 0.8895340415045545, 0.890063355602904, 0.8886240260406508, 0.8893237388061037, 0.8873758554901007, 0.8868058898246824, 0.887346413804684, 0.8876623349497936, 0.8866493297321859, 0.8864038249711955, 0.8856027511549177, 0.8873499469821082, 0.8863601049412715], 'acc': [0.6887787688200178, 0.889100640775198, 0.8952194096565339, 0.8974495594849315, 0.901129710734037, 0.9085922770152247, 0.914285670505029, 0.9186040178868471, 0.9222758026161885, 0.9254248129344227, 0.9276273519086564, 0.9293821072502593, 0.9309963565704892, 0.932721442174059, 0.93411799187637, 0.9357496384419765, 0.9368885254893247, 0.9382096699470095, 0.9393440364579062, 0.9405949850376327, 0.9416596320739591, 0.942495094907675, 0.9436374277205368, 0.9446263161560008, 0.9453874803335157, 0.9461605370265902, 0.946912745267973, 0.9475160858517262, 0.9480379633549822, 0.9488444925245075, 0.9492983214165414, 0.9498806890076721, 0.9504990284802213, 0.9510147384726471, 0.9514015682307007, 0.9517334441609184, 0.9521702122438948, 0.9525772381664225, 0.952779134260131, 0.953171398187064, 0.9536637869839083, 0.9537754748130605, 0.954002751675656, 0.9541756962976075, 0.9543583435525985, 0.954513151502947, 0.9547575429592531, 0.9548532040103429, 0.9550578575621709, 0.9551656130187638, 0.9553026786637172, 0.9553790586650489, 0.9555398613994418, 0.9556248104711907, 0.9556452157503266, 0.9558271362446114, 0.9558997890668763, 0.9560637471359766, 0.9560957930278673, 0.9562347462576115, 0.9563861134609686, 0.9563480617770334, 0.95644218763723, 0.9565594295253695, 0.956660705415006, 0.9566541264456918, 0.9567635801563643, 0.9569641481342589, 0.9570090751445944, 0.9571228720773082, 0.9571435282337125, 0.9573329862924799, 0.9573670850615253, 0.9574461910194808, 0.9574142253406365, 0.9574671434611549, 0.9575374890358525, 0.9577204334838711, 0.9578525203757415, 0.9579784987782781, 0.9580539060079067, 0.9581527758796012, 0.95815791028443, 0.958331695737204, 0.958433329823494, 0.9584578473462594, 0.9585050392354263, 0.9585243305032055, 0.9586638448019936, 0.9587298057683606, 0.9587503375849199, 0.9588162463724461, 0.958844606011935, 0.958909728072156, 0.9590348046718433, 0.9589476357845463, 0.9591540734281516, 0.9591318290143641, 0.95927058732609, 0.9592802198721797, 0.9593601007680789, 0.9592461818252405, 0.9593514468625193, 0.9593906548260045, 0.9593917309585792, 0.9594787758019144, 0.9595655673233547, 0.9596505517086079, 0.9596795167105532, 0.9598329049189973, 0.9597598818521786, 0.9597382344931394, 0.9597503347904506, 0.9598720832079932, 0.9598486707051855, 0.9598427304842827, 0.9598769074813723, 0.9599054620790967, 0.9599909209286979, 0.9600236647054026, 0.9600105925068015, 0.9599602151315777, 0.9599592226965754, 0.9598987544781961, 0.9596930713968177, 0.9596461985385747, 0.9595801465666101, 0.9595178715912335, 0.9594863788800858, 0.9595035586858567, 0.9594661148578393, 0.9594941664370602, 0.9594700793408963, 0.9594120580240344, 0.9594961615854116, 0.9595090832427138, 0.9595118488189461, 0.959539145094796, 0.9595255669044974, 0.9595335502125589, 0.9595335052009835, 0.9596051634481209, 0.9595275853816513, 0.9595942844671883, 0.9596000540411049, 0.9595634147795581, 0.959545229491202, 0.9595763669329279, 0.9596339753664708, 0.9595884046322914, 0.9595679435892458, 0.9596361876987619, 0.9596940137011797, 0.9597329259280716, 0.9596713621091587, 0.9597234813194573, 0.9598094317429186, 0.9597648723515334, 0.9598783938716627, 0.9598291948149708, 0.9598811099147664, 0.9599029226868209, 0.9599772659064638, 0.9599672732304261, 0.9599616502565306, 0.9599789223605922, 0.959976004108681, 0.9600626094478361, 0.9600880598009589, 0.9600682445041036, 0.960155515517032, 0.9600970945023211, 0.9600765326550635, 0.9601528812439518, 0.9601451582842904, 0.960168771731939, 0.9601388464112346, 0.9602035983446805, 0.9602830827629729, 0.9602475730509544, 0.960280870568569, 0.9602753073228423, 0.9603178430145933, 0.9603569105657487, 0.9603118193575294, 0.9603595867737947, 0.9604025375090988, 0.9604480984303008, 0.9603837982270395, 0.9604112753391703, 0.9604610369328309, 0.9604832616086305, 0.960477267353154, 0.9604813560381689, 0.9604629138284858, 0.9605401756016684, 0.9604513455720757, 0.960477349809741, 0.9605565396261374, 0.9605923756598691, 0.9605680232772172, 0.9606539733186159, 0.9605888927480998, 0.9605892297043687, 0.9606833875084493, 0.9606388599173156, 0.9607275344244319, 0.9606755424036486, 0.960747533493418, 0.960720589757971, 0.9607447189442238, 0.9607252104350519, 0.9608292443918098, 0.960833014201024, 0.9608625618398802, 0.9608659331893581, 0.9608763194646058, 0.960890944572614, 0.9608600097447397, 0.9608696095627115, 0.9610173631400406, 0.9610198603445573, 0.9609549026833056, 0.96105046451876, 0.9611078398337657, 0.9611560239769579, 0.9611031791090512, 0.9610981787909737, 0.9611501439065036, 0.9611509212862009, 0.9611387006707568, 0.9611817631206028, 0.9611903279021514, 0.9611684333888001, 0.9612213723791313, 0.9612292497064632, 0.9612953114827872, 0.9612344562579329, 0.9612813873706746, 0.9612246689994546, 0.9612655482054779, 0.9613065318438718, 0.9612850686792431, 0.9613170028738277, 0.9613571336191358, 0.9613432302561845, 0.9613445648785539, 0.9613322476673324, 0.9613422383066602, 0.9613962849827696, 0.9613636001586179, 0.9613524013366738, 0.9614068167951574, 0.96141991131426, 0.9614589948459754, 0.9615092514118428, 0.961542743160912, 0.9614936263740846, 0.9614966501787006, 0.961562702449421, 0.9615058998807872, 0.9615740335052563, 0.9615687239405057, 0.9616322892276032, 0.9615614096040214, 0.9615640525151645, 0.9616180072089747, 0.9616045869656704, 0.9616380261251096, 0.9616234669789239, 0.9616816727401171, 0.9615911471711003, 0.9616490207704799, 0.9617067109568102, 0.9616283562661504, 0.961674286139213, 0.9615854976998679, 0.9616601368571898, 0.9616284600263205, 0.9616694284511523, 0.9616250061800384, 0.9616730354212469, 0.9616878018464702, 0.9616947076143578, 0.9617552364284466, 0.9617777446550521, 0.9617516189817923, 0.9617413941725368, 0.9617626343648332, 0.9617676126439287, 0.9617454006365221, 0.9617428205675019, 0.9618201731423239, 0.9617356673925088, 0.9617779522759351, 0.9618278489617751, 0.9618232568560856, 0.961838766485072, 0.9617933377460535, 0.9618418118776635], 'mDice': [0.015806241351815756, 0.02774872483359836, 0.04211689662580588, 0.05568420832269398, 0.072339754685766, 0.10072064497030306, 0.1328935416467311, 0.16685488819214192, 0.20042015766243146, 0.22834668587969553, 0.2553283073562123, 0.2804445693553675, 0.3054534755842882, 0.3290652074542779, 0.3493414982522078, 0.37042920137521496, 0.3868273868176545, 0.4024958162429769, 0.41707816066337017, 0.43178770057413574, 0.44392885373376284, 0.45468069851338144, 0.46762353268162166, 0.47932566303857305, 0.48928085442215363, 0.49949329843853885, 0.5073329311456156, 0.5164342077877259, 0.5236563415974098, 0.5326517226638676, 0.5385748457033046, 0.5452315032013905, 0.5516376776565178, 0.5565443278864278, 0.5628872095541734, 0.5675899397350747, 0.5719367705073332, 0.5772385066851368, 0.580950510594361, 0.5843921814879955, 0.5902158113133563, 0.5937421812632668, 0.597331620450824, 0.5997609730151551, 0.6031332972153244, 0.6054511750811318, 0.6083361534269364, 0.6101521644036599, 0.6125860340601336, 0.6148934476898058, 0.6166037853105746, 0.6188730570636741, 0.6205182384339245, 0.6216930638129949, 0.6235218057977624, 0.6258510746544048, 0.6267033484942816, 0.6288795439534788, 0.629307770862356, 0.6316441937988767, 0.6334286830793996, 0.6332236428342156, 0.634860417918588, 0.6363873476665896, 0.6382867725414049, 0.638415604546832, 0.6396801486475634, 0.6412214154743034, 0.6427065549146596, 0.6436191470980719, 0.6442911648406736, 0.6460798698980565, 0.647521676480141, 0.6477166551142706, 0.6483355722482983, 0.6485690030254647, 0.6491410026315195, 0.650345946376344, 0.6512601018882704, 0.6526844599366596, 0.6532144555988538, 0.6541905530568025, 0.6542900422980018, 0.6551639200710458, 0.656318078662029, 0.6569123612041272, 0.6570688095993177, 0.6578205638698477, 0.6590903080959275, 0.6596454782045642, 0.6598915822580984, 0.660711510309892, 0.6609209849636919, 0.661711096548162, 0.6624218281208564, 0.6621072348804208, 0.663577209932786, 0.6642129079474789, 0.6649765003887486, 0.6647391910351663, 0.6658596761502773, 0.6650642829031201, 0.6663647390651762, 0.6667277113409684, 0.6666117582420445, 0.6674321562452508, 0.6682104039286423, 0.6695806620592002, 0.6693181019529354, 0.6706547235843905, 0.670198990179031, 0.6699787493447027, 0.6700053025460587, 0.6709425776336525, 0.6717229115657907, 0.6715717738068496, 0.6715223645237798, 0.6718687716176857, 0.673136383902625, 0.6737670857358584, 0.6739577604228373, 0.6739041341563938, 0.674666224235431, 0.675895679815171, 0.6760864924870892, 0.6771702937858295, 0.6776060862474323, 0.6778213362549235, 0.6781558022050261, 0.6787537522526027, 0.6789099941242631, 0.6791072185024927, 0.6801108732511465, 0.6788840585228587, 0.6797831503159913, 0.6803921650585436, 0.6811363244437041, 0.6807738498806282, 0.6811458557230851, 0.6811035644393915, 0.6820241365698632, 0.6824820372787026, 0.6821014087104448, 0.6829820850555153, 0.6825391402835874, 0.6832166066445053, 0.6837531442992446, 0.683581086631407, 0.6840880243420526, 0.6842063681735401, 0.6843054548815835, 0.6848912430278387, 0.6850145980216629, 0.685811640819967, 0.685235003539353, 0.6850525376348474, 0.6862183605379596, 0.6864311654249774, 0.6868336793169745, 0.6860181325613444, 0.6866425718995277, 0.6865642534301301, 0.6875092268156726, 0.6874859695566942, 0.687432991337435, 0.6875512802869166, 0.688002110639304, 0.6884652794634062, 0.6881709474156555, 0.6884529956582484, 0.6885519997718174, 0.6890205535964969, 0.6880793730042527, 0.6896468955290749, 0.689087972910331, 0.6887915149338992, 0.6889798474418691, 0.6891225140332268, 0.6901513671603822, 0.6896705601644997, 0.6898501871970713, 0.6898270756590825, 0.6898028961669578, 0.690720348460076, 0.6902650949598663, 0.6899172027913443, 0.6910975280770497, 0.6918483052640267, 0.6910184251583595, 0.6911605239759049, 0.6913634714226968, 0.6917629983748141, 0.6918390098322017, 0.6919112776444846, 0.6912460740436801, 0.6923578244627824, 0.6920764995710587, 0.6915723708108096, 0.6928375529992424, 0.6928194790979461, 0.6923184018030736, 0.6927324286931578, 0.6925625479809089, 0.6927300406695044, 0.6936181885540368, 0.6924906313473922, 0.6928946571962776, 0.6929720553485874, 0.6941317064232744, 0.6933909984627508, 0.6937653807061133, 0.6935205609030055, 0.6943365313316335, 0.694138611671212, 0.6947921038282953, 0.6950014174728544, 0.6944449805651488, 0.6956181890263926, 0.6943497773601185, 0.6943758507051596, 0.6953237248855106, 0.6949737160181366, 0.6948559249451123, 0.6953172685381959, 0.6955317845717528, 0.6959366798027434, 0.6959219408055961, 0.6959279029017397, 0.6955317345847452, 0.6954852928281767, 0.6964634247955469, 0.696539679078161, 0.6964840741782391, 0.6959669311521507, 0.696706922044294, 0.6965938430216475, 0.6976633419662598, 0.6970454814652488, 0.6966975412171595, 0.6968805236186492, 0.6962492201861327, 0.6979312426734013, 0.6965804278083557, 0.6972894999795813, 0.6980656729565395, 0.6980635799024816, 0.6981037357202915, 0.6979639376179041, 0.6979658147232625, 0.6982270143245685, 0.6985520609367715, 0.6982818627979125, 0.6982942556948816, 0.6984580173608027, 0.6981542269932723, 0.6990225516524313, 0.6993873227308489, 0.6985643837065736, 0.6989269293591548, 0.6992705399354996, 0.6991900692842594, 0.6993287687439753, 0.6994372606880722, 0.7002576254920457, 0.699649480005202, 0.6997004565954082, 0.6995198060773449, 0.6996960445877282, 0.7002490715030486, 0.7001673602886651, 0.7005082288301562, 0.6994764807080756, 0.7001997957982709, 0.7006280567681762, 0.7004967464545335, 0.700938242556957, 0.700072767714534, 0.7006896954225318, 0.7007219127571331, 0.7009040511621384, 0.700816553580318, 0.7009674017041774, 0.7007782746236912, 0.7010406253596284, 0.7011742223813807, 0.701611777283587, 0.7016085000191892, 0.7007010933048576, 0.7008216871951677, 0.7014598089060994, 0.7013895946661531, 0.7017325334269958, 0.7023077176400763, 0.7015058385693278, 0.7018346126076457, 0.7016966287886024, 0.7022918853663704, 0.7024684706172735, 0.7019222255752565, 0.701939783531118]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:02<00:28,  2.01s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:24,  1.89s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:23,  1.92s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:21,  1.95s/it]predicting test subjects:  33%|███▎      | 5/15 [00:09<00:20,  2.03s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:18,  2.10s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:15,  1.88s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:15<00:14,  2.02s/it]predicting test subjects:  60%|██████    | 9/15 [00:17<00:12,  2.02s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:19<00:09,  1.88s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:21<00:07,  1.84s/it]predicting test subjects:  80%|████████  | 12/15 [00:23<00:05,  1.92s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:25<00:03,  1.97s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:27<00:01,  1.91s/it]predicting test subjects: 100%|██████████| 15/15 [00:28<00:00,  1.90s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:08,  2.28s/it]predicting train subjects:   0%|          | 2/532 [00:03<18:15,  2.07s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:12,  1.95s/it]predicting train subjects:   1%|          | 4/532 [00:07<16:30,  1.88s/it]predicting train subjects:   1%|          | 5/532 [00:09<16:15,  1.85s/it]predicting train subjects:   1%|          | 6/532 [00:10<15:39,  1.79s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<15:36,  1.78s/it]predicting train subjects:   2%|▏         | 8/532 [00:14<15:06,  1.73s/it]predicting train subjects:   2%|▏         | 9/532 [00:16<15:49,  1.82s/it]predicting train subjects:   2%|▏         | 10/532 [00:17<15:20,  1.76s/it]predicting train subjects:   2%|▏         | 11/532 [00:19<14:25,  1.66s/it]predicting train subjects:   2%|▏         | 12/532 [00:21<15:33,  1.80s/it]predicting train subjects:   2%|▏         | 13/532 [00:22<14:36,  1.69s/it]predicting train subjects:   3%|▎         | 14/532 [00:24<13:51,  1.61s/it]predicting train subjects:   3%|▎         | 15/532 [00:25<13:45,  1.60s/it]predicting train subjects:   3%|▎         | 16/532 [00:27<14:21,  1.67s/it]predicting train subjects:   3%|▎         | 17/532 [00:28<13:44,  1.60s/it]predicting train subjects:   3%|▎         | 18/532 [00:30<14:32,  1.70s/it]predicting train subjects:   4%|▎         | 19/532 [00:32<13:36,  1.59s/it]predicting train subjects:   4%|▍         | 20/532 [00:33<14:00,  1.64s/it]predicting train subjects:   4%|▍         | 21/532 [00:35<14:50,  1.74s/it]predicting train subjects:   4%|▍         | 22/532 [00:37<14:15,  1.68s/it]predicting train subjects:   4%|▍         | 23/532 [00:39<14:12,  1.68s/it]predicting train subjects:   5%|▍         | 24/532 [00:40<13:27,  1.59s/it]predicting train subjects:   5%|▍         | 25/532 [00:42<14:49,  1.75s/it]predicting train subjects:   5%|▍         | 26/532 [00:44<14:10,  1.68s/it]predicting train subjects:   5%|▌         | 27/532 [00:46<15:33,  1.85s/it]predicting train subjects:   5%|▌         | 28/532 [00:48<15:03,  1.79s/it]predicting train subjects:   5%|▌         | 29/532 [00:50<15:58,  1.91s/it]predicting train subjects:   6%|▌         | 30/532 [00:51<14:55,  1.78s/it]predicting train subjects:   6%|▌         | 31/532 [00:53<14:33,  1.74s/it]predicting train subjects:   6%|▌         | 32/532 [00:55<14:25,  1.73s/it]predicting train subjects:   6%|▌         | 33/532 [00:56<13:46,  1.66s/it]predicting train subjects:   6%|▋         | 34/532 [00:58<14:51,  1.79s/it]predicting train subjects:   7%|▋         | 35/532 [01:00<14:50,  1.79s/it]predicting train subjects:   7%|▋         | 36/532 [01:02<15:07,  1.83s/it]predicting train subjects:   7%|▋         | 37/532 [01:04<14:57,  1.81s/it]predicting train subjects:   7%|▋         | 38/532 [01:06<15:21,  1.87s/it]predicting train subjects:   7%|▋         | 39/532 [01:07<15:03,  1.83s/it]predicting train subjects:   8%|▊         | 40/532 [01:09<14:29,  1.77s/it]predicting train subjects:   8%|▊         | 41/532 [01:11<14:47,  1.81s/it]predicting train subjects:   8%|▊         | 42/532 [01:13<14:53,  1.82s/it]predicting train subjects:   8%|▊         | 43/532 [01:14<14:06,  1.73s/it]predicting train subjects:   8%|▊         | 44/532 [01:16<13:15,  1.63s/it]predicting train subjects:   8%|▊         | 45/532 [01:17<13:09,  1.62s/it]predicting train subjects:   9%|▊         | 46/532 [01:19<13:28,  1.66s/it]predicting train subjects:   9%|▉         | 47/532 [01:21<14:32,  1.80s/it]predicting train subjects:   9%|▉         | 48/532 [01:23<14:34,  1.81s/it]predicting train subjects:   9%|▉         | 49/532 [01:25<13:58,  1.74s/it]predicting train subjects:   9%|▉         | 50/532 [01:27<14:50,  1.85s/it]predicting train subjects:  10%|▉         | 51/532 [01:28<14:26,  1.80s/it]predicting train subjects:  10%|▉         | 52/532 [01:30<14:28,  1.81s/it]predicting train subjects:  10%|▉         | 53/532 [01:32<13:54,  1.74s/it]predicting train subjects:  10%|█         | 54/532 [01:34<14:43,  1.85s/it]predicting train subjects:  10%|█         | 55/532 [01:36<14:40,  1.85s/it]predicting train subjects:  11%|█         | 56/532 [01:38<14:35,  1.84s/it]predicting train subjects:  11%|█         | 57/532 [01:39<14:10,  1.79s/it]predicting train subjects:  11%|█         | 58/532 [01:41<14:13,  1.80s/it]predicting train subjects:  11%|█         | 59/532 [01:43<15:14,  1.93s/it]predicting train subjects:  11%|█▏        | 60/532 [01:45<14:01,  1.78s/it]predicting train subjects:  11%|█▏        | 61/532 [01:46<13:25,  1.71s/it]predicting train subjects:  12%|█▏        | 62/532 [01:48<14:09,  1.81s/it]predicting train subjects:  12%|█▏        | 63/532 [01:50<14:42,  1.88s/it]predicting train subjects:  12%|█▏        | 64/532 [01:52<13:47,  1.77s/it]predicting train subjects:  12%|█▏        | 65/532 [01:54<13:49,  1.78s/it]predicting train subjects:  12%|█▏        | 66/532 [01:56<15:03,  1.94s/it]predicting train subjects:  13%|█▎        | 67/532 [01:58<15:35,  2.01s/it]predicting train subjects:  13%|█▎        | 68/532 [02:00<15:05,  1.95s/it]predicting train subjects:  13%|█▎        | 69/532 [02:02<14:33,  1.89s/it]predicting train subjects:  13%|█▎        | 70/532 [02:03<14:03,  1.83s/it]predicting train subjects:  13%|█▎        | 71/532 [02:05<13:26,  1.75s/it]predicting train subjects:  14%|█▎        | 72/532 [02:06<12:52,  1.68s/it]predicting train subjects:  14%|█▎        | 73/532 [02:08<13:26,  1.76s/it]predicting train subjects:  14%|█▍        | 74/532 [02:11<14:54,  1.95s/it]predicting train subjects:  14%|█▍        | 75/532 [02:14<16:57,  2.23s/it]predicting train subjects:  14%|█▍        | 76/532 [02:15<15:50,  2.08s/it]predicting train subjects:  14%|█▍        | 77/532 [02:17<15:21,  2.02s/it]predicting train subjects:  15%|█▍        | 78/532 [02:19<15:09,  2.00s/it]predicting train subjects:  15%|█▍        | 79/532 [02:21<14:53,  1.97s/it]predicting train subjects:  15%|█▌        | 80/532 [02:23<14:28,  1.92s/it]predicting train subjects:  15%|█▌        | 81/532 [02:25<14:18,  1.90s/it]predicting train subjects:  15%|█▌        | 82/532 [02:27<14:04,  1.88s/it]predicting train subjects:  16%|█▌        | 83/532 [02:28<13:18,  1.78s/it]predicting train subjects:  16%|█▌        | 84/532 [02:30<12:48,  1.71s/it]predicting train subjects:  16%|█▌        | 85/532 [02:31<12:24,  1.67s/it]predicting train subjects:  16%|█▌        | 86/532 [02:33<12:14,  1.65s/it]predicting train subjects:  16%|█▋        | 87/532 [02:35<12:05,  1.63s/it]predicting train subjects:  17%|█▋        | 88/532 [02:36<11:59,  1.62s/it]predicting train subjects:  17%|█▋        | 89/532 [02:38<12:22,  1.68s/it]predicting train subjects:  17%|█▋        | 90/532 [02:40<12:28,  1.69s/it]predicting train subjects:  17%|█▋        | 91/532 [02:41<12:37,  1.72s/it]predicting train subjects:  17%|█▋        | 92/532 [02:43<12:39,  1.73s/it]predicting train subjects:  17%|█▋        | 93/532 [02:45<12:39,  1.73s/it]predicting train subjects:  18%|█▊        | 94/532 [02:47<12:39,  1.73s/it]predicting train subjects:  18%|█▊        | 95/532 [02:49<13:24,  1.84s/it]predicting train subjects:  18%|█▊        | 96/532 [02:51<13:52,  1.91s/it]predicting train subjects:  18%|█▊        | 97/532 [02:53<14:11,  1.96s/it]predicting train subjects:  18%|█▊        | 98/532 [02:55<14:20,  1.98s/it]predicting train subjects:  19%|█▊        | 99/532 [02:57<14:31,  2.01s/it]predicting train subjects:  19%|█▉        | 100/532 [02:59<14:39,  2.04s/it]predicting train subjects:  19%|█▉        | 101/532 [03:01<13:36,  1.90s/it]predicting train subjects:  19%|█▉        | 102/532 [03:02<12:53,  1.80s/it]predicting train subjects:  19%|█▉        | 103/532 [03:04<12:13,  1.71s/it]predicting train subjects:  20%|█▉        | 104/532 [03:05<11:49,  1.66s/it]predicting train subjects:  20%|█▉        | 105/532 [03:07<11:35,  1.63s/it]predicting train subjects:  20%|█▉        | 106/532 [03:08<11:18,  1.59s/it]predicting train subjects:  20%|██        | 107/532 [03:10<11:12,  1.58s/it]predicting train subjects:  20%|██        | 108/532 [03:11<11:03,  1.56s/it]predicting train subjects:  20%|██        | 109/532 [03:13<10:55,  1.55s/it]predicting train subjects:  21%|██        | 110/532 [03:15<10:54,  1.55s/it]predicting train subjects:  21%|██        | 111/532 [03:16<10:49,  1.54s/it]predicting train subjects:  21%|██        | 112/532 [03:17<10:36,  1.51s/it]predicting train subjects:  21%|██        | 113/532 [03:19<11:09,  1.60s/it]predicting train subjects:  21%|██▏       | 114/532 [03:21<11:36,  1.67s/it]predicting train subjects:  22%|██▏       | 115/532 [03:23<12:02,  1.73s/it]predicting train subjects:  22%|██▏       | 116/532 [03:25<12:06,  1.75s/it]predicting train subjects:  22%|██▏       | 117/532 [03:27<12:12,  1.76s/it]predicting train subjects:  22%|██▏       | 118/532 [03:28<12:21,  1.79s/it]predicting train subjects:  22%|██▏       | 119/532 [03:30<12:23,  1.80s/it]predicting train subjects:  23%|██▎       | 120/532 [03:32<12:18,  1.79s/it]predicting train subjects:  23%|██▎       | 121/532 [03:34<12:21,  1.80s/it]predicting train subjects:  23%|██▎       | 122/532 [03:36<12:15,  1.79s/it]predicting train subjects:  23%|██▎       | 123/532 [03:37<12:07,  1.78s/it]predicting train subjects:  23%|██▎       | 124/532 [03:39<12:06,  1.78s/it]predicting train subjects:  23%|██▎       | 125/532 [03:41<12:15,  1.81s/it]predicting train subjects:  24%|██▎       | 126/532 [03:43<12:18,  1.82s/it]predicting train subjects:  24%|██▍       | 127/532 [03:45<12:19,  1.83s/it]predicting train subjects:  24%|██▍       | 128/532 [03:47<12:24,  1.84s/it]predicting train subjects:  24%|██▍       | 129/532 [03:48<12:15,  1.83s/it]predicting train subjects:  24%|██▍       | 130/532 [03:50<12:19,  1.84s/it]predicting train subjects:  25%|██▍       | 131/532 [03:52<13:00,  1.95s/it]predicting train subjects:  25%|██▍       | 132/532 [03:55<13:24,  2.01s/it]predicting train subjects:  25%|██▌       | 133/532 [03:57<13:49,  2.08s/it]predicting train subjects:  25%|██▌       | 134/532 [03:59<13:57,  2.11s/it]predicting train subjects:  25%|██▌       | 135/532 [04:01<14:03,  2.13s/it]predicting train subjects:  26%|██▌       | 136/532 [04:03<14:00,  2.12s/it]predicting train subjects:  26%|██▌       | 137/532 [04:06<14:15,  2.17s/it]predicting train subjects:  26%|██▌       | 138/532 [04:08<14:22,  2.19s/it]predicting train subjects:  26%|██▌       | 139/532 [04:10<14:27,  2.21s/it]predicting train subjects:  26%|██▋       | 140/532 [04:12<14:37,  2.24s/it]predicting train subjects:  27%|██▋       | 141/532 [04:15<14:27,  2.22s/it]predicting train subjects:  27%|██▋       | 142/532 [04:17<14:27,  2.22s/it]predicting train subjects:  27%|██▋       | 143/532 [04:18<13:13,  2.04s/it]predicting train subjects:  27%|██▋       | 144/532 [04:20<12:31,  1.94s/it]predicting train subjects:  27%|██▋       | 145/532 [04:22<12:00,  1.86s/it]predicting train subjects:  27%|██▋       | 146/532 [04:23<11:31,  1.79s/it]predicting train subjects:  28%|██▊       | 147/532 [04:25<11:05,  1.73s/it]predicting train subjects:  28%|██▊       | 148/532 [04:27<11:01,  1.72s/it]predicting train subjects:  28%|██▊       | 149/532 [04:28<10:55,  1.71s/it]predicting train subjects:  28%|██▊       | 150/532 [04:30<10:56,  1.72s/it]predicting train subjects:  28%|██▊       | 151/532 [04:32<10:51,  1.71s/it]predicting train subjects:  29%|██▊       | 152/532 [04:34<10:53,  1.72s/it]predicting train subjects:  29%|██▉       | 153/532 [04:35<10:50,  1.72s/it]predicting train subjects:  29%|██▉       | 154/532 [04:37<10:44,  1.70s/it]predicting train subjects:  29%|██▉       | 155/532 [04:39<11:50,  1.88s/it]predicting train subjects:  29%|██▉       | 156/532 [04:42<12:32,  2.00s/it]predicting train subjects:  30%|██▉       | 157/532 [04:44<13:00,  2.08s/it]predicting train subjects:  30%|██▉       | 158/532 [04:46<13:15,  2.13s/it]predicting train subjects:  30%|██▉       | 159/532 [04:48<13:37,  2.19s/it]predicting train subjects:  30%|███       | 160/532 [04:51<13:53,  2.24s/it]predicting train subjects:  30%|███       | 161/532 [04:53<13:06,  2.12s/it]predicting train subjects:  30%|███       | 162/532 [04:54<12:27,  2.02s/it]predicting train subjects:  31%|███       | 163/532 [04:56<11:53,  1.93s/it]predicting train subjects:  31%|███       | 164/532 [04:58<11:23,  1.86s/it]predicting train subjects:  31%|███       | 165/532 [04:59<11:02,  1.81s/it]predicting train subjects:  31%|███       | 166/532 [05:01<10:41,  1.75s/it]predicting train subjects:  31%|███▏      | 167/532 [05:03<10:43,  1.76s/it]predicting train subjects:  32%|███▏      | 168/532 [05:05<10:47,  1.78s/it]predicting train subjects:  32%|███▏      | 169/532 [05:07<10:52,  1.80s/it]predicting train subjects:  32%|███▏      | 170/532 [05:08<10:58,  1.82s/it]predicting train subjects:  32%|███▏      | 171/532 [05:10<11:02,  1.83s/it]predicting train subjects:  32%|███▏      | 172/532 [05:12<11:02,  1.84s/it]predicting train subjects:  33%|███▎      | 173/532 [05:14<10:39,  1.78s/it]predicting train subjects:  33%|███▎      | 174/532 [05:15<10:17,  1.72s/it]predicting train subjects:  33%|███▎      | 175/532 [05:17<10:07,  1.70s/it]predicting train subjects:  33%|███▎      | 176/532 [05:19<09:58,  1.68s/it]predicting train subjects:  33%|███▎      | 177/532 [05:20<10:06,  1.71s/it]predicting train subjects:  33%|███▎      | 178/532 [05:22<09:55,  1.68s/it]predicting train subjects:  34%|███▎      | 179/532 [05:24<09:51,  1.68s/it]predicting train subjects:  34%|███▍      | 180/532 [05:25<09:50,  1.68s/it]predicting train subjects:  34%|███▍      | 181/532 [05:27<09:48,  1.68s/it]predicting train subjects:  34%|███▍      | 182/532 [05:29<09:39,  1.66s/it]predicting train subjects:  34%|███▍      | 183/532 [05:30<09:32,  1.64s/it]predicting train subjects:  35%|███▍      | 184/532 [05:32<09:35,  1.65s/it]predicting train subjects:  35%|███▍      | 185/532 [05:34<09:27,  1.64s/it]predicting train subjects:  35%|███▍      | 186/532 [05:35<09:19,  1.62s/it]predicting train subjects:  35%|███▌      | 187/532 [05:37<09:16,  1.61s/it]predicting train subjects:  35%|███▌      | 188/532 [05:38<09:13,  1.61s/it]predicting train subjects:  36%|███▌      | 189/532 [05:40<09:11,  1.61s/it]predicting train subjects:  36%|███▌      | 190/532 [05:41<09:08,  1.60s/it]predicting train subjects:  36%|███▌      | 191/532 [05:44<10:24,  1.83s/it]predicting train subjects:  36%|███▌      | 192/532 [05:46<11:12,  1.98s/it]predicting train subjects:  36%|███▋      | 193/532 [05:48<11:42,  2.07s/it]predicting train subjects:  36%|███▋      | 194/532 [05:51<12:13,  2.17s/it]predicting train subjects:  37%|███▋      | 195/532 [05:53<12:28,  2.22s/it]predicting train subjects:  37%|███▋      | 196/532 [05:56<12:37,  2.26s/it]predicting train subjects:  37%|███▋      | 197/532 [05:58<12:10,  2.18s/it]predicting train subjects:  37%|███▋      | 198/532 [06:00<11:53,  2.14s/it]predicting train subjects:  37%|███▋      | 199/532 [06:02<11:41,  2.11s/it]predicting train subjects:  38%|███▊      | 200/532 [06:04<11:31,  2.08s/it]predicting train subjects:  38%|███▊      | 201/532 [06:06<11:26,  2.08s/it]predicting train subjects:  38%|███▊      | 202/532 [06:08<11:20,  2.06s/it]predicting train subjects:  38%|███▊      | 203/532 [06:09<10:44,  1.96s/it]predicting train subjects:  38%|███▊      | 204/532 [06:11<10:14,  1.87s/it]predicting train subjects:  39%|███▊      | 205/532 [06:13<09:54,  1.82s/it]predicting train subjects:  39%|███▊      | 206/532 [06:14<09:35,  1.77s/it]predicting train subjects:  39%|███▉      | 207/532 [06:16<09:26,  1.74s/it]predicting train subjects:  39%|███▉      | 208/532 [06:18<09:20,  1.73s/it]predicting train subjects:  39%|███▉      | 209/532 [06:19<08:54,  1.65s/it]predicting train subjects:  39%|███▉      | 210/532 [06:21<08:45,  1.63s/it]predicting train subjects:  40%|███▉      | 211/532 [06:22<08:32,  1.60s/it]predicting train subjects:  40%|███▉      | 212/532 [06:24<08:26,  1.58s/it]predicting train subjects:  40%|████      | 213/532 [06:25<08:18,  1.56s/it]predicting train subjects:  40%|████      | 214/532 [06:27<08:11,  1.55s/it]predicting train subjects:  40%|████      | 215/532 [06:29<09:13,  1.75s/it]predicting train subjects:  41%|████      | 216/532 [06:31<09:50,  1.87s/it]predicting train subjects:  41%|████      | 217/532 [06:34<10:21,  1.97s/it]predicting train subjects:  41%|████      | 218/532 [06:36<10:47,  2.06s/it]predicting train subjects:  41%|████      | 219/532 [06:38<10:58,  2.10s/it]predicting train subjects:  41%|████▏     | 220/532 [06:40<11:07,  2.14s/it]predicting train subjects:  42%|████▏     | 221/532 [06:42<10:04,  1.94s/it]predicting train subjects:  42%|████▏     | 222/532 [06:43<09:19,  1.80s/it]predicting train subjects:  42%|████▏     | 223/532 [06:45<08:50,  1.72s/it]predicting train subjects:  42%|████▏     | 224/532 [06:46<08:30,  1.66s/it]predicting train subjects:  42%|████▏     | 225/532 [06:48<08:10,  1.60s/it]predicting train subjects:  42%|████▏     | 226/532 [06:49<07:55,  1.55s/it]predicting train subjects:  43%|████▎     | 227/532 [06:51<07:45,  1.53s/it]predicting train subjects:  43%|████▎     | 228/532 [06:52<07:35,  1.50s/it]predicting train subjects:  43%|████▎     | 229/532 [06:54<07:27,  1.48s/it]predicting train subjects:  43%|████▎     | 230/532 [06:55<07:26,  1.48s/it]predicting train subjects:  43%|████▎     | 231/532 [06:56<07:23,  1.47s/it]predicting train subjects:  44%|████▎     | 232/532 [06:58<07:18,  1.46s/it]predicting train subjects:  44%|████▍     | 233/532 [07:00<07:35,  1.52s/it]predicting train subjects:  44%|████▍     | 234/532 [07:01<07:52,  1.58s/it]predicting train subjects:  44%|████▍     | 235/532 [07:03<07:59,  1.61s/it]predicting train subjects:  44%|████▍     | 236/532 [07:05<08:05,  1.64s/it]predicting train subjects:  45%|████▍     | 237/532 [07:06<08:05,  1.65s/it]predicting train subjects:  45%|████▍     | 238/532 [07:08<08:09,  1.67s/it]predicting train subjects:  45%|████▍     | 239/532 [07:10<08:26,  1.73s/it]predicting train subjects:  45%|████▌     | 240/532 [07:12<08:40,  1.78s/it]predicting train subjects:  45%|████▌     | 241/532 [07:14<08:47,  1.81s/it]predicting train subjects:  45%|████▌     | 242/532 [07:16<08:52,  1.84s/it]predicting train subjects:  46%|████▌     | 243/532 [07:17<08:52,  1.84s/it]predicting train subjects:  46%|████▌     | 244/532 [07:19<08:59,  1.87s/it]predicting train subjects:  46%|████▌     | 245/532 [07:21<08:24,  1.76s/it]predicting train subjects:  46%|████▌     | 246/532 [07:22<07:56,  1.67s/it]predicting train subjects:  46%|████▋     | 247/532 [07:24<07:41,  1.62s/it]predicting train subjects:  47%|████▋     | 248/532 [07:25<07:22,  1.56s/it]predicting train subjects:  47%|████▋     | 249/532 [07:27<07:11,  1.53s/it]predicting train subjects:  47%|████▋     | 250/532 [07:28<07:02,  1.50s/it]predicting train subjects:  47%|████▋     | 251/532 [07:30<07:03,  1.51s/it]predicting train subjects:  47%|████▋     | 252/532 [07:31<07:04,  1.52s/it]predicting train subjects:  48%|████▊     | 253/532 [07:33<07:10,  1.54s/it]predicting train subjects:  48%|████▊     | 254/532 [07:34<07:12,  1.56s/it]predicting train subjects:  48%|████▊     | 255/532 [07:36<07:07,  1.54s/it]predicting train subjects:  48%|████▊     | 256/532 [07:37<07:04,  1.54s/it]predicting train subjects:  48%|████▊     | 257/532 [07:39<07:38,  1.67s/it]predicting train subjects:  48%|████▊     | 258/532 [07:41<08:01,  1.76s/it]predicting train subjects:  49%|████▊     | 259/532 [07:43<08:18,  1.83s/it]predicting train subjects:  49%|████▉     | 260/532 [07:45<08:25,  1.86s/it]predicting train subjects:  49%|████▉     | 261/532 [07:47<08:29,  1.88s/it]predicting train subjects:  49%|████▉     | 262/532 [07:49<08:36,  1.91s/it]predicting train subjects:  49%|████▉     | 263/532 [07:51<07:52,  1.76s/it]predicting train subjects:  50%|████▉     | 264/532 [07:52<07:24,  1.66s/it]predicting train subjects:  50%|████▉     | 265/532 [07:53<07:05,  1.59s/it]predicting train subjects:  50%|█████     | 266/532 [07:55<06:48,  1.54s/it]predicting train subjects:  50%|█████     | 267/532 [07:56<06:38,  1.50s/it]predicting train subjects:  50%|█████     | 268/532 [07:58<06:25,  1.46s/it]predicting train subjects:  51%|█████     | 269/532 [07:59<06:49,  1.56s/it]predicting train subjects:  51%|█████     | 270/532 [08:01<06:59,  1.60s/it]predicting train subjects:  51%|█████     | 271/532 [08:03<07:14,  1.66s/it]predicting train subjects:  51%|█████     | 272/532 [08:05<07:21,  1.70s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:06<07:22,  1.71s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:08<07:27,  1.74s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:10<07:57,  1.86s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:13<08:17,  1.95s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:15<08:32,  2.01s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:17<08:44,  2.06s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:19<08:53,  2.11s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:21<08:58,  2.14s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:23<08:49,  2.11s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:25<08:42,  2.09s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:28<08:40,  2.09s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:30<08:32,  2.07s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:32<08:32,  2.07s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:34<08:29,  2.07s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:35<07:48,  1.91s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:37<07:20,  1.80s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:38<07:00,  1.73s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:40<06:45,  1.68s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:41<06:35,  1.64s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:43<06:30,  1.63s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:45<06:40,  1.68s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:47<06:50,  1.73s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:48<06:52,  1.74s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:50<06:53,  1.75s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:52<06:53,  1.76s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:54<06:56,  1.78s/it]predicting train subjects:  56%|█████▌    | 299/532 [08:55<06:30,  1.68s/it]predicting train subjects:  56%|█████▋    | 300/532 [08:57<06:11,  1.60s/it]predicting train subjects:  57%|█████▋    | 301/532 [08:58<05:57,  1.55s/it]predicting train subjects:  57%|█████▋    | 302/532 [09:00<05:48,  1.51s/it]predicting train subjects:  57%|█████▋    | 303/532 [09:01<05:40,  1.49s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:02<05:32,  1.46s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:04<06:14,  1.65s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:07<06:45,  1.79s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:09<07:05,  1.89s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:11<07:16,  1.95s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:13<07:27,  2.01s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:15<07:33,  2.04s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:18<08:21,  2.27s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:21<08:54,  2.43s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:24<09:21,  2.56s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:26<09:34,  2.63s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:29<09:41,  2.68s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:32<09:46,  2.72s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:34<08:32,  2.38s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:35<07:38,  2.14s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:37<07:03,  1.99s/it]predicting train subjects:  60%|██████    | 320/532 [09:38<06:35,  1.86s/it]predicting train subjects:  60%|██████    | 321/532 [09:40<06:15,  1.78s/it]predicting train subjects:  61%|██████    | 322/532 [09:42<06:06,  1.75s/it]predicting train subjects:  61%|██████    | 323/532 [09:44<06:39,  1.91s/it]predicting train subjects:  61%|██████    | 324/532 [09:46<06:59,  2.02s/it]predicting train subjects:  61%|██████    | 325/532 [09:48<07:16,  2.11s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:51<07:21,  2.14s/it]predicting train subjects:  61%|██████▏   | 327/532 [09:53<07:33,  2.21s/it]predicting train subjects:  62%|██████▏   | 328/532 [09:55<07:34,  2.23s/it]predicting train subjects:  62%|██████▏   | 329/532 [09:57<06:59,  2.07s/it]predicting train subjects:  62%|██████▏   | 330/532 [09:59<06:35,  1.96s/it]predicting train subjects:  62%|██████▏   | 331/532 [10:00<06:20,  1.89s/it]predicting train subjects:  62%|██████▏   | 332/532 [10:02<06:09,  1.85s/it]predicting train subjects:  63%|██████▎   | 333/532 [10:04<05:58,  1.80s/it]predicting train subjects:  63%|██████▎   | 334/532 [10:06<05:51,  1.78s/it]predicting train subjects:  63%|██████▎   | 335/532 [10:08<06:05,  1.85s/it]predicting train subjects:  63%|██████▎   | 336/532 [10:10<06:08,  1.88s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:12<06:14,  1.92s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:14<06:19,  1.96s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:16<06:20,  1.97s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:18<06:21,  1.99s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:19<05:49,  1.83s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:21<05:28,  1.73s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:22<05:15,  1.67s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:24<05:02,  1.61s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:25<04:51,  1.56s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:27<04:46,  1.54s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:28<04:56,  1.60s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:30<05:01,  1.64s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:32<05:05,  1.67s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:33<05:04,  1.68s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:35<05:05,  1.69s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:37<05:01,  1.67s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:38<04:57,  1.66s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:40<04:55,  1.66s/it]predicting train subjects:  67%|██████▋   | 355/532 [10:42<04:52,  1.65s/it]predicting train subjects:  67%|██████▋   | 356/532 [10:43<04:47,  1.64s/it]predicting train subjects:  67%|██████▋   | 357/532 [10:45<04:49,  1.65s/it]predicting train subjects:  67%|██████▋   | 358/532 [10:47<04:52,  1.68s/it]predicting train subjects:  67%|██████▋   | 359/532 [10:48<04:43,  1.64s/it]predicting train subjects:  68%|██████▊   | 360/532 [10:50<04:35,  1.60s/it]predicting train subjects:  68%|██████▊   | 361/532 [10:51<04:28,  1.57s/it]predicting train subjects:  68%|██████▊   | 362/532 [10:53<04:21,  1.54s/it]predicting train subjects:  68%|██████▊   | 363/532 [10:54<04:21,  1.55s/it]predicting train subjects:  68%|██████▊   | 364/532 [10:56<04:20,  1.55s/it]predicting train subjects:  69%|██████▊   | 365/532 [10:57<04:16,  1.54s/it]predicting train subjects:  69%|██████▉   | 366/532 [10:59<04:13,  1.53s/it]predicting train subjects:  69%|██████▉   | 367/532 [11:00<04:10,  1.52s/it]predicting train subjects:  69%|██████▉   | 368/532 [11:02<04:07,  1.51s/it]predicting train subjects:  69%|██████▉   | 369/532 [11:03<04:08,  1.52s/it]predicting train subjects:  70%|██████▉   | 370/532 [11:05<04:04,  1.51s/it]predicting train subjects:  70%|██████▉   | 371/532 [11:07<04:33,  1.70s/it]predicting train subjects:  70%|██████▉   | 372/532 [11:09<04:51,  1.82s/it]predicting train subjects:  70%|███████   | 373/532 [11:11<05:02,  1.90s/it]predicting train subjects:  70%|███████   | 374/532 [11:13<05:13,  1.99s/it]predicting train subjects:  70%|███████   | 375/532 [11:16<05:16,  2.01s/it]predicting train subjects:  71%|███████   | 376/532 [11:18<05:19,  2.05s/it]predicting train subjects:  71%|███████   | 377/532 [11:19<05:03,  1.96s/it]predicting train subjects:  71%|███████   | 378/532 [11:21<04:51,  1.90s/it]predicting train subjects:  71%|███████   | 379/532 [11:23<04:46,  1.87s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:25<04:34,  1.81s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:26<04:26,  1.77s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:28<04:23,  1.76s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:30<04:22,  1.76s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:32<04:22,  1.77s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:33<04:21,  1.78s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:35<04:18,  1.77s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:37<04:17,  1.77s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:39<04:12,  1.75s/it]predicting train subjects:  73%|███████▎  | 389/532 [11:40<04:13,  1.77s/it]predicting train subjects:  73%|███████▎  | 390/532 [11:42<04:16,  1.81s/it]predicting train subjects:  73%|███████▎  | 391/532 [11:44<04:18,  1.83s/it]predicting train subjects:  74%|███████▎  | 392/532 [11:46<04:18,  1.84s/it]predicting train subjects:  74%|███████▍  | 393/532 [11:48<04:20,  1.87s/it]predicting train subjects:  74%|███████▍  | 394/532 [11:50<04:20,  1.89s/it]predicting train subjects:  74%|███████▍  | 395/532 [11:52<04:15,  1.87s/it]predicting train subjects:  74%|███████▍  | 396/532 [11:54<04:12,  1.85s/it]predicting train subjects:  75%|███████▍  | 397/532 [11:55<04:09,  1.85s/it]predicting train subjects:  75%|███████▍  | 398/532 [11:57<04:05,  1.84s/it]predicting train subjects:  75%|███████▌  | 399/532 [11:59<04:02,  1.82s/it]predicting train subjects:  75%|███████▌  | 400/532 [12:01<04:02,  1.84s/it]predicting train subjects:  75%|███████▌  | 401/532 [12:03<04:06,  1.88s/it]predicting train subjects:  76%|███████▌  | 402/532 [12:05<04:08,  1.91s/it]predicting train subjects:  76%|███████▌  | 403/532 [12:07<04:12,  1.96s/it]predicting train subjects:  76%|███████▌  | 404/532 [12:09<04:13,  1.98s/it]predicting train subjects:  76%|███████▌  | 405/532 [12:11<04:12,  1.99s/it]predicting train subjects:  76%|███████▋  | 406/532 [12:13<04:08,  1.97s/it]predicting train subjects:  77%|███████▋  | 407/532 [12:15<03:56,  1.89s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:16<03:50,  1.86s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:18<03:43,  1.82s/it]predicting train subjects:  77%|███████▋  | 410/532 [12:20<03:37,  1.78s/it]predicting train subjects:  77%|███████▋  | 411/532 [12:22<03:34,  1.77s/it]predicting train subjects:  77%|███████▋  | 412/532 [12:23<03:31,  1.76s/it]predicting train subjects:  78%|███████▊  | 413/532 [12:25<03:24,  1.72s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:27<03:21,  1.71s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:28<03:21,  1.72s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:30<03:19,  1.72s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:32<03:15,  1.70s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:33<03:10,  1.67s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:35<03:15,  1.73s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:37<03:17,  1.76s/it]predicting train subjects:  79%|███████▉  | 421/532 [12:39<03:19,  1.80s/it]predicting train subjects:  79%|███████▉  | 422/532 [12:41<03:20,  1.82s/it]predicting train subjects:  80%|███████▉  | 423/532 [12:43<03:18,  1.82s/it]predicting train subjects:  80%|███████▉  | 424/532 [12:44<03:12,  1.79s/it]predicting train subjects:  80%|███████▉  | 425/532 [12:46<03:12,  1.79s/it]predicting train subjects:  80%|████████  | 426/532 [12:48<03:10,  1.80s/it]predicting train subjects:  80%|████████  | 427/532 [12:50<03:10,  1.82s/it]predicting train subjects:  80%|████████  | 428/532 [12:52<03:08,  1.81s/it]predicting train subjects:  81%|████████  | 429/532 [12:53<03:08,  1.83s/it]predicting train subjects:  81%|████████  | 430/532 [12:55<03:09,  1.85s/it]predicting train subjects:  81%|████████  | 431/532 [12:57<03:12,  1.90s/it]predicting train subjects:  81%|████████  | 432/532 [12:59<03:13,  1.93s/it]predicting train subjects:  81%|████████▏ | 433/532 [13:01<03:12,  1.94s/it]predicting train subjects:  82%|████████▏ | 434/532 [13:03<03:14,  1.99s/it]predicting train subjects:  82%|████████▏ | 435/532 [13:06<03:15,  2.02s/it]predicting train subjects:  82%|████████▏ | 436/532 [13:08<03:15,  2.04s/it]predicting train subjects:  82%|████████▏ | 437/532 [13:09<02:58,  1.88s/it]predicting train subjects:  82%|████████▏ | 438/532 [13:11<02:47,  1.78s/it]predicting train subjects:  83%|████████▎ | 439/532 [13:12<02:44,  1.76s/it]predicting train subjects:  83%|████████▎ | 440/532 [13:14<02:36,  1.70s/it]predicting train subjects:  83%|████████▎ | 441/532 [13:16<02:29,  1.65s/it]predicting train subjects:  83%|████████▎ | 442/532 [13:17<02:25,  1.61s/it]predicting train subjects:  83%|████████▎ | 443/532 [13:18<02:18,  1.56s/it]predicting train subjects:  83%|████████▎ | 444/532 [13:20<02:14,  1.53s/it]predicting train subjects:  84%|████████▎ | 445/532 [13:21<02:09,  1.48s/it]predicting train subjects:  84%|████████▍ | 446/532 [13:23<02:07,  1.48s/it]predicting train subjects:  84%|████████▍ | 447/532 [13:24<02:04,  1.47s/it]predicting train subjects:  84%|████████▍ | 448/532 [13:26<02:02,  1.45s/it]predicting train subjects:  84%|████████▍ | 449/532 [13:27<02:05,  1.51s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:29<02:05,  1.54s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:31<02:06,  1.57s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:32<02:05,  1.57s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:34<02:03,  1.56s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:35<02:02,  1.57s/it]predicting train subjects:  86%|████████▌ | 455/532 [13:37<02:06,  1.64s/it]predicting train subjects:  86%|████████▌ | 456/532 [13:39<02:09,  1.70s/it]predicting train subjects:  86%|████████▌ | 457/532 [13:41<02:09,  1.73s/it]predicting train subjects:  86%|████████▌ | 458/532 [13:43<02:10,  1.76s/it]predicting train subjects:  86%|████████▋ | 459/532 [13:44<02:11,  1.80s/it]predicting train subjects:  86%|████████▋ | 460/532 [13:46<02:09,  1.80s/it]predicting train subjects:  87%|████████▋ | 461/532 [13:48<02:15,  1.91s/it]predicting train subjects:  87%|████████▋ | 462/532 [13:51<02:19,  1.99s/it]predicting train subjects:  87%|████████▋ | 463/532 [13:53<02:21,  2.05s/it]predicting train subjects:  87%|████████▋ | 464/532 [13:55<02:23,  2.11s/it]predicting train subjects:  87%|████████▋ | 465/532 [13:57<02:24,  2.15s/it]predicting train subjects:  88%|████████▊ | 466/532 [13:59<02:23,  2.17s/it]predicting train subjects:  88%|████████▊ | 467/532 [14:01<02:13,  2.05s/it]predicting train subjects:  88%|████████▊ | 468/532 [14:03<02:05,  1.96s/it]predicting train subjects:  88%|████████▊ | 469/532 [14:05<02:00,  1.91s/it]predicting train subjects:  88%|████████▊ | 470/532 [14:07<01:55,  1.86s/it]predicting train subjects:  89%|████████▊ | 471/532 [14:08<01:50,  1.81s/it]predicting train subjects:  89%|████████▊ | 472/532 [14:10<01:47,  1.80s/it]predicting train subjects:  89%|████████▉ | 473/532 [14:12<01:47,  1.82s/it]predicting train subjects:  89%|████████▉ | 474/532 [14:14<01:46,  1.83s/it]predicting train subjects:  89%|████████▉ | 475/532 [14:16<01:45,  1.85s/it]predicting train subjects:  89%|████████▉ | 476/532 [14:17<01:44,  1.86s/it]predicting train subjects:  90%|████████▉ | 477/532 [14:19<01:43,  1.87s/it]predicting train subjects:  90%|████████▉ | 478/532 [14:21<01:42,  1.90s/it]predicting train subjects:  90%|█████████ | 479/532 [14:23<01:35,  1.81s/it]predicting train subjects:  90%|█████████ | 480/532 [14:24<01:29,  1.73s/it]predicting train subjects:  90%|█████████ | 481/532 [14:26<01:25,  1.68s/it]predicting train subjects:  91%|█████████ | 482/532 [14:28<01:22,  1.65s/it]predicting train subjects:  91%|█████████ | 483/532 [14:29<01:20,  1.64s/it]predicting train subjects:  91%|█████████ | 484/532 [14:31<01:18,  1.63s/it]predicting train subjects:  91%|█████████ | 485/532 [14:33<01:25,  1.82s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:35<01:27,  1.91s/it]predicting train subjects:  92%|█████████▏| 487/532 [14:37<01:28,  1.96s/it]predicting train subjects:  92%|█████████▏| 488/532 [14:39<01:28,  2.01s/it]predicting train subjects:  92%|█████████▏| 489/532 [14:41<01:26,  2.02s/it]predicting train subjects:  92%|█████████▏| 490/532 [14:44<01:25,  2.04s/it]predicting train subjects:  92%|█████████▏| 491/532 [14:45<01:19,  1.95s/it]predicting train subjects:  92%|█████████▏| 492/532 [14:47<01:14,  1.87s/it]predicting train subjects:  93%|█████████▎| 493/532 [14:49<01:10,  1.81s/it]predicting train subjects:  93%|█████████▎| 494/532 [14:50<01:07,  1.77s/it]predicting train subjects:  93%|█████████▎| 495/532 [14:52<01:04,  1.76s/it]predicting train subjects:  93%|█████████▎| 496/532 [14:54<01:03,  1.75s/it]predicting train subjects:  93%|█████████▎| 497/532 [14:56<01:02,  1.78s/it]predicting train subjects:  94%|█████████▎| 498/532 [14:58<01:03,  1.86s/it]predicting train subjects:  94%|█████████▍| 499/532 [14:59<01:00,  1.84s/it]predicting train subjects:  94%|█████████▍| 500/532 [15:01<00:58,  1.83s/it]predicting train subjects:  94%|█████████▍| 501/532 [15:03<00:56,  1.82s/it]predicting train subjects:  94%|█████████▍| 502/532 [15:05<00:54,  1.82s/it]predicting train subjects:  95%|█████████▍| 503/532 [15:07<00:52,  1.81s/it]predicting train subjects:  95%|█████████▍| 504/532 [15:08<00:49,  1.75s/it]predicting train subjects:  95%|█████████▍| 505/532 [15:10<00:46,  1.71s/it]predicting train subjects:  95%|█████████▌| 506/532 [15:12<00:43,  1.69s/it]predicting train subjects:  95%|█████████▌| 507/532 [15:13<00:41,  1.68s/it]predicting train subjects:  95%|█████████▌| 508/532 [15:15<00:40,  1.68s/it]predicting train subjects:  96%|█████████▌| 509/532 [15:17<00:41,  1.81s/it]predicting train subjects:  96%|█████████▌| 510/532 [15:19<00:42,  1.91s/it]predicting train subjects:  96%|█████████▌| 511/532 [15:21<00:41,  1.96s/it]predicting train subjects:  96%|█████████▌| 512/532 [15:23<00:40,  2.02s/it]predicting train subjects:  96%|█████████▋| 513/532 [15:26<00:39,  2.07s/it]predicting train subjects:  97%|█████████▋| 514/532 [15:28<00:37,  2.07s/it]predicting train subjects:  97%|█████████▋| 515/532 [15:29<00:33,  1.98s/it]predicting train subjects:  97%|█████████▋| 516/532 [15:31<00:30,  1.89s/it]predicting train subjects:  97%|█████████▋| 517/532 [15:33<00:27,  1.85s/it]predicting train subjects:  97%|█████████▋| 518/532 [15:35<00:25,  1.82s/it]predicting train subjects:  98%|█████████▊| 519/532 [15:36<00:23,  1.80s/it]predicting train subjects:  98%|█████████▊| 520/532 [15:38<00:21,  1.77s/it]predicting train subjects:  98%|█████████▊| 521/532 [15:40<00:19,  1.80s/it]predicting train subjects:  98%|█████████▊| 522/532 [15:42<00:17,  1.80s/it]predicting train subjects:  98%|█████████▊| 523/532 [15:44<00:16,  1.80s/it]predicting train subjects:  98%|█████████▊| 524/532 [15:45<00:14,  1.80s/it]predicting train subjects:  99%|█████████▊| 525/532 [15:47<00:12,  1.81s/it]predicting train subjects:  99%|█████████▉| 526/532 [15:49<00:10,  1.82s/it]predicting train subjects:  99%|█████████▉| 527/532 [15:51<00:08,  1.79s/it]predicting train subjects:  99%|█████████▉| 528/532 [15:52<00:07,  1.76s/it]predicting train subjects:  99%|█████████▉| 529/532 [15:54<00:05,  1.74s/it]predicting train subjects: 100%|█████████▉| 530/532 [15:56<00:03,  1.73s/it]predicting train subjects: 100%|█████████▉| 531/532 [15:58<00:01,  1.72s/it]predicting train subjects: 100%|██████████| 532/532 [15:59<00:00,  1.71s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<14:41,  1.66s/it]Loading train:   0%|          | 2/532 [00:02<13:09,  1.49s/it]Loading train:   1%|          | 3/532 [00:03<11:46,  1.34s/it]Loading train:   1%|          | 4/532 [00:05<12:00,  1.37s/it]Loading train:   1%|          | 5/532 [00:06<11:45,  1.34s/it]Loading train:   1%|          | 6/532 [00:07<11:26,  1.30s/it]Loading train:   1%|▏         | 7/532 [00:08<11:00,  1.26s/it]Loading train:   2%|▏         | 8/532 [00:09<10:22,  1.19s/it]Loading train:   2%|▏         | 9/532 [00:11<10:43,  1.23s/it]Loading train:   2%|▏         | 10/532 [00:12<10:10,  1.17s/it]Loading train:   2%|▏         | 11/532 [00:13<09:59,  1.15s/it]Loading train:   2%|▏         | 12/532 [00:14<10:17,  1.19s/it]Loading train:   2%|▏         | 13/532 [00:15<09:51,  1.14s/it]Loading train:   3%|▎         | 14/532 [00:16<09:27,  1.10s/it]Loading train:   3%|▎         | 15/532 [00:18<11:19,  1.31s/it]Loading train:   3%|▎         | 16/532 [00:19<11:49,  1.37s/it]Loading train:   3%|▎         | 17/532 [00:21<11:43,  1.37s/it]Loading train:   3%|▎         | 18/532 [00:22<11:38,  1.36s/it]Loading train:   4%|▎         | 19/532 [00:23<11:01,  1.29s/it]Loading train:   4%|▍         | 20/532 [00:24<10:25,  1.22s/it]Loading train:   4%|▍         | 21/532 [00:26<10:19,  1.21s/it]Loading train:   4%|▍         | 22/532 [00:27<09:52,  1.16s/it]Loading train:   4%|▍         | 23/532 [00:28<09:31,  1.12s/it]Loading train:   5%|▍         | 24/532 [00:28<08:50,  1.04s/it]Loading train:   5%|▍         | 25/532 [00:30<09:21,  1.11s/it]Loading train:   5%|▍         | 26/532 [00:31<09:10,  1.09s/it]Loading train:   5%|▌         | 27/532 [00:32<09:44,  1.16s/it]Loading train:   5%|▌         | 28/532 [00:33<09:21,  1.11s/it]Loading train:   5%|▌         | 29/532 [00:34<09:21,  1.12s/it]Loading train:   6%|▌         | 30/532 [00:35<08:55,  1.07s/it]Loading train:   6%|▌         | 31/532 [00:36<08:42,  1.04s/it]Loading train:   6%|▌         | 32/532 [00:37<08:27,  1.01s/it]Loading train:   6%|▌         | 33/532 [00:38<08:19,  1.00s/it]Loading train:   6%|▋         | 34/532 [00:39<08:59,  1.08s/it]Loading train:   7%|▋         | 35/532 [00:40<08:41,  1.05s/it]Loading train:   7%|▋         | 36/532 [00:41<08:56,  1.08s/it]Loading train:   7%|▋         | 37/532 [00:43<09:01,  1.09s/it]Loading train:   7%|▋         | 38/532 [00:44<09:00,  1.10s/it]Loading train:   7%|▋         | 39/532 [00:45<08:56,  1.09s/it]Loading train:   8%|▊         | 40/532 [00:46<08:29,  1.04s/it]Loading train:   8%|▊         | 41/532 [00:47<08:44,  1.07s/it]Loading train:   8%|▊         | 42/532 [00:48<08:45,  1.07s/it]Loading train:   8%|▊         | 43/532 [00:49<08:08,  1.00it/s]Loading train:   8%|▊         | 44/532 [00:50<07:51,  1.04it/s]Loading train:   8%|▊         | 45/532 [00:51<07:47,  1.04it/s]Loading train:   9%|▊         | 46/532 [00:52<07:59,  1.01it/s]Loading train:   9%|▉         | 47/532 [00:53<08:24,  1.04s/it]Loading train:   9%|▉         | 48/532 [00:54<08:35,  1.07s/it]Loading train:   9%|▉         | 49/532 [00:55<08:30,  1.06s/it]Loading train:   9%|▉         | 50/532 [00:56<08:54,  1.11s/it]Loading train:  10%|▉         | 51/532 [00:57<08:41,  1.08s/it]Loading train:  10%|▉         | 52/532 [00:58<08:41,  1.09s/it]Loading train:  10%|▉         | 53/532 [00:59<08:20,  1.04s/it]Loading train:  10%|█         | 54/532 [01:00<08:40,  1.09s/it]Loading train:  10%|█         | 55/532 [01:02<08:46,  1.10s/it]Loading train:  11%|█         | 56/532 [01:03<08:43,  1.10s/it]Loading train:  11%|█         | 57/532 [01:04<08:23,  1.06s/it]Loading train:  11%|█         | 58/532 [01:05<08:34,  1.09s/it]Loading train:  11%|█         | 59/532 [01:06<08:52,  1.13s/it]Loading train:  11%|█▏        | 60/532 [01:07<08:24,  1.07s/it]Loading train:  11%|█▏        | 61/532 [01:08<08:02,  1.02s/it]Loading train:  12%|█▏        | 62/532 [01:09<08:27,  1.08s/it]Loading train:  12%|█▏        | 63/532 [01:10<08:51,  1.13s/it]Loading train:  12%|█▏        | 64/532 [01:11<08:31,  1.09s/it]Loading train:  12%|█▏        | 65/532 [01:12<08:20,  1.07s/it]Loading train:  12%|█▏        | 66/532 [01:14<08:53,  1.14s/it]Loading train:  13%|█▎        | 67/532 [01:15<09:00,  1.16s/it]Loading train:  13%|█▎        | 68/532 [01:16<08:34,  1.11s/it]Loading train:  13%|█▎        | 69/532 [01:17<08:17,  1.07s/it]Loading train:  13%|█▎        | 70/532 [01:18<08:02,  1.04s/it]Loading train:  13%|█▎        | 71/532 [01:19<07:46,  1.01s/it]Loading train:  14%|█▎        | 72/532 [01:20<07:45,  1.01s/it]Loading train:  14%|█▎        | 73/532 [01:21<07:58,  1.04s/it]Loading train:  14%|█▍        | 74/532 [01:22<08:18,  1.09s/it]Loading train:  14%|█▍        | 75/532 [01:24<09:10,  1.21s/it]Loading train:  14%|█▍        | 76/532 [01:25<08:42,  1.15s/it]Loading train:  14%|█▍        | 77/532 [01:26<08:24,  1.11s/it]Loading train:  15%|█▍        | 78/532 [01:27<08:10,  1.08s/it]Loading train:  15%|█▍        | 79/532 [01:28<08:05,  1.07s/it]Loading train:  15%|█▌        | 80/532 [01:29<08:03,  1.07s/it]Loading train:  15%|█▌        | 81/532 [01:30<08:00,  1.07s/it]Loading train:  15%|█▌        | 82/532 [01:31<07:56,  1.06s/it]Loading train:  16%|█▌        | 83/532 [01:32<07:37,  1.02s/it]Loading train:  16%|█▌        | 84/532 [01:33<07:22,  1.01it/s]Loading train:  16%|█▌        | 85/532 [01:34<07:08,  1.04it/s]Loading train:  16%|█▌        | 86/532 [01:34<06:57,  1.07it/s]Loading train:  16%|█▋        | 87/532 [01:35<06:47,  1.09it/s]Loading train:  17%|█▋        | 88/532 [01:36<06:50,  1.08it/s]Loading train:  17%|█▋        | 89/532 [01:37<07:14,  1.02it/s]Loading train:  17%|█▋        | 90/532 [01:38<07:09,  1.03it/s]Loading train:  17%|█▋        | 91/532 [01:39<07:09,  1.03it/s]Loading train:  17%|█▋        | 92/532 [01:40<07:21,  1.00s/it]Loading train:  17%|█▋        | 93/532 [01:41<07:20,  1.00s/it]Loading train:  18%|█▊        | 94/532 [01:42<07:09,  1.02it/s]Loading train:  18%|█▊        | 95/532 [01:44<07:53,  1.08s/it]Loading train:  18%|█▊        | 96/532 [01:45<08:01,  1.11s/it]Loading train:  18%|█▊        | 97/532 [01:46<08:08,  1.12s/it]Loading train:  18%|█▊        | 98/532 [01:47<08:13,  1.14s/it]Loading train:  19%|█▊        | 99/532 [01:48<08:09,  1.13s/it]Loading train:  19%|█▉        | 100/532 [01:49<08:08,  1.13s/it]Loading train:  19%|█▉        | 101/532 [01:50<07:59,  1.11s/it]Loading train:  19%|█▉        | 102/532 [01:51<07:41,  1.07s/it]Loading train:  19%|█▉        | 103/532 [01:52<07:25,  1.04s/it]Loading train:  20%|█▉        | 104/532 [01:53<07:19,  1.03s/it]Loading train:  20%|█▉        | 105/532 [01:54<07:00,  1.02it/s]Loading train:  20%|█▉        | 106/532 [01:55<06:51,  1.04it/s]Loading train:  20%|██        | 107/532 [01:56<06:53,  1.03it/s]Loading train:  20%|██        | 108/532 [01:57<06:47,  1.04it/s]Loading train:  20%|██        | 109/532 [01:58<06:38,  1.06it/s]Loading train:  21%|██        | 110/532 [01:59<06:45,  1.04it/s]Loading train:  21%|██        | 111/532 [02:00<06:49,  1.03it/s]Loading train:  21%|██        | 112/532 [02:01<06:37,  1.06it/s]Loading train:  21%|██        | 113/532 [02:02<06:56,  1.01it/s]Loading train:  21%|██▏       | 114/532 [02:03<06:57,  1.00it/s]Loading train:  22%|██▏       | 115/532 [02:04<06:55,  1.00it/s]Loading train:  22%|██▏       | 116/532 [02:05<06:57,  1.00s/it]Loading train:  22%|██▏       | 117/532 [02:06<07:02,  1.02s/it]Loading train:  22%|██▏       | 118/532 [02:07<07:09,  1.04s/it]Loading train:  22%|██▏       | 119/532 [02:08<07:13,  1.05s/it]Loading train:  23%|██▎       | 120/532 [02:09<07:14,  1.06s/it]Loading train:  23%|██▎       | 121/532 [02:10<07:04,  1.03s/it]Loading train:  23%|██▎       | 122/532 [02:11<07:03,  1.03s/it]Loading train:  23%|██▎       | 123/532 [02:12<07:06,  1.04s/it]Loading train:  23%|██▎       | 124/532 [02:13<07:04,  1.04s/it]Loading train:  23%|██▎       | 125/532 [02:15<07:35,  1.12s/it]Loading train:  24%|██▎       | 126/532 [02:16<07:29,  1.11s/it]Loading train:  24%|██▍       | 127/532 [02:17<07:25,  1.10s/it]Loading train:  24%|██▍       | 128/532 [02:18<07:12,  1.07s/it]Loading train:  24%|██▍       | 129/532 [02:19<07:08,  1.06s/it]Loading train:  24%|██▍       | 130/532 [02:20<07:08,  1.07s/it]Loading train:  25%|██▍       | 131/532 [02:21<07:33,  1.13s/it]Loading train:  25%|██▍       | 132/532 [02:23<07:49,  1.17s/it]Loading train:  25%|██▌       | 133/532 [02:24<07:53,  1.19s/it]Loading train:  25%|██▌       | 134/532 [02:25<07:53,  1.19s/it]Loading train:  25%|██▌       | 135/532 [02:26<07:44,  1.17s/it]Loading train:  26%|██▌       | 136/532 [02:27<07:49,  1.19s/it]Loading train:  26%|██▌       | 137/532 [02:28<07:51,  1.19s/it]Loading train:  26%|██▌       | 138/532 [02:30<07:50,  1.20s/it]Loading train:  26%|██▌       | 139/532 [02:31<07:48,  1.19s/it]Loading train:  26%|██▋       | 140/532 [02:32<07:50,  1.20s/it]Loading train:  27%|██▋       | 141/532 [02:33<07:45,  1.19s/it]Loading train:  27%|██▋       | 142/532 [02:34<07:44,  1.19s/it]Loading train:  27%|██▋       | 143/532 [02:36<07:28,  1.15s/it]Loading train:  27%|██▋       | 144/532 [02:36<06:59,  1.08s/it]Loading train:  27%|██▋       | 145/532 [02:37<06:41,  1.04s/it]Loading train:  27%|██▋       | 146/532 [02:38<06:20,  1.01it/s]Loading train:  28%|██▊       | 147/532 [02:39<06:11,  1.04it/s]Loading train:  28%|██▊       | 148/532 [02:40<06:23,  1.00it/s]Loading train:  28%|██▊       | 149/532 [02:41<06:25,  1.01s/it]Loading train:  28%|██▊       | 150/532 [02:42<06:31,  1.02s/it]Loading train:  28%|██▊       | 151/532 [02:43<06:21,  1.00s/it]Loading train:  29%|██▊       | 152/532 [02:44<06:21,  1.00s/it]Loading train:  29%|██▉       | 153/532 [02:45<06:07,  1.03it/s]Loading train:  29%|██▉       | 154/532 [02:46<06:02,  1.04it/s]Loading train:  29%|██▉       | 155/532 [02:47<06:41,  1.07s/it]Loading train:  29%|██▉       | 156/532 [02:49<06:56,  1.11s/it]Loading train:  30%|██▉       | 157/532 [02:50<07:11,  1.15s/it]Loading train:  30%|██▉       | 158/532 [02:51<07:14,  1.16s/it]Loading train:  30%|██▉       | 159/532 [02:52<07:31,  1.21s/it]Loading train:  30%|███       | 160/532 [02:54<07:28,  1.21s/it]Loading train:  30%|███       | 161/532 [02:55<07:13,  1.17s/it]Loading train:  30%|███       | 162/532 [02:56<07:10,  1.16s/it]Loading train:  31%|███       | 163/532 [02:57<06:56,  1.13s/it]Loading train:  31%|███       | 164/532 [02:58<06:48,  1.11s/it]Loading train:  31%|███       | 165/532 [02:59<06:33,  1.07s/it]Loading train:  31%|███       | 166/532 [03:00<06:23,  1.05s/it]Loading train:  31%|███▏      | 167/532 [03:01<06:28,  1.06s/it]Loading train:  32%|███▏      | 168/532 [03:02<06:22,  1.05s/it]Loading train:  32%|███▏      | 169/532 [03:03<06:21,  1.05s/it]Loading train:  32%|███▏      | 170/532 [03:04<06:15,  1.04s/it]Loading train:  32%|███▏      | 171/532 [03:05<06:08,  1.02s/it]Loading train:  32%|███▏      | 172/532 [03:06<06:09,  1.03s/it]Loading train:  33%|███▎      | 173/532 [03:07<06:09,  1.03s/it]Loading train:  33%|███▎      | 174/532 [03:08<05:57,  1.00it/s]Loading train:  33%|███▎      | 175/532 [03:09<05:50,  1.02it/s]Loading train:  33%|███▎      | 176/532 [03:10<05:45,  1.03it/s]Loading train:  33%|███▎      | 177/532 [03:11<05:40,  1.04it/s]Loading train:  33%|███▎      | 178/532 [03:12<05:37,  1.05it/s]Loading train:  34%|███▎      | 179/532 [03:13<05:29,  1.07it/s]Loading train:  34%|███▍      | 180/532 [03:14<05:25,  1.08it/s]Loading train:  34%|███▍      | 181/532 [03:15<05:21,  1.09it/s]Loading train:  34%|███▍      | 182/532 [03:15<05:14,  1.11it/s]Loading train:  34%|███▍      | 183/532 [03:16<05:10,  1.12it/s]Loading train:  35%|███▍      | 184/532 [03:17<05:04,  1.14it/s]Loading train:  35%|███▍      | 185/532 [03:18<05:21,  1.08it/s]Loading train:  35%|███▍      | 186/532 [03:19<05:26,  1.06it/s]Loading train:  35%|███▌      | 187/532 [03:20<05:24,  1.06it/s]Loading train:  35%|███▌      | 188/532 [03:21<05:27,  1.05it/s]Loading train:  36%|███▌      | 189/532 [03:22<05:27,  1.05it/s]Loading train:  36%|███▌      | 190/532 [03:23<05:26,  1.05it/s]Loading train:  36%|███▌      | 191/532 [03:24<05:56,  1.04s/it]Loading train:  36%|███▌      | 192/532 [03:25<06:17,  1.11s/it]Loading train:  36%|███▋      | 193/532 [03:27<06:37,  1.17s/it]Loading train:  36%|███▋      | 194/532 [03:28<06:53,  1.22s/it]Loading train:  37%|███▋      | 195/532 [03:29<06:57,  1.24s/it]Loading train:  37%|███▋      | 196/532 [03:31<07:05,  1.27s/it]Loading train:  37%|███▋      | 197/532 [03:32<07:12,  1.29s/it]Loading train:  37%|███▋      | 198/532 [03:33<06:57,  1.25s/it]Loading train:  37%|███▋      | 199/532 [03:34<06:39,  1.20s/it]Loading train:  38%|███▊      | 200/532 [03:35<06:29,  1.17s/it]Loading train:  38%|███▊      | 201/532 [03:36<06:17,  1.14s/it]Loading train:  38%|███▊      | 202/532 [03:38<06:12,  1.13s/it]Loading train:  38%|███▊      | 203/532 [03:39<06:00,  1.10s/it]Loading train:  38%|███▊      | 204/532 [03:40<05:40,  1.04s/it]Loading train:  39%|███▊      | 205/532 [03:40<05:22,  1.01it/s]Loading train:  39%|███▊      | 206/532 [03:41<05:20,  1.02it/s]Loading train:  39%|███▉      | 207/532 [03:42<05:12,  1.04it/s]Loading train:  39%|███▉      | 208/532 [03:43<05:18,  1.02it/s]Loading train:  39%|███▉      | 209/532 [03:44<05:14,  1.03it/s]Loading train:  39%|███▉      | 210/532 [03:45<04:59,  1.07it/s]Loading train:  40%|███▉      | 211/532 [03:46<04:49,  1.11it/s]Loading train:  40%|███▉      | 212/532 [03:47<04:39,  1.15it/s]Loading train:  40%|████      | 213/532 [03:48<04:34,  1.16it/s]Loading train:  40%|████      | 214/532 [03:48<04:41,  1.13it/s]Loading train:  40%|████      | 215/532 [03:50<05:17,  1.00s/it]Loading train:  41%|████      | 216/532 [03:51<05:25,  1.03s/it]Loading train:  41%|████      | 217/532 [03:52<05:32,  1.05s/it]Loading train:  41%|████      | 218/532 [03:53<05:33,  1.06s/it]Loading train:  41%|████      | 219/532 [03:54<05:39,  1.08s/it]Loading train:  41%|████▏     | 220/532 [03:55<05:42,  1.10s/it]Loading train:  42%|████▏     | 221/532 [03:56<05:17,  1.02s/it]Loading train:  42%|████▏     | 222/532 [03:57<05:03,  1.02it/s]Loading train:  42%|████▏     | 223/532 [03:58<04:54,  1.05it/s]Loading train:  42%|████▏     | 224/532 [03:59<04:48,  1.07it/s]Loading train:  42%|████▏     | 225/532 [04:00<04:40,  1.09it/s]Loading train:  42%|████▏     | 226/532 [04:01<04:31,  1.13it/s]Loading train:  43%|████▎     | 227/532 [04:01<04:36,  1.10it/s]Loading train:  43%|████▎     | 228/532 [04:02<04:32,  1.11it/s]Loading train:  43%|████▎     | 229/532 [04:03<04:27,  1.13it/s]Loading train:  43%|████▎     | 230/532 [04:04<04:37,  1.09it/s]Loading train:  43%|████▎     | 231/532 [04:05<04:27,  1.12it/s]Loading train:  44%|████▎     | 232/532 [04:06<04:15,  1.18it/s]Loading train:  44%|████▍     | 233/532 [04:07<04:27,  1.12it/s]Loading train:  44%|████▍     | 234/532 [04:08<04:26,  1.12it/s]Loading train:  44%|████▍     | 235/532 [04:09<04:31,  1.09it/s]Loading train:  44%|████▍     | 236/532 [04:10<04:37,  1.07it/s]Loading train:  45%|████▍     | 237/532 [04:11<04:35,  1.07it/s]Loading train:  45%|████▍     | 238/532 [04:11<04:31,  1.08it/s]Loading train:  45%|████▍     | 239/532 [04:12<04:31,  1.08it/s]Loading train:  45%|████▌     | 240/532 [04:13<04:30,  1.08it/s]Loading train:  45%|████▌     | 241/532 [04:14<04:29,  1.08it/s]Loading train:  45%|████▌     | 242/532 [04:15<04:29,  1.08it/s]Loading train:  46%|████▌     | 243/532 [04:16<04:36,  1.04it/s]Loading train:  46%|████▌     | 244/532 [04:17<04:28,  1.07it/s]Loading train:  46%|████▌     | 245/532 [04:18<04:19,  1.10it/s]Loading train:  46%|████▌     | 246/532 [04:19<04:12,  1.13it/s]Loading train:  46%|████▋     | 247/532 [04:19<04:01,  1.18it/s]Loading train:  47%|████▋     | 248/532 [04:20<04:07,  1.15it/s]Loading train:  47%|████▋     | 249/532 [04:21<04:08,  1.14it/s]Loading train:  47%|████▋     | 250/532 [04:22<04:10,  1.13it/s]Loading train:  47%|████▋     | 251/532 [04:23<04:12,  1.11it/s]Loading train:  47%|████▋     | 252/532 [04:24<04:09,  1.12it/s]Loading train:  48%|████▊     | 253/532 [04:25<04:18,  1.08it/s]Loading train:  48%|████▊     | 254/532 [04:26<04:16,  1.08it/s]Loading train:  48%|████▊     | 255/532 [04:27<04:22,  1.05it/s]Loading train:  48%|████▊     | 256/532 [04:28<04:18,  1.07it/s]Loading train:  48%|████▊     | 257/532 [04:29<04:38,  1.01s/it]Loading train:  48%|████▊     | 258/532 [04:30<04:47,  1.05s/it]Loading train:  49%|████▊     | 259/532 [04:31<04:50,  1.06s/it]Loading train:  49%|████▉     | 260/532 [04:32<04:49,  1.06s/it]Loading train:  49%|████▉     | 261/532 [04:33<04:49,  1.07s/it]Loading train:  49%|████▉     | 262/532 [04:35<04:49,  1.07s/it]Loading train:  49%|████▉     | 263/532 [04:35<04:35,  1.02s/it]Loading train:  50%|████▉     | 264/532 [04:36<04:24,  1.01it/s]Loading train:  50%|████▉     | 265/532 [04:37<04:07,  1.08it/s]Loading train:  50%|█████     | 266/532 [04:38<03:58,  1.12it/s]Loading train:  50%|█████     | 267/532 [04:39<03:54,  1.13it/s]Loading train:  50%|█████     | 268/532 [04:40<03:50,  1.15it/s]Loading train:  51%|█████     | 269/532 [04:41<04:00,  1.09it/s]Loading train:  51%|█████     | 270/532 [04:42<04:00,  1.09it/s]Loading train:  51%|█████     | 271/532 [04:42<03:58,  1.09it/s]Loading train:  51%|█████     | 272/532 [04:43<03:58,  1.09it/s]Loading train:  51%|█████▏    | 273/532 [04:44<04:00,  1.08it/s]Loading train:  52%|█████▏    | 274/532 [04:45<03:57,  1.08it/s]Loading train:  52%|█████▏    | 275/532 [04:47<04:27,  1.04s/it]Loading train:  52%|█████▏    | 276/532 [04:48<04:47,  1.12s/it]Loading train:  52%|█████▏    | 277/532 [04:49<04:52,  1.15s/it]Loading train:  52%|█████▏    | 278/532 [04:50<04:53,  1.15s/it]Loading train:  52%|█████▏    | 279/532 [04:51<04:54,  1.16s/it]Loading train:  53%|█████▎    | 280/532 [04:53<04:54,  1.17s/it]Loading train:  53%|█████▎    | 281/532 [04:54<04:46,  1.14s/it]Loading train:  53%|█████▎    | 282/532 [04:55<04:41,  1.12s/it]Loading train:  53%|█████▎    | 283/532 [04:56<04:46,  1.15s/it]Loading train:  53%|█████▎    | 284/532 [04:57<04:39,  1.13s/it]Loading train:  54%|█████▎    | 285/532 [04:58<04:36,  1.12s/it]Loading train:  54%|█████▍    | 286/532 [04:59<04:33,  1.11s/it]Loading train:  54%|█████▍    | 287/532 [05:00<04:21,  1.07s/it]Loading train:  54%|█████▍    | 288/532 [05:01<04:07,  1.01s/it]Loading train:  54%|█████▍    | 289/532 [05:02<04:03,  1.00s/it]Loading train:  55%|█████▍    | 290/532 [05:03<03:57,  1.02it/s]Loading train:  55%|█████▍    | 291/532 [05:04<03:55,  1.02it/s]Loading train:  55%|█████▍    | 292/532 [05:05<03:57,  1.01it/s]Loading train:  55%|█████▌    | 293/532 [05:06<04:09,  1.04s/it]Loading train:  55%|█████▌    | 294/532 [05:07<04:04,  1.03s/it]Loading train:  55%|█████▌    | 295/532 [05:08<03:59,  1.01s/it]Loading train:  56%|█████▌    | 296/532 [05:09<03:55,  1.00it/s]Loading train:  56%|█████▌    | 297/532 [05:10<03:51,  1.02it/s]Loading train:  56%|█████▌    | 298/532 [05:11<03:57,  1.01s/it]Loading train:  56%|█████▌    | 299/532 [05:12<03:45,  1.03it/s]Loading train:  56%|█████▋    | 300/532 [05:13<03:33,  1.08it/s]Loading train:  57%|█████▋    | 301/532 [05:14<03:31,  1.09it/s]Loading train:  57%|█████▋    | 302/532 [05:15<03:21,  1.14it/s]Loading train:  57%|█████▋    | 303/532 [05:15<03:14,  1.18it/s]Loading train:  57%|█████▋    | 304/532 [05:16<03:10,  1.20it/s]Loading train:  57%|█████▋    | 305/532 [05:17<03:31,  1.07it/s]Loading train:  58%|█████▊    | 306/532 [05:18<03:46,  1.00s/it]Loading train:  58%|█████▊    | 307/532 [05:20<03:55,  1.05s/it]Loading train:  58%|█████▊    | 308/532 [05:21<04:03,  1.08s/it]Loading train:  58%|█████▊    | 309/532 [05:22<04:12,  1.13s/it]Loading train:  58%|█████▊    | 310/532 [05:23<04:13,  1.14s/it]Loading train:  58%|█████▊    | 311/532 [05:25<04:40,  1.27s/it]Loading train:  59%|█████▊    | 312/532 [05:26<04:50,  1.32s/it]Loading train:  59%|█████▉    | 313/532 [05:28<05:03,  1.38s/it]Loading train:  59%|█████▉    | 314/532 [05:29<05:11,  1.43s/it]Loading train:  59%|█████▉    | 315/532 [05:31<05:15,  1.46s/it]Loading train:  59%|█████▉    | 316/532 [05:32<05:21,  1.49s/it]Loading train:  60%|█████▉    | 317/532 [05:33<04:45,  1.33s/it]Loading train:  60%|█████▉    | 318/532 [05:34<04:20,  1.22s/it]Loading train:  60%|█████▉    | 319/532 [05:35<04:09,  1.17s/it]Loading train:  60%|██████    | 320/532 [05:36<03:58,  1.12s/it]Loading train:  60%|██████    | 321/532 [05:37<03:55,  1.11s/it]Loading train:  61%|██████    | 322/532 [05:38<03:45,  1.08s/it]Loading train:  61%|██████    | 323/532 [05:40<04:03,  1.16s/it]Loading train:  61%|██████    | 324/532 [05:41<04:11,  1.21s/it]Loading train:  61%|██████    | 325/532 [05:42<04:22,  1.27s/it]Loading train:  61%|██████▏   | 326/532 [05:44<04:21,  1.27s/it]Loading train:  61%|██████▏   | 327/532 [05:45<04:20,  1.27s/it]Loading train:  62%|██████▏   | 328/532 [05:46<04:29,  1.32s/it]Loading train:  62%|██████▏   | 329/532 [05:47<04:08,  1.22s/it]Loading train:  62%|██████▏   | 330/532 [05:49<03:58,  1.18s/it]Loading train:  62%|██████▏   | 331/532 [05:50<03:46,  1.13s/it]Loading train:  62%|██████▏   | 332/532 [05:51<03:46,  1.13s/it]Loading train:  63%|██████▎   | 333/532 [05:52<03:43,  1.12s/it]Loading train:  63%|██████▎   | 334/532 [05:53<03:38,  1.10s/it]Loading train:  63%|██████▎   | 335/532 [05:54<03:57,  1.20s/it]Loading train:  63%|██████▎   | 336/532 [05:56<04:02,  1.24s/it]Loading train:  63%|██████▎   | 337/532 [05:57<03:57,  1.22s/it]Loading train:  64%|██████▎   | 338/532 [05:58<03:58,  1.23s/it]Loading train:  64%|██████▎   | 339/532 [05:59<03:52,  1.21s/it]Loading train:  64%|██████▍   | 340/532 [06:00<03:52,  1.21s/it]Loading train:  64%|██████▍   | 341/532 [06:01<03:34,  1.12s/it]Loading train:  64%|██████▍   | 342/532 [06:02<03:20,  1.06s/it]Loading train:  64%|██████▍   | 343/532 [06:03<03:10,  1.01s/it]Loading train:  65%|██████▍   | 344/532 [06:04<03:08,  1.00s/it]Loading train:  65%|██████▍   | 345/532 [06:05<03:01,  1.03it/s]Loading train:  65%|██████▌   | 346/532 [06:06<02:58,  1.04it/s]Loading train:  65%|██████▌   | 347/532 [06:07<03:01,  1.02it/s]Loading train:  65%|██████▌   | 348/532 [06:08<02:57,  1.04it/s]Loading train:  66%|██████▌   | 349/532 [06:09<03:00,  1.02it/s]Loading train:  66%|██████▌   | 350/532 [06:10<03:01,  1.00it/s]Loading train:  66%|██████▌   | 351/532 [06:11<03:02,  1.01s/it]Loading train:  66%|██████▌   | 352/532 [06:12<03:04,  1.03s/it]Loading train:  66%|██████▋   | 353/532 [06:13<03:03,  1.02s/it]Loading train:  67%|██████▋   | 354/532 [06:14<02:57,  1.00it/s]Loading train:  67%|██████▋   | 355/532 [06:15<02:55,  1.01it/s]Loading train:  67%|██████▋   | 356/532 [06:16<02:56,  1.00s/it]Loading train:  67%|██████▋   | 357/532 [06:17<02:56,  1.01s/it]Loading train:  67%|██████▋   | 358/532 [06:18<02:57,  1.02s/it]Loading train:  67%|██████▋   | 359/532 [06:19<02:52,  1.01it/s]Loading train:  68%|██████▊   | 360/532 [06:20<02:52,  1.00s/it]Loading train:  68%|██████▊   | 361/532 [06:21<02:47,  1.02it/s]Loading train:  68%|██████▊   | 362/532 [06:22<02:43,  1.04it/s]Loading train:  68%|██████▊   | 363/532 [06:23<02:38,  1.07it/s]Loading train:  68%|██████▊   | 364/532 [06:24<02:32,  1.10it/s]Loading train:  69%|██████▊   | 365/532 [06:25<02:34,  1.08it/s]Loading train:  69%|██████▉   | 366/532 [06:26<02:34,  1.07it/s]Loading train:  69%|██████▉   | 367/532 [06:27<02:37,  1.04it/s]Loading train:  69%|██████▉   | 368/532 [06:28<02:37,  1.04it/s]Loading train:  69%|██████▉   | 369/532 [06:28<02:33,  1.06it/s]Loading train:  70%|██████▉   | 370/532 [06:29<02:33,  1.06it/s]Loading train:  70%|██████▉   | 371/532 [06:31<02:45,  1.03s/it]Loading train:  70%|██████▉   | 372/532 [06:32<03:00,  1.13s/it]Loading train:  70%|███████   | 373/532 [06:33<03:01,  1.14s/it]Loading train:  70%|███████   | 374/532 [06:34<03:02,  1.15s/it]Loading train:  70%|███████   | 375/532 [06:36<03:06,  1.19s/it]Loading train:  71%|███████   | 376/532 [06:37<03:08,  1.21s/it]Loading train:  71%|███████   | 377/532 [06:38<02:59,  1.16s/it]Loading train:  71%|███████   | 378/532 [06:39<03:08,  1.23s/it]Loading train:  71%|███████   | 379/532 [06:41<03:10,  1.25s/it]Loading train:  71%|███████▏  | 380/532 [06:42<03:06,  1.23s/it]Loading train:  72%|███████▏  | 381/532 [06:43<03:02,  1.21s/it]Loading train:  72%|███████▏  | 382/532 [06:44<02:54,  1.17s/it]Loading train:  72%|███████▏  | 383/532 [06:45<02:51,  1.15s/it]Loading train:  72%|███████▏  | 384/532 [06:46<02:39,  1.08s/it]Loading train:  72%|███████▏  | 385/532 [06:47<02:31,  1.03s/it]Loading train:  73%|███████▎  | 386/532 [06:48<02:25,  1.00it/s]Loading train:  73%|███████▎  | 387/532 [06:49<02:26,  1.01s/it]Loading train:  73%|███████▎  | 388/532 [06:50<02:27,  1.02s/it]Loading train:  73%|███████▎  | 389/532 [06:51<02:27,  1.03s/it]Loading train:  73%|███████▎  | 390/532 [06:52<02:28,  1.05s/it]Loading train:  73%|███████▎  | 391/532 [06:53<02:28,  1.05s/it]Loading train:  74%|███████▎  | 392/532 [06:54<02:25,  1.04s/it]Loading train:  74%|███████▍  | 393/532 [06:55<02:23,  1.03s/it]Loading train:  74%|███████▍  | 394/532 [06:56<02:22,  1.03s/it]Loading train:  74%|███████▍  | 395/532 [06:57<02:20,  1.02s/it]Loading train:  74%|███████▍  | 396/532 [06:58<02:16,  1.01s/it]Loading train:  75%|███████▍  | 397/532 [06:59<02:17,  1.02s/it]Loading train:  75%|███████▍  | 398/532 [07:00<02:18,  1.03s/it]Loading train:  75%|███████▌  | 399/532 [07:01<02:17,  1.03s/it]Loading train:  75%|███████▌  | 400/532 [07:02<02:17,  1.04s/it]Loading train:  75%|███████▌  | 401/532 [07:04<02:21,  1.08s/it]Loading train:  76%|███████▌  | 402/532 [07:05<02:21,  1.09s/it]Loading train:  76%|███████▌  | 403/532 [07:06<02:18,  1.07s/it]Loading train:  76%|███████▌  | 404/532 [07:07<02:16,  1.07s/it]Loading train:  76%|███████▌  | 405/532 [07:08<02:15,  1.07s/it]Loading train:  76%|███████▋  | 406/532 [07:09<02:13,  1.06s/it]Loading train:  77%|███████▋  | 407/532 [07:10<02:12,  1.06s/it]Loading train:  77%|███████▋  | 408/532 [07:11<02:10,  1.05s/it]Loading train:  77%|███████▋  | 409/532 [07:12<02:08,  1.05s/it]Loading train:  77%|███████▋  | 410/532 [07:13<02:04,  1.02s/it]Loading train:  77%|███████▋  | 411/532 [07:14<02:00,  1.00it/s]Loading train:  77%|███████▋  | 412/532 [07:15<01:58,  1.02it/s]Loading train:  78%|███████▊  | 413/532 [07:16<01:55,  1.03it/s]Loading train:  78%|███████▊  | 414/532 [07:17<01:52,  1.05it/s]Loading train:  78%|███████▊  | 415/532 [07:18<01:50,  1.06it/s]Loading train:  78%|███████▊  | 416/532 [07:18<01:48,  1.07it/s]Loading train:  78%|███████▊  | 417/532 [07:19<01:45,  1.09it/s]Loading train:  79%|███████▊  | 418/532 [07:20<01:45,  1.08it/s]Loading train:  79%|███████▉  | 419/532 [07:21<01:49,  1.03it/s]Loading train:  79%|███████▉  | 420/532 [07:22<01:51,  1.00it/s]Loading train:  79%|███████▉  | 421/532 [07:24<01:53,  1.02s/it]Loading train:  79%|███████▉  | 422/532 [07:25<01:54,  1.04s/it]Loading train:  80%|███████▉  | 423/532 [07:26<01:55,  1.06s/it]Loading train:  80%|███████▉  | 424/532 [07:27<01:55,  1.07s/it]Loading train:  80%|███████▉  | 425/532 [07:28<01:52,  1.05s/it]Loading train:  80%|████████  | 426/532 [07:29<01:53,  1.07s/it]Loading train:  80%|████████  | 427/532 [07:30<01:49,  1.05s/it]Loading train:  80%|████████  | 428/532 [07:31<01:49,  1.06s/it]Loading train:  81%|████████  | 429/532 [07:32<01:49,  1.07s/it]Loading train:  81%|████████  | 430/532 [07:33<01:47,  1.05s/it]Loading train:  81%|████████  | 431/532 [07:34<01:47,  1.06s/it]Loading train:  81%|████████  | 432/532 [07:35<01:47,  1.07s/it]Loading train:  81%|████████▏ | 433/532 [07:36<01:46,  1.07s/it]Loading train:  82%|████████▏ | 434/532 [07:37<01:46,  1.08s/it]Loading train:  82%|████████▏ | 435/532 [07:39<01:45,  1.09s/it]Loading train:  82%|████████▏ | 436/532 [07:40<01:44,  1.09s/it]Loading train:  82%|████████▏ | 437/532 [07:41<01:37,  1.03s/it]Loading train:  82%|████████▏ | 438/532 [07:41<01:30,  1.03it/s]Loading train:  83%|████████▎ | 439/532 [07:42<01:28,  1.05it/s]Loading train:  83%|████████▎ | 440/532 [07:43<01:25,  1.08it/s]Loading train:  83%|████████▎ | 441/532 [07:44<01:22,  1.11it/s]Loading train:  83%|████████▎ | 442/532 [07:45<01:19,  1.14it/s]Loading train:  83%|████████▎ | 443/532 [07:46<01:17,  1.15it/s]Loading train:  83%|████████▎ | 444/532 [07:47<01:15,  1.16it/s]Loading train:  84%|████████▎ | 445/532 [07:47<01:16,  1.14it/s]Loading train:  84%|████████▍ | 446/532 [07:48<01:15,  1.14it/s]Loading train:  84%|████████▍ | 447/532 [07:49<01:14,  1.14it/s]Loading train:  84%|████████▍ | 448/532 [07:50<01:13,  1.14it/s]Loading train:  84%|████████▍ | 449/532 [07:51<01:13,  1.13it/s]Loading train:  85%|████████▍ | 450/532 [07:52<01:17,  1.05it/s]Loading train:  85%|████████▍ | 451/532 [07:53<01:16,  1.07it/s]Loading train:  85%|████████▍ | 452/532 [07:54<01:13,  1.09it/s]Loading train:  85%|████████▌ | 453/532 [07:55<01:11,  1.11it/s]Loading train:  85%|████████▌ | 454/532 [07:56<01:10,  1.10it/s]Loading train:  86%|████████▌ | 455/532 [07:57<01:14,  1.03it/s]Loading train:  86%|████████▌ | 456/532 [07:58<01:15,  1.01it/s]Loading train:  86%|████████▌ | 457/532 [07:59<01:14,  1.01it/s]Loading train:  86%|████████▌ | 458/532 [08:00<01:18,  1.06s/it]Loading train:  86%|████████▋ | 459/532 [08:01<01:16,  1.05s/it]Loading train:  86%|████████▋ | 460/532 [08:02<01:12,  1.01s/it]Loading train:  87%|████████▋ | 461/532 [08:03<01:14,  1.06s/it]Loading train:  87%|████████▋ | 462/532 [08:04<01:15,  1.08s/it]Loading train:  87%|████████▋ | 463/532 [08:06<01:18,  1.14s/it]Loading train:  87%|████████▋ | 464/532 [08:07<01:16,  1.13s/it]Loading train:  87%|████████▋ | 465/532 [08:08<01:14,  1.11s/it]Loading train:  88%|████████▊ | 466/532 [08:09<01:14,  1.13s/it]Loading train:  88%|████████▊ | 467/532 [08:10<01:11,  1.10s/it]Loading train:  88%|████████▊ | 468/532 [08:11<01:08,  1.06s/it]Loading train:  88%|████████▊ | 469/532 [08:12<01:05,  1.03s/it]Loading train:  88%|████████▊ | 470/532 [08:13<01:02,  1.00s/it]Loading train:  89%|████████▊ | 471/532 [08:14<00:59,  1.03it/s]Loading train:  89%|████████▊ | 472/532 [08:15<00:57,  1.04it/s]Loading train:  89%|████████▉ | 473/532 [08:16<00:59,  1.01s/it]Loading train:  89%|████████▉ | 474/532 [08:17<00:58,  1.02s/it]Loading train:  89%|████████▉ | 475/532 [08:18<00:59,  1.05s/it]Loading train:  89%|████████▉ | 476/532 [08:19<00:58,  1.04s/it]Loading train:  90%|████████▉ | 477/532 [08:20<00:58,  1.06s/it]Loading train:  90%|████████▉ | 478/532 [08:21<00:56,  1.04s/it]Loading train:  90%|█████████ | 479/532 [08:22<00:51,  1.02it/s]Loading train:  90%|█████████ | 480/532 [08:23<00:50,  1.02it/s]Loading train:  90%|█████████ | 481/532 [08:24<00:49,  1.03it/s]Loading train:  91%|█████████ | 482/532 [08:25<00:49,  1.02it/s]Loading train:  91%|█████████ | 483/532 [08:26<00:47,  1.03it/s]Loading train:  91%|█████████ | 484/532 [08:27<00:47,  1.01it/s]Loading train:  91%|█████████ | 485/532 [08:28<00:48,  1.03s/it]Loading train:  91%|█████████▏| 486/532 [08:29<00:48,  1.05s/it]Loading train:  92%|█████████▏| 487/532 [08:30<00:47,  1.06s/it]Loading train:  92%|█████████▏| 488/532 [08:31<00:46,  1.06s/it]Loading train:  92%|█████████▏| 489/532 [08:32<00:45,  1.05s/it]Loading train:  92%|█████████▏| 490/532 [08:33<00:43,  1.04s/it]Loading train:  92%|█████████▏| 491/532 [08:34<00:41,  1.01s/it]Loading train:  92%|█████████▏| 492/532 [08:35<00:39,  1.01it/s]Loading train:  93%|█████████▎| 493/532 [08:36<00:37,  1.04it/s]Loading train:  93%|█████████▎| 494/532 [08:37<00:35,  1.07it/s]Loading train:  93%|█████████▎| 495/532 [08:38<00:33,  1.10it/s]Loading train:  93%|█████████▎| 496/532 [08:39<00:32,  1.10it/s]Loading train:  93%|█████████▎| 497/532 [08:40<00:32,  1.07it/s]Loading train:  94%|█████████▎| 498/532 [08:41<00:31,  1.06it/s]Loading train:  94%|█████████▍| 499/532 [08:41<00:30,  1.08it/s]Loading train:  94%|█████████▍| 500/532 [08:42<00:29,  1.10it/s]Loading train:  94%|█████████▍| 501/532 [08:43<00:28,  1.09it/s]Loading train:  94%|█████████▍| 502/532 [08:44<00:28,  1.05it/s]Loading train:  95%|█████████▍| 503/532 [08:45<00:27,  1.04it/s]Loading train:  95%|█████████▍| 504/532 [08:46<00:26,  1.06it/s]Loading train:  95%|█████████▍| 505/532 [08:47<00:24,  1.08it/s]Loading train:  95%|█████████▌| 506/532 [08:48<00:24,  1.07it/s]Loading train:  95%|█████████▌| 507/532 [08:49<00:23,  1.08it/s]Loading train:  95%|█████████▌| 508/532 [08:50<00:22,  1.07it/s]Loading train:  96%|█████████▌| 509/532 [08:51<00:23,  1.01s/it]Loading train:  96%|█████████▌| 510/532 [08:52<00:23,  1.07s/it]Loading train:  96%|█████████▌| 511/532 [08:53<00:22,  1.08s/it]Loading train:  96%|█████████▌| 512/532 [08:54<00:21,  1.10s/it]Loading train:  96%|█████████▋| 513/532 [08:56<00:21,  1.11s/it]Loading train:  97%|█████████▋| 514/532 [08:57<00:20,  1.12s/it]Loading train:  97%|█████████▋| 515/532 [08:58<00:18,  1.09s/it]Loading train:  97%|█████████▋| 516/532 [08:59<00:16,  1.05s/it]Loading train:  97%|█████████▋| 517/532 [09:00<00:15,  1.02s/it]Loading train:  97%|█████████▋| 518/532 [09:01<00:14,  1.00s/it]Loading train:  98%|█████████▊| 519/532 [09:02<00:12,  1.03it/s]Loading train:  98%|█████████▊| 520/532 [09:02<00:11,  1.05it/s]Loading train:  98%|█████████▊| 521/532 [09:04<00:10,  1.00it/s]Loading train:  98%|█████████▊| 522/532 [09:05<00:10,  1.01s/it]Loading train:  98%|█████████▊| 523/532 [09:06<00:09,  1.04s/it]Loading train:  98%|█████████▊| 524/532 [09:07<00:08,  1.06s/it]Loading train:  99%|█████████▊| 525/532 [09:08<00:07,  1.06s/it]Loading train:  99%|█████████▉| 526/532 [09:09<00:06,  1.08s/it]Loading train:  99%|█████████▉| 527/532 [09:10<00:05,  1.06s/it]Loading train:  99%|█████████▉| 528/532 [09:11<00:04,  1.02s/it]Loading train:  99%|█████████▉| 529/532 [09:12<00:02,  1.01it/s]Loading train: 100%|█████████▉| 530/532 [09:13<00:01,  1.03it/s]Loading train: 100%|█████████▉| 531/532 [09:14<00:00,  1.07it/s]Loading train: 100%|██████████| 532/532 [09:15<00:00,  1.04it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   4%|▍         | 21/532 [00:00<00:02, 209.64it/s]concatenating: train:   8%|▊         | 44/532 [00:00<00:02, 213.74it/s]concatenating: train:  13%|█▎        | 70/532 [00:00<00:02, 224.18it/s]concatenating: train:  18%|█▊        | 94/532 [00:00<00:01, 226.80it/s]concatenating: train:  23%|██▎       | 120/532 [00:00<00:01, 234.26it/s]concatenating: train:  28%|██▊       | 147/532 [00:00<00:01, 242.16it/s]concatenating: train:  32%|███▏      | 172/532 [00:00<00:01, 243.07it/s]concatenating: train:  37%|███▋      | 197/532 [00:00<00:01, 242.71it/s]concatenating: train:  42%|████▏     | 221/532 [00:00<00:01, 228.87it/s]concatenating: train:  46%|████▌     | 244/532 [00:01<00:01, 226.87it/s]concatenating: train:  50%|█████     | 267/532 [00:01<00:01, 221.05it/s]concatenating: train:  56%|█████▌    | 296/532 [00:01<00:00, 236.66it/s]concatenating: train:  61%|██████    | 323/532 [00:01<00:00, 245.37it/s]concatenating: train:  65%|██████▌   | 348/532 [00:01<00:00, 216.26it/s]concatenating: train:  70%|██████▉   | 372/532 [00:01<00:00, 221.70it/s]concatenating: train:  75%|███████▌  | 399/532 [00:01<00:00, 233.57it/s]concatenating: train:  80%|████████  | 427/532 [00:01<00:00, 241.96it/s]concatenating: train:  85%|████████▍ | 452/532 [00:01<00:00, 214.41it/s]concatenating: train:  89%|████████▉ | 475/532 [00:02<00:00, 216.39it/s]concatenating: train:  94%|█████████▎| 498/532 [00:02<00:00, 211.82it/s]concatenating: train:  99%|█████████▉| 529/532 [00:02<00:00, 233.73it/s]concatenating: train: 100%|██████████| 532/532 [00:02<00:00, 232.84it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:12,  1.12it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:11,  1.11it/s]Loading test:  20%|██        | 3/15 [00:02<00:11,  1.03it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:10,  1.02it/s]Loading test:  33%|███▎      | 5/15 [00:05<00:10,  1.04s/it]Loading test:  40%|████      | 6/15 [00:06<00:10,  1.11s/it]Loading test:  47%|████▋     | 7/15 [00:07<00:08,  1.04s/it]Loading test:  53%|█████▎    | 8/15 [00:08<00:07,  1.10s/it]Loading test:  60%|██████    | 9/15 [00:09<00:06,  1.09s/it]Loading test:  67%|██████▋   | 10/15 [00:10<00:05,  1.04s/it]Loading test:  73%|███████▎  | 11/15 [00:11<00:03,  1.00it/s]Loading test:  80%|████████  | 12/15 [00:12<00:03,  1.02s/it]Loading test:  87%|████████▋ | 13/15 [00:13<00:02,  1.05s/it]Loading test:  93%|█████████▎| 14/15 [00:14<00:01,  1.06s/it]Loading test: 100%|██████████| 15/15 [00:15<00:00,  1.05s/it]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 303.87it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 56, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 56, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 56, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 56, 10)   910         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 56, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 56, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 11)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 11)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 28, 11)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 20)   2000        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 20)   3620        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 31)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 31)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 14, 31)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 14, 40)   11200       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 14, 40)   160         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 14, 40)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 14, 40)   14440       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 14, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 14, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 14, 71)   0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 71)   0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 20)   5700        dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 51)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 20)   9200        concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 20)   3620        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 20)   80          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 20)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 71)   0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-05 23:35:12.157320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 23:35:12.157433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 23:35:12.157451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 23:35:12.157462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 23:35:12.157887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 28, 28, 71)   0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 56, 10)   2850        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 56, 21)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 56, 10)   1900        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 56, 10)   40          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 56, 10)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 56, 10)   910         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 56, 10)   40          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 56, 10)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 56, 31)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 56, 31)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 56, 13)   416         dropout_5[0][0]                  
==================================================================================================
Total params: 57,666
Trainable params: 57,266
Non-trainable params: 400
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53974061e-02 2.89048015e-02 1.16758472e-01 1.00223856e-02
 3.03440156e-02 5.80063118e-03 6.84518755e-02 1.28261328e-01
 7.55818021e-02 1.22545826e-02 2.73712232e-01 1.84335085e-01
 1.75382711e-04]
Train on 33496 samples, validate on 969 samples
Epoch 1/300
 - 27s - loss: 50.3094 - acc: 0.8437 - mDice: 0.0181 - val_loss: 4.0126 - val_acc: 0.9217 - val_mDice: 0.0330

Epoch 00001: val_mDice improved from -inf to 0.03298, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 21s - loss: 5.5035 - acc: 0.9021 - mDice: 0.0442 - val_loss: 3.1947 - val_acc: 0.9135 - val_mDice: 0.0692

Epoch 00002: val_mDice improved from 0.03298 to 0.06925, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 19s - loss: 4.1118 - acc: 0.9063 - mDice: 0.0781 - val_loss: 2.7109 - val_acc: 0.9261 - val_mDice: 0.1236

Epoch 00003: val_mDice improved from 0.06925 to 0.12363, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 19s - loss: 3.5122 - acc: 0.9116 - mDice: 0.1223 - val_loss: 2.2703 - val_acc: 0.9288 - val_mDice: 0.1965

Epoch 00004: val_mDice improved from 0.12363 to 0.19646, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300
 - 20s - loss: 3.0654 - acc: 0.9145 - mDice: 0.1705 - val_loss: 1.9654 - val_acc: 0.9307 - val_mDice: 0.2615

Epoch 00005: val_mDice improved from 0.19646 to 0.26151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 19s - loss: 2.7591 - acc: 0.9190 - mDice: 0.2120 - val_loss: 1.7199 - val_acc: 0.9394 - val_mDice: 0.3207

Epoch 00006: val_mDice improved from 0.26151 to 0.32066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 19s - loss: 2.5304 - acc: 0.9239 - mDice: 0.2476 - val_loss: 1.5663 - val_acc: 0.9539 - val_mDice: 0.3690

Epoch 00007: val_mDice improved from 0.32066 to 0.36904, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300
 - 19s - loss: 2.3493 - acc: 0.9289 - mDice: 0.2810 - val_loss: 1.4244 - val_acc: 0.9604 - val_mDice: 0.4106

Epoch 00008: val_mDice improved from 0.36904 to 0.41063, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 20s - loss: 2.1766 - acc: 0.9337 - mDice: 0.3175 - val_loss: 1.3039 - val_acc: 0.9644 - val_mDice: 0.4588

Epoch 00009: val_mDice improved from 0.41063 to 0.45875, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 19s - loss: 2.0229 - acc: 0.9378 - mDice: 0.3531 - val_loss: 1.2351 - val_acc: 0.9669 - val_mDice: 0.4901

Epoch 00010: val_mDice improved from 0.45875 to 0.49008, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300
 - 19s - loss: 1.8976 - acc: 0.9409 - mDice: 0.3829 - val_loss: 1.1221 - val_acc: 0.9698 - val_mDice: 0.5294

Epoch 00011: val_mDice improved from 0.49008 to 0.52937, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300
 - 19s - loss: 1.7801 - acc: 0.9436 - mDice: 0.4120 - val_loss: 1.0688 - val_acc: 0.9709 - val_mDice: 0.5543

Epoch 00012: val_mDice improved from 0.52937 to 0.55433, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 13/300
 - 19s - loss: 1.6680 - acc: 0.9454 - mDice: 0.4378 - val_loss: 1.0078 - val_acc: 0.9712 - val_mDice: 0.5763

Epoch 00013: val_mDice improved from 0.55433 to 0.57635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 14/300
 - 20s - loss: 1.5704 - acc: 0.9469 - mDice: 0.4616 - val_loss: 1.0027 - val_acc: 0.9718 - val_mDice: 0.5847

Epoch 00014: val_mDice improved from 0.57635 to 0.58468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 15/300
 - 19s - loss: 1.5032 - acc: 0.9481 - mDice: 0.4792 - val_loss: 0.9525 - val_acc: 0.9733 - val_mDice: 0.6021

Epoch 00015: val_mDice improved from 0.58468 to 0.60209, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 16/300
 - 19s - loss: 1.4510 - acc: 0.9492 - mDice: 0.4950 - val_loss: 0.9599 - val_acc: 0.9730 - val_mDice: 0.6061

Epoch 00016: val_mDice improved from 0.60209 to 0.60608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 17/300
 - 19s - loss: 1.4047 - acc: 0.9502 - mDice: 0.5082 - val_loss: 0.9118 - val_acc: 0.9732 - val_mDice: 0.6216

Epoch 00017: val_mDice improved from 0.60608 to 0.62164, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300
 - 21s - loss: 1.3633 - acc: 0.9513 - mDice: 0.5213 - val_loss: 0.8821 - val_acc: 0.9726 - val_mDice: 0.6390

Epoch 00018: val_mDice improved from 0.62164 to 0.63900, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 19/300
 - 19s - loss: 1.3277 - acc: 0.9521 - mDice: 0.5313 - val_loss: 0.8629 - val_acc: 0.9737 - val_mDice: 0.6414

Epoch 00019: val_mDice improved from 0.63900 to 0.64141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 20/300
 - 19s - loss: 1.2983 - acc: 0.9527 - mDice: 0.5405 - val_loss: 0.8550 - val_acc: 0.9743 - val_mDice: 0.6466

Epoch 00020: val_mDice improved from 0.64141 to 0.64656, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 21/300
 - 19s - loss: 1.2647 - acc: 0.9534 - mDice: 0.5508 - val_loss: 0.8562 - val_acc: 0.9743 - val_mDice: 0.6530

Epoch 00021: val_mDice improved from 0.64656 to 0.65299, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300
 - 21s - loss: 1.2319 - acc: 0.9535 - mDice: 0.5613 - val_loss: 0.8385 - val_acc: 0.9741 - val_mDice: 0.6540

Epoch 00022: val_mDice improved from 0.65299 to 0.65398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 23/300
 - 19s - loss: 1.2097 - acc: 0.9543 - mDice: 0.5688 - val_loss: 0.8294 - val_acc: 0.9748 - val_mDice: 0.6607

Epoch 00023: val_mDice improved from 0.65398 to 0.66066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 24/300
 - 19s - loss: 1.1853 - acc: 0.9552 - mDice: 0.5765 - val_loss: 0.8460 - val_acc: 0.9746 - val_mDice: 0.6566

Epoch 00024: val_mDice did not improve from 0.66066
Epoch 25/300
 - 19s - loss: 1.1672 - acc: 0.9557 - mDice: 0.5828 - val_loss: 0.8110 - val_acc: 0.9749 - val_mDice: 0.6730

Epoch 00025: val_mDice improved from 0.66066 to 0.67303, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 26/300
 - 20s - loss: 1.1450 - acc: 0.9560 - mDice: 0.5894 - val_loss: 0.8177 - val_acc: 0.9750 - val_mDice: 0.6735

Epoch 00026: val_mDice improved from 0.67303 to 0.67355, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 27/300
 - 19s - loss: 1.1294 - acc: 0.9562 - mDice: 0.5951 - val_loss: 0.8017 - val_acc: 0.9742 - val_mDice: 0.6774

Epoch 00027: val_mDice improved from 0.67355 to 0.67742, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 28/300
 - 19s - loss: 1.1161 - acc: 0.9565 - mDice: 0.5997 - val_loss: 0.7847 - val_acc: 0.9752 - val_mDice: 0.6871

Epoch 00028: val_mDice improved from 0.67742 to 0.68706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 29/300
 - 19s - loss: 1.1060 - acc: 0.9568 - mDice: 0.6030 - val_loss: 0.8020 - val_acc: 0.9756 - val_mDice: 0.6846

Epoch 00029: val_mDice did not improve from 0.68706
Epoch 30/300
 - 19s - loss: 1.0878 - acc: 0.9571 - mDice: 0.6088 - val_loss: 0.7799 - val_acc: 0.9768 - val_mDice: 0.6861

Epoch 00030: val_mDice did not improve from 0.68706
Epoch 31/300
 - 20s - loss: 1.0780 - acc: 0.9573 - mDice: 0.6124 - val_loss: 0.7908 - val_acc: 0.9757 - val_mDice: 0.6832

Epoch 00031: val_mDice did not improve from 0.68706
Epoch 32/300
 - 19s - loss: 1.0666 - acc: 0.9576 - mDice: 0.6158 - val_loss: 0.7736 - val_acc: 0.9761 - val_mDice: 0.6914

Epoch 00032: val_mDice improved from 0.68706 to 0.69137, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 33/300
 - 19s - loss: 1.0571 - acc: 0.9577 - mDice: 0.6194 - val_loss: 0.7773 - val_acc: 0.9767 - val_mDice: 0.6870

Epoch 00033: val_mDice did not improve from 0.69137
Epoch 34/300
 - 19s - loss: 1.0503 - acc: 0.9579 - mDice: 0.6220 - val_loss: 0.7665 - val_acc: 0.9766 - val_mDice: 0.6892

Epoch 00034: val_mDice did not improve from 0.69137
Epoch 35/300
 - 20s - loss: 1.0436 - acc: 0.9580 - mDice: 0.6240 - val_loss: 0.7829 - val_acc: 0.9767 - val_mDice: 0.6902

Epoch 00035: val_mDice did not improve from 0.69137
Epoch 36/300
 - 20s - loss: 1.0336 - acc: 0.9581 - mDice: 0.6269 - val_loss: 0.7488 - val_acc: 0.9769 - val_mDice: 0.6952

Epoch 00036: val_mDice improved from 0.69137 to 0.69516, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 37/300
 - 19s - loss: 1.0260 - acc: 0.9582 - mDice: 0.6300 - val_loss: 0.7327 - val_acc: 0.9773 - val_mDice: 0.7009

Epoch 00037: val_mDice improved from 0.69516 to 0.70094, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 38/300
 - 19s - loss: 1.0207 - acc: 0.9582 - mDice: 0.6318 - val_loss: 0.7382 - val_acc: 0.9774 - val_mDice: 0.6994

Epoch 00038: val_mDice did not improve from 0.70094
Epoch 39/300
 - 19s - loss: 1.0131 - acc: 0.9583 - mDice: 0.6342 - val_loss: 0.7696 - val_acc: 0.9762 - val_mDice: 0.6951

Epoch 00039: val_mDice did not improve from 0.70094
Epoch 40/300
 - 20s - loss: 1.0059 - acc: 0.9584 - mDice: 0.6366 - val_loss: 0.7327 - val_acc: 0.9770 - val_mDice: 0.7038

Epoch 00040: val_mDice improved from 0.70094 to 0.70379, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 41/300
 - 19s - loss: 1.0039 - acc: 0.9583 - mDice: 0.6376 - val_loss: 0.7558 - val_acc: 0.9767 - val_mDice: 0.7001

Epoch 00041: val_mDice did not improve from 0.70379
Epoch 42/300
 - 19s - loss: 0.9933 - acc: 0.9586 - mDice: 0.6413 - val_loss: 0.7365 - val_acc: 0.9772 - val_mDice: 0.7042

Epoch 00042: val_mDice improved from 0.70379 to 0.70425, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 43/300
 - 19s - loss: 0.9917 - acc: 0.9586 - mDice: 0.6414 - val_loss: 0.7613 - val_acc: 0.9768 - val_mDice: 0.6986

Epoch 00043: val_mDice did not improve from 0.70425
Epoch 44/300
 - 19s - loss: 0.9886 - acc: 0.9586 - mDice: 0.6429 - val_loss: 0.7322 - val_acc: 0.9769 - val_mDice: 0.7040

Epoch 00044: val_mDice did not improve from 0.70425
Epoch 45/300
 - 20s - loss: 0.9844 - acc: 0.9587 - mDice: 0.6443 - val_loss: 0.7215 - val_acc: 0.9774 - val_mDice: 0.7061

Epoch 00045: val_mDice improved from 0.70425 to 0.70614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 46/300
 - 19s - loss: 0.9795 - acc: 0.9587 - mDice: 0.6461 - val_loss: 0.7296 - val_acc: 0.9774 - val_mDice: 0.7063

Epoch 00046: val_mDice improved from 0.70614 to 0.70634, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 47/300
 - 19s - loss: 0.9711 - acc: 0.9588 - mDice: 0.6489 - val_loss: 0.7237 - val_acc: 0.9774 - val_mDice: 0.7097

Epoch 00047: val_mDice improved from 0.70634 to 0.70965, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 48/300
 - 19s - loss: 0.9677 - acc: 0.9589 - mDice: 0.6506 - val_loss: 0.7230 - val_acc: 0.9774 - val_mDice: 0.7114

Epoch 00048: val_mDice improved from 0.70965 to 0.71139, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 49/300
 - 20s - loss: 0.9646 - acc: 0.9590 - mDice: 0.6516 - val_loss: 0.7276 - val_acc: 0.9777 - val_mDice: 0.7102

Epoch 00049: val_mDice did not improve from 0.71139
Epoch 50/300
 - 19s - loss: 0.9613 - acc: 0.9592 - mDice: 0.6523 - val_loss: 0.7217 - val_acc: 0.9778 - val_mDice: 0.7130

Epoch 00050: val_mDice improved from 0.71139 to 0.71297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 51/300
 - 19s - loss: 0.9549 - acc: 0.9594 - mDice: 0.6553 - val_loss: 0.7233 - val_acc: 0.9778 - val_mDice: 0.7134

Epoch 00051: val_mDice improved from 0.71297 to 0.71340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 52/300
 - 19s - loss: 0.9511 - acc: 0.9596 - mDice: 0.6556 - val_loss: 0.7115 - val_acc: 0.9782 - val_mDice: 0.7142

Epoch 00052: val_mDice improved from 0.71340 to 0.71418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 53/300
 - 19s - loss: 0.9494 - acc: 0.9598 - mDice: 0.6569 - val_loss: 0.7198 - val_acc: 0.9780 - val_mDice: 0.7144

Epoch 00053: val_mDice improved from 0.71418 to 0.71441, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 54/300
 - 21s - loss: 0.9445 - acc: 0.9600 - mDice: 0.6583 - val_loss: 0.7271 - val_acc: 0.9778 - val_mDice: 0.7099

Epoch 00054: val_mDice did not improve from 0.71441
Epoch 55/300
 - 19s - loss: 0.9432 - acc: 0.9601 - mDice: 0.6586 - val_loss: 0.7109 - val_acc: 0.9779 - val_mDice: 0.7169

Epoch 00055: val_mDice improved from 0.71441 to 0.71686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 56/300
 - 19s - loss: 0.9415 - acc: 0.9603 - mDice: 0.6595 - val_loss: 0.7162 - val_acc: 0.9777 - val_mDice: 0.7149

Epoch 00056: val_mDice did not improve from 0.71686
Epoch 57/300
 - 20s - loss: 0.9355 - acc: 0.9604 - mDice: 0.6613 - val_loss: 0.7238 - val_acc: 0.9783 - val_mDice: 0.7135

Epoch 00057: val_mDice did not improve from 0.71686
Epoch 58/300
 - 19s - loss: 0.9337 - acc: 0.9605 - mDice: 0.6620 - val_loss: 0.7106 - val_acc: 0.9781 - val_mDice: 0.7127

Epoch 00058: val_mDice did not improve from 0.71686
Epoch 59/300
 - 19s - loss: 0.9333 - acc: 0.9606 - mDice: 0.6622 - val_loss: 0.7091 - val_acc: 0.9779 - val_mDice: 0.7187

Epoch 00059: val_mDice improved from 0.71686 to 0.71871, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 60/300
 - 19s - loss: 0.9293 - acc: 0.9607 - mDice: 0.6635 - val_loss: 0.7071 - val_acc: 0.9780 - val_mDice: 0.7157

Epoch 00060: val_mDice did not improve from 0.71871
Epoch 61/300
 - 20s - loss: 0.9268 - acc: 0.9608 - mDice: 0.6642 - val_loss: 0.7080 - val_acc: 0.9780 - val_mDice: 0.7164

Epoch 00061: val_mDice did not improve from 0.71871
Epoch 62/300
 - 19s - loss: 0.9226 - acc: 0.9610 - mDice: 0.6657 - val_loss: 0.6990 - val_acc: 0.9784 - val_mDice: 0.7190

Epoch 00062: val_mDice improved from 0.71871 to 0.71900, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 63/300
 - 19s - loss: 0.9200 - acc: 0.9611 - mDice: 0.6665 - val_loss: 0.7049 - val_acc: 0.9783 - val_mDice: 0.7209

Epoch 00063: val_mDice improved from 0.71900 to 0.72091, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 64/300
 - 19s - loss: 0.9185 - acc: 0.9611 - mDice: 0.6671 - val_loss: 0.7038 - val_acc: 0.9781 - val_mDice: 0.7192

Epoch 00064: val_mDice did not improve from 0.72091
Epoch 65/300
 - 20s - loss: 0.9182 - acc: 0.9611 - mDice: 0.6674 - val_loss: 0.7023 - val_acc: 0.9783 - val_mDice: 0.7200

Epoch 00065: val_mDice did not improve from 0.72091
Epoch 66/300
 - 20s - loss: 0.9168 - acc: 0.9613 - mDice: 0.6681 - val_loss: 0.6960 - val_acc: 0.9788 - val_mDice: 0.7202

Epoch 00066: val_mDice did not improve from 0.72091
Epoch 67/300
 - 19s - loss: 0.9128 - acc: 0.9613 - mDice: 0.6694 - val_loss: 0.6949 - val_acc: 0.9786 - val_mDice: 0.7229

Epoch 00067: val_mDice improved from 0.72091 to 0.72291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 68/300
 - 19s - loss: 0.9122 - acc: 0.9614 - mDice: 0.6698 - val_loss: 0.7085 - val_acc: 0.9781 - val_mDice: 0.7213

Epoch 00068: val_mDice did not improve from 0.72291
Epoch 69/300
 - 19s - loss: 0.9115 - acc: 0.9614 - mDice: 0.6698 - val_loss: 0.6967 - val_acc: 0.9787 - val_mDice: 0.7197

Epoch 00069: val_mDice did not improve from 0.72291
Epoch 70/300
 - 19s - loss: 0.9079 - acc: 0.9615 - mDice: 0.6710 - val_loss: 0.6943 - val_acc: 0.9781 - val_mDice: 0.7210

Epoch 00070: val_mDice did not improve from 0.72291
Epoch 71/300
 - 20s - loss: 0.9056 - acc: 0.9616 - mDice: 0.6716 - val_loss: 0.6932 - val_acc: 0.9789 - val_mDice: 0.7225

Epoch 00071: val_mDice did not improve from 0.72291
Epoch 72/300
 - 19s - loss: 0.9067 - acc: 0.9616 - mDice: 0.6716 - val_loss: 0.6994 - val_acc: 0.9782 - val_mDice: 0.7166

Epoch 00072: val_mDice did not improve from 0.72291
Epoch 73/300
 - 19s - loss: 0.9039 - acc: 0.9617 - mDice: 0.6720 - val_loss: 0.7080 - val_acc: 0.9786 - val_mDice: 0.7234

Epoch 00073: val_mDice improved from 0.72291 to 0.72343, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 74/300
 - 19s - loss: 0.9019 - acc: 0.9618 - mDice: 0.6731 - val_loss: 0.6841 - val_acc: 0.9786 - val_mDice: 0.7221

Epoch 00074: val_mDice did not improve from 0.72343
Epoch 75/300
 - 20s - loss: 0.9009 - acc: 0.9618 - mDice: 0.6735 - val_loss: 0.7005 - val_acc: 0.9787 - val_mDice: 0.7227

Epoch 00075: val_mDice did not improve from 0.72343
Epoch 76/300
 - 20s - loss: 0.8999 - acc: 0.9618 - mDice: 0.6736 - val_loss: 0.7078 - val_acc: 0.9785 - val_mDice: 0.7188

Epoch 00076: val_mDice did not improve from 0.72343
Epoch 77/300
 - 19s - loss: 0.8948 - acc: 0.9620 - mDice: 0.6754 - val_loss: 0.6932 - val_acc: 0.9788 - val_mDice: 0.7237

Epoch 00077: val_mDice improved from 0.72343 to 0.72369, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 78/300
 - 19s - loss: 0.8974 - acc: 0.9619 - mDice: 0.6742 - val_loss: 0.6834 - val_acc: 0.9789 - val_mDice: 0.7256

Epoch 00078: val_mDice improved from 0.72369 to 0.72561, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 79/300
 - 19s - loss: 0.8927 - acc: 0.9621 - mDice: 0.6764 - val_loss: 0.6894 - val_acc: 0.9789 - val_mDice: 0.7261

Epoch 00079: val_mDice improved from 0.72561 to 0.72610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 80/300
 - 19s - loss: 0.8956 - acc: 0.9621 - mDice: 0.6751 - val_loss: 0.6905 - val_acc: 0.9787 - val_mDice: 0.7238

Epoch 00080: val_mDice did not improve from 0.72610
Epoch 81/300
 - 20s - loss: 0.8941 - acc: 0.9620 - mDice: 0.6755 - val_loss: 0.6875 - val_acc: 0.9784 - val_mDice: 0.7261

Epoch 00081: val_mDice did not improve from 0.72610
Epoch 82/300
 - 19s - loss: 0.8901 - acc: 0.9621 - mDice: 0.6770 - val_loss: 0.7339 - val_acc: 0.9776 - val_mDice: 0.7142

Epoch 00082: val_mDice did not improve from 0.72610
Epoch 83/300
 - 19s - loss: 0.8906 - acc: 0.9622 - mDice: 0.6770 - val_loss: 0.6915 - val_acc: 0.9785 - val_mDice: 0.7252

Epoch 00083: val_mDice did not improve from 0.72610
Epoch 84/300
 - 19s - loss: 0.8880 - acc: 0.9623 - mDice: 0.6781 - val_loss: 0.6847 - val_acc: 0.9785 - val_mDice: 0.7249

Epoch 00084: val_mDice did not improve from 0.72610
Epoch 85/300
 - 19s - loss: 0.8866 - acc: 0.9623 - mDice: 0.6785 - val_loss: 0.6862 - val_acc: 0.9789 - val_mDice: 0.7253

Epoch 00085: val_mDice did not improve from 0.72610
Epoch 86/300
 - 20s - loss: 0.8870 - acc: 0.9623 - mDice: 0.6778 - val_loss: 0.6753 - val_acc: 0.9791 - val_mDice: 0.7312

Epoch 00086: val_mDice improved from 0.72610 to 0.73116, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 87/300
 - 19s - loss: 0.8848 - acc: 0.9623 - mDice: 0.6792 - val_loss: 0.6725 - val_acc: 0.9792 - val_mDice: 0.7296

Epoch 00087: val_mDice did not improve from 0.73116
Epoch 88/300
 - 19s - loss: 0.8843 - acc: 0.9624 - mDice: 0.6787 - val_loss: 0.6877 - val_acc: 0.9785 - val_mDice: 0.7278

Epoch 00088: val_mDice did not improve from 0.73116
Epoch 89/300
 - 19s - loss: 0.8817 - acc: 0.9625 - mDice: 0.6797 - val_loss: 0.6879 - val_acc: 0.9783 - val_mDice: 0.7271

Epoch 00089: val_mDice did not improve from 0.73116
Epoch 90/300
 - 19s - loss: 0.8826 - acc: 0.9625 - mDice: 0.6800 - val_loss: 0.6807 - val_acc: 0.9789 - val_mDice: 0.7286

Epoch 00090: val_mDice did not improve from 0.73116
Epoch 91/300
 - 21s - loss: 0.8802 - acc: 0.9625 - mDice: 0.6806 - val_loss: 0.6885 - val_acc: 0.9787 - val_mDice: 0.7267

Epoch 00091: val_mDice did not improve from 0.73116
Epoch 92/300
 - 19s - loss: 0.8799 - acc: 0.9625 - mDice: 0.6809 - val_loss: 0.6861 - val_acc: 0.9783 - val_mDice: 0.7294

Epoch 00092: val_mDice did not improve from 0.73116
Epoch 93/300
 - 19s - loss: 0.8777 - acc: 0.9625 - mDice: 0.6817 - val_loss: 0.6830 - val_acc: 0.9784 - val_mDice: 0.7294

Epoch 00093: val_mDice did not improve from 0.73116
Epoch 94/300
 - 19s - loss: 0.8753 - acc: 0.9625 - mDice: 0.6825 - val_loss: 0.6858 - val_acc: 0.9787 - val_mDice: 0.7305

Epoch 00094: val_mDice did not improve from 0.73116
Epoch 95/300
 - 19s - loss: 0.8758 - acc: 0.9625 - mDice: 0.6821 - val_loss: 0.6781 - val_acc: 0.9787 - val_mDice: 0.7325

Epoch 00095: val_mDice improved from 0.73116 to 0.73248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 96/300
 - 21s - loss: 0.8754 - acc: 0.9625 - mDice: 0.6819 - val_loss: 0.6845 - val_acc: 0.9786 - val_mDice: 0.7295

Epoch 00096: val_mDice did not improve from 0.73248
Epoch 97/300
 - 19s - loss: 0.8765 - acc: 0.9624 - mDice: 0.6820 - val_loss: 0.6754 - val_acc: 0.9785 - val_mDice: 0.7291

Epoch 00097: val_mDice did not improve from 0.73248
Epoch 98/300
 - 20s - loss: 0.8726 - acc: 0.9624 - mDice: 0.6834 - val_loss: 0.6708 - val_acc: 0.9787 - val_mDice: 0.7294

Epoch 00098: val_mDice did not improve from 0.73248
Epoch 99/300
 - 19s - loss: 0.8743 - acc: 0.9624 - mDice: 0.6826 - val_loss: 0.6896 - val_acc: 0.9784 - val_mDice: 0.7274

Epoch 00099: val_mDice did not improve from 0.73248
Epoch 100/300
 - 19s - loss: 0.8703 - acc: 0.9625 - mDice: 0.6841 - val_loss: 0.6948 - val_acc: 0.9785 - val_mDice: 0.7297

Epoch 00100: val_mDice did not improve from 0.73248
Epoch 101/300
 - 20s - loss: 0.8703 - acc: 0.9625 - mDice: 0.6838 - val_loss: 0.6874 - val_acc: 0.9787 - val_mDice: 0.7284

Epoch 00101: val_mDice did not improve from 0.73248
Epoch 102/300
 - 20s - loss: 0.8714 - acc: 0.9625 - mDice: 0.6837 - val_loss: 0.6719 - val_acc: 0.9791 - val_mDice: 0.7319

Epoch 00102: val_mDice did not improve from 0.73248
Epoch 103/300
 - 19s - loss: 0.8688 - acc: 0.9626 - mDice: 0.6845 - val_loss: 0.6757 - val_acc: 0.9789 - val_mDice: 0.7310

Epoch 00103: val_mDice did not improve from 0.73248
Epoch 104/300
 - 19s - loss: 0.8685 - acc: 0.9626 - mDice: 0.6845 - val_loss: 0.6761 - val_acc: 0.9785 - val_mDice: 0.7318

Epoch 00104: val_mDice did not improve from 0.73248
Epoch 105/300
 - 20s - loss: 0.8663 - acc: 0.9627 - mDice: 0.6855 - val_loss: 0.6898 - val_acc: 0.9784 - val_mDice: 0.7292

Epoch 00105: val_mDice did not improve from 0.73248
Epoch 106/300
 - 19s - loss: 0.8661 - acc: 0.9627 - mDice: 0.6857 - val_loss: 0.6804 - val_acc: 0.9789 - val_mDice: 0.7314

Epoch 00106: val_mDice did not improve from 0.73248
Epoch 107/300
 - 19s - loss: 0.8645 - acc: 0.9627 - mDice: 0.6858 - val_loss: 0.6790 - val_acc: 0.9788 - val_mDice: 0.7316

Epoch 00107: val_mDice did not improve from 0.73248
Epoch 108/300
 - 19s - loss: 0.8659 - acc: 0.9626 - mDice: 0.6855 - val_loss: 0.6952 - val_acc: 0.9782 - val_mDice: 0.7309

Epoch 00108: val_mDice did not improve from 0.73248
Epoch 109/300
 - 20s - loss: 0.8656 - acc: 0.9627 - mDice: 0.6857 - val_loss: 0.6700 - val_acc: 0.9792 - val_mDice: 0.7351

Epoch 00109: val_mDice improved from 0.73248 to 0.73506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 110/300
 - 19s - loss: 0.8636 - acc: 0.9627 - mDice: 0.6866 - val_loss: 0.6748 - val_acc: 0.9784 - val_mDice: 0.7304

Epoch 00110: val_mDice did not improve from 0.73506
Epoch 111/300
 - 19s - loss: 0.8627 - acc: 0.9628 - mDice: 0.6867 - val_loss: 0.6746 - val_acc: 0.9786 - val_mDice: 0.7323

Epoch 00111: val_mDice did not improve from 0.73506
Epoch 112/300
 - 19s - loss: 0.8634 - acc: 0.9627 - mDice: 0.6865 - val_loss: 0.6747 - val_acc: 0.9783 - val_mDice: 0.7323

Epoch 00112: val_mDice did not improve from 0.73506
Epoch 113/300
 - 21s - loss: 0.8611 - acc: 0.9628 - mDice: 0.6870 - val_loss: 0.6753 - val_acc: 0.9783 - val_mDice: 0.7360

Epoch 00113: val_mDice improved from 0.73506 to 0.73605, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 114/300
 - 20s - loss: 0.8629 - acc: 0.9628 - mDice: 0.6864 - val_loss: 0.6857 - val_acc: 0.9786 - val_mDice: 0.7309

Epoch 00114: val_mDice did not improve from 0.73605
Epoch 115/300
 - 19s - loss: 0.8593 - acc: 0.9629 - mDice: 0.6878 - val_loss: 0.6789 - val_acc: 0.9788 - val_mDice: 0.7359

Epoch 00115: val_mDice did not improve from 0.73605
Epoch 116/300
 - 19s - loss: 0.8586 - acc: 0.9629 - mDice: 0.6881 - val_loss: 0.6787 - val_acc: 0.9788 - val_mDice: 0.7301

Epoch 00116: val_mDice did not improve from 0.73605
Epoch 117/300
 - 20s - loss: 0.8576 - acc: 0.9629 - mDice: 0.6883 - val_loss: 0.6783 - val_acc: 0.9788 - val_mDice: 0.7325

Epoch 00117: val_mDice did not improve from 0.73605
Epoch 118/300
 - 20s - loss: 0.8592 - acc: 0.9628 - mDice: 0.6879 - val_loss: 0.6698 - val_acc: 0.9787 - val_mDice: 0.7331

Epoch 00118: val_mDice did not improve from 0.73605
Epoch 119/300
 - 19s - loss: 0.8565 - acc: 0.9630 - mDice: 0.6886 - val_loss: 0.6707 - val_acc: 0.9792 - val_mDice: 0.7331

Epoch 00119: val_mDice did not improve from 0.73605
Epoch 120/300
 - 19s - loss: 0.8572 - acc: 0.9629 - mDice: 0.6883 - val_loss: 0.6716 - val_acc: 0.9789 - val_mDice: 0.7326

Epoch 00120: val_mDice did not improve from 0.73605
Epoch 121/300
 - 19s - loss: 0.8561 - acc: 0.9629 - mDice: 0.6889 - val_loss: 0.6775 - val_acc: 0.9784 - val_mDice: 0.7329

Epoch 00121: val_mDice did not improve from 0.73605
Epoch 122/300
 - 20s - loss: 0.8559 - acc: 0.9630 - mDice: 0.6886 - val_loss: 0.6900 - val_acc: 0.9778 - val_mDice: 0.7267

Epoch 00122: val_mDice did not improve from 0.73605
Epoch 123/300
 - 20s - loss: 0.8562 - acc: 0.9630 - mDice: 0.6890 - val_loss: 0.6782 - val_acc: 0.9785 - val_mDice: 0.7296

Epoch 00123: val_mDice did not improve from 0.73605
Epoch 124/300
 - 19s - loss: 0.8534 - acc: 0.9630 - mDice: 0.6897 - val_loss: 0.6726 - val_acc: 0.9790 - val_mDice: 0.7347

Epoch 00124: val_mDice did not improve from 0.73605
Epoch 125/300
 - 19s - loss: 0.8531 - acc: 0.9630 - mDice: 0.6897 - val_loss: 0.6793 - val_acc: 0.9781 - val_mDice: 0.7343

Epoch 00125: val_mDice did not improve from 0.73605
Epoch 126/300
 - 19s - loss: 0.8528 - acc: 0.9630 - mDice: 0.6899 - val_loss: 0.6789 - val_acc: 0.9787 - val_mDice: 0.7305

Epoch 00126: val_mDice did not improve from 0.73605
Epoch 127/300
 - 20s - loss: 0.8540 - acc: 0.9630 - mDice: 0.6894 - val_loss: 0.6704 - val_acc: 0.9789 - val_mDice: 0.7348

Epoch 00127: val_mDice did not improve from 0.73605
Epoch 128/300
 - 20s - loss: 0.8526 - acc: 0.9630 - mDice: 0.6902 - val_loss: 0.6664 - val_acc: 0.9787 - val_mDice: 0.7355

Epoch 00128: val_mDice did not improve from 0.73605
Epoch 129/300
 - 19s - loss: 0.8492 - acc: 0.9631 - mDice: 0.6909 - val_loss: 0.6768 - val_acc: 0.9785 - val_mDice: 0.7314

Epoch 00129: val_mDice did not improve from 0.73605
Epoch 130/300
 - 19s - loss: 0.8501 - acc: 0.9630 - mDice: 0.6908 - val_loss: 0.6837 - val_acc: 0.9785 - val_mDice: 0.7316

Epoch 00130: val_mDice did not improve from 0.73605
Epoch 131/300
 - 19s - loss: 0.8496 - acc: 0.9631 - mDice: 0.6907 - val_loss: 0.6963 - val_acc: 0.9782 - val_mDice: 0.7278

Epoch 00131: val_mDice did not improve from 0.73605
Epoch 132/300
 - 20s - loss: 0.8489 - acc: 0.9630 - mDice: 0.6913 - val_loss: 0.6643 - val_acc: 0.9787 - val_mDice: 0.7358

Epoch 00132: val_mDice did not improve from 0.73605
Epoch 133/300
 - 20s - loss: 0.8487 - acc: 0.9631 - mDice: 0.6912 - val_loss: 0.6769 - val_acc: 0.9785 - val_mDice: 0.7317

Epoch 00133: val_mDice did not improve from 0.73605
Epoch 134/300
 - 19s - loss: 0.8500 - acc: 0.9630 - mDice: 0.6906 - val_loss: 0.6806 - val_acc: 0.9786 - val_mDice: 0.7339

Epoch 00134: val_mDice did not improve from 0.73605
Epoch 135/300
 - 19s - loss: 0.8492 - acc: 0.9631 - mDice: 0.6913 - val_loss: 0.6718 - val_acc: 0.9788 - val_mDice: 0.7341

Epoch 00135: val_mDice did not improve from 0.73605
Epoch 136/300
 - 19s - loss: 0.8489 - acc: 0.9630 - mDice: 0.6912 - val_loss: 0.6635 - val_acc: 0.9787 - val_mDice: 0.7346

Epoch 00136: val_mDice did not improve from 0.73605
Epoch 137/300
 - 20s - loss: 0.8464 - acc: 0.9631 - mDice: 0.6922 - val_loss: 0.6680 - val_acc: 0.9792 - val_mDice: 0.7353

Epoch 00137: val_mDice did not improve from 0.73605
Epoch 138/300
 - 20s - loss: 0.8487 - acc: 0.9631 - mDice: 0.6911 - val_loss: 0.6729 - val_acc: 0.9786 - val_mDice: 0.7366

Epoch 00138: val_mDice improved from 0.73605 to 0.73657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 139/300
 - 19s - loss: 0.8456 - acc: 0.9631 - mDice: 0.6923 - val_loss: 0.6903 - val_acc: 0.9784 - val_mDice: 0.7319

Epoch 00139: val_mDice did not improve from 0.73657
Epoch 140/300
 - 19s - loss: 0.8448 - acc: 0.9631 - mDice: 0.6927 - val_loss: 0.6761 - val_acc: 0.9788 - val_mDice: 0.7327

Epoch 00140: val_mDice did not improve from 0.73657
Epoch 141/300
 - 19s - loss: 0.8455 - acc: 0.9631 - mDice: 0.6921 - val_loss: 0.6737 - val_acc: 0.9787 - val_mDice: 0.7346

Epoch 00141: val_mDice did not improve from 0.73657
Epoch 142/300
 - 20s - loss: 0.8435 - acc: 0.9631 - mDice: 0.6932 - val_loss: 0.6618 - val_acc: 0.9789 - val_mDice: 0.7404

Epoch 00142: val_mDice improved from 0.73657 to 0.74044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 143/300
 - 19s - loss: 0.8426 - acc: 0.9631 - mDice: 0.6933 - val_loss: 0.6632 - val_acc: 0.9787 - val_mDice: 0.7354

Epoch 00143: val_mDice did not improve from 0.74044
Epoch 144/300
 - 19s - loss: 0.8435 - acc: 0.9632 - mDice: 0.6931 - val_loss: 0.6639 - val_acc: 0.9790 - val_mDice: 0.7342

Epoch 00144: val_mDice did not improve from 0.74044
Epoch 145/300
 - 19s - loss: 0.8446 - acc: 0.9631 - mDice: 0.6926 - val_loss: 0.6726 - val_acc: 0.9787 - val_mDice: 0.7353

Epoch 00145: val_mDice did not improve from 0.74044
Epoch 146/300
 - 19s - loss: 0.8420 - acc: 0.9631 - mDice: 0.6937 - val_loss: 0.6656 - val_acc: 0.9788 - val_mDice: 0.7351

Epoch 00146: val_mDice did not improve from 0.74044
Epoch 147/300
 - 20s - loss: 0.8403 - acc: 0.9631 - mDice: 0.6944 - val_loss: 0.6834 - val_acc: 0.9784 - val_mDice: 0.7360

Epoch 00147: val_mDice did not improve from 0.74044
Epoch 148/300
 - 19s - loss: 0.8409 - acc: 0.9631 - mDice: 0.6941 - val_loss: 0.6830 - val_acc: 0.9784 - val_mDice: 0.7321

Epoch 00148: val_mDice did not improve from 0.74044
Epoch 149/300
 - 19s - loss: 0.8408 - acc: 0.9631 - mDice: 0.6941 - val_loss: 0.6806 - val_acc: 0.9787 - val_mDice: 0.7371

Epoch 00149: val_mDice did not improve from 0.74044
Epoch 150/300
 - 19s - loss: 0.8395 - acc: 0.9631 - mDice: 0.6944 - val_loss: 0.6651 - val_acc: 0.9788 - val_mDice: 0.7385

Epoch 00150: val_mDice did not improve from 0.74044
Epoch 151/300
 - 19s - loss: 0.8397 - acc: 0.9631 - mDice: 0.6946 - val_loss: 0.6866 - val_acc: 0.9784 - val_mDice: 0.7339

Epoch 00151: val_mDice did not improve from 0.74044
Epoch 152/300
 - 20s - loss: 0.8378 - acc: 0.9632 - mDice: 0.6954 - val_loss: 0.6619 - val_acc: 0.9790 - val_mDice: 0.7375

Epoch 00152: val_mDice did not improve from 0.74044
Epoch 153/300
 - 19s - loss: 0.8373 - acc: 0.9631 - mDice: 0.6954 - val_loss: 0.6781 - val_acc: 0.9788 - val_mDice: 0.7354

Epoch 00153: val_mDice did not improve from 0.74044
Epoch 154/300
 - 19s - loss: 0.8358 - acc: 0.9632 - mDice: 0.6958 - val_loss: 0.6806 - val_acc: 0.9785 - val_mDice: 0.7302

Epoch 00154: val_mDice did not improve from 0.74044
Epoch 155/300
 - 19s - loss: 0.8387 - acc: 0.9632 - mDice: 0.6948 - val_loss: 0.6905 - val_acc: 0.9785 - val_mDice: 0.7336

Epoch 00155: val_mDice did not improve from 0.74044
Epoch 156/300
 - 19s - loss: 0.8374 - acc: 0.9631 - mDice: 0.6954 - val_loss: 0.6649 - val_acc: 0.9789 - val_mDice: 0.7392

Epoch 00156: val_mDice did not improve from 0.74044
Epoch 157/300
 - 20s - loss: 0.8372 - acc: 0.9632 - mDice: 0.6955 - val_loss: 0.6798 - val_acc: 0.9787 - val_mDice: 0.7336

Epoch 00157: val_mDice did not improve from 0.74044
Epoch 158/300
 - 20s - loss: 0.8337 - acc: 0.9632 - mDice: 0.6967 - val_loss: 0.6676 - val_acc: 0.9788 - val_mDice: 0.7356

Epoch 00158: val_mDice did not improve from 0.74044
Epoch 159/300
 - 19s - loss: 0.8356 - acc: 0.9631 - mDice: 0.6958 - val_loss: 0.6795 - val_acc: 0.9788 - val_mDice: 0.7356

Epoch 00159: val_mDice did not improve from 0.74044
Epoch 160/300
 - 19s - loss: 0.8346 - acc: 0.9631 - mDice: 0.6968 - val_loss: 0.6663 - val_acc: 0.9783 - val_mDice: 0.7408

Epoch 00160: val_mDice improved from 0.74044 to 0.74085, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 161/300
 - 19s - loss: 0.8340 - acc: 0.9632 - mDice: 0.6964 - val_loss: 0.6756 - val_acc: 0.9785 - val_mDice: 0.7333

Epoch 00161: val_mDice did not improve from 0.74085
Epoch 162/300
 - 20s - loss: 0.8338 - acc: 0.9632 - mDice: 0.6970 - val_loss: 0.6643 - val_acc: 0.9787 - val_mDice: 0.7380

Epoch 00162: val_mDice did not improve from 0.74085
Epoch 163/300
 - 19s - loss: 0.8323 - acc: 0.9632 - mDice: 0.6973 - val_loss: 0.6700 - val_acc: 0.9786 - val_mDice: 0.7376

Epoch 00163: val_mDice did not improve from 0.74085
Epoch 164/300
 - 19s - loss: 0.8356 - acc: 0.9631 - mDice: 0.6965 - val_loss: 0.6646 - val_acc: 0.9788 - val_mDice: 0.7383

Epoch 00164: val_mDice did not improve from 0.74085
Epoch 165/300
 - 19s - loss: 0.8319 - acc: 0.9633 - mDice: 0.6971 - val_loss: 0.6860 - val_acc: 0.9784 - val_mDice: 0.7379

Epoch 00165: val_mDice did not improve from 0.74085
Epoch 166/300
 - 19s - loss: 0.8335 - acc: 0.9632 - mDice: 0.6970 - val_loss: 0.6746 - val_acc: 0.9788 - val_mDice: 0.7371

Epoch 00166: val_mDice did not improve from 0.74085
Epoch 167/300
 - 20s - loss: 0.8305 - acc: 0.9632 - mDice: 0.6976 - val_loss: 0.6765 - val_acc: 0.9786 - val_mDice: 0.7354

Epoch 00167: val_mDice did not improve from 0.74085
Epoch 168/300
 - 19s - loss: 0.8325 - acc: 0.9632 - mDice: 0.6977 - val_loss: 0.6597 - val_acc: 0.9789 - val_mDice: 0.7393

Epoch 00168: val_mDice did not improve from 0.74085
Epoch 169/300
 - 19s - loss: 0.8305 - acc: 0.9632 - mDice: 0.6981 - val_loss: 0.6902 - val_acc: 0.9783 - val_mDice: 0.7306

Epoch 00169: val_mDice did not improve from 0.74085
Epoch 170/300
 - 19s - loss: 0.8321 - acc: 0.9632 - mDice: 0.6975 - val_loss: 0.6732 - val_acc: 0.9786 - val_mDice: 0.7356

Epoch 00170: val_mDice did not improve from 0.74085
Epoch 171/300
 - 19s - loss: 0.8312 - acc: 0.9632 - mDice: 0.6978 - val_loss: 0.6558 - val_acc: 0.9791 - val_mDice: 0.7381

Epoch 00171: val_mDice did not improve from 0.74085
Epoch 172/300
 - 20s - loss: 0.8314 - acc: 0.9632 - mDice: 0.6978 - val_loss: 0.6714 - val_acc: 0.9783 - val_mDice: 0.7389

Epoch 00172: val_mDice did not improve from 0.74085
Epoch 173/300
 - 20s - loss: 0.8312 - acc: 0.9632 - mDice: 0.6981 - val_loss: 0.6805 - val_acc: 0.9784 - val_mDice: 0.7330

Epoch 00173: val_mDice did not improve from 0.74085
Epoch 174/300
 - 19s - loss: 0.8276 - acc: 0.9633 - mDice: 0.6989 - val_loss: 0.6613 - val_acc: 0.9789 - val_mDice: 0.7405

Epoch 00174: val_mDice did not improve from 0.74085
Epoch 175/300
 - 19s - loss: 0.8283 - acc: 0.9633 - mDice: 0.6988 - val_loss: 0.6727 - val_acc: 0.9786 - val_mDice: 0.7350

Epoch 00175: val_mDice did not improve from 0.74085
Epoch 176/300
 - 19s - loss: 0.8307 - acc: 0.9632 - mDice: 0.6981 - val_loss: 0.6621 - val_acc: 0.9789 - val_mDice: 0.7365

Epoch 00176: val_mDice did not improve from 0.74085
Epoch 177/300
 - 20s - loss: 0.8282 - acc: 0.9633 - mDice: 0.6987 - val_loss: 0.6636 - val_acc: 0.9789 - val_mDice: 0.7351

Epoch 00177: val_mDice did not improve from 0.74085
Epoch 178/300
 - 20s - loss: 0.8279 - acc: 0.9633 - mDice: 0.6991 - val_loss: 0.6776 - val_acc: 0.9786 - val_mDice: 0.7332

Epoch 00178: val_mDice did not improve from 0.74085
Epoch 179/300
 - 19s - loss: 0.8282 - acc: 0.9633 - mDice: 0.6987 - val_loss: 0.6666 - val_acc: 0.9789 - val_mDice: 0.7381

Epoch 00179: val_mDice did not improve from 0.74085
Epoch 180/300
 - 19s - loss: 0.8275 - acc: 0.9633 - mDice: 0.6993 - val_loss: 0.6690 - val_acc: 0.9788 - val_mDice: 0.7348

Epoch 00180: val_mDice did not improve from 0.74085
Epoch 181/300
 - 21s - loss: 0.8281 - acc: 0.9633 - mDice: 0.6986 - val_loss: 0.6796 - val_acc: 0.9783 - val_mDice: 0.7360

Epoch 00181: val_mDice did not improve from 0.74085
Epoch 182/300
 - 20s - loss: 0.8262 - acc: 0.9634 - mDice: 0.6995 - val_loss: 0.6720 - val_acc: 0.9783 - val_mDice: 0.7390

Epoch 00182: val_mDice did not improve from 0.74085
Epoch 183/300
 - 19s - loss: 0.8266 - acc: 0.9634 - mDice: 0.6995 - val_loss: 0.6762 - val_acc: 0.9787 - val_mDice: 0.7354

Epoch 00183: val_mDice did not improve from 0.74085
Epoch 184/300
 - 19s - loss: 0.8249 - acc: 0.9634 - mDice: 0.7002 - val_loss: 0.6781 - val_acc: 0.9788 - val_mDice: 0.7370

Epoch 00184: val_mDice did not improve from 0.74085
Epoch 185/300
 - 21s - loss: 0.8268 - acc: 0.9634 - mDice: 0.6992 - val_loss: 0.6635 - val_acc: 0.9786 - val_mDice: 0.7407

Epoch 00185: val_mDice did not improve from 0.74085
Epoch 186/300
 - 19s - loss: 0.8265 - acc: 0.9634 - mDice: 0.6995 - val_loss: 0.6711 - val_acc: 0.9786 - val_mDice: 0.7347

Epoch 00186: val_mDice did not improve from 0.74085
Epoch 187/300
 - 19s - loss: 0.8257 - acc: 0.9634 - mDice: 0.6998 - val_loss: 0.6835 - val_acc: 0.9785 - val_mDice: 0.7343

Epoch 00187: val_mDice did not improve from 0.74085
Epoch 188/300
 - 19s - loss: 0.8247 - acc: 0.9634 - mDice: 0.7000 - val_loss: 0.6659 - val_acc: 0.9789 - val_mDice: 0.7377

Epoch 00188: val_mDice did not improve from 0.74085
Epoch 189/300
 - 20s - loss: 0.8266 - acc: 0.9634 - mDice: 0.6993 - val_loss: 0.6744 - val_acc: 0.9786 - val_mDice: 0.7386

Epoch 00189: val_mDice did not improve from 0.74085
Epoch 190/300
 - 20s - loss: 0.8267 - acc: 0.9634 - mDice: 0.6995 - val_loss: 0.6631 - val_acc: 0.9790 - val_mDice: 0.7394

Epoch 00190: val_mDice did not improve from 0.74085
Restoring model weights from the end of the best epoch
Epoch 00190: early stopping
{'val_loss': [4.012624092023316, 3.19470776197711, 2.710921264654343, 2.270313621059414, 1.9653843049540485, 1.7199168110533518, 1.566262639959038, 1.4243645046148508, 1.3039370513798898, 1.2350965562373615, 1.122079284444559, 1.0688070699772476, 1.0078224779036515, 1.0026614340589266, 0.9525133548506273, 0.9599304707303751, 0.9117679536896225, 0.8821410952583801, 0.8628925452291412, 0.8550287942768251, 0.8561629693574581, 0.8385033709592002, 0.8293967833829, 0.8460258909177238, 0.8109904209649723, 0.8177278386304007, 0.8016589424804514, 0.7847451686982154, 0.8020175797528404, 0.7798909597844654, 0.7908038877727324, 0.7736324424345058, 0.7772949094988859, 0.7665259251899641, 0.7828570786398384, 0.7488357171923753, 0.7327390094782669, 0.7381825581792707, 0.7696207973364091, 0.7327358844787097, 0.7557768609194071, 0.7364875883079288, 0.7612636471926489, 0.7321935173035652, 0.721476543706268, 0.7296152534991718, 0.7236857802690736, 0.7229889148406077, 0.7275853559021118, 0.7217015167323428, 0.7233153304509949, 0.7114580821080596, 0.7198076517896879, 0.7271247449243524, 0.7108733188380152, 0.7162222110573107, 0.7238438039003142, 0.7105527964907903, 0.709056541677353, 0.707074038446011, 0.7079670883492174, 0.699032104944174, 0.7048556800043619, 0.7037807557851046, 0.7022677420401106, 0.6959622807424012, 0.6948688555551141, 0.7084969434698792, 0.696660474265692, 0.6943108942917134, 0.6931837360740816, 0.6993841603013876, 0.7080380996867969, 0.684093940953106, 0.7005380371468232, 0.7077511986650303, 0.6931668000201568, 0.6833707556451437, 0.6893536914735878, 0.6904757371012763, 0.6874627331954161, 0.7338951535453737, 0.6915352356200125, 0.6847271002114004, 0.6862453268100849, 0.6753092224568406, 0.6724667911743601, 0.6877207767729666, 0.6878770689169565, 0.6807232979835003, 0.6885299281562199, 0.6860524516541153, 0.6830202147010925, 0.6858076448288003, 0.6781468998229417, 0.6844531018729058, 0.6754460426429969, 0.6708497120869049, 0.6896261185622928, 0.6948210820940134, 0.6873625819835624, 0.6718977632537346, 0.675680247715014, 0.6760731150921899, 0.689836874373557, 0.6804440392736804, 0.6789572503421575, 0.6951543141014189, 0.6700474323687539, 0.6747864641948873, 0.674631381643815, 0.6747157832048257, 0.675348282290932, 0.6856985951171202, 0.6788814885020871, 0.6787439103712115, 0.6782837364019132, 0.6698043808356404, 0.6707298306304235, 0.6715939175356775, 0.677539377697235, 0.6900258516379554, 0.6781918591328573, 0.6726150550217328, 0.6793108697215593, 0.6788657538908062, 0.6703657658599601, 0.6664272947208062, 0.6767550365167013, 0.6837085378735919, 0.6962942909776119, 0.6643286732143662, 0.6769239367961392, 0.6805974860493982, 0.6717720447863588, 0.663473204747073, 0.6679719083080351, 0.6729150184106285, 0.6903166311751702, 0.6761498645916811, 0.6737279993154193, 0.6618310094371792, 0.6631903126758933, 0.6638742585361803, 0.6726334285686875, 0.6655985905228508, 0.6834168670654789, 0.6829563931661001, 0.680604018693861, 0.6650743216306925, 0.6866235530966944, 0.6619275727139169, 0.6780912518747327, 0.6805794453719329, 0.6904532583689911, 0.6648864008263292, 0.6797720508801802, 0.6676400355202741, 0.6794725423073252, 0.6663205361895271, 0.6755533257443592, 0.6643103671455285, 0.6700199967321351, 0.6645875348948842, 0.6860152769014931, 0.6746143660247633, 0.6764920078010382, 0.6597429268500384, 0.6902220520749304, 0.6732219523751206, 0.655783775421119, 0.6713959529796006, 0.6804861173233626, 0.6613189643873641, 0.6726596548276789, 0.6621000003088862, 0.6636465436041785, 0.6775917150349317, 0.6666448309141046, 0.6689868593671128, 0.6795860320421934, 0.6720025118967082, 0.6761538330677, 0.67809071318776, 0.6634568084873282, 0.6711006870395259, 0.6834677900999815, 0.6658662692312116, 0.6743795463605808, 0.6630881299236857], 'val_acc': [0.9217265926770505, 0.9134693407790949, 0.9261342729331293, 0.928778420291818, 0.9306590953847572, 0.9393764105620646, 0.9539124822345927, 0.9603936925638079, 0.9644031920792272, 0.9669173669888877, 0.9697642280836469, 0.9709189612922039, 0.9711851870681479, 0.9718413662984275, 0.9732567418839541, 0.972974394681653, 0.973237977123851, 0.972590354943054, 0.9736878295554957, 0.9743097883382941, 0.9743209750659695, 0.974126819917169, 0.9747994524283552, 0.974587525807175, 0.9749324080006626, 0.9750090796881047, 0.9741972446195605, 0.9752348299119987, 0.9755671936538074, 0.9768354714347359, 0.9757261455243587, 0.9761164342163763, 0.9766584257100266, 0.976570560897713, 0.976683116795724, 0.9768637690627784, 0.9773172445218506, 0.9774004932035479, 0.9762174653195006, 0.9770003426800817, 0.9766873807729951, 0.9771849581946775, 0.9768460046027098, 0.9769341917968017, 0.977427483650676, 0.9773517971314389, 0.9773909547995734, 0.9773639623225658, 0.9777137835205401, 0.9777983469490665, 0.9777786147975823, 0.9782218881435808, 0.9780280594978293, 0.9778151379896268, 0.9779454606851434, 0.9777078589787793, 0.9782577489797791, 0.9780701788217291, 0.9778664802865225, 0.9779855960785913, 0.9780069870722429, 0.9783591092924585, 0.9782903225798356, 0.978077416018928, 0.9782735488240072, 0.9788283790597236, 0.9786361901260628, 0.9780895848392333, 0.9786536214398402, 0.978085648226172, 0.9788576644394543, 0.9782116849232999, 0.9786467082237189, 0.9786328947704028, 0.9787148503573194, 0.9785068773878863, 0.9787576250862657, 0.9788573304320022, 0.9789185392352203, 0.978731303761249, 0.9783979419338199, 0.9776436988537279, 0.9784565188444313, 0.9784841627282378, 0.9788573301859561, 0.9790778225297406, 0.9791897001404265, 0.9785045637557873, 0.9783403433021254, 0.9789317020198756, 0.978740182700418, 0.9783140207468787, 0.9784038666601151, 0.9787368890055685, 0.9787391896587407, 0.9785766308152639, 0.9785275929852536, 0.9787408438260341, 0.9783913518376148, 0.9784989667012596, 0.9786727263585456, 0.9791001932170738, 0.9788671974919283, 0.9785279183810955, 0.9784232696635559, 0.9789290703118032, 0.978776054178106, 0.978156729071748, 0.9791646915938709, 0.9784308496643516, 0.9786266497537202, 0.9782751935185293, 0.9783334431264422, 0.978581571972653, 0.9787898848546425, 0.9788152018813772, 0.9787770357171325, 0.9786789821397409, 0.979174230797495, 0.9789112991469809, 0.9784443305864915, 0.9778470547698722, 0.9784637548729116, 0.9789610001324869, 0.9781455326252555, 0.9787375514229262, 0.9789182087954353, 0.9786796336080513, 0.9785394447137696, 0.9785140993302328, 0.9782037771277138, 0.9787217510866061, 0.9784548695365465, 0.9785993366787678, 0.9788431814334464, 0.9787474034125345, 0.9791877219920557, 0.9785960469821658, 0.9784196667503893, 0.9788102777626738, 0.9787194501873878, 0.9788767517043587, 0.9787243926980301, 0.9789754777254827, 0.9786579098986891, 0.978784605383996, 0.9783933441951425, 0.9784127579630959, 0.9786756776803788, 0.9788461495479194, 0.9784117547105095, 0.978993577484745, 0.9787605801603004, 0.978487119586106, 0.9785157439632436, 0.9788589722354838, 0.9786881810002283, 0.9788319935370525, 0.9787836142491753, 0.9783087480424973, 0.9785335212177044, 0.9786980571638566, 0.9786493632446501, 0.9787661695258906, 0.9784236087764626, 0.9787678296597996, 0.9785812356892754, 0.9789405984283108, 0.9782541370859333, 0.9785851877417234, 0.9790709080833891, 0.9783192885918514, 0.97842229593649, 0.9789251136459926, 0.978644098290718, 0.9789284028505024, 0.9789047154479721, 0.9785670953023299, 0.9788905706937099, 0.9788306711012857, 0.9782541351175653, 0.9782617006631582, 0.9786615257907824, 0.9788254138362912, 0.9786223719978726, 0.9786227025606807, 0.9784680323212016, 0.978867537958088, 0.9786292777711024, 0.9789705403818065], 'val_mDice': [0.032984009967930425, 0.06924611334645711, 0.12363208453598652, 0.1964595864062708, 0.26151460833593787, 0.3206644413456459, 0.3690371462927268, 0.4106314404338014, 0.4587519894689475, 0.49008065855786037, 0.5293676497028339, 0.5543310037584374, 0.576348931366199, 0.5846772768426114, 0.6020852129279768, 0.606078684760567, 0.6216376385698623, 0.639000770292784, 0.6414091929070598, 0.6465630570063281, 0.6529945362709132, 0.653977723682628, 0.6606580455113736, 0.6566470915565058, 0.6730280898057762, 0.6735455994758567, 0.6774230847294732, 0.6870583578775049, 0.6846353455347666, 0.686108179198207, 0.6831653053546468, 0.6913685424039977, 0.6870463134334552, 0.6892300863876185, 0.6901610888563812, 0.6951606723300198, 0.7009417163698297, 0.6993776625031903, 0.6950973443325582, 0.7037855793325033, 0.7000581654109699, 0.7042475701362602, 0.6985839605331421, 0.7040269627044567, 0.7061405232939312, 0.7063370249219724, 0.7096542872880635, 0.7113922091337427, 0.7101765345124638, 0.7129663829956016, 0.7134044869150293, 0.7141789786586821, 0.7144064353346455, 0.7099025740958097, 0.7168649820105333, 0.7148648157208327, 0.7134678168193475, 0.7126630087632021, 0.7187075555878158, 0.715655874726204, 0.7164203496909363, 0.7189994544190642, 0.7209080163047287, 0.7192055144423178, 0.7199682640094384, 0.7201939906129158, 0.722908705749748, 0.721339694117614, 0.719653518453348, 0.7210329141779211, 0.7225228256485411, 0.7165773545994478, 0.7234300700380584, 0.7220629258052483, 0.7227093573079144, 0.7187643513098836, 0.7236933665747982, 0.725614003168171, 0.7261047528132073, 0.723821480453814, 0.7260737195226553, 0.7142496811470134, 0.7252131836456165, 0.7249352949199539, 0.7252676379938982, 0.7311601662783431, 0.7295668101778218, 0.7278289261002043, 0.7270945653211713, 0.7285529461807511, 0.7267285705965985, 0.7294202100627809, 0.7294337079620952, 0.7304569447126674, 0.7324849931202191, 0.7295413209188834, 0.7291214950805601, 0.729380165829378, 0.7274229189436748, 0.7297244718446328, 0.7283820536975644, 0.7319406244530889, 0.7309862002499702, 0.7317991053725913, 0.7292170760186218, 0.7313897409183199, 0.7315802873718726, 0.7309417899302039, 0.7350621049987274, 0.7303507233920851, 0.7322705450810885, 0.732288470024664, 0.7360494357636592, 0.7309011325501559, 0.7359349653201699, 0.7300708191805703, 0.732547799807707, 0.7331205727638229, 0.7330964422939485, 0.7326263079825324, 0.7328990215610548, 0.7267083847977921, 0.7296270138215969, 0.734717514000687, 0.7343465098409584, 0.7305272884659231, 0.7348322203407839, 0.7354811342631085, 0.7313859979434648, 0.7316152263967123, 0.7277788488488448, 0.7358310743628148, 0.7316733651608998, 0.7338979000031518, 0.7341360070264992, 0.7346126087186753, 0.7353168924034441, 0.7365700533023437, 0.7319140515460318, 0.7326885524548983, 0.7345624836113677, 0.7404369607921478, 0.7353829529263287, 0.7342220493145403, 0.7352635578720439, 0.7351183859310406, 0.7359941713704167, 0.7320944832328427, 0.7370985791287062, 0.7385054250988798, 0.7338541501323751, 0.7374759074089082, 0.7354293161132387, 0.7302468445524958, 0.7336268725163919, 0.7392414631120192, 0.7336382498066983, 0.7356380187321004, 0.7355715841577765, 0.7408462879089379, 0.7333250018096191, 0.7379691717671413, 0.7376328881433997, 0.7383408478785103, 0.7378609420345295, 0.7370501510622085, 0.7354274376750115, 0.7393215408758236, 0.7305651232677102, 0.7356170439252666, 0.738075825882647, 0.7388829915516147, 0.7329905763253093, 0.7405472273551028, 0.7349942448953602, 0.7364516726957386, 0.7351469060461834, 0.7332050703750429, 0.73805755672071, 0.7348406058346894, 0.7359842384931842, 0.738979675390895, 0.7354271153547446, 0.7369647953040337, 0.7407167045201557, 0.7346977015766936, 0.7343416063901195, 0.737740485045686, 0.7386121288541669, 0.7393684157769131], 'loss': [50.30936203322947, 5.503518383571861, 4.1117988772741905, 3.5122436800349264, 3.0653512652973185, 2.7590679622989067, 2.530395774645426, 2.349328213128663, 2.1765889345759675, 2.022873317665777, 1.897562749863582, 1.7801360393413246, 1.667979175631061, 1.5703505565763118, 1.5031703494316586, 1.451038042144329, 1.4046574718262603, 1.3632617039045, 1.3277356322238176, 1.2982576115335802, 1.2647214226259205, 1.2319043756015322, 1.2097070180760496, 1.1852578380696097, 1.1671955271299055, 1.14496162488206, 1.1293804060738042, 1.1161422987584428, 1.1060115085785276, 1.0878395809815782, 1.0780147260260917, 1.0666056656242413, 1.0570600970650676, 1.0502646719039925, 1.0436286415636953, 1.0335532308180007, 1.0260086304423404, 1.0207177501306177, 1.0131477230000046, 1.0059473456617811, 1.0039314897982294, 0.9932746426826279, 0.9917341845400555, 0.9886377763642825, 0.9843563764739327, 0.9795177813600691, 0.971057192654891, 0.9676884362442498, 0.9646220973092046, 0.9612827309922669, 0.954941293000036, 0.9511447046490367, 0.9494492689635607, 0.9444992972610388, 0.9432414786354978, 0.941487106818244, 0.9355101690091351, 0.9337479069433948, 0.9333232592890078, 0.9293326706954054, 0.9268449554088607, 0.9226262150401285, 0.9199870436180739, 0.9185419526509007, 0.9181773695593834, 0.9167562072163753, 0.9128057497880725, 0.9122341832754177, 0.9114811096722111, 0.9079458442128926, 0.9056128759941623, 0.9067211777989665, 0.9039165661902709, 0.9019186651120187, 0.9009315918304484, 0.899892762876337, 0.8948160217247916, 0.8974168503201888, 0.8927138297361948, 0.8956221310981908, 0.8940927979767906, 0.8900647254641596, 0.8906219298910177, 0.8879882782975272, 0.8865505636720632, 0.8870498340355345, 0.8847659146597826, 0.8843125942885406, 0.8817295390593854, 0.8826050250576366, 0.8802088188732157, 0.879915556917334, 0.8776836878634992, 0.8753239572873854, 0.8758109612228023, 0.8754082207666082, 0.8765190330060194, 0.8726216322529535, 0.8742992553102791, 0.8703459964992609, 0.8702809436383326, 0.8713699312818799, 0.8687809856987979, 0.8685449681462551, 0.8663239359200812, 0.8660706139234361, 0.8645184635760137, 0.8658609781881378, 0.8656130888765571, 0.8635832553963858, 0.8626990227261392, 0.8633971875468957, 0.8610955743892739, 0.8629316414259544, 0.8592603814994803, 0.8585524524357201, 0.8576092099001734, 0.8592337732998944, 0.8565101152548849, 0.8572223574912489, 0.8561328185225319, 0.855864600695942, 0.8561683414316644, 0.8533740691006084, 0.8530941836754804, 0.8528143036243082, 0.8539523606813406, 0.8525711270158206, 0.8492392959530951, 0.8501489715926939, 0.8495592876026731, 0.8489190471579472, 0.8486622343890161, 0.8500081672712873, 0.8491871792076595, 0.8488755188837864, 0.8463637952178766, 0.8486685808957093, 0.8456311412022169, 0.8447827430095427, 0.8455397547045841, 0.8434781105870816, 0.8426316849067041, 0.8435422769659369, 0.8446365622836933, 0.841953243265751, 0.8402714365018932, 0.8408871284597723, 0.8407632220978665, 0.8395035878724234, 0.8397016914826975, 0.8377873931789216, 0.8373042017413632, 0.8357941711008734, 0.8386663570021399, 0.8374465142154511, 0.8372119465918025, 0.8336833343034374, 0.835603522370361, 0.8346276788487194, 0.8340073572954415, 0.8338245191863365, 0.8322556813178326, 0.8356351097571927, 0.831882056378225, 0.8335424079896728, 0.8304731424978874, 0.8324715018357813, 0.8305175748564959, 0.8321288019198064, 0.8312342529197795, 0.831428321884958, 0.8311755110831998, 0.8276082065542757, 0.8282552602782521, 0.8306589264672669, 0.8281553312762415, 0.8279214359238007, 0.8281881265860057, 0.8274722502813551, 0.8281052301201639, 0.8261511139989839, 0.8265946668814952, 0.8248556787995583, 0.8268023899938544, 0.8264813245709426, 0.8256892808545083, 0.8246661523716928, 0.8265643883981425, 0.8266754967943706], 'acc': [0.843705827523764, 0.9021316194821862, 0.9063424400492331, 0.9116422723465385, 0.914547481128587, 0.9190301972939401, 0.9239489782645192, 0.928892351888033, 0.9336883300510089, 0.9378372405177231, 0.9409201755952459, 0.9436018857504378, 0.9454035006974232, 0.9468560023853879, 0.948052041287749, 0.9492246329656272, 0.9502418516343893, 0.9512769433544962, 0.9520723216426039, 0.9526750897561163, 0.9534033620260711, 0.9535478356916921, 0.9542767703177282, 0.955243257113052, 0.9557410218221976, 0.9560092255910883, 0.956186970990798, 0.956470911247166, 0.9567618666513455, 0.9571054184425409, 0.9573076128290581, 0.9576034871823526, 0.9577184040192804, 0.9578746344555185, 0.9579853990751603, 0.9580866424827266, 0.9581787561584493, 0.9582378366966079, 0.9583074267991892, 0.9583858803274069, 0.9583465152319787, 0.9586307034305721, 0.9586076437928723, 0.958637688868593, 0.9586973513883823, 0.9586692587811707, 0.9588216056765649, 0.9588700882255932, 0.9590175530279568, 0.9591775314105221, 0.9594350174756274, 0.9596141889239371, 0.9598263030778772, 0.959996708870959, 0.9600661851168533, 0.9602757266335366, 0.9604102421382005, 0.9604786320620401, 0.9605782941314751, 0.960748444223905, 0.9607762231059765, 0.9609594343669209, 0.9610604020158118, 0.9611179863022881, 0.9611420893842804, 0.9612502486104466, 0.9613241220406026, 0.9614430267798864, 0.9614442725553984, 0.961492963293614, 0.9615834232358338, 0.9616200548312189, 0.961677223123683, 0.961760408060631, 0.9618191828100215, 0.9618291776562352, 0.9619603340256205, 0.961923254299255, 0.9620525911610537, 0.9620792563672791, 0.9620141898614852, 0.9621385852889682, 0.9621505895249166, 0.9622606295644739, 0.9622746515464805, 0.9622910912534574, 0.9623462595018875, 0.96240395944586, 0.9624514350365533, 0.9624782140567589, 0.9625088778491917, 0.9624509609112855, 0.9624741302132749, 0.9625064510489979, 0.9624754268385102, 0.9624559955677213, 0.9624007034264745, 0.962423493789835, 0.9624296914130371, 0.9625108856445341, 0.9625267270914205, 0.9624757582372032, 0.9625837714412431, 0.9625928820973226, 0.9626533238691904, 0.9626519410186546, 0.96272984421108, 0.9626374349321931, 0.9626809504868052, 0.9627110322549003, 0.9627501301259223, 0.9627420387918351, 0.9628199480415273, 0.9627713114058798, 0.9628818655799846, 0.9629171559854485, 0.9629107303000989, 0.9628396929591603, 0.9629558830606303, 0.9629428980385799, 0.9629382917825208, 0.9629569986511706, 0.9629954481739857, 0.9629737445739849, 0.9629946489848867, 0.963028159002316, 0.9629597964963282, 0.9630285969619282, 0.9630650956533335, 0.9630386773643671, 0.9630586783456608, 0.9630146295237946, 0.9630539396553982, 0.9629989889347386, 0.963050101177488, 0.9630346788778094, 0.9631287655640765, 0.9630614216896524, 0.963111885427603, 0.9631281835041497, 0.9630997964183897, 0.9630891432661358, 0.9631360281701016, 0.9631530318648541, 0.9630897033960517, 0.9631370463247712, 0.9631413123418017, 0.9631105806809324, 0.9630982343553216, 0.9631453768461622, 0.9631269684776305, 0.9631848193265556, 0.9631483659685612, 0.9631895762170901, 0.963174507139348, 0.9631454326925968, 0.9631921000687923, 0.9631657582750632, 0.9631270232421559, 0.9631426728993178, 0.9632031911094803, 0.9631756509378185, 0.9631599622038118, 0.9631400162859922, 0.9632530840336636, 0.9631788671614591, 0.9632455065280576, 0.963177992224494, 0.9632356073311413, 0.9631923840983774, 0.9632212680864368, 0.963213320055468, 0.9632464307630606, 0.9633187130396081, 0.9633005306121128, 0.9632496091625928, 0.9633146107396107, 0.963347208060027, 0.9632773973761295, 0.9633064716306996, 0.9632960576586317, 0.9634017958002157, 0.9633845353086666, 0.9633769375528559, 0.9633711010025485, 0.9633965305120984, 0.9634312039375988, 0.9634178932538202, 0.9634022795131902, 0.963449678701178], 'mDice': [0.01807214604952696, 0.04419654395483774, 0.07809399583685903, 0.12230485466497737, 0.1704743406494022, 0.21195203006972213, 0.24756964926029806, 0.28097199604666145, 0.31752935536641735, 0.35311350649912715, 0.38293849308678096, 0.4120171085686908, 0.437827644742899, 0.46161930470526974, 0.4791759423838614, 0.4950255382906602, 0.5082419364754727, 0.5213208755018719, 0.5313179885925414, 0.5405342137369415, 0.5508469369307833, 0.5612742963769939, 0.56882644693294, 0.576453657267855, 0.5828458420880689, 0.5894440236212901, 0.595083460564086, 0.5996592101630545, 0.6030253232422267, 0.6088197612309758, 0.6124070584845196, 0.6158071658279323, 0.6194427312844005, 0.621989903428374, 0.623956288959976, 0.6268973884575118, 0.6300460122025665, 0.6317757812951669, 0.6341575432612385, 0.6365812686838055, 0.6376302817470281, 0.6412569950880799, 0.6414365697183215, 0.6429366303893869, 0.6442503821801422, 0.6461137637108073, 0.6488790370796186, 0.6505515001143024, 0.651625664266334, 0.6523476270636255, 0.6552814590580742, 0.6555560198324462, 0.6568621560514813, 0.6583312279303432, 0.658617962615314, 0.6595132979567705, 0.6612945108331699, 0.6620420604652398, 0.6621653066064112, 0.6634692363930341, 0.6642356142312722, 0.6656513616339472, 0.6664834236981961, 0.6671073863806405, 0.667368267668384, 0.6680776752177416, 0.6693851289314674, 0.6697847932356481, 0.669786616416031, 0.6709803928331285, 0.6715535397585976, 0.6716020818909171, 0.6720187600113937, 0.6731215820894453, 0.6734832399267839, 0.6735959550504158, 0.67537876258951, 0.674163768420733, 0.6764432102448905, 0.6751436843973428, 0.6755251867520032, 0.676983344297236, 0.676996951381375, 0.6780522508911053, 0.6785491820961301, 0.6777532436852243, 0.6792325123713926, 0.6787340571195891, 0.6797247573302702, 0.6800262425063702, 0.6806188017621028, 0.680885179005234, 0.6816570545783156, 0.6825017509552697, 0.682128235708423, 0.6819245162288072, 0.6820451371248963, 0.68335122915649, 0.6825599333096553, 0.6840929470356649, 0.6838248215770562, 0.6837066917978041, 0.6845020287740706, 0.6845119326046892, 0.6854999976338081, 0.6856580183589718, 0.6858192415917321, 0.6855204703060744, 0.6856949274782523, 0.6866231704914174, 0.6866838698501601, 0.6865063535873891, 0.6869753469408968, 0.686433704137973, 0.6877536123610921, 0.6880600769166193, 0.6882838030077263, 0.6878723742243268, 0.6886135421378361, 0.6882688312306163, 0.6888666978123252, 0.6885749757119058, 0.6890255725338205, 0.6897445985175898, 0.6897235679361682, 0.6898987069886141, 0.6893825794436467, 0.690215745510621, 0.6908631900599899, 0.6907844615987642, 0.690682602193494, 0.6913246815347546, 0.691202414905595, 0.6905926242417357, 0.6913397855182978, 0.6912294718981188, 0.692226493688879, 0.6910640799204888, 0.6922820208677393, 0.69267308472135, 0.6921169699476873, 0.6931578106404148, 0.6932575045193982, 0.6931048569387145, 0.6925915903170466, 0.6936573064395571, 0.6943988235178442, 0.6941351630122843, 0.6940742035184865, 0.6944010936191778, 0.6946004939187476, 0.695354449121201, 0.6953853114154648, 0.6957964966483352, 0.6947596989214377, 0.6954268172030976, 0.6955080090999489, 0.6967465675244446, 0.6957955044452462, 0.696828036518122, 0.6964039324162427, 0.6969986983036864, 0.6972789701980995, 0.6964944551518916, 0.6971285729667922, 0.6970143950096542, 0.6976372898763821, 0.6976764477720003, 0.6980519057217036, 0.6974857003955873, 0.6977621779354711, 0.6977806793410595, 0.6981255742297526, 0.6989366976748453, 0.6988222604723799, 0.6981301831478773, 0.6986655611353556, 0.6990860074879532, 0.6986510847017422, 0.6992706862303417, 0.6986490336442751, 0.6994734030223161, 0.6995158433657943, 0.7001528389376446, 0.6991637561809256, 0.6995324972195759, 0.6998286097138544, 0.7000196945391095, 0.6993282439673405, 0.6995272203436438]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:02<00:28,  2.03s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:25,  1.95s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:23,  1.99s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:21,  1.99s/it]predicting test subjects:  33%|███▎      | 5/15 [00:10<00:21,  2.15s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:19,  2.22s/it]predicting test subjects:  47%|████▋     | 7/15 [00:14<00:16,  2.03s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:16<00:15,  2.18s/it]predicting test subjects:  60%|██████    | 9/15 [00:18<00:12,  2.13s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:20<00:09,  1.99s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:22<00:07,  1.93s/it]predicting test subjects:  80%|████████  | 12/15 [00:24<00:05,  1.96s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:26<00:03,  1.99s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:28<00:01,  1.94s/it]predicting test subjects: 100%|██████████| 15/15 [00:30<00:00,  1.93s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:11,  2.28s/it]predicting train subjects:   0%|          | 2/532 [00:03<18:35,  2.11s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:56,  2.03s/it]predicting train subjects:   1%|          | 4/532 [00:07<17:13,  1.96s/it]predicting train subjects:   1%|          | 5/532 [00:09<16:54,  1.92s/it]predicting train subjects:   1%|          | 6/532 [00:11<16:28,  1.88s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<16:05,  1.84s/it]predicting train subjects:   2%|▏         | 8/532 [00:14<15:41,  1.80s/it]predicting train subjects:   2%|▏         | 9/532 [00:16<16:23,  1.88s/it]predicting train subjects:   2%|▏         | 10/532 [00:18<16:01,  1.84s/it]predicting train subjects:   2%|▏         | 11/532 [00:20<15:26,  1.78s/it]predicting train subjects:   2%|▏         | 12/532 [00:22<16:36,  1.92s/it]predicting train subjects:   2%|▏         | 13/532 [00:23<15:41,  1.81s/it]predicting train subjects:   3%|▎         | 14/532 [00:25<14:49,  1.72s/it]predicting train subjects:   3%|▎         | 15/532 [00:27<15:04,  1.75s/it]predicting train subjects:   3%|▎         | 16/532 [00:29<15:28,  1.80s/it]predicting train subjects:   3%|▎         | 17/532 [00:30<15:01,  1.75s/it]predicting train subjects:   3%|▎         | 18/532 [00:32<15:47,  1.84s/it]predicting train subjects:   4%|▎         | 19/532 [00:34<14:45,  1.73s/it]predicting train subjects:   4%|▍         | 20/532 [00:36<14:52,  1.74s/it]predicting train subjects:   4%|▍         | 21/532 [00:38<15:40,  1.84s/it]predicting train subjects:   4%|▍         | 22/532 [00:39<15:00,  1.77s/it]predicting train subjects:   4%|▍         | 23/532 [00:41<15:05,  1.78s/it]predicting train subjects:   5%|▍         | 24/532 [00:43<14:26,  1.71s/it]predicting train subjects:   5%|▍         | 25/532 [00:45<15:45,  1.87s/it]predicting train subjects:   5%|▍         | 26/532 [00:47<15:27,  1.83s/it]predicting train subjects:   5%|▌         | 27/532 [00:49<16:49,  2.00s/it]predicting train subjects:   5%|▌         | 28/532 [00:51<16:28,  1.96s/it]predicting train subjects:   5%|▌         | 29/532 [00:53<16:44,  2.00s/it]predicting train subjects:   6%|▌         | 30/532 [00:55<15:54,  1.90s/it]predicting train subjects:   6%|▌         | 31/532 [00:56<15:42,  1.88s/it]predicting train subjects:   6%|▌         | 32/532 [00:58<15:33,  1.87s/it]predicting train subjects:   6%|▌         | 33/532 [01:00<14:49,  1.78s/it]predicting train subjects:   6%|▋         | 34/532 [01:02<15:51,  1.91s/it]predicting train subjects:   7%|▋         | 35/532 [01:04<15:31,  1.87s/it]predicting train subjects:   7%|▋         | 36/532 [01:06<15:41,  1.90s/it]predicting train subjects:   7%|▋         | 37/532 [01:08<15:40,  1.90s/it]predicting train subjects:   7%|▋         | 38/532 [01:10<15:59,  1.94s/it]predicting train subjects:   7%|▋         | 39/532 [01:12<15:51,  1.93s/it]predicting train subjects:   8%|▊         | 40/532 [01:13<15:24,  1.88s/it]predicting train subjects:   8%|▊         | 41/532 [01:15<15:44,  1.92s/it]predicting train subjects:   8%|▊         | 42/532 [01:17<15:35,  1.91s/it]predicting train subjects:   8%|▊         | 43/532 [01:19<14:37,  1.79s/it]predicting train subjects:   8%|▊         | 44/532 [01:20<13:53,  1.71s/it]predicting train subjects:   8%|▊         | 45/532 [01:22<13:45,  1.70s/it]predicting train subjects:   9%|▊         | 46/532 [01:24<14:18,  1.77s/it]predicting train subjects:   9%|▉         | 47/532 [01:26<15:21,  1.90s/it]predicting train subjects:   9%|▉         | 48/532 [01:28<15:34,  1.93s/it]predicting train subjects:   9%|▉         | 49/532 [01:30<15:03,  1.87s/it]predicting train subjects:   9%|▉         | 50/532 [01:32<15:44,  1.96s/it]predicting train subjects:  10%|▉         | 51/532 [01:34<15:24,  1.92s/it]predicting train subjects:  10%|▉         | 52/532 [01:36<15:13,  1.90s/it]predicting train subjects:  10%|▉         | 53/532 [01:38<14:47,  1.85s/it]predicting train subjects:  10%|█         | 54/532 [01:40<15:29,  1.94s/it]predicting train subjects:  10%|█         | 55/532 [01:42<15:32,  1.96s/it]predicting train subjects:  11%|█         | 56/532 [01:44<15:28,  1.95s/it]predicting train subjects:  11%|█         | 57/532 [01:45<15:11,  1.92s/it]predicting train subjects:  11%|█         | 58/532 [01:47<15:13,  1.93s/it]predicting train subjects:  11%|█         | 59/532 [01:50<15:55,  2.02s/it]predicting train subjects:  11%|█▏        | 60/532 [01:51<14:39,  1.86s/it]predicting train subjects:  11%|█▏        | 61/532 [01:53<13:56,  1.78s/it]predicting train subjects:  12%|█▏        | 62/532 [01:55<14:34,  1.86s/it]predicting train subjects:  12%|█▏        | 63/532 [01:57<15:08,  1.94s/it]predicting train subjects:  12%|█▏        | 64/532 [01:59<14:38,  1.88s/it]predicting train subjects:  12%|█▏        | 65/532 [02:00<14:32,  1.87s/it]predicting train subjects:  12%|█▏        | 66/532 [02:03<15:47,  2.03s/it]predicting train subjects:  13%|█▎        | 67/532 [02:05<16:17,  2.10s/it]predicting train subjects:  13%|█▎        | 68/532 [02:07<15:57,  2.06s/it]predicting train subjects:  13%|█▎        | 69/532 [02:09<15:18,  1.98s/it]predicting train subjects:  13%|█▎        | 70/532 [02:11<14:46,  1.92s/it]predicting train subjects:  13%|█▎        | 71/532 [02:12<14:07,  1.84s/it]predicting train subjects:  14%|█▎        | 72/532 [02:14<13:34,  1.77s/it]predicting train subjects:  14%|█▎        | 73/532 [02:16<13:55,  1.82s/it]predicting train subjects:  14%|█▍        | 74/532 [02:18<15:04,  1.97s/it]predicting train subjects:  14%|█▍        | 75/532 [02:21<17:07,  2.25s/it]predicting train subjects:  14%|█▍        | 76/532 [02:23<15:59,  2.11s/it]predicting train subjects:  14%|█▍        | 77/532 [02:25<15:35,  2.06s/it]predicting train subjects:  15%|█▍        | 78/532 [02:27<15:21,  2.03s/it]predicting train subjects:  15%|█▍        | 79/532 [02:29<15:14,  2.02s/it]predicting train subjects:  15%|█▌        | 80/532 [02:31<14:55,  1.98s/it]predicting train subjects:  15%|█▌        | 81/532 [02:33<14:37,  1.95s/it]predicting train subjects:  15%|█▌        | 82/532 [02:34<14:29,  1.93s/it]predicting train subjects:  16%|█▌        | 83/532 [02:36<13:59,  1.87s/it]predicting train subjects:  16%|█▌        | 84/532 [02:38<13:22,  1.79s/it]predicting train subjects:  16%|█▌        | 85/532 [02:39<13:08,  1.76s/it]predicting train subjects:  16%|█▌        | 86/532 [02:41<12:49,  1.72s/it]predicting train subjects:  16%|█▋        | 87/532 [02:43<12:34,  1.70s/it]predicting train subjects:  17%|█▋        | 88/532 [02:44<12:18,  1.66s/it]predicting train subjects:  17%|█▋        | 89/532 [02:46<12:50,  1.74s/it]predicting train subjects:  17%|█▋        | 90/532 [02:48<12:51,  1.75s/it]predicting train subjects:  17%|█▋        | 91/532 [02:50<12:58,  1.77s/it]predicting train subjects:  17%|█▋        | 92/532 [02:52<13:15,  1.81s/it]predicting train subjects:  17%|█▋        | 93/532 [02:54<13:18,  1.82s/it]predicting train subjects:  18%|█▊        | 94/532 [02:55<13:20,  1.83s/it]predicting train subjects:  18%|█▊        | 95/532 [02:58<14:07,  1.94s/it]predicting train subjects:  18%|█▊        | 96/532 [03:00<14:32,  2.00s/it]predicting train subjects:  18%|█▊        | 97/532 [03:02<14:50,  2.05s/it]predicting train subjects:  18%|█▊        | 98/532 [03:04<15:08,  2.09s/it]predicting train subjects:  19%|█▊        | 99/532 [03:06<15:12,  2.11s/it]predicting train subjects:  19%|█▉        | 100/532 [03:08<15:20,  2.13s/it]predicting train subjects:  19%|█▉        | 101/532 [03:10<14:27,  2.01s/it]predicting train subjects:  19%|█▉        | 102/532 [03:12<13:45,  1.92s/it]predicting train subjects:  19%|█▉        | 103/532 [03:14<13:14,  1.85s/it]predicting train subjects:  20%|█▉        | 104/532 [03:15<12:53,  1.81s/it]predicting train subjects:  20%|█▉        | 105/532 [03:17<12:48,  1.80s/it]predicting train subjects:  20%|█▉        | 106/532 [03:19<12:40,  1.78s/it]predicting train subjects:  20%|██        | 107/532 [03:20<12:25,  1.75s/it]predicting train subjects:  20%|██        | 108/532 [03:22<12:12,  1.73s/it]predicting train subjects:  20%|██        | 109/532 [03:24<12:07,  1.72s/it]predicting train subjects:  21%|██        | 110/532 [03:25<11:56,  1.70s/it]predicting train subjects:  21%|██        | 111/532 [03:27<11:50,  1.69s/it]predicting train subjects:  21%|██        | 112/532 [03:29<11:48,  1.69s/it]predicting train subjects:  21%|██        | 113/532 [03:31<12:30,  1.79s/it]predicting train subjects:  21%|██▏       | 114/532 [03:33<12:49,  1.84s/it]predicting train subjects:  22%|██▏       | 115/532 [03:35<13:08,  1.89s/it]predicting train subjects:  22%|██▏       | 116/532 [03:37<13:14,  1.91s/it]predicting train subjects:  22%|██▏       | 117/532 [03:39<13:32,  1.96s/it]predicting train subjects:  22%|██▏       | 118/532 [03:41<13:27,  1.95s/it]predicting train subjects:  22%|██▏       | 119/532 [03:43<13:14,  1.92s/it]predicting train subjects:  23%|██▎       | 120/532 [03:45<13:06,  1.91s/it]predicting train subjects:  23%|██▎       | 121/532 [03:46<12:55,  1.89s/it]predicting train subjects:  23%|██▎       | 122/532 [03:48<12:53,  1.89s/it]predicting train subjects:  23%|██▎       | 123/532 [03:50<12:47,  1.88s/it]predicting train subjects:  23%|██▎       | 124/532 [03:52<12:31,  1.84s/it]predicting train subjects:  23%|██▎       | 125/532 [03:54<12:48,  1.89s/it]predicting train subjects:  24%|██▎       | 126/532 [03:56<12:47,  1.89s/it]predicting train subjects:  24%|██▍       | 127/532 [03:58<12:57,  1.92s/it]predicting train subjects:  24%|██▍       | 128/532 [04:00<13:02,  1.94s/it]predicting train subjects:  24%|██▍       | 129/532 [04:02<13:07,  1.95s/it]predicting train subjects:  24%|██▍       | 130/532 [04:04<13:09,  1.96s/it]predicting train subjects:  25%|██▍       | 131/532 [04:06<13:37,  2.04s/it]predicting train subjects:  25%|██▍       | 132/532 [04:08<14:04,  2.11s/it]predicting train subjects:  25%|██▌       | 133/532 [04:11<14:35,  2.19s/it]predicting train subjects:  25%|██▌       | 134/532 [04:13<14:53,  2.25s/it]predicting train subjects:  25%|██▌       | 135/532 [04:15<15:04,  2.28s/it]predicting train subjects:  26%|██▌       | 136/532 [04:18<15:01,  2.28s/it]predicting train subjects:  26%|██▌       | 137/532 [04:20<15:04,  2.29s/it]predicting train subjects:  26%|██▌       | 138/532 [04:22<15:07,  2.30s/it]predicting train subjects:  26%|██▌       | 139/532 [04:25<15:01,  2.29s/it]predicting train subjects:  26%|██▋       | 140/532 [04:27<14:58,  2.29s/it]predicting train subjects:  27%|██▋       | 141/532 [04:29<14:58,  2.30s/it]predicting train subjects:  27%|██▋       | 142/532 [04:31<14:53,  2.29s/it]predicting train subjects:  27%|██▋       | 143/532 [04:33<13:42,  2.11s/it]predicting train subjects:  27%|██▋       | 144/532 [04:35<12:57,  2.00s/it]predicting train subjects:  27%|██▋       | 145/532 [04:37<12:27,  1.93s/it]predicting train subjects:  27%|██▋       | 146/532 [04:38<12:01,  1.87s/it]predicting train subjects:  28%|██▊       | 147/532 [04:40<11:42,  1.83s/it]predicting train subjects:  28%|██▊       | 148/532 [04:42<11:28,  1.79s/it]predicting train subjects:  28%|██▊       | 149/532 [04:44<11:37,  1.82s/it]predicting train subjects:  28%|██▊       | 150/532 [04:45<11:33,  1.81s/it]predicting train subjects:  28%|██▊       | 151/532 [04:47<11:53,  1.87s/it]predicting train subjects:  29%|██▊       | 152/532 [04:49<11:50,  1.87s/it]predicting train subjects:  29%|██▉       | 153/532 [04:51<11:48,  1.87s/it]predicting train subjects:  29%|██▉       | 154/532 [04:53<11:47,  1.87s/it]predicting train subjects:  29%|██▉       | 155/532 [04:55<12:29,  1.99s/it]predicting train subjects:  29%|██▉       | 156/532 [04:58<13:04,  2.09s/it]predicting train subjects:  30%|██▉       | 157/532 [05:00<13:38,  2.18s/it]predicting train subjects:  30%|██▉       | 158/532 [05:02<13:51,  2.22s/it]predicting train subjects:  30%|██▉       | 159/532 [05:05<14:02,  2.26s/it]predicting train subjects:  30%|███       | 160/532 [05:07<14:08,  2.28s/it]predicting train subjects:  30%|███       | 161/532 [05:09<13:12,  2.14s/it]predicting train subjects:  30%|███       | 162/532 [05:11<12:39,  2.05s/it]predicting train subjects:  31%|███       | 163/532 [05:13<12:19,  2.01s/it]predicting train subjects:  31%|███       | 164/532 [05:14<12:01,  1.96s/it]predicting train subjects:  31%|███       | 165/532 [05:16<11:49,  1.93s/it]predicting train subjects:  31%|███       | 166/532 [05:18<11:44,  1.92s/it]predicting train subjects:  31%|███▏      | 167/532 [05:20<11:32,  1.90s/it]predicting train subjects:  32%|███▏      | 168/532 [05:22<11:29,  1.89s/it]predicting train subjects:  32%|███▏      | 169/532 [05:24<11:21,  1.88s/it]predicting train subjects:  32%|███▏      | 170/532 [05:26<11:11,  1.86s/it]predicting train subjects:  32%|███▏      | 171/532 [05:27<11:09,  1.85s/it]predicting train subjects:  32%|███▏      | 172/532 [05:29<11:00,  1.84s/it]predicting train subjects:  33%|███▎      | 173/532 [05:31<11:10,  1.87s/it]predicting train subjects:  33%|███▎      | 174/532 [05:33<10:56,  1.83s/it]predicting train subjects:  33%|███▎      | 175/532 [05:35<10:40,  1.79s/it]predicting train subjects:  33%|███▎      | 176/532 [05:36<10:29,  1.77s/it]predicting train subjects:  33%|███▎      | 177/532 [05:38<10:13,  1.73s/it]predicting train subjects:  33%|███▎      | 178/532 [05:40<10:12,  1.73s/it]predicting train subjects:  34%|███▎      | 179/532 [05:41<10:18,  1.75s/it]predicting train subjects:  34%|███▍      | 180/532 [05:43<10:17,  1.75s/it]predicting train subjects:  34%|███▍      | 181/532 [05:45<10:15,  1.75s/it]predicting train subjects:  34%|███▍      | 182/532 [05:47<10:02,  1.72s/it]predicting train subjects:  34%|███▍      | 183/532 [05:48<10:04,  1.73s/it]predicting train subjects:  35%|███▍      | 184/532 [05:50<10:05,  1.74s/it]predicting train subjects:  35%|███▍      | 185/532 [05:52<09:57,  1.72s/it]predicting train subjects:  35%|███▍      | 186/532 [05:54<09:55,  1.72s/it]predicting train subjects:  35%|███▌      | 187/532 [05:55<09:48,  1.71s/it]predicting train subjects:  35%|███▌      | 188/532 [05:57<09:46,  1.70s/it]predicting train subjects:  36%|███▌      | 189/532 [05:59<09:41,  1.69s/it]predicting train subjects:  36%|███▌      | 190/532 [06:00<09:41,  1.70s/it]predicting train subjects:  36%|███▌      | 191/532 [06:03<10:45,  1.89s/it]predicting train subjects:  36%|███▌      | 192/532 [06:05<11:29,  2.03s/it]predicting train subjects:  36%|███▋      | 193/532 [06:07<12:05,  2.14s/it]predicting train subjects:  36%|███▋      | 194/532 [06:10<12:23,  2.20s/it]predicting train subjects:  37%|███▋      | 195/532 [06:12<12:35,  2.24s/it]predicting train subjects:  37%|███▋      | 196/532 [06:14<12:47,  2.28s/it]predicting train subjects:  37%|███▋      | 197/532 [06:17<12:24,  2.22s/it]predicting train subjects:  37%|███▋      | 198/532 [06:19<12:07,  2.18s/it]predicting train subjects:  37%|███▋      | 199/532 [06:21<11:52,  2.14s/it]predicting train subjects:  38%|███▊      | 200/532 [06:23<11:48,  2.13s/it]predicting train subjects:  38%|███▊      | 201/532 [06:25<11:48,  2.14s/it]predicting train subjects:  38%|███▊      | 202/532 [06:27<11:41,  2.13s/it]predicting train subjects:  38%|███▊      | 203/532 [06:29<11:08,  2.03s/it]predicting train subjects:  38%|███▊      | 204/532 [06:31<10:44,  1.97s/it]predicting train subjects:  39%|███▊      | 205/532 [06:32<10:24,  1.91s/it]predicting train subjects:  39%|███▊      | 206/532 [06:34<10:06,  1.86s/it]predicting train subjects:  39%|███▉      | 207/532 [06:36<09:55,  1.83s/it]predicting train subjects:  39%|███▉      | 208/532 [06:38<09:54,  1.84s/it]predicting train subjects:  39%|███▉      | 209/532 [06:39<09:29,  1.76s/it]predicting train subjects:  39%|███▉      | 210/532 [06:41<09:09,  1.71s/it]predicting train subjects:  40%|███▉      | 211/532 [06:43<09:00,  1.68s/it]predicting train subjects:  40%|███▉      | 212/532 [06:44<09:00,  1.69s/it]predicting train subjects:  40%|████      | 213/532 [06:46<08:50,  1.66s/it]predicting train subjects:  40%|████      | 214/532 [06:48<08:42,  1.64s/it]predicting train subjects:  40%|████      | 215/532 [06:50<09:37,  1.82s/it]predicting train subjects:  41%|████      | 216/532 [06:52<10:12,  1.94s/it]predicting train subjects:  41%|████      | 217/532 [06:54<10:31,  2.00s/it]predicting train subjects:  41%|████      | 218/532 [06:56<10:50,  2.07s/it]predicting train subjects:  41%|████      | 219/532 [06:59<11:06,  2.13s/it]predicting train subjects:  41%|████▏     | 220/532 [07:01<11:09,  2.14s/it]predicting train subjects:  42%|████▏     | 221/532 [07:02<10:20,  2.00s/it]predicting train subjects:  42%|████▏     | 222/532 [07:04<09:42,  1.88s/it]predicting train subjects:  42%|████▏     | 223/532 [07:06<09:18,  1.81s/it]predicting train subjects:  42%|████▏     | 224/532 [07:07<09:01,  1.76s/it]predicting train subjects:  42%|████▏     | 225/532 [07:09<08:46,  1.71s/it]predicting train subjects:  42%|████▏     | 226/532 [07:11<08:36,  1.69s/it]predicting train subjects:  43%|████▎     | 227/532 [07:12<08:19,  1.64s/it]predicting train subjects:  43%|████▎     | 228/532 [07:14<08:16,  1.63s/it]predicting train subjects:  43%|████▎     | 229/532 [07:15<08:04,  1.60s/it]predicting train subjects:  43%|████▎     | 230/532 [07:17<07:56,  1.58s/it]predicting train subjects:  43%|████▎     | 231/532 [07:18<07:55,  1.58s/it]predicting train subjects:  44%|████▎     | 232/532 [07:20<07:59,  1.60s/it]predicting train subjects:  44%|████▍     | 233/532 [07:22<08:21,  1.68s/it]predicting train subjects:  44%|████▍     | 234/532 [07:24<08:30,  1.71s/it]predicting train subjects:  44%|████▍     | 235/532 [07:25<08:39,  1.75s/it]predicting train subjects:  44%|████▍     | 236/532 [07:27<08:44,  1.77s/it]predicting train subjects:  45%|████▍     | 237/532 [07:29<08:44,  1.78s/it]predicting train subjects:  45%|████▍     | 238/532 [07:31<08:44,  1.79s/it]predicting train subjects:  45%|████▍     | 239/532 [07:33<09:04,  1.86s/it]predicting train subjects:  45%|████▌     | 240/532 [07:35<09:10,  1.89s/it]predicting train subjects:  45%|████▌     | 241/532 [07:37<09:16,  1.91s/it]predicting train subjects:  45%|████▌     | 242/532 [07:39<09:18,  1.93s/it]predicting train subjects:  46%|████▌     | 243/532 [07:41<09:36,  1.99s/it]predicting train subjects:  46%|████▌     | 244/532 [07:43<09:40,  2.01s/it]predicting train subjects:  46%|████▌     | 245/532 [07:45<09:04,  1.90s/it]predicting train subjects:  46%|████▌     | 246/532 [07:46<08:38,  1.81s/it]predicting train subjects:  46%|████▋     | 247/532 [07:48<08:15,  1.74s/it]predicting train subjects:  47%|████▋     | 248/532 [07:49<08:02,  1.70s/it]predicting train subjects:  47%|████▋     | 249/532 [07:51<07:47,  1.65s/it]predicting train subjects:  47%|████▋     | 250/532 [07:53<07:40,  1.63s/it]predicting train subjects:  47%|████▋     | 251/532 [07:54<07:45,  1.66s/it]predicting train subjects:  47%|████▋     | 252/532 [07:56<07:51,  1.68s/it]predicting train subjects:  48%|████▊     | 253/532 [07:58<07:47,  1.68s/it]predicting train subjects:  48%|████▊     | 254/532 [07:59<07:43,  1.67s/it]predicting train subjects:  48%|████▊     | 255/532 [08:01<07:45,  1.68s/it]predicting train subjects:  48%|████▊     | 256/532 [08:03<07:44,  1.68s/it]predicting train subjects:  48%|████▊     | 257/532 [08:05<08:18,  1.81s/it]predicting train subjects:  48%|████▊     | 258/532 [08:07<08:40,  1.90s/it]predicting train subjects:  49%|████▊     | 259/532 [08:09<08:55,  1.96s/it]predicting train subjects:  49%|████▉     | 260/532 [08:11<09:02,  2.00s/it]predicting train subjects:  49%|████▉     | 261/532 [08:13<09:06,  2.02s/it]predicting train subjects:  49%|████▉     | 262/532 [08:15<09:10,  2.04s/it]predicting train subjects:  49%|████▉     | 263/532 [08:17<08:37,  1.92s/it]predicting train subjects:  50%|████▉     | 264/532 [08:18<08:06,  1.81s/it]predicting train subjects:  50%|████▉     | 265/532 [08:20<07:49,  1.76s/it]predicting train subjects:  50%|█████     | 266/532 [08:22<07:33,  1.70s/it]predicting train subjects:  50%|█████     | 267/532 [08:23<07:23,  1.68s/it]predicting train subjects:  50%|█████     | 268/532 [08:25<07:08,  1.62s/it]predicting train subjects:  51%|█████     | 269/532 [08:27<07:31,  1.72s/it]predicting train subjects:  51%|█████     | 270/532 [08:29<07:40,  1.76s/it]predicting train subjects:  51%|█████     | 271/532 [08:31<07:50,  1.80s/it]predicting train subjects:  51%|█████     | 272/532 [08:32<07:57,  1.84s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:34<07:55,  1.84s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:36<07:54,  1.84s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:38<08:26,  1.97s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:41<08:48,  2.07s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:43<09:05,  2.14s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:45<09:10,  2.17s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:47<09:14,  2.19s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:50<09:21,  2.23s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:52<09:12,  2.20s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:54<09:04,  2.18s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:56<08:58,  2.16s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:58<08:51,  2.14s/it]predicting train subjects:  54%|█████▎    | 285/532 [09:00<08:47,  2.14s/it]predicting train subjects:  54%|█████▍    | 286/532 [09:02<08:42,  2.12s/it]predicting train subjects:  54%|█████▍    | 287/532 [09:04<08:13,  2.01s/it]predicting train subjects:  54%|█████▍    | 288/532 [09:06<07:52,  1.93s/it]predicting train subjects:  54%|█████▍    | 289/532 [09:08<07:32,  1.86s/it]predicting train subjects:  55%|█████▍    | 290/532 [09:09<07:22,  1.83s/it]predicting train subjects:  55%|█████▍    | 291/532 [09:11<07:17,  1.81s/it]predicting train subjects:  55%|█████▍    | 292/532 [09:13<07:16,  1.82s/it]predicting train subjects:  55%|█████▌    | 293/532 [09:15<07:19,  1.84s/it]predicting train subjects:  55%|█████▌    | 294/532 [09:17<07:22,  1.86s/it]predicting train subjects:  55%|█████▌    | 295/532 [09:19<07:22,  1.87s/it]predicting train subjects:  56%|█████▌    | 296/532 [09:21<07:25,  1.89s/it]predicting train subjects:  56%|█████▌    | 297/532 [09:22<07:21,  1.88s/it]predicting train subjects:  56%|█████▌    | 298/532 [09:24<07:18,  1.87s/it]predicting train subjects:  56%|█████▌    | 299/532 [09:26<06:56,  1.79s/it]predicting train subjects:  56%|█████▋    | 300/532 [09:27<06:38,  1.72s/it]predicting train subjects:  57%|█████▋    | 301/532 [09:29<06:24,  1.66s/it]predicting train subjects:  57%|█████▋    | 302/532 [09:31<06:13,  1.62s/it]predicting train subjects:  57%|█████▋    | 303/532 [09:32<06:08,  1.61s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:34<06:00,  1.58s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:36<06:45,  1.79s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:38<07:14,  1.92s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:40<07:40,  2.05s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:43<07:54,  2.12s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:45<08:01,  2.16s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:47<08:10,  2.21s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:50<08:53,  2.41s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:53<09:19,  2.54s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:56<09:39,  2.65s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:59<09:50,  2.71s/it]predicting train subjects:  59%|█████▉    | 315/532 [10:02<10:02,  2.78s/it]predicting train subjects:  59%|█████▉    | 316/532 [10:05<10:01,  2.79s/it]predicting train subjects:  60%|█████▉    | 317/532 [10:06<08:52,  2.48s/it]predicting train subjects:  60%|█████▉    | 318/532 [10:08<07:58,  2.24s/it]predicting train subjects:  60%|█████▉    | 319/532 [10:10<07:21,  2.07s/it]predicting train subjects:  60%|██████    | 320/532 [10:11<06:56,  1.96s/it]predicting train subjects:  60%|██████    | 321/532 [10:13<06:39,  1.89s/it]predicting train subjects:  61%|██████    | 322/532 [10:15<06:22,  1.82s/it]predicting train subjects:  61%|██████    | 323/532 [10:17<06:50,  1.96s/it]predicting train subjects:  61%|██████    | 324/532 [10:19<07:08,  2.06s/it]predicting train subjects:  61%|██████    | 325/532 [10:22<07:20,  2.13s/it]predicting train subjects:  61%|██████▏   | 326/532 [10:24<07:31,  2.19s/it]predicting train subjects:  61%|██████▏   | 327/532 [10:26<07:38,  2.24s/it]predicting train subjects:  62%|██████▏   | 328/532 [10:29<07:42,  2.27s/it]predicting train subjects:  62%|██████▏   | 329/532 [10:30<07:10,  2.12s/it]predicting train subjects:  62%|██████▏   | 330/532 [10:32<06:49,  2.03s/it]predicting train subjects:  62%|██████▏   | 331/532 [10:34<06:32,  1.95s/it]predicting train subjects:  62%|██████▏   | 332/532 [10:36<06:19,  1.90s/it]predicting train subjects:  63%|██████▎   | 333/532 [10:38<06:09,  1.86s/it]predicting train subjects:  63%|██████▎   | 334/532 [10:39<06:01,  1.82s/it]predicting train subjects:  63%|██████▎   | 335/532 [10:41<06:08,  1.87s/it]predicting train subjects:  63%|██████▎   | 336/532 [10:43<06:10,  1.89s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:45<06:13,  1.92s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:47<06:13,  1.92s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:49<06:09,  1.91s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:51<06:07,  1.92s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:53<05:46,  1.81s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:54<05:30,  1.74s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:56<05:18,  1.68s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:57<05:11,  1.66s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:59<05:08,  1.65s/it]predicting train subjects:  65%|██████▌   | 346/532 [11:00<05:04,  1.64s/it]predicting train subjects:  65%|██████▌   | 347/532 [11:02<05:12,  1.69s/it]predicting train subjects:  65%|██████▌   | 348/532 [11:04<05:15,  1.72s/it]predicting train subjects:  66%|██████▌   | 349/532 [11:06<05:20,  1.75s/it]predicting train subjects:  66%|██████▌   | 350/532 [11:08<05:21,  1.77s/it]predicting train subjects:  66%|██████▌   | 351/532 [11:10<05:21,  1.78s/it]predicting train subjects:  66%|██████▌   | 352/532 [11:11<05:19,  1.78s/it]predicting train subjects:  66%|██████▋   | 353/532 [11:13<05:19,  1.78s/it]predicting train subjects:  67%|██████▋   | 354/532 [11:15<05:16,  1.78s/it]predicting train subjects:  67%|██████▋   | 355/532 [11:17<05:16,  1.79s/it]predicting train subjects:  67%|██████▋   | 356/532 [11:18<05:12,  1.78s/it]predicting train subjects:  67%|██████▋   | 357/532 [11:20<05:08,  1.76s/it]predicting train subjects:  67%|██████▋   | 358/532 [11:22<05:07,  1.76s/it]predicting train subjects:  67%|██████▋   | 359/532 [11:23<04:51,  1.68s/it]predicting train subjects:  68%|██████▊   | 360/532 [11:25<04:39,  1.62s/it]predicting train subjects:  68%|██████▊   | 361/532 [11:26<04:32,  1.59s/it]predicting train subjects:  68%|██████▊   | 362/532 [11:28<04:29,  1.59s/it]predicting train subjects:  68%|██████▊   | 363/532 [11:30<04:28,  1.59s/it]predicting train subjects:  68%|██████▊   | 364/532 [11:31<04:27,  1.59s/it]predicting train subjects:  69%|██████▊   | 365/532 [11:33<04:31,  1.63s/it]predicting train subjects:  69%|██████▉   | 366/532 [11:35<04:30,  1.63s/it]predicting train subjects:  69%|██████▉   | 367/532 [11:36<04:30,  1.64s/it]predicting train subjects:  69%|██████▉   | 368/532 [11:38<04:26,  1.63s/it]predicting train subjects:  69%|██████▉   | 369/532 [11:39<04:25,  1.63s/it]predicting train subjects:  70%|██████▉   | 370/532 [11:41<04:24,  1.64s/it]predicting train subjects:  70%|██████▉   | 371/532 [11:43<04:50,  1.80s/it]predicting train subjects:  70%|██████▉   | 372/532 [11:45<05:05,  1.91s/it]predicting train subjects:  70%|███████   | 373/532 [11:48<05:16,  1.99s/it]predicting train subjects:  70%|███████   | 374/532 [11:50<05:23,  2.05s/it]predicting train subjects:  70%|███████   | 375/532 [11:52<05:27,  2.09s/it]predicting train subjects:  71%|███████   | 376/532 [11:54<05:30,  2.12s/it]predicting train subjects:  71%|███████   | 377/532 [11:56<05:13,  2.02s/it]predicting train subjects:  71%|███████   | 378/532 [11:58<04:59,  1.94s/it]predicting train subjects:  71%|███████   | 379/532 [12:00<04:50,  1.90s/it]predicting train subjects:  71%|███████▏  | 380/532 [12:01<04:42,  1.86s/it]predicting train subjects:  72%|███████▏  | 381/532 [12:03<04:36,  1.83s/it]predicting train subjects:  72%|███████▏  | 382/532 [12:05<04:32,  1.81s/it]predicting train subjects:  72%|███████▏  | 383/532 [12:07<04:35,  1.85s/it]predicting train subjects:  72%|███████▏  | 384/532 [12:09<04:37,  1.87s/it]predicting train subjects:  72%|███████▏  | 385/532 [12:11<04:34,  1.87s/it]predicting train subjects:  73%|███████▎  | 386/532 [12:12<04:36,  1.89s/it]predicting train subjects:  73%|███████▎  | 387/532 [12:14<04:35,  1.90s/it]predicting train subjects:  73%|███████▎  | 388/532 [12:16<04:32,  1.89s/it]predicting train subjects:  73%|███████▎  | 389/532 [12:18<04:32,  1.91s/it]predicting train subjects:  73%|███████▎  | 390/532 [12:20<04:33,  1.93s/it]predicting train subjects:  73%|███████▎  | 391/532 [12:22<04:34,  1.94s/it]predicting train subjects:  74%|███████▎  | 392/532 [12:24<04:31,  1.94s/it]predicting train subjects:  74%|███████▍  | 393/532 [12:26<04:27,  1.92s/it]predicting train subjects:  74%|███████▍  | 394/532 [12:28<04:27,  1.94s/it]predicting train subjects:  74%|███████▍  | 395/532 [12:30<04:23,  1.92s/it]predicting train subjects:  74%|███████▍  | 396/532 [12:32<04:23,  1.94s/it]predicting train subjects:  75%|███████▍  | 397/532 [12:34<04:20,  1.93s/it]predicting train subjects:  75%|███████▍  | 398/532 [12:36<04:19,  1.94s/it]predicting train subjects:  75%|███████▌  | 399/532 [12:38<04:17,  1.93s/it]predicting train subjects:  75%|███████▌  | 400/532 [12:40<04:18,  1.96s/it]predicting train subjects:  75%|███████▌  | 401/532 [12:42<04:20,  1.99s/it]predicting train subjects:  76%|███████▌  | 402/532 [12:44<04:20,  2.01s/it]predicting train subjects:  76%|███████▌  | 403/532 [12:46<04:21,  2.03s/it]predicting train subjects:  76%|███████▌  | 404/532 [12:48<04:19,  2.03s/it]predicting train subjects:  76%|███████▌  | 405/532 [12:50<04:15,  2.01s/it]predicting train subjects:  76%|███████▋  | 406/532 [12:52<04:15,  2.03s/it]predicting train subjects:  77%|███████▋  | 407/532 [12:54<04:06,  1.97s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:56<03:56,  1.91s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:57<03:50,  1.88s/it]predicting train subjects:  77%|███████▋  | 410/532 [12:59<03:45,  1.85s/it]predicting train subjects:  77%|███████▋  | 411/532 [13:01<03:49,  1.90s/it]predicting train subjects:  77%|███████▋  | 412/532 [13:03<03:44,  1.87s/it]predicting train subjects:  78%|███████▊  | 413/532 [13:05<03:37,  1.83s/it]predicting train subjects:  78%|███████▊  | 414/532 [13:06<03:32,  1.80s/it]predicting train subjects:  78%|███████▊  | 415/532 [13:08<03:25,  1.76s/it]predicting train subjects:  78%|███████▊  | 416/532 [13:10<03:22,  1.74s/it]predicting train subjects:  78%|███████▊  | 417/532 [13:11<03:18,  1.73s/it]predicting train subjects:  79%|███████▊  | 418/532 [13:13<03:14,  1.71s/it]predicting train subjects:  79%|███████▉  | 419/532 [13:15<03:19,  1.77s/it]predicting train subjects:  79%|███████▉  | 420/532 [13:17<03:22,  1.81s/it]predicting train subjects:  79%|███████▉  | 421/532 [13:19<03:23,  1.84s/it]predicting train subjects:  79%|███████▉  | 422/532 [13:21<03:26,  1.87s/it]predicting train subjects:  80%|███████▉  | 423/532 [13:23<03:26,  1.89s/it]predicting train subjects:  80%|███████▉  | 424/532 [13:25<03:25,  1.91s/it]predicting train subjects:  80%|███████▉  | 425/532 [13:27<03:25,  1.92s/it]predicting train subjects:  80%|████████  | 426/532 [13:29<03:25,  1.94s/it]predicting train subjects:  80%|████████  | 427/532 [13:30<03:21,  1.92s/it]predicting train subjects:  80%|████████  | 428/532 [13:32<03:16,  1.89s/it]predicting train subjects:  81%|████████  | 429/532 [13:34<03:17,  1.92s/it]predicting train subjects:  81%|████████  | 430/532 [13:36<03:16,  1.92s/it]predicting train subjects:  81%|████████  | 431/532 [13:38<03:16,  1.95s/it]predicting train subjects:  81%|████████  | 432/532 [13:40<03:18,  1.98s/it]predicting train subjects:  81%|████████▏ | 433/532 [13:42<03:19,  2.02s/it]predicting train subjects:  82%|████████▏ | 434/532 [13:44<03:19,  2.03s/it]predicting train subjects:  82%|████████▏ | 435/532 [13:47<03:19,  2.05s/it]predicting train subjects:  82%|████████▏ | 436/532 [13:49<03:15,  2.04s/it]predicting train subjects:  82%|████████▏ | 437/532 [13:50<02:58,  1.88s/it]predicting train subjects:  82%|████████▏ | 438/532 [13:52<02:46,  1.77s/it]predicting train subjects:  83%|████████▎ | 439/532 [13:53<02:38,  1.70s/it]predicting train subjects:  83%|████████▎ | 440/532 [13:55<02:32,  1.66s/it]predicting train subjects:  83%|████████▎ | 441/532 [13:56<02:26,  1.62s/it]predicting train subjects:  83%|████████▎ | 442/532 [13:58<02:24,  1.60s/it]predicting train subjects:  83%|████████▎ | 443/532 [13:59<02:21,  1.59s/it]predicting train subjects:  83%|████████▎ | 444/532 [14:01<02:17,  1.56s/it]predicting train subjects:  84%|████████▎ | 445/532 [14:02<02:15,  1.56s/it]predicting train subjects:  84%|████████▍ | 446/532 [14:04<02:15,  1.58s/it]predicting train subjects:  84%|████████▍ | 447/532 [14:06<02:14,  1.58s/it]predicting train subjects:  84%|████████▍ | 448/532 [14:07<02:12,  1.58s/it]predicting train subjects:  84%|████████▍ | 449/532 [14:09<02:14,  1.62s/it]predicting train subjects:  85%|████████▍ | 450/532 [14:11<02:15,  1.65s/it]predicting train subjects:  85%|████████▍ | 451/532 [14:12<02:13,  1.65s/it]predicting train subjects:  85%|████████▍ | 452/532 [14:14<02:10,  1.63s/it]predicting train subjects:  85%|████████▌ | 453/532 [14:15<02:09,  1.64s/it]predicting train subjects:  85%|████████▌ | 454/532 [14:17<02:11,  1.68s/it]predicting train subjects:  86%|████████▌ | 455/532 [14:19<02:13,  1.74s/it]predicting train subjects:  86%|████████▌ | 456/532 [14:21<02:16,  1.80s/it]predicting train subjects:  86%|████████▌ | 457/532 [14:23<02:18,  1.85s/it]predicting train subjects:  86%|████████▌ | 458/532 [14:25<02:18,  1.87s/it]predicting train subjects:  86%|████████▋ | 459/532 [14:27<02:17,  1.88s/it]predicting train subjects:  86%|████████▋ | 460/532 [14:29<02:16,  1.90s/it]predicting train subjects:  87%|████████▋ | 461/532 [14:31<02:21,  2.00s/it]predicting train subjects:  87%|████████▋ | 462/532 [14:33<02:24,  2.07s/it]predicting train subjects:  87%|████████▋ | 463/532 [14:36<02:26,  2.12s/it]predicting train subjects:  87%|████████▋ | 464/532 [14:38<02:25,  2.14s/it]predicting train subjects:  87%|████████▋ | 465/532 [14:40<02:25,  2.17s/it]predicting train subjects:  88%|████████▊ | 466/532 [14:42<02:24,  2.19s/it]predicting train subjects:  88%|████████▊ | 467/532 [14:44<02:15,  2.09s/it]predicting train subjects:  88%|████████▊ | 468/532 [14:46<02:06,  1.98s/it]predicting train subjects:  88%|████████▊ | 469/532 [14:47<01:59,  1.90s/it]predicting train subjects:  88%|████████▊ | 470/532 [14:49<01:55,  1.87s/it]predicting train subjects:  89%|████████▊ | 471/532 [14:51<01:52,  1.85s/it]predicting train subjects:  89%|████████▊ | 472/532 [14:53<01:48,  1.82s/it]predicting train subjects:  89%|████████▉ | 473/532 [14:55<01:52,  1.90s/it]predicting train subjects:  89%|████████▉ | 474/532 [14:57<01:53,  1.96s/it]predicting train subjects:  89%|████████▉ | 475/532 [14:59<01:52,  1.98s/it]predicting train subjects:  89%|████████▉ | 476/532 [15:01<01:50,  1.97s/it]predicting train subjects:  90%|████████▉ | 477/532 [15:03<01:48,  1.97s/it]predicting train subjects:  90%|████████▉ | 478/532 [15:05<01:45,  1.96s/it]predicting train subjects:  90%|█████████ | 479/532 [15:07<01:40,  1.89s/it]predicting train subjects:  90%|█████████ | 480/532 [15:08<01:36,  1.85s/it]predicting train subjects:  90%|█████████ | 481/532 [15:10<01:31,  1.80s/it]predicting train subjects:  91%|█████████ | 482/532 [15:12<01:27,  1.76s/it]predicting train subjects:  91%|█████████ | 483/532 [15:13<01:25,  1.74s/it]predicting train subjects:  91%|█████████ | 484/532 [15:15<01:23,  1.74s/it]predicting train subjects:  91%|█████████ | 485/532 [15:17<01:27,  1.86s/it]predicting train subjects:  91%|█████████▏| 486/532 [15:19<01:28,  1.93s/it]predicting train subjects:  92%|█████████▏| 487/532 [15:22<01:30,  2.01s/it]predicting train subjects:  92%|█████████▏| 488/532 [15:24<01:30,  2.05s/it]predicting train subjects:  92%|█████████▏| 489/532 [15:26<01:29,  2.08s/it]predicting train subjects:  92%|█████████▏| 490/532 [15:28<01:29,  2.12s/it]predicting train subjects:  92%|█████████▏| 491/532 [15:30<01:23,  2.04s/it]predicting train subjects:  92%|█████████▏| 492/532 [15:32<01:19,  1.99s/it]predicting train subjects:  93%|█████████▎| 493/532 [15:34<01:15,  1.95s/it]predicting train subjects:  93%|█████████▎| 494/532 [15:35<01:12,  1.91s/it]predicting train subjects:  93%|█████████▎| 495/532 [15:37<01:09,  1.89s/it]predicting train subjects:  93%|█████████▎| 496/532 [15:39<01:07,  1.86s/it]predicting train subjects:  93%|█████████▎| 497/532 [15:41<01:05,  1.88s/it]predicting train subjects:  94%|█████████▎| 498/532 [15:43<01:03,  1.86s/it]predicting train subjects:  94%|█████████▍| 499/532 [15:45<01:01,  1.87s/it]predicting train subjects:  94%|█████████▍| 500/532 [15:47<00:59,  1.86s/it]predicting train subjects:  94%|█████████▍| 501/532 [15:49<00:58,  1.90s/it]predicting train subjects:  94%|█████████▍| 502/532 [15:51<00:57,  1.92s/it]predicting train subjects:  95%|█████████▍| 503/532 [15:52<00:54,  1.88s/it]predicting train subjects:  95%|█████████▍| 504/532 [15:54<00:51,  1.84s/it]predicting train subjects:  95%|█████████▍| 505/532 [15:56<00:49,  1.82s/it]predicting train subjects:  95%|█████████▌| 506/532 [15:58<00:47,  1.82s/it]predicting train subjects:  95%|█████████▌| 507/532 [15:59<00:45,  1.81s/it]predicting train subjects:  95%|█████████▌| 508/532 [16:01<00:43,  1.80s/it]predicting train subjects:  96%|█████████▌| 509/532 [16:03<00:44,  1.92s/it]predicting train subjects:  96%|█████████▌| 510/532 [16:06<00:44,  2.00s/it]predicting train subjects:  96%|█████████▌| 511/532 [16:08<00:43,  2.06s/it]predicting train subjects:  96%|█████████▌| 512/532 [16:10<00:42,  2.10s/it]predicting train subjects:  96%|█████████▋| 513/532 [16:12<00:40,  2.14s/it]predicting train subjects:  97%|█████████▋| 514/532 [16:14<00:38,  2.16s/it]predicting train subjects:  97%|█████████▋| 515/532 [16:16<00:35,  2.07s/it]predicting train subjects:  97%|█████████▋| 516/532 [16:18<00:32,  2.01s/it]predicting train subjects:  97%|█████████▋| 517/532 [16:20<00:29,  1.95s/it]predicting train subjects:  97%|█████████▋| 518/532 [16:22<00:26,  1.91s/it]predicting train subjects:  98%|█████████▊| 519/532 [16:24<00:24,  1.89s/it]predicting train subjects:  98%|█████████▊| 520/532 [16:26<00:22,  1.88s/it]predicting train subjects:  98%|█████████▊| 521/532 [16:28<00:21,  1.92s/it]predicting train subjects:  98%|█████████▊| 522/532 [16:30<00:19,  1.95s/it]predicting train subjects:  98%|█████████▊| 523/532 [16:32<00:17,  1.96s/it]predicting train subjects:  98%|█████████▊| 524/532 [16:34<00:15,  1.97s/it]predicting train subjects:  99%|█████████▊| 525/532 [16:35<00:13,  1.97s/it]predicting train subjects:  99%|█████████▉| 526/532 [16:37<00:11,  1.97s/it]predicting train subjects:  99%|█████████▉| 527/532 [16:39<00:09,  1.91s/it]predicting train subjects:  99%|█████████▉| 528/532 [16:41<00:07,  1.89s/it]predicting train subjects:  99%|█████████▉| 529/532 [16:43<00:05,  1.84s/it]predicting train subjects: 100%|█████████▉| 530/532 [16:45<00:03,  1.82s/it]predicting train subjects: 100%|█████████▉| 531/532 [16:46<00:01,  1.81s/it]predicting train subjects: 100%|██████████| 532/532 [16:48<00:00,  1.82s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<12:16,  1.39s/it]Loading train:   0%|          | 2/532 [00:02<10:56,  1.24s/it]Loading train:   1%|          | 3/532 [00:03<10:16,  1.17s/it]Loading train:   1%|          | 4/532 [00:04<09:39,  1.10s/it]Loading train:   1%|          | 5/532 [00:05<09:24,  1.07s/it]Loading train:   1%|          | 6/532 [00:06<09:00,  1.03s/it]Loading train:   1%|▏         | 7/532 [00:07<08:39,  1.01it/s]Loading train:   2%|▏         | 8/532 [00:07<07:59,  1.09it/s]Loading train:   2%|▏         | 9/532 [00:08<08:19,  1.05it/s]Loading train:   2%|▏         | 10/532 [00:09<08:05,  1.08it/s]Loading train:   2%|▏         | 11/532 [00:10<07:31,  1.15it/s]Loading train:   2%|▏         | 12/532 [00:11<08:30,  1.02it/s]Loading train:   2%|▏         | 13/532 [00:12<08:14,  1.05it/s]Loading train:   3%|▎         | 14/532 [00:13<07:47,  1.11it/s]Loading train:   3%|▎         | 15/532 [00:14<07:35,  1.14it/s]Loading train:   3%|▎         | 16/532 [00:15<07:35,  1.13it/s]Loading train:   3%|▎         | 17/532 [00:15<07:20,  1.17it/s]Loading train:   3%|▎         | 18/532 [00:16<07:38,  1.12it/s]Loading train:   4%|▎         | 19/532 [00:17<07:11,  1.19it/s]Loading train:   4%|▍         | 20/532 [00:18<07:24,  1.15it/s]Loading train:   4%|▍         | 21/532 [00:19<08:21,  1.02it/s]Loading train:   4%|▍         | 22/532 [00:20<07:57,  1.07it/s]Loading train:   4%|▍         | 23/532 [00:21<08:02,  1.05it/s]Loading train:   5%|▍         | 24/532 [00:22<07:32,  1.12it/s]Loading train:   5%|▍         | 25/532 [00:23<08:10,  1.03it/s]Loading train:   5%|▍         | 26/532 [00:24<07:52,  1.07it/s]Loading train:   5%|▌         | 27/532 [00:25<08:29,  1.01s/it]Loading train:   5%|▌         | 28/532 [00:26<08:12,  1.02it/s]Loading train:   5%|▌         | 29/532 [00:27<08:23,  1.00s/it]Loading train:   6%|▌         | 30/532 [00:28<08:05,  1.03it/s]Loading train:   6%|▌         | 31/532 [00:29<08:01,  1.04it/s]Loading train:   6%|▌         | 32/532 [00:30<07:45,  1.08it/s]Loading train:   6%|▌         | 33/532 [00:30<07:23,  1.13it/s]Loading train:   6%|▋         | 34/532 [00:32<08:15,  1.00it/s]Loading train:   7%|▋         | 35/532 [00:32<07:49,  1.06it/s]Loading train:   7%|▋         | 36/532 [00:33<07:51,  1.05it/s]Loading train:   7%|▋         | 37/532 [00:34<07:42,  1.07it/s]Loading train:   7%|▋         | 38/532 [00:36<08:12,  1.00it/s]Loading train:   7%|▋         | 39/532 [00:36<07:57,  1.03it/s]Loading train:   8%|▊         | 40/532 [00:37<07:31,  1.09it/s]Loading train:   8%|▊         | 41/532 [00:38<07:34,  1.08it/s]Loading train:   8%|▊         | 42/532 [00:39<07:44,  1.05it/s]Loading train:   8%|▊         | 43/532 [00:40<07:14,  1.13it/s]Loading train:   8%|▊         | 44/532 [00:41<07:09,  1.14it/s]Loading train:   8%|▊         | 45/532 [00:42<07:32,  1.08it/s]Loading train:   9%|▊         | 46/532 [00:43<07:59,  1.01it/s]Loading train:   9%|▉         | 47/532 [00:44<08:25,  1.04s/it]Loading train:   9%|▉         | 48/532 [00:45<08:44,  1.08s/it]Loading train:   9%|▉         | 49/532 [00:46<08:52,  1.10s/it]Loading train:   9%|▉         | 50/532 [00:48<09:25,  1.17s/it]Loading train:  10%|▉         | 51/532 [00:49<08:57,  1.12s/it]Loading train:  10%|▉         | 52/532 [00:50<08:55,  1.11s/it]Loading train:  10%|▉         | 53/532 [00:51<08:43,  1.09s/it]Loading train:  10%|█         | 54/532 [00:52<09:18,  1.17s/it]Loading train:  10%|█         | 55/532 [00:53<09:09,  1.15s/it]Loading train:  11%|█         | 56/532 [00:55<09:10,  1.16s/it]Loading train:  11%|█         | 57/532 [00:56<09:36,  1.21s/it]Loading train:  11%|█         | 58/532 [00:57<09:47,  1.24s/it]Loading train:  11%|█         | 59/532 [00:59<10:18,  1.31s/it]Loading train:  11%|█▏        | 60/532 [01:00<09:39,  1.23s/it]Loading train:  11%|█▏        | 61/532 [01:01<09:09,  1.17s/it]Loading train:  12%|█▏        | 62/532 [01:02<09:18,  1.19s/it]Loading train:  12%|█▏        | 63/532 [01:03<09:57,  1.27s/it]Loading train:  12%|█▏        | 64/532 [01:04<09:21,  1.20s/it]Loading train:  12%|█▏        | 65/532 [01:06<09:47,  1.26s/it]Loading train:  12%|█▏        | 66/532 [01:07<10:07,  1.30s/it]Loading train:  13%|█▎        | 67/532 [01:09<10:14,  1.32s/it]Loading train:  13%|█▎        | 68/532 [01:10<09:51,  1.27s/it]Loading train:  13%|█▎        | 69/532 [01:11<09:32,  1.24s/it]Loading train:  13%|█▎        | 70/532 [01:12<09:19,  1.21s/it]Loading train:  13%|█▎        | 71/532 [01:13<08:46,  1.14s/it]Loading train:  14%|█▎        | 72/532 [01:14<08:15,  1.08s/it]Loading train:  14%|█▎        | 73/532 [01:15<08:29,  1.11s/it]Loading train:  14%|█▍        | 74/532 [01:17<09:16,  1.22s/it]Loading train:  14%|█▍        | 75/532 [01:19<11:00,  1.45s/it]Loading train:  14%|█▍        | 76/532 [01:20<10:10,  1.34s/it]Loading train:  14%|█▍        | 77/532 [01:21<09:47,  1.29s/it]Loading train:  15%|█▍        | 78/532 [01:22<09:27,  1.25s/it]Loading train:  15%|█▍        | 79/532 [01:23<09:08,  1.21s/it]Loading train:  15%|█▌        | 80/532 [01:24<08:46,  1.17s/it]Loading train:  15%|█▌        | 81/532 [01:25<08:33,  1.14s/it]Loading train:  15%|█▌        | 82/532 [01:26<08:17,  1.11s/it]Loading train:  16%|█▌        | 83/532 [01:27<07:49,  1.05s/it]Loading train:  16%|█▌        | 84/532 [01:28<07:25,  1.01it/s]Loading train:  16%|█▌        | 85/532 [01:29<07:07,  1.05it/s]Loading train:  16%|█▌        | 86/532 [01:30<07:10,  1.03it/s]Loading train:  16%|█▋        | 87/532 [01:31<07:03,  1.05it/s]Loading train:  17%|█▋        | 88/532 [01:32<06:47,  1.09it/s]Loading train:  17%|█▋        | 89/532 [01:33<07:04,  1.04it/s]Loading train:  17%|█▋        | 90/532 [01:34<07:04,  1.04it/s]Loading train:  17%|█▋        | 91/532 [01:35<07:10,  1.03it/s]Loading train:  17%|█▋        | 92/532 [01:36<07:28,  1.02s/it]Loading train:  17%|█▋        | 93/532 [01:37<07:09,  1.02it/s]Loading train:  18%|█▊        | 94/532 [01:38<07:05,  1.03it/s]Loading train:  18%|█▊        | 95/532 [01:39<07:35,  1.04s/it]Loading train:  18%|█▊        | 96/532 [01:40<07:47,  1.07s/it]Loading train:  18%|█▊        | 97/532 [01:41<08:08,  1.12s/it]Loading train:  18%|█▊        | 98/532 [01:43<08:33,  1.18s/it]Loading train:  19%|█▊        | 99/532 [01:44<08:28,  1.17s/it]Loading train:  19%|█▉        | 100/532 [01:45<09:01,  1.25s/it]Loading train:  19%|█▉        | 101/532 [01:47<09:07,  1.27s/it]Loading train:  19%|█▉        | 102/532 [01:48<08:35,  1.20s/it]Loading train:  19%|█▉        | 103/532 [01:48<07:54,  1.11s/it]Loading train:  20%|█▉        | 104/532 [01:49<07:28,  1.05s/it]Loading train:  20%|█▉        | 105/532 [01:50<06:51,  1.04it/s]Loading train:  20%|█▉        | 106/532 [01:51<06:33,  1.08it/s]Loading train:  20%|██        | 107/532 [01:52<06:37,  1.07it/s]Loading train:  20%|██        | 108/532 [01:53<06:30,  1.09it/s]Loading train:  20%|██        | 109/532 [01:54<06:46,  1.04it/s]Loading train:  21%|██        | 110/532 [01:55<06:46,  1.04it/s]Loading train:  21%|██        | 111/532 [01:56<06:41,  1.05it/s]Loading train:  21%|██        | 112/532 [01:57<06:37,  1.06it/s]Loading train:  21%|██        | 113/532 [01:58<06:51,  1.02it/s]Loading train:  21%|██▏       | 114/532 [01:59<06:38,  1.05it/s]Loading train:  22%|██▏       | 115/532 [02:00<06:32,  1.06it/s]Loading train:  22%|██▏       | 116/532 [02:00<06:21,  1.09it/s]Loading train:  22%|██▏       | 117/532 [02:01<06:24,  1.08it/s]Loading train:  22%|██▏       | 118/532 [02:02<06:27,  1.07it/s]Loading train:  22%|██▏       | 119/532 [02:03<06:33,  1.05it/s]Loading train:  23%|██▎       | 120/532 [02:04<06:30,  1.05it/s]Loading train:  23%|██▎       | 121/532 [02:05<06:27,  1.06it/s]Loading train:  23%|██▎       | 122/532 [02:06<06:33,  1.04it/s]Loading train:  23%|██▎       | 123/532 [02:07<06:18,  1.08it/s]Loading train:  23%|██▎       | 124/532 [02:08<06:11,  1.10it/s]Loading train:  23%|██▎       | 125/532 [02:09<06:19,  1.07it/s]Loading train:  24%|██▎       | 126/532 [02:10<06:17,  1.08it/s]Loading train:  24%|██▍       | 127/532 [02:11<06:16,  1.08it/s]Loading train:  24%|██▍       | 128/532 [02:12<06:19,  1.06it/s]Loading train:  24%|██▍       | 129/532 [02:13<06:21,  1.06it/s]Loading train:  24%|██▍       | 130/532 [02:14<06:32,  1.02it/s]Loading train:  25%|██▍       | 131/532 [02:15<06:54,  1.03s/it]Loading train:  25%|██▍       | 132/532 [02:16<07:06,  1.07s/it]Loading train:  25%|██▌       | 133/532 [02:17<07:21,  1.11s/it]Loading train:  25%|██▌       | 134/532 [02:18<07:25,  1.12s/it]Loading train:  25%|██▌       | 135/532 [02:20<07:43,  1.17s/it]Loading train:  26%|██▌       | 136/532 [02:21<07:32,  1.14s/it]Loading train:  26%|██▌       | 137/532 [02:22<07:30,  1.14s/it]Loading train:  26%|██▌       | 138/532 [02:23<07:25,  1.13s/it]Loading train:  26%|██▌       | 139/532 [02:24<07:19,  1.12s/it]Loading train:  26%|██▋       | 140/532 [02:25<07:33,  1.16s/it]Loading train:  27%|██▋       | 141/532 [02:26<07:26,  1.14s/it]Loading train:  27%|██▋       | 142/532 [02:27<07:19,  1.13s/it]Loading train:  27%|██▋       | 143/532 [02:28<07:01,  1.08s/it]Loading train:  27%|██▋       | 144/532 [02:29<06:30,  1.01s/it]Loading train:  27%|██▋       | 145/532 [02:30<06:07,  1.05it/s]Loading train:  27%|██▋       | 146/532 [02:31<05:41,  1.13it/s]Loading train:  28%|██▊       | 147/532 [02:32<05:32,  1.16it/s]Loading train:  28%|██▊       | 148/532 [02:32<05:27,  1.17it/s]Loading train:  28%|██▊       | 149/532 [02:34<05:54,  1.08it/s]Loading train:  28%|██▊       | 150/532 [02:34<05:41,  1.12it/s]Loading train:  28%|██▊       | 151/532 [02:35<05:29,  1.15it/s]Loading train:  29%|██▊       | 152/532 [02:36<05:26,  1.16it/s]Loading train:  29%|██▉       | 153/532 [02:37<05:23,  1.17it/s]Loading train:  29%|██▉       | 154/532 [02:38<05:27,  1.16it/s]Loading train:  29%|██▉       | 155/532 [02:39<06:21,  1.01s/it]Loading train:  29%|██▉       | 156/532 [02:40<06:41,  1.07s/it]Loading train:  30%|██▉       | 157/532 [02:42<06:54,  1.11s/it]Loading train:  30%|██▉       | 158/532 [02:43<07:07,  1.14s/it]Loading train:  30%|██▉       | 159/532 [02:44<07:12,  1.16s/it]Loading train:  30%|███       | 160/532 [02:45<07:18,  1.18s/it]Loading train:  30%|███       | 161/532 [02:46<06:57,  1.13s/it]Loading train:  30%|███       | 162/532 [02:47<06:24,  1.04s/it]Loading train:  31%|███       | 163/532 [02:48<06:03,  1.02it/s]Loading train:  31%|███       | 164/532 [02:49<05:43,  1.07it/s]Loading train:  31%|███       | 165/532 [02:49<05:22,  1.14it/s]Loading train:  31%|███       | 166/532 [02:50<05:20,  1.14it/s]Loading train:  31%|███▏      | 167/532 [02:51<05:30,  1.11it/s]Loading train:  32%|███▏      | 168/532 [02:52<05:28,  1.11it/s]Loading train:  32%|███▏      | 169/532 [02:53<05:21,  1.13it/s]Loading train:  32%|███▏      | 170/532 [02:54<05:15,  1.15it/s]Loading train:  32%|███▏      | 171/532 [02:55<05:16,  1.14it/s]Loading train:  32%|███▏      | 172/532 [02:56<05:19,  1.13it/s]Loading train:  33%|███▎      | 173/532 [02:56<05:07,  1.17it/s]Loading train:  33%|███▎      | 174/532 [02:57<05:10,  1.15it/s]Loading train:  33%|███▎      | 175/532 [02:58<05:06,  1.17it/s]Loading train:  33%|███▎      | 176/532 [02:59<05:04,  1.17it/s]Loading train:  33%|███▎      | 177/532 [03:00<05:20,  1.11it/s]Loading train:  33%|███▎      | 178/532 [03:01<05:09,  1.14it/s]Loading train:  34%|███▎      | 179/532 [03:02<05:19,  1.10it/s]Loading train:  34%|███▍      | 180/532 [03:03<05:21,  1.10it/s]Loading train:  34%|███▍      | 181/532 [03:04<05:10,  1.13it/s]Loading train:  34%|███▍      | 182/532 [03:04<05:03,  1.15it/s]Loading train:  34%|███▍      | 183/532 [03:05<04:50,  1.20it/s]Loading train:  35%|███▍      | 184/532 [03:06<04:43,  1.23it/s]Loading train:  35%|███▍      | 185/532 [03:07<04:43,  1.22it/s]Loading train:  35%|███▍      | 186/532 [03:08<04:44,  1.22it/s]Loading train:  35%|███▌      | 187/532 [03:08<04:43,  1.22it/s]Loading train:  35%|███▌      | 188/532 [03:09<04:38,  1.23it/s]Loading train:  36%|███▌      | 189/532 [03:10<04:36,  1.24it/s]Loading train:  36%|███▌      | 190/532 [03:11<04:26,  1.28it/s]Loading train:  36%|███▌      | 191/532 [03:12<05:01,  1.13it/s]Loading train:  36%|███▌      | 192/532 [03:13<05:42,  1.01s/it]Loading train:  36%|███▋      | 193/532 [03:15<06:36,  1.17s/it]Loading train:  36%|███▋      | 194/532 [03:16<06:38,  1.18s/it]Loading train:  37%|███▋      | 195/532 [03:17<06:33,  1.17s/it]Loading train:  37%|███▋      | 196/532 [03:18<06:33,  1.17s/it]Loading train:  37%|███▋      | 197/532 [03:19<06:11,  1.11s/it]Loading train:  37%|███▋      | 198/532 [03:20<06:00,  1.08s/it]Loading train:  37%|███▋      | 199/532 [03:21<05:49,  1.05s/it]Loading train:  38%|███▊      | 200/532 [03:22<05:38,  1.02s/it]Loading train:  38%|███▊      | 201/532 [03:23<05:37,  1.02s/it]Loading train:  38%|███▊      | 202/532 [03:24<05:28,  1.00it/s]Loading train:  38%|███▊      | 203/532 [03:25<05:24,  1.01it/s]Loading train:  38%|███▊      | 204/532 [03:26<05:06,  1.07it/s]Loading train:  39%|███▊      | 205/532 [03:27<04:50,  1.12it/s]Loading train:  39%|███▊      | 206/532 [03:27<04:46,  1.14it/s]Loading train:  39%|███▉      | 207/532 [03:28<04:37,  1.17it/s]Loading train:  39%|███▉      | 208/532 [03:29<04:28,  1.21it/s]Loading train:  39%|███▉      | 209/532 [03:30<04:18,  1.25it/s]Loading train:  39%|███▉      | 210/532 [03:31<04:13,  1.27it/s]Loading train:  40%|███▉      | 211/532 [03:31<04:07,  1.30it/s]Loading train:  40%|███▉      | 212/532 [03:32<04:02,  1.32it/s]Loading train:  40%|████      | 213/532 [03:33<03:58,  1.34it/s]Loading train:  40%|████      | 214/532 [03:33<03:56,  1.34it/s]Loading train:  40%|████      | 215/532 [03:35<04:27,  1.18it/s]Loading train:  41%|████      | 216/532 [03:36<04:49,  1.09it/s]Loading train:  41%|████      | 217/532 [03:37<05:06,  1.03it/s]Loading train:  41%|████      | 218/532 [03:38<05:13,  1.00it/s]Loading train:  41%|████      | 219/532 [03:39<05:28,  1.05s/it]Loading train:  41%|████▏     | 220/532 [03:40<05:30,  1.06s/it]Loading train:  42%|████▏     | 221/532 [03:41<05:18,  1.03s/it]Loading train:  42%|████▏     | 222/532 [03:42<04:44,  1.09it/s]Loading train:  42%|████▏     | 223/532 [03:43<04:41,  1.10it/s]Loading train:  42%|████▏     | 224/532 [03:43<04:36,  1.11it/s]Loading train:  42%|████▏     | 225/532 [03:44<04:25,  1.15it/s]Loading train:  42%|████▏     | 226/532 [03:45<04:15,  1.20it/s]Loading train:  43%|████▎     | 227/532 [03:46<04:18,  1.18it/s]Loading train:  43%|████▎     | 228/532 [03:47<04:20,  1.17it/s]Loading train:  43%|████▎     | 229/532 [03:47<04:04,  1.24it/s]Loading train:  43%|████▎     | 230/532 [03:48<04:04,  1.24it/s]Loading train:  43%|████▎     | 231/532 [03:49<03:55,  1.28it/s]Loading train:  44%|████▎     | 232/532 [03:50<03:46,  1.33it/s]Loading train:  44%|████▍     | 233/532 [03:51<03:55,  1.27it/s]Loading train:  44%|████▍     | 234/532 [03:51<04:02,  1.23it/s]Loading train:  44%|████▍     | 235/532 [03:52<03:57,  1.25it/s]Loading train:  44%|████▍     | 236/532 [03:53<04:11,  1.18it/s]Loading train:  45%|████▍     | 237/532 [03:54<04:14,  1.16it/s]Loading train:  45%|████▍     | 238/532 [03:55<04:14,  1.15it/s]Loading train:  45%|████▍     | 239/532 [03:56<04:22,  1.12it/s]Loading train:  45%|████▌     | 240/532 [03:57<04:11,  1.16it/s]Loading train:  45%|████▌     | 241/532 [03:58<04:12,  1.15it/s]Loading train:  45%|████▌     | 242/532 [03:58<04:20,  1.11it/s]Loading train:  46%|████▌     | 243/532 [03:59<04:15,  1.13it/s]Loading train:  46%|████▌     | 244/532 [04:00<04:10,  1.15it/s]Loading train:  46%|████▌     | 245/532 [04:01<04:02,  1.18it/s]Loading train:  46%|████▌     | 246/532 [04:02<03:54,  1.22it/s]Loading train:  46%|████▋     | 247/532 [04:02<03:49,  1.24it/s]Loading train:  47%|████▋     | 248/532 [04:03<03:41,  1.28it/s]Loading train:  47%|████▋     | 249/532 [04:04<03:32,  1.33it/s]Loading train:  47%|████▋     | 250/532 [04:05<03:30,  1.34it/s]Loading train:  47%|████▋     | 251/532 [04:05<03:35,  1.30it/s]Loading train:  47%|████▋     | 252/532 [04:06<03:36,  1.29it/s]Loading train:  48%|████▊     | 253/532 [04:07<03:28,  1.34it/s]Loading train:  48%|████▊     | 254/532 [04:08<03:28,  1.34it/s]Loading train:  48%|████▊     | 255/532 [04:08<03:28,  1.33it/s]Loading train:  48%|████▊     | 256/532 [04:09<03:24,  1.35it/s]Loading train:  48%|████▊     | 257/532 [04:10<03:42,  1.23it/s]Loading train:  48%|████▊     | 258/532 [04:11<03:53,  1.17it/s]Loading train:  49%|████▊     | 259/532 [04:12<04:01,  1.13it/s]Loading train:  49%|████▉     | 260/532 [04:13<04:02,  1.12it/s]Loading train:  49%|████▉     | 261/532 [04:14<04:02,  1.12it/s]Loading train:  49%|████▉     | 262/532 [04:15<04:03,  1.11it/s]Loading train:  49%|████▉     | 263/532 [04:16<03:54,  1.15it/s]Loading train:  50%|████▉     | 264/532 [04:16<03:38,  1.23it/s]Loading train:  50%|████▉     | 265/532 [04:17<03:30,  1.27it/s]Loading train:  50%|█████     | 266/532 [04:18<03:21,  1.32it/s]Loading train:  50%|█████     | 267/532 [04:18<03:15,  1.35it/s]Loading train:  50%|█████     | 268/532 [04:19<03:09,  1.39it/s]Loading train:  51%|█████     | 269/532 [04:20<03:23,  1.29it/s]Loading train:  51%|█████     | 270/532 [04:21<03:28,  1.26it/s]Loading train:  51%|█████     | 271/532 [04:22<03:30,  1.24it/s]Loading train:  51%|█████     | 272/532 [04:22<03:31,  1.23it/s]Loading train:  51%|█████▏    | 273/532 [04:23<03:32,  1.22it/s]Loading train:  52%|█████▏    | 274/532 [04:24<03:28,  1.24it/s]Loading train:  52%|█████▏    | 275/532 [04:25<03:49,  1.12it/s]Loading train:  52%|█████▏    | 276/532 [04:26<04:04,  1.05it/s]Loading train:  52%|█████▏    | 277/532 [04:27<04:15,  1.00s/it]Loading train:  52%|█████▏    | 278/532 [04:28<04:21,  1.03s/it]Loading train:  52%|█████▏    | 279/532 [04:29<04:20,  1.03s/it]Loading train:  53%|█████▎    | 280/532 [04:31<04:20,  1.03s/it]Loading train:  53%|█████▎    | 281/532 [04:32<04:20,  1.04s/it]Loading train:  53%|█████▎    | 282/532 [04:33<04:19,  1.04s/it]Loading train:  53%|█████▎    | 283/532 [04:34<04:17,  1.03s/it]Loading train:  53%|█████▎    | 284/532 [04:35<04:18,  1.04s/it]Loading train:  54%|█████▎    | 285/532 [04:36<04:10,  1.02s/it]Loading train:  54%|█████▍    | 286/532 [04:37<04:06,  1.00s/it]Loading train:  54%|█████▍    | 287/532 [04:37<03:49,  1.07it/s]Loading train:  54%|█████▍    | 288/532 [04:38<03:34,  1.14it/s]Loading train:  54%|█████▍    | 289/532 [04:39<03:21,  1.21it/s]Loading train:  55%|█████▍    | 290/532 [04:40<03:13,  1.25it/s]Loading train:  55%|█████▍    | 291/532 [04:40<03:09,  1.27it/s]Loading train:  55%|█████▍    | 292/532 [04:41<03:08,  1.27it/s]Loading train:  55%|█████▌    | 293/532 [04:42<03:13,  1.24it/s]Loading train:  55%|█████▌    | 294/532 [04:43<03:13,  1.23it/s]Loading train:  55%|█████▌    | 295/532 [04:44<03:12,  1.23it/s]Loading train:  56%|█████▌    | 296/532 [04:44<03:13,  1.22it/s]Loading train:  56%|█████▌    | 297/532 [04:45<03:12,  1.22it/s]Loading train:  56%|█████▌    | 298/532 [04:46<03:08,  1.24it/s]Loading train:  56%|█████▌    | 299/532 [04:47<03:05,  1.26it/s]Loading train:  56%|█████▋    | 300/532 [04:48<02:57,  1.31it/s]Loading train:  57%|█████▋    | 301/532 [04:48<02:58,  1.29it/s]Loading train:  57%|█████▋    | 302/532 [04:49<02:51,  1.34it/s]Loading train:  57%|█████▋    | 303/532 [04:50<02:52,  1.33it/s]Loading train:  57%|█████▋    | 304/532 [04:50<02:48,  1.35it/s]Loading train:  57%|█████▋    | 305/532 [04:52<03:13,  1.17it/s]Loading train:  58%|█████▊    | 306/532 [04:53<03:23,  1.11it/s]Loading train:  58%|█████▊    | 307/532 [04:54<03:33,  1.05it/s]Loading train:  58%|█████▊    | 308/532 [04:55<03:38,  1.03it/s]Loading train:  58%|█████▊    | 309/532 [04:56<03:44,  1.01s/it]Loading train:  58%|█████▊    | 310/532 [04:57<03:46,  1.02s/it]Loading train:  58%|█████▊    | 311/532 [04:58<04:17,  1.16s/it]Loading train:  59%|█████▊    | 312/532 [05:00<04:33,  1.24s/it]Loading train:  59%|█████▉    | 313/532 [05:01<04:43,  1.30s/it]Loading train:  59%|█████▉    | 314/532 [05:03<04:51,  1.34s/it]Loading train:  59%|█████▉    | 315/532 [05:04<04:54,  1.36s/it]Loading train:  59%|█████▉    | 316/532 [05:05<04:58,  1.38s/it]Loading train:  60%|█████▉    | 317/532 [05:06<04:24,  1.23s/it]Loading train:  60%|█████▉    | 318/532 [05:07<03:51,  1.08s/it]Loading train:  60%|█████▉    | 319/532 [05:08<03:31,  1.01it/s]Loading train:  60%|██████    | 320/532 [05:09<03:16,  1.08it/s]Loading train:  60%|██████    | 321/532 [05:09<03:06,  1.13it/s]Loading train:  61%|██████    | 322/532 [05:10<02:58,  1.18it/s]Loading train:  61%|██████    | 323/532 [05:11<03:21,  1.04it/s]Loading train:  61%|██████    | 324/532 [05:13<03:30,  1.01s/it]Loading train:  61%|██████    | 325/532 [05:14<03:36,  1.05s/it]Loading train:  61%|██████▏   | 326/532 [05:15<03:40,  1.07s/it]Loading train:  61%|██████▏   | 327/532 [05:16<03:39,  1.07s/it]Loading train:  62%|██████▏   | 328/532 [05:17<03:48,  1.12s/it]Loading train:  62%|██████▏   | 329/532 [05:18<03:38,  1.08s/it]Loading train:  62%|██████▏   | 330/532 [05:19<03:17,  1.02it/s]Loading train:  62%|██████▏   | 331/532 [05:20<03:05,  1.08it/s]Loading train:  62%|██████▏   | 332/532 [05:20<02:56,  1.13it/s]Loading train:  63%|██████▎   | 333/532 [05:21<02:49,  1.17it/s]Loading train:  63%|██████▎   | 334/532 [05:22<02:44,  1.20it/s]Loading train:  63%|██████▎   | 335/532 [05:23<02:55,  1.12it/s]Loading train:  63%|██████▎   | 336/532 [05:24<03:01,  1.08it/s]Loading train:  63%|██████▎   | 337/532 [05:25<03:06,  1.04it/s]Loading train:  64%|██████▎   | 338/532 [05:26<03:10,  1.02it/s]Loading train:  64%|██████▎   | 339/532 [05:27<03:09,  1.02it/s]Loading train:  64%|██████▍   | 340/532 [05:28<03:09,  1.01it/s]Loading train:  64%|██████▍   | 341/532 [05:29<02:55,  1.09it/s]Loading train:  64%|██████▍   | 342/532 [05:30<02:43,  1.16it/s]Loading train:  64%|██████▍   | 343/532 [05:30<02:34,  1.22it/s]Loading train:  65%|██████▍   | 344/532 [05:31<02:27,  1.27it/s]Loading train:  65%|██████▍   | 345/532 [05:32<02:23,  1.31it/s]Loading train:  65%|██████▌   | 346/532 [05:32<02:20,  1.33it/s]Loading train:  65%|██████▌   | 347/532 [05:33<02:33,  1.21it/s]Loading train:  65%|██████▌   | 348/532 [05:34<02:30,  1.23it/s]Loading train:  66%|██████▌   | 349/532 [05:35<02:29,  1.23it/s]Loading train:  66%|██████▌   | 350/532 [05:36<02:28,  1.23it/s]Loading train:  66%|██████▌   | 351/532 [05:37<02:26,  1.23it/s]Loading train:  66%|██████▌   | 352/532 [05:37<02:26,  1.23it/s]Loading train:  66%|██████▋   | 353/532 [05:38<02:33,  1.17it/s]Loading train:  67%|██████▋   | 354/532 [05:39<02:29,  1.19it/s]Loading train:  67%|██████▋   | 355/532 [05:40<02:26,  1.21it/s]Loading train:  67%|██████▋   | 356/532 [05:41<02:23,  1.23it/s]Loading train:  67%|██████▋   | 357/532 [05:42<02:22,  1.23it/s]Loading train:  67%|██████▋   | 358/532 [05:42<02:22,  1.22it/s]Loading train:  67%|██████▋   | 359/532 [05:43<02:23,  1.20it/s]Loading train:  68%|██████▊   | 360/532 [05:44<02:18,  1.24it/s]Loading train:  68%|██████▊   | 361/532 [05:45<02:18,  1.24it/s]Loading train:  68%|██████▊   | 362/532 [05:46<02:14,  1.27it/s]Loading train:  68%|██████▊   | 363/532 [05:46<02:09,  1.30it/s]Loading train:  68%|██████▊   | 364/532 [05:47<02:05,  1.34it/s]Loading train:  69%|██████▊   | 365/532 [05:48<02:04,  1.34it/s]Loading train:  69%|██████▉   | 366/532 [05:49<02:06,  1.31it/s]Loading train:  69%|██████▉   | 367/532 [05:49<02:04,  1.32it/s]Loading train:  69%|██████▉   | 368/532 [05:50<02:01,  1.35it/s]Loading train:  69%|██████▉   | 369/532 [05:51<02:01,  1.34it/s]Loading train:  70%|██████▉   | 370/532 [05:51<02:00,  1.35it/s]Loading train:  70%|██████▉   | 371/532 [05:52<02:11,  1.22it/s]Loading train:  70%|██████▉   | 372/532 [05:54<02:23,  1.12it/s]Loading train:  70%|███████   | 373/532 [05:55<02:28,  1.07it/s]Loading train:  70%|███████   | 374/532 [05:56<02:33,  1.03it/s]Loading train:  70%|███████   | 375/532 [05:57<02:35,  1.01it/s]Loading train:  71%|███████   | 376/532 [05:58<02:35,  1.01it/s]Loading train:  71%|███████   | 377/532 [05:59<02:27,  1.05it/s]Loading train:  71%|███████   | 378/532 [05:59<02:18,  1.11it/s]Loading train:  71%|███████   | 379/532 [06:00<02:15,  1.13it/s]Loading train:  71%|███████▏  | 380/532 [06:01<02:09,  1.17it/s]Loading train:  72%|███████▏  | 381/532 [06:02<02:03,  1.22it/s]Loading train:  72%|███████▏  | 382/532 [06:03<02:03,  1.21it/s]Loading train:  72%|███████▏  | 383/532 [06:04<02:09,  1.15it/s]Loading train:  72%|███████▏  | 384/532 [06:04<02:07,  1.16it/s]Loading train:  72%|███████▏  | 385/532 [06:05<02:07,  1.15it/s]Loading train:  73%|███████▎  | 386/532 [06:06<02:07,  1.15it/s]Loading train:  73%|███████▎  | 387/532 [06:07<02:08,  1.13it/s]Loading train:  73%|███████▎  | 388/532 [06:08<02:07,  1.13it/s]Loading train:  73%|███████▎  | 389/532 [06:09<02:11,  1.09it/s]Loading train:  73%|███████▎  | 390/532 [06:10<02:07,  1.11it/s]Loading train:  73%|███████▎  | 391/532 [06:11<02:05,  1.12it/s]Loading train:  74%|███████▎  | 392/532 [06:12<02:04,  1.13it/s]Loading train:  74%|███████▍  | 393/532 [06:12<02:01,  1.14it/s]Loading train:  74%|███████▍  | 394/532 [06:13<02:01,  1.13it/s]Loading train:  74%|███████▍  | 395/532 [06:14<02:08,  1.07it/s]Loading train:  74%|███████▍  | 396/532 [06:15<02:04,  1.09it/s]Loading train:  75%|███████▍  | 397/532 [06:16<02:05,  1.08it/s]Loading train:  75%|███████▍  | 398/532 [06:17<02:04,  1.07it/s]Loading train:  75%|███████▌  | 399/532 [06:18<02:00,  1.10it/s]Loading train:  75%|███████▌  | 400/532 [06:19<01:58,  1.12it/s]Loading train:  75%|███████▌  | 401/532 [06:20<02:01,  1.08it/s]Loading train:  76%|███████▌  | 402/532 [06:21<02:01,  1.07it/s]Loading train:  76%|███████▌  | 403/532 [06:22<02:01,  1.06it/s]Loading train:  76%|███████▌  | 404/532 [06:23<02:01,  1.06it/s]Loading train:  76%|███████▌  | 405/532 [06:24<02:00,  1.06it/s]Loading train:  76%|███████▋  | 406/532 [06:25<02:00,  1.05it/s]Loading train:  77%|███████▋  | 407/532 [06:25<01:55,  1.08it/s]Loading train:  77%|███████▋  | 408/532 [06:26<01:53,  1.09it/s]Loading train:  77%|███████▋  | 409/532 [06:27<01:50,  1.11it/s]Loading train:  77%|███████▋  | 410/532 [06:28<01:47,  1.14it/s]Loading train:  77%|███████▋  | 411/532 [06:29<01:45,  1.15it/s]Loading train:  77%|███████▋  | 412/532 [06:30<01:41,  1.18it/s]Loading train:  78%|███████▊  | 413/532 [06:31<01:41,  1.17it/s]Loading train:  78%|███████▊  | 414/532 [06:31<01:38,  1.19it/s]Loading train:  78%|███████▊  | 415/532 [06:32<01:36,  1.21it/s]Loading train:  78%|███████▊  | 416/532 [06:33<01:36,  1.20it/s]Loading train:  78%|███████▊  | 417/532 [06:34<01:36,  1.19it/s]Loading train:  79%|███████▊  | 418/532 [06:35<01:36,  1.18it/s]Loading train:  79%|███████▉  | 419/532 [06:36<01:39,  1.14it/s]Loading train:  79%|███████▉  | 420/532 [06:37<01:39,  1.12it/s]Loading train:  79%|███████▉  | 421/532 [06:38<01:45,  1.06it/s]Loading train:  79%|███████▉  | 422/532 [06:39<01:44,  1.05it/s]Loading train:  80%|███████▉  | 423/532 [06:40<01:42,  1.07it/s]Loading train:  80%|███████▉  | 424/532 [06:40<01:40,  1.07it/s]Loading train:  80%|███████▉  | 425/532 [06:41<01:40,  1.07it/s]Loading train:  80%|████████  | 426/532 [06:42<01:37,  1.08it/s]Loading train:  80%|████████  | 427/532 [06:43<01:33,  1.12it/s]Loading train:  80%|████████  | 428/532 [06:44<01:31,  1.14it/s]Loading train:  81%|████████  | 429/532 [06:45<01:27,  1.17it/s]Loading train:  81%|████████  | 430/532 [06:46<01:30,  1.13it/s]Loading train:  81%|████████  | 431/532 [06:47<01:38,  1.03it/s]Loading train:  81%|████████  | 432/532 [06:48<01:40,  1.00s/it]Loading train:  81%|████████▏ | 433/532 [06:49<01:38,  1.00it/s]Loading train:  82%|████████▏ | 434/532 [06:50<01:39,  1.02s/it]Loading train:  82%|████████▏ | 435/532 [06:51<01:38,  1.01s/it]Loading train:  82%|████████▏ | 436/532 [06:52<01:37,  1.01s/it]Loading train:  82%|████████▏ | 437/532 [06:53<01:30,  1.05it/s]Loading train:  82%|████████▏ | 438/532 [06:54<01:26,  1.09it/s]Loading train:  83%|████████▎ | 439/532 [06:54<01:19,  1.17it/s]Loading train:  83%|████████▎ | 440/532 [06:55<01:17,  1.19it/s]Loading train:  83%|████████▎ | 441/532 [06:56<01:15,  1.21it/s]Loading train:  83%|████████▎ | 442/532 [06:57<01:12,  1.24it/s]Loading train:  83%|████████▎ | 443/532 [06:57<01:09,  1.28it/s]Loading train:  83%|████████▎ | 444/532 [06:58<01:08,  1.29it/s]Loading train:  84%|████████▎ | 445/532 [06:59<01:05,  1.33it/s]Loading train:  84%|████████▍ | 446/532 [07:00<01:05,  1.31it/s]Loading train:  84%|████████▍ | 447/532 [07:00<01:03,  1.33it/s]Loading train:  84%|████████▍ | 448/532 [07:01<01:01,  1.36it/s]Loading train:  84%|████████▍ | 449/532 [07:02<01:04,  1.29it/s]Loading train:  85%|████████▍ | 450/532 [07:03<01:03,  1.29it/s]Loading train:  85%|████████▍ | 451/532 [07:04<01:03,  1.28it/s]Loading train:  85%|████████▍ | 452/532 [07:04<01:02,  1.28it/s]Loading train:  85%|████████▌ | 453/532 [07:05<01:02,  1.26it/s]Loading train:  85%|████████▌ | 454/532 [07:06<01:03,  1.22it/s]Loading train:  86%|████████▌ | 455/532 [07:07<01:06,  1.16it/s]Loading train:  86%|████████▌ | 456/532 [07:08<01:06,  1.14it/s]Loading train:  86%|████████▌ | 457/532 [07:09<01:07,  1.11it/s]Loading train:  86%|████████▌ | 458/532 [07:10<01:06,  1.12it/s]Loading train:  86%|████████▋ | 459/532 [07:11<01:04,  1.13it/s]Loading train:  86%|████████▋ | 460/532 [07:12<01:05,  1.10it/s]Loading train:  87%|████████▋ | 461/532 [07:13<01:10,  1.01it/s]Loading train:  87%|████████▋ | 462/532 [07:14<01:10,  1.01s/it]Loading train:  87%|████████▋ | 463/532 [07:15<01:09,  1.01s/it]Loading train:  87%|████████▋ | 464/532 [07:16<01:09,  1.02s/it]Loading train:  87%|████████▋ | 465/532 [07:17<01:08,  1.02s/it]Loading train:  88%|████████▊ | 466/532 [07:18<01:06,  1.01s/it]Loading train:  88%|████████▊ | 467/532 [07:19<01:02,  1.05it/s]Loading train:  88%|████████▊ | 468/532 [07:19<00:57,  1.11it/s]Loading train:  88%|████████▊ | 469/532 [07:20<00:53,  1.17it/s]Loading train:  88%|████████▊ | 470/532 [07:21<00:51,  1.20it/s]Loading train:  89%|████████▊ | 471/532 [07:22<00:50,  1.21it/s]Loading train:  89%|████████▊ | 472/532 [07:23<00:49,  1.22it/s]Loading train:  89%|████████▉ | 473/532 [07:24<00:50,  1.18it/s]Loading train:  89%|████████▉ | 474/532 [07:24<00:49,  1.17it/s]Loading train:  89%|████████▉ | 475/532 [07:25<00:50,  1.13it/s]Loading train:  89%|████████▉ | 476/532 [07:26<00:49,  1.13it/s]Loading train:  90%|████████▉ | 477/532 [07:27<00:50,  1.08it/s]Loading train:  90%|████████▉ | 478/532 [07:28<00:49,  1.09it/s]Loading train:  90%|█████████ | 479/532 [07:29<00:46,  1.13it/s]Loading train:  90%|█████████ | 480/532 [07:30<00:44,  1.17it/s]Loading train:  90%|█████████ | 481/532 [07:31<00:42,  1.19it/s]Loading train:  91%|█████████ | 482/532 [07:31<00:42,  1.17it/s]Loading train:  91%|█████████ | 483/532 [07:32<00:40,  1.21it/s]Loading train:  91%|█████████ | 484/532 [07:33<00:39,  1.21it/s]Loading train:  91%|█████████ | 485/532 [07:34<00:42,  1.11it/s]Loading train:  91%|█████████▏| 486/532 [07:35<00:42,  1.09it/s]Loading train:  92%|█████████▏| 487/532 [07:36<00:42,  1.06it/s]Loading train:  92%|█████████▏| 488/532 [07:37<00:43,  1.02it/s]Loading train:  92%|█████████▏| 489/532 [07:38<00:41,  1.03it/s]Loading train:  92%|█████████▏| 490/532 [07:39<00:41,  1.02it/s]Loading train:  92%|█████████▏| 491/532 [07:40<00:39,  1.03it/s]Loading train:  92%|█████████▏| 492/532 [07:41<00:37,  1.05it/s]Loading train:  93%|█████████▎| 493/532 [07:42<00:35,  1.08it/s]Loading train:  93%|█████████▎| 494/532 [07:43<00:34,  1.11it/s]Loading train:  93%|█████████▎| 495/532 [07:44<00:32,  1.13it/s]Loading train:  93%|█████████▎| 496/532 [07:44<00:30,  1.16it/s]Loading train:  93%|█████████▎| 497/532 [07:45<00:30,  1.17it/s]Loading train:  94%|█████████▎| 498/532 [07:46<00:29,  1.17it/s]Loading train:  94%|█████████▍| 499/532 [07:47<00:28,  1.15it/s]Loading train:  94%|█████████▍| 500/532 [07:48<00:27,  1.14it/s]Loading train:  94%|█████████▍| 501/532 [07:49<00:27,  1.14it/s]Loading train:  94%|█████████▍| 502/532 [07:50<00:26,  1.15it/s]Loading train:  95%|█████████▍| 503/532 [07:50<00:24,  1.17it/s]Loading train:  95%|█████████▍| 504/532 [07:51<00:23,  1.19it/s]Loading train:  95%|█████████▍| 505/532 [07:52<00:22,  1.20it/s]Loading train:  95%|█████████▌| 506/532 [07:53<00:21,  1.23it/s]Loading train:  95%|█████████▌| 507/532 [07:54<00:19,  1.25it/s]Loading train:  95%|█████████▌| 508/532 [07:54<00:18,  1.26it/s]Loading train:  96%|█████████▌| 509/532 [07:55<00:19,  1.19it/s]Loading train:  96%|█████████▌| 510/532 [07:56<00:19,  1.15it/s]Loading train:  96%|█████████▌| 511/532 [07:57<00:18,  1.12it/s]Loading train:  96%|█████████▌| 512/532 [07:58<00:18,  1.10it/s]Loading train:  96%|█████████▋| 513/532 [07:59<00:17,  1.09it/s]Loading train:  97%|█████████▋| 514/532 [08:00<00:16,  1.10it/s]Loading train:  97%|█████████▋| 515/532 [08:01<00:15,  1.13it/s]Loading train:  97%|█████████▋| 516/532 [08:02<00:13,  1.16it/s]Loading train:  97%|█████████▋| 517/532 [08:02<00:12,  1.16it/s]Loading train:  97%|█████████▋| 518/532 [08:03<00:11,  1.18it/s]Loading train:  98%|█████████▊| 519/532 [08:04<00:11,  1.17it/s]Loading train:  98%|█████████▊| 520/532 [08:05<00:10,  1.19it/s]Loading train:  98%|█████████▊| 521/532 [08:06<00:09,  1.14it/s]Loading train:  98%|█████████▊| 522/532 [08:07<00:08,  1.13it/s]Loading train:  98%|█████████▊| 523/532 [08:08<00:07,  1.13it/s]Loading train:  98%|█████████▊| 524/532 [08:09<00:07,  1.12it/s]Loading train:  99%|█████████▊| 525/532 [08:09<00:06,  1.11it/s]Loading train:  99%|█████████▉| 526/532 [08:10<00:05,  1.10it/s]Loading train:  99%|█████████▉| 527/532 [08:11<00:04,  1.12it/s]Loading train:  99%|█████████▉| 528/532 [08:12<00:03,  1.14it/s]Loading train:  99%|█████████▉| 529/532 [08:13<00:02,  1.18it/s]Loading train: 100%|█████████▉| 530/532 [08:14<00:01,  1.20it/s]Loading train: 100%|█████████▉| 531/532 [08:15<00:00,  1.19it/s]Loading train: 100%|██████████| 532/532 [08:15<00:00,  1.20it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   5%|▌         | 28/532 [00:00<00:01, 273.39it/s]concatenating: train:  10%|█         | 54/532 [00:00<00:01, 267.85it/s]concatenating: train:  15%|█▌        | 82/532 [00:00<00:01, 270.11it/s]concatenating: train:  21%|██        | 113/532 [00:00<00:01, 280.36it/s]concatenating: train:  27%|██▋       | 142/532 [00:00<00:01, 281.26it/s]concatenating: train:  33%|███▎      | 174/532 [00:00<00:01, 289.77it/s]concatenating: train:  38%|███▊      | 204/532 [00:00<00:01, 287.45it/s]concatenating: train:  44%|████▍     | 235/532 [00:00<00:01, 287.89it/s]concatenating: train:  50%|████▉     | 265/532 [00:00<00:00, 288.97it/s]concatenating: train:  55%|█████▌    | 295/532 [00:01<00:00, 291.10it/s]concatenating: train:  61%|██████▏   | 326/532 [00:01<00:00, 295.18it/s]concatenating: train:  67%|██████▋   | 358/532 [00:01<00:00, 298.97it/s]concatenating: train:  73%|███████▎  | 388/532 [00:01<00:00, 296.04it/s]concatenating: train:  79%|███████▊  | 418/532 [00:01<00:00, 284.77it/s]concatenating: train:  84%|████████▍ | 447/532 [00:01<00:00, 272.24it/s]concatenating: train:  89%|████████▉ | 475/532 [00:01<00:00, 258.08it/s]concatenating: train:  94%|█████████▍| 502/532 [00:01<00:00, 245.41it/s]concatenating: train: 100%|██████████| 532/532 [00:01<00:00, 280.29it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:11,  1.20it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:10,  1.19it/s]Loading test:  20%|██        | 3/15 [00:02<00:10,  1.15it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:09,  1.13it/s]Loading test:  33%|███▎      | 5/15 [00:04<00:09,  1.05it/s]Loading test:  40%|████      | 6/15 [00:05<00:09,  1.02s/it]Loading test:  47%|████▋     | 7/15 [00:06<00:07,  1.07it/s]Loading test:  53%|█████▎    | 8/15 [00:07<00:07,  1.01s/it]Loading test:  60%|██████    | 9/15 [00:08<00:06,  1.03s/it]Loading test:  67%|██████▋   | 10/15 [00:09<00:04,  1.06it/s]Loading test:  73%|███████▎  | 11/15 [00:10<00:03,  1.05it/s]Loading test:  80%|████████  | 12/15 [00:11<00:02,  1.04it/s]Loading test:  87%|████████▋ | 13/15 [00:12<00:02,  1.02s/it]Loading test:  93%|█████████▎| 14/15 [00:13<00:00,  1.03it/s]Loading test: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 543.99it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 56, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 56, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 56, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 56, 10)   910         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 56, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 56, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 84, 56, 11)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 28, 11)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 28, 11)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 28, 20)   2000        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 28, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 28, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 28, 20)   3620        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 28, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 28, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 42, 28, 31)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 14, 31)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 14, 31)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 14, 40)   11200       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 14, 40)   160         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 14, 40)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 14, 40)   14440       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 14, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 14, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 21, 14, 71)   0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 14, 71)   0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 28, 20)   5700        dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 28, 51)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 28, 20)   9200        concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 28, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 28, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 28, 20)   3620        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 28, 20)   80          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 28, 20)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 42, 28, 71)   0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-06 01:03:19.180309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 01:03:19.180430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 01:03:19.180447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 01:03:19.180456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 01:03:19.180879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 42, 28, 71)   0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 56, 10)   2850        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 56, 21)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 56, 10)   1900        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 56, 10)   40          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 56, 10)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 56, 10)   910         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 56, 10)   40          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 56, 10)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 84, 56, 31)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 56, 31)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 56, 13)   416         dropout_5[0][0]                  
==================================================================================================
Total params: 57,666
Trainable params: 57,266
Non-trainable params: 400
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53807809e-02 2.88974534e-02 1.16728790e-01 1.00198377e-02
 3.03363016e-02 5.79915656e-03 6.86746312e-02 1.28228722e-01
 7.55625878e-02 1.22514673e-02 2.73642650e-01 1.84278063e-01
 1.99559502e-04]
Train on 19871 samples, validate on 569 samples
Epoch 1/300
 - 23s - loss: 95.0270 - acc: 0.7513 - mDice: 0.0156 - val_loss: 7.9543 - val_acc: 0.9112 - val_mDice: 0.0168

Epoch 00001: val_mDice improved from -inf to 0.01678, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 17s - loss: 13.2229 - acc: 0.8920 - mDice: 0.0261 - val_loss: 6.5479 - val_acc: 0.9112 - val_mDice: 0.0254

Epoch 00002: val_mDice improved from 0.01678 to 0.02536, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 15s - loss: 10.2233 - acc: 0.8929 - mDice: 0.0352 - val_loss: 5.8615 - val_acc: 0.9112 - val_mDice: 0.0461

Epoch 00003: val_mDice improved from 0.02536 to 0.04613, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 15s - loss: 8.7329 - acc: 0.8932 - mDice: 0.0440 - val_loss: 5.4529 - val_acc: 0.9112 - val_mDice: 0.0606

Epoch 00004: val_mDice improved from 0.04613 to 0.06056, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 15s - loss: 7.8441 - acc: 0.8932 - mDice: 0.0541 - val_loss: 5.1272 - val_acc: 0.9107 - val_mDice: 0.0742

Epoch 00005: val_mDice improved from 0.06056 to 0.07424, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 16s - loss: 7.2363 - acc: 0.8936 - mDice: 0.0653 - val_loss: 4.8447 - val_acc: 0.9109 - val_mDice: 0.0963

Epoch 00006: val_mDice improved from 0.07424 to 0.09633, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 15s - loss: 6.7325 - acc: 0.8945 - mDice: 0.0784 - val_loss: 4.5559 - val_acc: 0.9103 - val_mDice: 0.1173

Epoch 00007: val_mDice improved from 0.09633 to 0.11730, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300
 - 15s - loss: 6.2957 - acc: 0.8964 - mDice: 0.0943 - val_loss: 4.3858 - val_acc: 0.9106 - val_mDice: 0.1367

Epoch 00008: val_mDice improved from 0.11730 to 0.13670, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300
 - 15s - loss: 5.9017 - acc: 0.9014 - mDice: 0.1150 - val_loss: 3.9701 - val_acc: 0.9257 - val_mDice: 0.1771

Epoch 00009: val_mDice improved from 0.13670 to 0.17711, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300
 - 15s - loss: 5.5298 - acc: 0.9071 - mDice: 0.1377 - val_loss: 3.7422 - val_acc: 0.9315 - val_mDice: 0.2089

Epoch 00010: val_mDice improved from 0.17711 to 0.20888, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300
 - 16s - loss: 5.1987 - acc: 0.9107 - mDice: 0.1601 - val_loss: 3.4790 - val_acc: 0.9330 - val_mDice: 0.2357

Epoch 00011: val_mDice improved from 0.20888 to 0.23570, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300
 - 15s - loss: 4.8645 - acc: 0.9125 - mDice: 0.1846 - val_loss: 3.2851 - val_acc: 0.9331 - val_mDice: 0.2634

Epoch 00012: val_mDice improved from 0.23570 to 0.26340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 13/300
 - 15s - loss: 4.4915 - acc: 0.9143 - mDice: 0.2168 - val_loss: 3.0541 - val_acc: 0.9348 - val_mDice: 0.3025

Epoch 00013: val_mDice improved from 0.26340 to 0.30246, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300
 - 15s - loss: 3.9751 - acc: 0.9190 - mDice: 0.2650 - val_loss: 2.6239 - val_acc: 0.9483 - val_mDice: 0.3745

Epoch 00014: val_mDice improved from 0.30246 to 0.37452, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 15/300
 - 15s - loss: 3.6875 - acc: 0.9237 - mDice: 0.3022 - val_loss: 2.5135 - val_acc: 0.9507 - val_mDice: 0.4044

Epoch 00015: val_mDice improved from 0.37452 to 0.40440, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 16/300
 - 16s - loss: 3.4661 - acc: 0.9275 - mDice: 0.3326 - val_loss: 2.2514 - val_acc: 0.9560 - val_mDice: 0.4494

Epoch 00016: val_mDice improved from 0.40440 to 0.44936, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 17/300
 - 15s - loss: 3.2901 - acc: 0.9298 - mDice: 0.3574 - val_loss: 2.1973 - val_acc: 0.9572 - val_mDice: 0.4720

Epoch 00017: val_mDice improved from 0.44936 to 0.47197, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 18/300
 - 15s - loss: 3.1331 - acc: 0.9315 - mDice: 0.3799 - val_loss: 2.0042 - val_acc: 0.9607 - val_mDice: 0.5093

Epoch 00018: val_mDice improved from 0.47197 to 0.50930, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300
 - 15s - loss: 2.9933 - acc: 0.9335 - mDice: 0.4020 - val_loss: 1.9180 - val_acc: 0.9607 - val_mDice: 0.5278

Epoch 00019: val_mDice improved from 0.50930 to 0.52785, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 20/300
 - 15s - loss: 2.8912 - acc: 0.9352 - mDice: 0.4193 - val_loss: 1.8598 - val_acc: 0.9628 - val_mDice: 0.5488

Epoch 00020: val_mDice improved from 0.52785 to 0.54876, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 21/300
 - 16s - loss: 2.7781 - acc: 0.9370 - mDice: 0.4384 - val_loss: 1.8278 - val_acc: 0.9638 - val_mDice: 0.5553

Epoch 00021: val_mDice improved from 0.54876 to 0.55528, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300
 - 15s - loss: 2.7091 - acc: 0.9382 - mDice: 0.4512 - val_loss: 1.7069 - val_acc: 0.9651 - val_mDice: 0.5780

Epoch 00022: val_mDice improved from 0.55528 to 0.57802, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 23/300
 - 15s - loss: 2.6306 - acc: 0.9395 - mDice: 0.4654 - val_loss: 1.7919 - val_acc: 0.9641 - val_mDice: 0.5790

Epoch 00023: val_mDice improved from 0.57802 to 0.57898, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 24/300
 - 15s - loss: 2.5685 - acc: 0.9405 - mDice: 0.4760 - val_loss: 1.6285 - val_acc: 0.9665 - val_mDice: 0.5937

Epoch 00024: val_mDice improved from 0.57898 to 0.59372, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 25/300
 - 15s - loss: 2.5088 - acc: 0.9417 - mDice: 0.4869 - val_loss: 1.6054 - val_acc: 0.9660 - val_mDice: 0.6071

Epoch 00025: val_mDice improved from 0.59372 to 0.60714, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 26/300
 - 16s - loss: 2.4544 - acc: 0.9427 - mDice: 0.4966 - val_loss: 1.5738 - val_acc: 0.9685 - val_mDice: 0.6059

Epoch 00026: val_mDice did not improve from 0.60714
Epoch 27/300
 - 16s - loss: 2.4137 - acc: 0.9434 - mDice: 0.5036 - val_loss: 1.5532 - val_acc: 0.9681 - val_mDice: 0.6214

Epoch 00027: val_mDice improved from 0.60714 to 0.62138, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 28/300
 - 15s - loss: 2.3617 - acc: 0.9444 - mDice: 0.5121 - val_loss: 1.5261 - val_acc: 0.9691 - val_mDice: 0.6213

Epoch 00028: val_mDice did not improve from 0.62138
Epoch 29/300
 - 15s - loss: 2.3238 - acc: 0.9450 - mDice: 0.5188 - val_loss: 1.5390 - val_acc: 0.9694 - val_mDice: 0.6206

Epoch 00029: val_mDice did not improve from 0.62138
Epoch 30/300
 - 15s - loss: 2.2896 - acc: 0.9457 - mDice: 0.5257 - val_loss: 1.4824 - val_acc: 0.9702 - val_mDice: 0.6360

Epoch 00030: val_mDice improved from 0.62138 to 0.63602, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 31/300
 - 15s - loss: 2.2442 - acc: 0.9464 - mDice: 0.5328 - val_loss: 1.4913 - val_acc: 0.9704 - val_mDice: 0.6379

Epoch 00031: val_mDice improved from 0.63602 to 0.63787, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 32/300
 - 16s - loss: 2.2192 - acc: 0.9469 - mDice: 0.5380 - val_loss: 1.4678 - val_acc: 0.9705 - val_mDice: 0.6384

Epoch 00032: val_mDice improved from 0.63787 to 0.63835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 33/300
 - 15s - loss: 2.1795 - acc: 0.9476 - mDice: 0.5450 - val_loss: 1.4184 - val_acc: 0.9709 - val_mDice: 0.6472

Epoch 00033: val_mDice improved from 0.63835 to 0.64719, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 34/300
 - 15s - loss: 2.1557 - acc: 0.9480 - mDice: 0.5499 - val_loss: 1.4263 - val_acc: 0.9712 - val_mDice: 0.6553

Epoch 00034: val_mDice improved from 0.64719 to 0.65526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 35/300
 - 15s - loss: 2.1165 - acc: 0.9486 - mDice: 0.5574 - val_loss: 1.3998 - val_acc: 0.9711 - val_mDice: 0.6602

Epoch 00035: val_mDice improved from 0.65526 to 0.66017, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 36/300
 - 15s - loss: 2.0928 - acc: 0.9490 - mDice: 0.5621 - val_loss: 1.3737 - val_acc: 0.9721 - val_mDice: 0.6626

Epoch 00036: val_mDice improved from 0.66017 to 0.66260, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 37/300
 - 16s - loss: 2.0649 - acc: 0.9495 - mDice: 0.5671 - val_loss: 1.3492 - val_acc: 0.9721 - val_mDice: 0.6681

Epoch 00037: val_mDice improved from 0.66260 to 0.66806, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300
 - 16s - loss: 2.0372 - acc: 0.9499 - mDice: 0.5721 - val_loss: 1.3834 - val_acc: 0.9716 - val_mDice: 0.6685

Epoch 00038: val_mDice improved from 0.66806 to 0.66852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 39/300
 - 15s - loss: 2.0061 - acc: 0.9505 - mDice: 0.5776 - val_loss: 1.3586 - val_acc: 0.9727 - val_mDice: 0.6739

Epoch 00039: val_mDice improved from 0.66852 to 0.67385, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 40/300
 - 15s - loss: 1.9953 - acc: 0.9505 - mDice: 0.5805 - val_loss: 1.3294 - val_acc: 0.9732 - val_mDice: 0.6733

Epoch 00040: val_mDice did not improve from 0.67385
Epoch 41/300
 - 17s - loss: 1.9683 - acc: 0.9509 - mDice: 0.5851 - val_loss: 1.3474 - val_acc: 0.9725 - val_mDice: 0.6795

Epoch 00041: val_mDice improved from 0.67385 to 0.67949, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 42/300
 - 16s - loss: 1.9523 - acc: 0.9512 - mDice: 0.5886 - val_loss: 1.3459 - val_acc: 0.9721 - val_mDice: 0.6765

Epoch 00042: val_mDice did not improve from 0.67949
Epoch 43/300
 - 16s - loss: 1.9323 - acc: 0.9516 - mDice: 0.5925 - val_loss: 1.3250 - val_acc: 0.9736 - val_mDice: 0.6757

Epoch 00043: val_mDice did not improve from 0.67949
Epoch 44/300
 - 15s - loss: 1.9193 - acc: 0.9517 - mDice: 0.5947 - val_loss: 1.3197 - val_acc: 0.9737 - val_mDice: 0.6813

Epoch 00044: val_mDice improved from 0.67949 to 0.68126, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 45/300
 - 15s - loss: 1.9120 - acc: 0.9519 - mDice: 0.5969 - val_loss: 1.2861 - val_acc: 0.9735 - val_mDice: 0.6905

Epoch 00045: val_mDice improved from 0.68126 to 0.69052, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 46/300
 - 15s - loss: 1.8895 - acc: 0.9522 - mDice: 0.6010 - val_loss: 1.3344 - val_acc: 0.9725 - val_mDice: 0.6814

Epoch 00046: val_mDice did not improve from 0.69052
Epoch 47/300
 - 16s - loss: 1.8740 - acc: 0.9524 - mDice: 0.6033 - val_loss: 1.2880 - val_acc: 0.9736 - val_mDice: 0.6872

Epoch 00047: val_mDice did not improve from 0.69052
Epoch 48/300
 - 15s - loss: 1.8646 - acc: 0.9526 - mDice: 0.6057 - val_loss: 1.3009 - val_acc: 0.9734 - val_mDice: 0.6944

Epoch 00048: val_mDice improved from 0.69052 to 0.69438, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 49/300
 - 15s - loss: 1.8543 - acc: 0.9527 - mDice: 0.6077 - val_loss: 1.2780 - val_acc: 0.9741 - val_mDice: 0.6907

Epoch 00049: val_mDice did not improve from 0.69438
Epoch 50/300
 - 15s - loss: 1.8380 - acc: 0.9528 - mDice: 0.6106 - val_loss: 1.2620 - val_acc: 0.9740 - val_mDice: 0.6969

Epoch 00050: val_mDice improved from 0.69438 to 0.69695, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 51/300
 - 16s - loss: 1.8190 - acc: 0.9528 - mDice: 0.6146 - val_loss: 1.3218 - val_acc: 0.9731 - val_mDice: 0.6883

Epoch 00051: val_mDice did not improve from 0.69695
Epoch 52/300
 - 16s - loss: 1.8142 - acc: 0.9529 - mDice: 0.6155 - val_loss: 1.2762 - val_acc: 0.9736 - val_mDice: 0.6963

Epoch 00052: val_mDice did not improve from 0.69695
Epoch 53/300
 - 15s - loss: 1.8017 - acc: 0.9530 - mDice: 0.6180 - val_loss: 1.2728 - val_acc: 0.9740 - val_mDice: 0.6992

Epoch 00053: val_mDice improved from 0.69695 to 0.69920, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 54/300
 - 15s - loss: 1.7973 - acc: 0.9532 - mDice: 0.6188 - val_loss: 1.2546 - val_acc: 0.9743 - val_mDice: 0.7011

Epoch 00054: val_mDice improved from 0.69920 to 0.70111, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 55/300
 - 15s - loss: 1.7824 - acc: 0.9533 - mDice: 0.6214 - val_loss: 1.2361 - val_acc: 0.9747 - val_mDice: 0.7065

Epoch 00055: val_mDice improved from 0.70111 to 0.70649, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 56/300
 - 15s - loss: 1.7757 - acc: 0.9534 - mDice: 0.6233 - val_loss: 1.2514 - val_acc: 0.9747 - val_mDice: 0.7038

Epoch 00056: val_mDice did not improve from 0.70649
Epoch 57/300
 - 16s - loss: 1.7652 - acc: 0.9535 - mDice: 0.6252 - val_loss: 1.2421 - val_acc: 0.9745 - val_mDice: 0.7034

Epoch 00057: val_mDice did not improve from 0.70649
Epoch 58/300
 - 15s - loss: 1.7571 - acc: 0.9536 - mDice: 0.6265 - val_loss: 1.2266 - val_acc: 0.9751 - val_mDice: 0.7071

Epoch 00058: val_mDice improved from 0.70649 to 0.70715, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 59/300
 - 15s - loss: 1.7503 - acc: 0.9537 - mDice: 0.6281 - val_loss: 1.2364 - val_acc: 0.9748 - val_mDice: 0.7016

Epoch 00059: val_mDice did not improve from 0.70715
Epoch 60/300
 - 15s - loss: 1.7457 - acc: 0.9536 - mDice: 0.6282 - val_loss: 1.2418 - val_acc: 0.9746 - val_mDice: 0.7036

Epoch 00060: val_mDice did not improve from 0.70715
Epoch 61/300
 - 15s - loss: 1.7351 - acc: 0.9538 - mDice: 0.6306 - val_loss: 1.2199 - val_acc: 0.9750 - val_mDice: 0.7119

Epoch 00061: val_mDice improved from 0.70715 to 0.71194, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 62/300
 - 16s - loss: 1.7261 - acc: 0.9539 - mDice: 0.6333 - val_loss: 1.2403 - val_acc: 0.9751 - val_mDice: 0.7048

Epoch 00062: val_mDice did not improve from 0.71194
Epoch 63/300
 - 15s - loss: 1.7276 - acc: 0.9539 - mDice: 0.6325 - val_loss: 1.2315 - val_acc: 0.9753 - val_mDice: 0.7060

Epoch 00063: val_mDice did not improve from 0.71194
Epoch 64/300
 - 15s - loss: 1.7172 - acc: 0.9539 - mDice: 0.6346 - val_loss: 1.2919 - val_acc: 0.9736 - val_mDice: 0.7020

Epoch 00064: val_mDice did not improve from 0.71194
Epoch 65/300
 - 15s - loss: 1.7150 - acc: 0.9539 - mDice: 0.6351 - val_loss: 1.2285 - val_acc: 0.9745 - val_mDice: 0.7088

Epoch 00065: val_mDice did not improve from 0.71194
Epoch 66/300
 - 15s - loss: 1.7071 - acc: 0.9541 - mDice: 0.6365 - val_loss: 1.2357 - val_acc: 0.9742 - val_mDice: 0.7105

Epoch 00066: val_mDice did not improve from 0.71194
Epoch 67/300
 - 15s - loss: 1.7085 - acc: 0.9540 - mDice: 0.6366 - val_loss: 1.2295 - val_acc: 0.9750 - val_mDice: 0.7074

Epoch 00067: val_mDice did not improve from 0.71194
Epoch 68/300
 - 16s - loss: 1.7051 - acc: 0.9540 - mDice: 0.6371 - val_loss: 1.2159 - val_acc: 0.9751 - val_mDice: 0.7073

Epoch 00068: val_mDice did not improve from 0.71194
Epoch 69/300
 - 16s - loss: 1.6994 - acc: 0.9541 - mDice: 0.6379 - val_loss: 1.2239 - val_acc: 0.9750 - val_mDice: 0.7094

Epoch 00069: val_mDice did not improve from 0.71194
Epoch 70/300
 - 15s - loss: 1.6903 - acc: 0.9542 - mDice: 0.6394 - val_loss: 1.2249 - val_acc: 0.9751 - val_mDice: 0.7112

Epoch 00070: val_mDice did not improve from 0.71194
Epoch 71/300
 - 15s - loss: 1.6838 - acc: 0.9544 - mDice: 0.6414 - val_loss: 1.2129 - val_acc: 0.9753 - val_mDice: 0.7125

Epoch 00071: val_mDice improved from 0.71194 to 0.71254, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 72/300
 - 15s - loss: 1.6836 - acc: 0.9543 - mDice: 0.6408 - val_loss: 1.2218 - val_acc: 0.9748 - val_mDice: 0.7155

Epoch 00072: val_mDice improved from 0.71254 to 0.71548, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 73/300
 - 15s - loss: 1.6717 - acc: 0.9545 - mDice: 0.6434 - val_loss: 1.2134 - val_acc: 0.9751 - val_mDice: 0.7107

Epoch 00073: val_mDice did not improve from 0.71548
Epoch 74/300
 - 16s - loss: 1.6666 - acc: 0.9546 - mDice: 0.6446 - val_loss: 1.2236 - val_acc: 0.9747 - val_mDice: 0.7133

Epoch 00074: val_mDice did not improve from 0.71548
Epoch 75/300
 - 15s - loss: 1.6686 - acc: 0.9545 - mDice: 0.6443 - val_loss: 1.1967 - val_acc: 0.9754 - val_mDice: 0.7143

Epoch 00075: val_mDice did not improve from 0.71548
Epoch 76/300
 - 15s - loss: 1.6629 - acc: 0.9546 - mDice: 0.6451 - val_loss: 1.2341 - val_acc: 0.9749 - val_mDice: 0.7100

Epoch 00076: val_mDice did not improve from 0.71548
Epoch 77/300
 - 15s - loss: 1.6610 - acc: 0.9546 - mDice: 0.6456 - val_loss: 1.1852 - val_acc: 0.9754 - val_mDice: 0.7178

Epoch 00077: val_mDice improved from 0.71548 to 0.71782, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 78/300
 - 15s - loss: 1.6571 - acc: 0.9548 - mDice: 0.6465 - val_loss: 1.1836 - val_acc: 0.9753 - val_mDice: 0.7195

Epoch 00078: val_mDice improved from 0.71782 to 0.71954, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 79/300
 - 15s - loss: 1.6501 - acc: 0.9548 - mDice: 0.6481 - val_loss: 1.2009 - val_acc: 0.9753 - val_mDice: 0.7141

Epoch 00079: val_mDice did not improve from 0.71954
Epoch 80/300
 - 16s - loss: 1.6453 - acc: 0.9549 - mDice: 0.6486 - val_loss: 1.1821 - val_acc: 0.9751 - val_mDice: 0.7168

Epoch 00080: val_mDice did not improve from 0.71954
Epoch 81/300
 - 15s - loss: 1.6488 - acc: 0.9548 - mDice: 0.6483 - val_loss: 1.1809 - val_acc: 0.9754 - val_mDice: 0.7215

Epoch 00081: val_mDice improved from 0.71954 to 0.72155, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 82/300
 - 15s - loss: 1.6478 - acc: 0.9550 - mDice: 0.6484 - val_loss: 1.1981 - val_acc: 0.9750 - val_mDice: 0.7147

Epoch 00082: val_mDice did not improve from 0.72155
Epoch 83/300
 - 15s - loss: 1.6396 - acc: 0.9550 - mDice: 0.6499 - val_loss: 1.1707 - val_acc: 0.9756 - val_mDice: 0.7224

Epoch 00083: val_mDice improved from 0.72155 to 0.72239, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 84/300
 - 15s - loss: 1.6387 - acc: 0.9551 - mDice: 0.6503 - val_loss: 1.1893 - val_acc: 0.9753 - val_mDice: 0.7136

Epoch 00084: val_mDice did not improve from 0.72239
Epoch 85/300
 - 15s - loss: 1.6302 - acc: 0.9552 - mDice: 0.6521 - val_loss: 1.1665 - val_acc: 0.9756 - val_mDice: 0.7221

Epoch 00085: val_mDice did not improve from 0.72239
Epoch 86/300
 - 16s - loss: 1.6297 - acc: 0.9552 - mDice: 0.6523 - val_loss: 1.1906 - val_acc: 0.9753 - val_mDice: 0.7181

Epoch 00086: val_mDice did not improve from 0.72239
Epoch 87/300
 - 15s - loss: 1.6243 - acc: 0.9552 - mDice: 0.6532 - val_loss: 1.1747 - val_acc: 0.9755 - val_mDice: 0.7229

Epoch 00087: val_mDice improved from 0.72239 to 0.72287, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 88/300
 - 15s - loss: 1.6255 - acc: 0.9552 - mDice: 0.6531 - val_loss: 1.1737 - val_acc: 0.9757 - val_mDice: 0.7170

Epoch 00088: val_mDice did not improve from 0.72287
Epoch 89/300
 - 15s - loss: 1.6213 - acc: 0.9554 - mDice: 0.6541 - val_loss: 1.1769 - val_acc: 0.9753 - val_mDice: 0.7181

Epoch 00089: val_mDice did not improve from 0.72287
Epoch 90/300
 - 15s - loss: 1.6190 - acc: 0.9554 - mDice: 0.6543 - val_loss: 1.1863 - val_acc: 0.9752 - val_mDice: 0.7226

Epoch 00090: val_mDice did not improve from 0.72287
Epoch 91/300
 - 15s - loss: 1.6214 - acc: 0.9554 - mDice: 0.6538 - val_loss: 1.1758 - val_acc: 0.9759 - val_mDice: 0.7222

Epoch 00091: val_mDice did not improve from 0.72287
Epoch 92/300
 - 16s - loss: 1.6145 - acc: 0.9555 - mDice: 0.6554 - val_loss: 1.1709 - val_acc: 0.9757 - val_mDice: 0.7204

Epoch 00092: val_mDice did not improve from 0.72287
Epoch 93/300
 - 16s - loss: 1.6096 - acc: 0.9556 - mDice: 0.6559 - val_loss: 1.1839 - val_acc: 0.9756 - val_mDice: 0.7184

Epoch 00093: val_mDice did not improve from 0.72287
Epoch 94/300
 - 15s - loss: 1.6095 - acc: 0.9556 - mDice: 0.6562 - val_loss: 1.1701 - val_acc: 0.9755 - val_mDice: 0.7200

Epoch 00094: val_mDice did not improve from 0.72287
Epoch 95/300
 - 15s - loss: 1.6037 - acc: 0.9557 - mDice: 0.6572 - val_loss: 1.1764 - val_acc: 0.9749 - val_mDice: 0.7208

Epoch 00095: val_mDice did not improve from 0.72287
Epoch 96/300
 - 15s - loss: 1.6041 - acc: 0.9557 - mDice: 0.6571 - val_loss: 1.1993 - val_acc: 0.9753 - val_mDice: 0.7199

Epoch 00096: val_mDice did not improve from 0.72287
Epoch 97/300
 - 15s - loss: 1.6024 - acc: 0.9558 - mDice: 0.6581 - val_loss: 1.1658 - val_acc: 0.9755 - val_mDice: 0.7216

Epoch 00097: val_mDice did not improve from 0.72287
Epoch 98/300
 - 15s - loss: 1.5966 - acc: 0.9558 - mDice: 0.6585 - val_loss: 1.1710 - val_acc: 0.9755 - val_mDice: 0.7231

Epoch 00098: val_mDice improved from 0.72287 to 0.72314, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 99/300
 - 16s - loss: 1.5901 - acc: 0.9559 - mDice: 0.6598 - val_loss: 1.1616 - val_acc: 0.9755 - val_mDice: 0.7230

Epoch 00099: val_mDice did not improve from 0.72314
Epoch 100/300
 - 15s - loss: 1.5908 - acc: 0.9560 - mDice: 0.6599 - val_loss: 1.1642 - val_acc: 0.9756 - val_mDice: 0.7259

Epoch 00100: val_mDice improved from 0.72314 to 0.72593, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 101/300
 - 15s - loss: 1.5909 - acc: 0.9561 - mDice: 0.6599 - val_loss: 1.1957 - val_acc: 0.9752 - val_mDice: 0.7204

Epoch 00101: val_mDice did not improve from 0.72593
Epoch 102/300
 - 15s - loss: 1.5877 - acc: 0.9561 - mDice: 0.6602 - val_loss: 1.1507 - val_acc: 0.9760 - val_mDice: 0.7256

Epoch 00102: val_mDice did not improve from 0.72593
Epoch 103/300
 - 15s - loss: 1.5874 - acc: 0.9561 - mDice: 0.6608 - val_loss: 1.1619 - val_acc: 0.9756 - val_mDice: 0.7267

Epoch 00103: val_mDice improved from 0.72593 to 0.72672, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 104/300
 - 15s - loss: 1.5871 - acc: 0.9562 - mDice: 0.6609 - val_loss: 1.1484 - val_acc: 0.9755 - val_mDice: 0.7280

Epoch 00104: val_mDice improved from 0.72672 to 0.72801, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 105/300
 - 16s - loss: 1.5866 - acc: 0.9562 - mDice: 0.6610 - val_loss: 1.1607 - val_acc: 0.9756 - val_mDice: 0.7265

Epoch 00105: val_mDice did not improve from 0.72801
Epoch 106/300
 - 15s - loss: 1.5776 - acc: 0.9563 - mDice: 0.6624 - val_loss: 1.1698 - val_acc: 0.9761 - val_mDice: 0.7278

Epoch 00106: val_mDice did not improve from 0.72801
Epoch 107/300
 - 15s - loss: 1.5770 - acc: 0.9563 - mDice: 0.6630 - val_loss: 1.1638 - val_acc: 0.9755 - val_mDice: 0.7248

Epoch 00107: val_mDice did not improve from 0.72801
Epoch 108/300
 - 15s - loss: 1.5739 - acc: 0.9564 - mDice: 0.6634 - val_loss: 1.1627 - val_acc: 0.9757 - val_mDice: 0.7267

Epoch 00108: val_mDice did not improve from 0.72801
Epoch 109/300
 - 15s - loss: 1.5727 - acc: 0.9565 - mDice: 0.6636 - val_loss: 1.1623 - val_acc: 0.9760 - val_mDice: 0.7262

Epoch 00109: val_mDice did not improve from 0.72801
Epoch 110/300
 - 15s - loss: 1.5752 - acc: 0.9565 - mDice: 0.6634 - val_loss: 1.1464 - val_acc: 0.9758 - val_mDice: 0.7303

Epoch 00110: val_mDice improved from 0.72801 to 0.73031, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 111/300
 - 16s - loss: 1.5681 - acc: 0.9565 - mDice: 0.6648 - val_loss: 1.1547 - val_acc: 0.9759 - val_mDice: 0.7276

Epoch 00111: val_mDice did not improve from 0.73031
Epoch 112/300
 - 16s - loss: 1.5705 - acc: 0.9566 - mDice: 0.6644 - val_loss: 1.1499 - val_acc: 0.9757 - val_mDice: 0.7294

Epoch 00112: val_mDice did not improve from 0.73031
Epoch 113/300
 - 15s - loss: 1.5652 - acc: 0.9566 - mDice: 0.6656 - val_loss: 1.1481 - val_acc: 0.9756 - val_mDice: 0.7292

Epoch 00113: val_mDice did not improve from 0.73031
Epoch 114/300
 - 15s - loss: 1.5648 - acc: 0.9567 - mDice: 0.6656 - val_loss: 1.1629 - val_acc: 0.9752 - val_mDice: 0.7271

Epoch 00114: val_mDice did not improve from 0.73031
Epoch 115/300
 - 15s - loss: 1.5599 - acc: 0.9567 - mDice: 0.6665 - val_loss: 1.1646 - val_acc: 0.9749 - val_mDice: 0.7275

Epoch 00115: val_mDice did not improve from 0.73031
Epoch 116/300
 - 15s - loss: 1.5574 - acc: 0.9568 - mDice: 0.6672 - val_loss: 1.1595 - val_acc: 0.9760 - val_mDice: 0.7282

Epoch 00116: val_mDice did not improve from 0.73031
Epoch 117/300
 - 15s - loss: 1.5555 - acc: 0.9569 - mDice: 0.6673 - val_loss: 1.1456 - val_acc: 0.9758 - val_mDice: 0.7302

Epoch 00117: val_mDice did not improve from 0.73031
Epoch 118/300
 - 16s - loss: 1.5565 - acc: 0.9568 - mDice: 0.6673 - val_loss: 1.1678 - val_acc: 0.9760 - val_mDice: 0.7257

Epoch 00118: val_mDice did not improve from 0.73031
Epoch 119/300
 - 15s - loss: 1.5550 - acc: 0.9569 - mDice: 0.6675 - val_loss: 1.1462 - val_acc: 0.9760 - val_mDice: 0.7321

Epoch 00119: val_mDice improved from 0.73031 to 0.73211, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 120/300
 - 15s - loss: 1.5521 - acc: 0.9570 - mDice: 0.6684 - val_loss: 1.1735 - val_acc: 0.9757 - val_mDice: 0.7231

Epoch 00120: val_mDice did not improve from 0.73211
Epoch 121/300
 - 15s - loss: 1.5496 - acc: 0.9570 - mDice: 0.6689 - val_loss: 1.1525 - val_acc: 0.9754 - val_mDice: 0.7322

Epoch 00121: val_mDice improved from 0.73211 to 0.73222, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 122/300
 - 15s - loss: 1.5460 - acc: 0.9571 - mDice: 0.6692 - val_loss: 1.1630 - val_acc: 0.9757 - val_mDice: 0.7323

Epoch 00122: val_mDice improved from 0.73222 to 0.73232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 123/300
 - 15s - loss: 1.5468 - acc: 0.9571 - mDice: 0.6693 - val_loss: 1.1551 - val_acc: 0.9751 - val_mDice: 0.7262

Epoch 00123: val_mDice did not improve from 0.73232
Epoch 124/300
 - 16s - loss: 1.5438 - acc: 0.9571 - mDice: 0.6698 - val_loss: 1.1558 - val_acc: 0.9760 - val_mDice: 0.7264

Epoch 00124: val_mDice did not improve from 0.73232
Epoch 125/300
 - 15s - loss: 1.5436 - acc: 0.9571 - mDice: 0.6696 - val_loss: 1.1531 - val_acc: 0.9759 - val_mDice: 0.7320

Epoch 00125: val_mDice did not improve from 0.73232
Epoch 126/300
 - 15s - loss: 1.5432 - acc: 0.9572 - mDice: 0.6701 - val_loss: 1.1451 - val_acc: 0.9759 - val_mDice: 0.7335

Epoch 00126: val_mDice improved from 0.73232 to 0.73347, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 127/300
 - 15s - loss: 1.5396 - acc: 0.9572 - mDice: 0.6711 - val_loss: 1.1474 - val_acc: 0.9760 - val_mDice: 0.7314

Epoch 00127: val_mDice did not improve from 0.73347
Epoch 128/300
 - 15s - loss: 1.5374 - acc: 0.9573 - mDice: 0.6714 - val_loss: 1.1491 - val_acc: 0.9759 - val_mDice: 0.7314

Epoch 00128: val_mDice did not improve from 0.73347
Epoch 129/300
 - 15s - loss: 1.5449 - acc: 0.9572 - mDice: 0.6698 - val_loss: 1.1429 - val_acc: 0.9761 - val_mDice: 0.7343

Epoch 00129: val_mDice improved from 0.73347 to 0.73432, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 130/300
 - 16s - loss: 1.5367 - acc: 0.9573 - mDice: 0.6711 - val_loss: 1.1627 - val_acc: 0.9759 - val_mDice: 0.7286

Epoch 00130: val_mDice did not improve from 0.73432
Epoch 131/300
 - 15s - loss: 1.5383 - acc: 0.9574 - mDice: 0.6713 - val_loss: 1.1539 - val_acc: 0.9752 - val_mDice: 0.7335

Epoch 00131: val_mDice did not improve from 0.73432
Epoch 132/300
 - 15s - loss: 1.5361 - acc: 0.9574 - mDice: 0.6716 - val_loss: 1.1462 - val_acc: 0.9759 - val_mDice: 0.7330

Epoch 00132: val_mDice did not improve from 0.73432
Epoch 133/300
 - 15s - loss: 1.5360 - acc: 0.9573 - mDice: 0.6717 - val_loss: 1.1408 - val_acc: 0.9757 - val_mDice: 0.7319

Epoch 00133: val_mDice did not improve from 0.73432
Epoch 134/300
 - 15s - loss: 1.5333 - acc: 0.9574 - mDice: 0.6720 - val_loss: 1.1640 - val_acc: 0.9758 - val_mDice: 0.7284

Epoch 00134: val_mDice did not improve from 0.73432
Epoch 135/300
 - 15s - loss: 1.5314 - acc: 0.9575 - mDice: 0.6725 - val_loss: 1.1568 - val_acc: 0.9761 - val_mDice: 0.7299

Epoch 00135: val_mDice did not improve from 0.73432
Epoch 136/300
 - 16s - loss: 1.5321 - acc: 0.9575 - mDice: 0.6722 - val_loss: 1.1499 - val_acc: 0.9755 - val_mDice: 0.7315

Epoch 00136: val_mDice did not improve from 0.73432
Epoch 137/300
 - 15s - loss: 1.5280 - acc: 0.9575 - mDice: 0.6732 - val_loss: 1.1322 - val_acc: 0.9762 - val_mDice: 0.7359

Epoch 00137: val_mDice improved from 0.73432 to 0.73590, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 138/300
 - 15s - loss: 1.5304 - acc: 0.9575 - mDice: 0.6726 - val_loss: 1.1436 - val_acc: 0.9758 - val_mDice: 0.7318

Epoch 00138: val_mDice did not improve from 0.73590
Epoch 139/300
 - 15s - loss: 1.5235 - acc: 0.9576 - mDice: 0.6738 - val_loss: 1.1462 - val_acc: 0.9757 - val_mDice: 0.7348

Epoch 00139: val_mDice did not improve from 0.73590
Epoch 140/300
 - 15s - loss: 1.5232 - acc: 0.9576 - mDice: 0.6744 - val_loss: 1.1409 - val_acc: 0.9759 - val_mDice: 0.7341

Epoch 00140: val_mDice did not improve from 0.73590
Epoch 141/300
 - 15s - loss: 1.5231 - acc: 0.9576 - mDice: 0.6741 - val_loss: 1.1684 - val_acc: 0.9755 - val_mDice: 0.7310

Epoch 00141: val_mDice did not improve from 0.73590
Epoch 142/300
 - 15s - loss: 1.5232 - acc: 0.9576 - mDice: 0.6742 - val_loss: 1.1377 - val_acc: 0.9765 - val_mDice: 0.7372

Epoch 00142: val_mDice improved from 0.73590 to 0.73724, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 143/300
 - 16s - loss: 1.5238 - acc: 0.9577 - mDice: 0.6739 - val_loss: 1.1470 - val_acc: 0.9759 - val_mDice: 0.7346

Epoch 00143: val_mDice did not improve from 0.73724
Epoch 144/300
 - 15s - loss: 1.5219 - acc: 0.9577 - mDice: 0.6745 - val_loss: 1.1577 - val_acc: 0.9759 - val_mDice: 0.7326

Epoch 00144: val_mDice did not improve from 0.73724
Epoch 145/300
 - 15s - loss: 1.5193 - acc: 0.9577 - mDice: 0.6747 - val_loss: 1.1459 - val_acc: 0.9762 - val_mDice: 0.7325

Epoch 00145: val_mDice did not improve from 0.73724
Epoch 146/300
 - 15s - loss: 1.5192 - acc: 0.9577 - mDice: 0.6746 - val_loss: 1.1555 - val_acc: 0.9758 - val_mDice: 0.7356

Epoch 00146: val_mDice did not improve from 0.73724
Epoch 147/300
 - 15s - loss: 1.5193 - acc: 0.9577 - mDice: 0.6751 - val_loss: 1.1442 - val_acc: 0.9762 - val_mDice: 0.7373

Epoch 00147: val_mDice improved from 0.73724 to 0.73735, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 148/300
 - 15s - loss: 1.5203 - acc: 0.9577 - mDice: 0.6747 - val_loss: 1.1722 - val_acc: 0.9759 - val_mDice: 0.7293

Epoch 00148: val_mDice did not improve from 0.73735
Epoch 149/300
 - 16s - loss: 1.5164 - acc: 0.9578 - mDice: 0.6754 - val_loss: 1.1560 - val_acc: 0.9759 - val_mDice: 0.7358

Epoch 00149: val_mDice did not improve from 0.73735
Epoch 150/300
 - 15s - loss: 1.5181 - acc: 0.9578 - mDice: 0.6748 - val_loss: 1.1369 - val_acc: 0.9760 - val_mDice: 0.7318

Epoch 00150: val_mDice did not improve from 0.73735
Epoch 151/300
 - 15s - loss: 1.5172 - acc: 0.9578 - mDice: 0.6755 - val_loss: 1.1523 - val_acc: 0.9760 - val_mDice: 0.7396

Epoch 00151: val_mDice improved from 0.73735 to 0.73962, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 152/300
 - 15s - loss: 1.5079 - acc: 0.9580 - mDice: 0.6771 - val_loss: 1.1530 - val_acc: 0.9763 - val_mDice: 0.7374

Epoch 00152: val_mDice did not improve from 0.73962
Epoch 153/300
 - 15s - loss: 1.5107 - acc: 0.9579 - mDice: 0.6764 - val_loss: 1.1392 - val_acc: 0.9754 - val_mDice: 0.7341

Epoch 00153: val_mDice did not improve from 0.73962
Epoch 154/300
 - 16s - loss: 1.5096 - acc: 0.9579 - mDice: 0.6771 - val_loss: 1.1285 - val_acc: 0.9762 - val_mDice: 0.7357

Epoch 00154: val_mDice did not improve from 0.73962
Epoch 155/300
 - 16s - loss: 1.5077 - acc: 0.9580 - mDice: 0.6774 - val_loss: 1.1372 - val_acc: 0.9762 - val_mDice: 0.7388

Epoch 00155: val_mDice did not improve from 0.73962
Epoch 156/300
 - 15s - loss: 1.5104 - acc: 0.9579 - mDice: 0.6765 - val_loss: 1.1557 - val_acc: 0.9762 - val_mDice: 0.7342

Epoch 00156: val_mDice did not improve from 0.73962
Epoch 157/300
 - 15s - loss: 1.5088 - acc: 0.9580 - mDice: 0.6768 - val_loss: 1.1480 - val_acc: 0.9762 - val_mDice: 0.7355

Epoch 00157: val_mDice did not improve from 0.73962
Epoch 158/300
 - 15s - loss: 1.5056 - acc: 0.9580 - mDice: 0.6773 - val_loss: 1.1297 - val_acc: 0.9763 - val_mDice: 0.7400

Epoch 00158: val_mDice improved from 0.73962 to 0.73998, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 159/300
 - 15s - loss: 1.5064 - acc: 0.9580 - mDice: 0.6775 - val_loss: 1.1234 - val_acc: 0.9761 - val_mDice: 0.7380

Epoch 00159: val_mDice did not improve from 0.73998
Epoch 160/300
 - 16s - loss: 1.5018 - acc: 0.9581 - mDice: 0.6784 - val_loss: 1.1350 - val_acc: 0.9760 - val_mDice: 0.7355

Epoch 00160: val_mDice did not improve from 0.73998
Epoch 161/300
 - 16s - loss: 1.5022 - acc: 0.9581 - mDice: 0.6781 - val_loss: 1.1513 - val_acc: 0.9756 - val_mDice: 0.7354

Epoch 00161: val_mDice did not improve from 0.73998
Epoch 162/300
 - 15s - loss: 1.5049 - acc: 0.9581 - mDice: 0.6776 - val_loss: 1.1473 - val_acc: 0.9760 - val_mDice: 0.7368

Epoch 00162: val_mDice did not improve from 0.73998
Epoch 163/300
 - 15s - loss: 1.5050 - acc: 0.9581 - mDice: 0.6778 - val_loss: 1.1416 - val_acc: 0.9761 - val_mDice: 0.7345

Epoch 00163: val_mDice did not improve from 0.73998
Epoch 164/300
 - 15s - loss: 1.5045 - acc: 0.9581 - mDice: 0.6777 - val_loss: 1.1732 - val_acc: 0.9754 - val_mDice: 0.7329

Epoch 00164: val_mDice did not improve from 0.73998
Epoch 165/300
 - 16s - loss: 1.5029 - acc: 0.9581 - mDice: 0.6781 - val_loss: 1.1361 - val_acc: 0.9761 - val_mDice: 0.7391

Epoch 00165: val_mDice did not improve from 0.73998
Epoch 166/300
 - 16s - loss: 1.4990 - acc: 0.9582 - mDice: 0.6786 - val_loss: 1.1470 - val_acc: 0.9763 - val_mDice: 0.7362

Epoch 00166: val_mDice did not improve from 0.73998
Epoch 167/300
 - 15s - loss: 1.4990 - acc: 0.9582 - mDice: 0.6790 - val_loss: 1.1440 - val_acc: 0.9762 - val_mDice: 0.7400

Epoch 00167: val_mDice improved from 0.73998 to 0.74002, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 168/300
 - 15s - loss: 1.5012 - acc: 0.9581 - mDice: 0.6785 - val_loss: 1.1400 - val_acc: 0.9759 - val_mDice: 0.7385

Epoch 00168: val_mDice did not improve from 0.74002
Epoch 169/300
 - 15s - loss: 1.4983 - acc: 0.9582 - mDice: 0.6791 - val_loss: 1.1377 - val_acc: 0.9760 - val_mDice: 0.7393

Epoch 00169: val_mDice did not improve from 0.74002
Epoch 170/300
 - 15s - loss: 1.4965 - acc: 0.9583 - mDice: 0.6795 - val_loss: 1.1622 - val_acc: 0.9761 - val_mDice: 0.7347

Epoch 00170: val_mDice did not improve from 0.74002
Epoch 171/300
 - 16s - loss: 1.4996 - acc: 0.9581 - mDice: 0.6792 - val_loss: 1.1459 - val_acc: 0.9763 - val_mDice: 0.7363

Epoch 00171: val_mDice did not improve from 0.74002
Epoch 172/300
 - 16s - loss: 1.4986 - acc: 0.9583 - mDice: 0.6788 - val_loss: 1.1385 - val_acc: 0.9762 - val_mDice: 0.7372

Epoch 00172: val_mDice did not improve from 0.74002
Epoch 173/300
 - 15s - loss: 1.4926 - acc: 0.9583 - mDice: 0.6798 - val_loss: 1.1450 - val_acc: 0.9763 - val_mDice: 0.7369

Epoch 00173: val_mDice did not improve from 0.74002
Epoch 174/300
 - 15s - loss: 1.4944 - acc: 0.9583 - mDice: 0.6798 - val_loss: 1.1481 - val_acc: 0.9758 - val_mDice: 0.7371

Epoch 00174: val_mDice did not improve from 0.74002
Epoch 175/300
 - 15s - loss: 1.4950 - acc: 0.9583 - mDice: 0.6800 - val_loss: 1.1383 - val_acc: 0.9760 - val_mDice: 0.7374

Epoch 00175: val_mDice did not improve from 0.74002
Epoch 176/300
 - 15s - loss: 1.4931 - acc: 0.9583 - mDice: 0.6802 - val_loss: 1.1460 - val_acc: 0.9761 - val_mDice: 0.7335

Epoch 00176: val_mDice did not improve from 0.74002
Epoch 177/300
 - 15s - loss: 1.4912 - acc: 0.9583 - mDice: 0.6804 - val_loss: 1.1418 - val_acc: 0.9763 - val_mDice: 0.7406

Epoch 00177: val_mDice improved from 0.74002 to 0.74063, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 178/300
 - 16s - loss: 1.4904 - acc: 0.9584 - mDice: 0.6807 - val_loss: 1.1669 - val_acc: 0.9759 - val_mDice: 0.7343

Epoch 00178: val_mDice did not improve from 0.74063
Epoch 179/300
 - 15s - loss: 1.4934 - acc: 0.9583 - mDice: 0.6798 - val_loss: 1.1287 - val_acc: 0.9761 - val_mDice: 0.7400

Epoch 00179: val_mDice did not improve from 0.74063
Epoch 180/300
 - 15s - loss: 1.4940 - acc: 0.9583 - mDice: 0.6800 - val_loss: 1.1639 - val_acc: 0.9760 - val_mDice: 0.7339

Epoch 00180: val_mDice did not improve from 0.74063
Epoch 181/300
 - 15s - loss: 1.4890 - acc: 0.9584 - mDice: 0.6810 - val_loss: 1.1337 - val_acc: 0.9763 - val_mDice: 0.7405

Epoch 00181: val_mDice did not improve from 0.74063
Epoch 182/300
 - 15s - loss: 1.4859 - acc: 0.9585 - mDice: 0.6813 - val_loss: 1.1424 - val_acc: 0.9764 - val_mDice: 0.7381

Epoch 00182: val_mDice did not improve from 0.74063
Epoch 183/300
 - 15s - loss: 1.4906 - acc: 0.9584 - mDice: 0.6806 - val_loss: 1.1537 - val_acc: 0.9760 - val_mDice: 0.7377

Epoch 00183: val_mDice did not improve from 0.74063
Epoch 184/300
 - 15s - loss: 1.4870 - acc: 0.9585 - mDice: 0.6813 - val_loss: 1.1334 - val_acc: 0.9760 - val_mDice: 0.7335

Epoch 00184: val_mDice did not improve from 0.74063
Epoch 185/300
 - 16s - loss: 1.4883 - acc: 0.9584 - mDice: 0.6809 - val_loss: 1.1427 - val_acc: 0.9747 - val_mDice: 0.7406

Epoch 00185: val_mDice did not improve from 0.74063
Epoch 186/300
 - 15s - loss: 1.4869 - acc: 0.9585 - mDice: 0.6812 - val_loss: 1.1389 - val_acc: 0.9761 - val_mDice: 0.7400

Epoch 00186: val_mDice did not improve from 0.74063
Epoch 187/300
 - 15s - loss: 1.4833 - acc: 0.9585 - mDice: 0.6822 - val_loss: 1.1400 - val_acc: 0.9761 - val_mDice: 0.7340

Epoch 00187: val_mDice did not improve from 0.74063
Epoch 188/300
 - 15s - loss: 1.4835 - acc: 0.9585 - mDice: 0.6819 - val_loss: 1.1211 - val_acc: 0.9763 - val_mDice: 0.7407

Epoch 00188: val_mDice improved from 0.74063 to 0.74066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 189/300
 - 15s - loss: 1.4853 - acc: 0.9585 - mDice: 0.6813 - val_loss: 1.1305 - val_acc: 0.9761 - val_mDice: 0.7388

Epoch 00189: val_mDice did not improve from 0.74066
Epoch 190/300
 - 15s - loss: 1.4828 - acc: 0.9586 - mDice: 0.6821 - val_loss: 1.1472 - val_acc: 0.9765 - val_mDice: 0.7360

Epoch 00190: val_mDice did not improve from 0.74066
Epoch 191/300
 - 16s - loss: 1.4817 - acc: 0.9585 - mDice: 0.6822 - val_loss: 1.1349 - val_acc: 0.9761 - val_mDice: 0.7361

Epoch 00191: val_mDice did not improve from 0.74066
Epoch 192/300
 - 15s - loss: 1.4808 - acc: 0.9586 - mDice: 0.6822 - val_loss: 1.1269 - val_acc: 0.9760 - val_mDice: 0.7335

Epoch 00192: val_mDice did not improve from 0.74066
Epoch 193/300
 - 15s - loss: 1.4808 - acc: 0.9586 - mDice: 0.6824 - val_loss: 1.1297 - val_acc: 0.9766 - val_mDice: 0.7395

Epoch 00193: val_mDice did not improve from 0.74066
Epoch 194/300
 - 15s - loss: 1.4773 - acc: 0.9586 - mDice: 0.6830 - val_loss: 1.1506 - val_acc: 0.9762 - val_mDice: 0.7376

Epoch 00194: val_mDice did not improve from 0.74066
Epoch 195/300
 - 15s - loss: 1.4845 - acc: 0.9585 - mDice: 0.6816 - val_loss: 1.1470 - val_acc: 0.9761 - val_mDice: 0.7410

Epoch 00195: val_mDice improved from 0.74066 to 0.74099, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 196/300
 - 15s - loss: 1.4861 - acc: 0.9586 - mDice: 0.6815 - val_loss: 1.1292 - val_acc: 0.9751 - val_mDice: 0.7402

Epoch 00196: val_mDice did not improve from 0.74099
Epoch 197/300
 - 15s - loss: 1.4843 - acc: 0.9585 - mDice: 0.6822 - val_loss: 1.1276 - val_acc: 0.9765 - val_mDice: 0.7401

Epoch 00197: val_mDice did not improve from 0.74099
Epoch 198/300
 - 16s - loss: 1.4827 - acc: 0.9586 - mDice: 0.6821 - val_loss: 1.1341 - val_acc: 0.9758 - val_mDice: 0.7393

Epoch 00198: val_mDice did not improve from 0.74099
Epoch 199/300
 - 15s - loss: 1.4838 - acc: 0.9586 - mDice: 0.6820 - val_loss: 1.1470 - val_acc: 0.9753 - val_mDice: 0.7355

Epoch 00199: val_mDice did not improve from 0.74099
Epoch 200/300
 - 15s - loss: 1.4793 - acc: 0.9586 - mDice: 0.6827 - val_loss: 1.1236 - val_acc: 0.9763 - val_mDice: 0.7393

Epoch 00200: val_mDice did not improve from 0.74099
Epoch 201/300
 - 15s - loss: 1.4767 - acc: 0.9588 - mDice: 0.6833 - val_loss: 1.1532 - val_acc: 0.9762 - val_mDice: 0.7343

Epoch 00201: val_mDice did not improve from 0.74099
Epoch 202/300
 - 15s - loss: 1.4763 - acc: 0.9587 - mDice: 0.6834 - val_loss: 1.1316 - val_acc: 0.9761 - val_mDice: 0.7395

Epoch 00202: val_mDice did not improve from 0.74099
Epoch 203/300
 - 15s - loss: 1.4786 - acc: 0.9587 - mDice: 0.6833 - val_loss: 1.1596 - val_acc: 0.9757 - val_mDice: 0.7344

Epoch 00203: val_mDice did not improve from 0.74099
Epoch 204/300
 - 15s - loss: 1.4768 - acc: 0.9587 - mDice: 0.6835 - val_loss: 1.1346 - val_acc: 0.9758 - val_mDice: 0.7377

Epoch 00204: val_mDice did not improve from 0.74099
Epoch 205/300
 - 16s - loss: 1.4759 - acc: 0.9587 - mDice: 0.6833 - val_loss: 1.1325 - val_acc: 0.9763 - val_mDice: 0.7393

Epoch 00205: val_mDice did not improve from 0.74099
Epoch 206/300
 - 15s - loss: 1.4771 - acc: 0.9588 - mDice: 0.6834 - val_loss: 1.1400 - val_acc: 0.9761 - val_mDice: 0.7387

Epoch 00206: val_mDice did not improve from 0.74099
Epoch 207/300
 - 15s - loss: 1.4735 - acc: 0.9588 - mDice: 0.6839 - val_loss: 1.1530 - val_acc: 0.9762 - val_mDice: 0.7366

Epoch 00207: val_mDice did not improve from 0.74099
Epoch 208/300
 - 15s - loss: 1.4772 - acc: 0.9587 - mDice: 0.6834 - val_loss: 1.1434 - val_acc: 0.9760 - val_mDice: 0.7392

Epoch 00208: val_mDice did not improve from 0.74099
Epoch 209/300
 - 15s - loss: 1.4763 - acc: 0.9588 - mDice: 0.6836 - val_loss: 1.1151 - val_acc: 0.9766 - val_mDice: 0.7393

Epoch 00209: val_mDice did not improve from 0.74099
Epoch 210/300
 - 15s - loss: 1.4725 - acc: 0.9588 - mDice: 0.6845 - val_loss: 1.1317 - val_acc: 0.9762 - val_mDice: 0.7425

Epoch 00210: val_mDice improved from 0.74099 to 0.74253, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 211/300
 - 16s - loss: 1.4750 - acc: 0.9588 - mDice: 0.6838 - val_loss: 1.1464 - val_acc: 0.9758 - val_mDice: 0.7403

Epoch 00211: val_mDice did not improve from 0.74253
Epoch 212/300
 - 15s - loss: 1.4713 - acc: 0.9589 - mDice: 0.6841 - val_loss: 1.1387 - val_acc: 0.9764 - val_mDice: 0.7402

Epoch 00212: val_mDice did not improve from 0.74253
Epoch 213/300
 - 15s - loss: 1.4714 - acc: 0.9588 - mDice: 0.6842 - val_loss: 1.1194 - val_acc: 0.9765 - val_mDice: 0.7415

Epoch 00213: val_mDice did not improve from 0.74253
Epoch 214/300
 - 15s - loss: 1.4724 - acc: 0.9588 - mDice: 0.6841 - val_loss: 1.1326 - val_acc: 0.9765 - val_mDice: 0.7398

Epoch 00214: val_mDice did not improve from 0.74253
Epoch 215/300
 - 15s - loss: 1.4702 - acc: 0.9589 - mDice: 0.6846 - val_loss: 1.1247 - val_acc: 0.9761 - val_mDice: 0.7429

Epoch 00215: val_mDice improved from 0.74253 to 0.74291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 216/300
 - 15s - loss: 1.4712 - acc: 0.9588 - mDice: 0.6843 - val_loss: 1.1509 - val_acc: 0.9766 - val_mDice: 0.7408

Epoch 00216: val_mDice did not improve from 0.74291
Epoch 217/300
 - 16s - loss: 1.4728 - acc: 0.9588 - mDice: 0.6844 - val_loss: 1.1455 - val_acc: 0.9762 - val_mDice: 0.7397

Epoch 00217: val_mDice did not improve from 0.74291
Epoch 218/300
 - 15s - loss: 1.4711 - acc: 0.9589 - mDice: 0.6848 - val_loss: 1.1288 - val_acc: 0.9762 - val_mDice: 0.7381

Epoch 00218: val_mDice did not improve from 0.74291
Epoch 219/300
 - 15s - loss: 1.4726 - acc: 0.9589 - mDice: 0.6843 - val_loss: 1.1283 - val_acc: 0.9764 - val_mDice: 0.7383

Epoch 00219: val_mDice did not improve from 0.74291
Epoch 220/300
 - 15s - loss: 1.4689 - acc: 0.9589 - mDice: 0.6848 - val_loss: 1.1260 - val_acc: 0.9761 - val_mDice: 0.7385

Epoch 00220: val_mDice did not improve from 0.74291
Epoch 221/300
 - 15s - loss: 1.4698 - acc: 0.9589 - mDice: 0.6850 - val_loss: 1.1342 - val_acc: 0.9757 - val_mDice: 0.7388

Epoch 00221: val_mDice did not improve from 0.74291
Epoch 222/300
 - 15s - loss: 1.4671 - acc: 0.9590 - mDice: 0.6854 - val_loss: 1.1114 - val_acc: 0.9764 - val_mDice: 0.7388

Epoch 00222: val_mDice did not improve from 0.74291
Epoch 223/300
 - 15s - loss: 1.4695 - acc: 0.9589 - mDice: 0.6844 - val_loss: 1.1292 - val_acc: 0.9764 - val_mDice: 0.7402

Epoch 00223: val_mDice did not improve from 0.74291
Epoch 224/300
 - 16s - loss: 1.4679 - acc: 0.9589 - mDice: 0.6851 - val_loss: 1.1359 - val_acc: 0.9758 - val_mDice: 0.7383

Epoch 00224: val_mDice did not improve from 0.74291
Epoch 225/300
 - 15s - loss: 1.4660 - acc: 0.9590 - mDice: 0.6855 - val_loss: 1.1394 - val_acc: 0.9763 - val_mDice: 0.7400

Epoch 00225: val_mDice did not improve from 0.74291
Epoch 226/300
 - 15s - loss: 1.4650 - acc: 0.9590 - mDice: 0.6855 - val_loss: 1.1430 - val_acc: 0.9759 - val_mDice: 0.7370

Epoch 00226: val_mDice did not improve from 0.74291
Epoch 227/300
 - 15s - loss: 1.4646 - acc: 0.9590 - mDice: 0.6857 - val_loss: 1.1517 - val_acc: 0.9758 - val_mDice: 0.7417

Epoch 00227: val_mDice did not improve from 0.74291
Epoch 228/300
 - 15s - loss: 1.4685 - acc: 0.9590 - mDice: 0.6852 - val_loss: 1.1256 - val_acc: 0.9761 - val_mDice: 0.7394

Epoch 00228: val_mDice did not improve from 0.74291
Epoch 229/300
 - 15s - loss: 1.4661 - acc: 0.9590 - mDice: 0.6854 - val_loss: 1.1325 - val_acc: 0.9765 - val_mDice: 0.7455

Epoch 00229: val_mDice improved from 0.74291 to 0.74552, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 230/300
 - 16s - loss: 1.4648 - acc: 0.9590 - mDice: 0.6859 - val_loss: 1.1217 - val_acc: 0.9758 - val_mDice: 0.7408

Epoch 00230: val_mDice did not improve from 0.74552
Epoch 231/300
 - 16s - loss: 1.4647 - acc: 0.9590 - mDice: 0.6860 - val_loss: 1.1282 - val_acc: 0.9764 - val_mDice: 0.7371

Epoch 00231: val_mDice did not improve from 0.74552
Epoch 232/300
 - 15s - loss: 1.4631 - acc: 0.9590 - mDice: 0.6861 - val_loss: 1.1376 - val_acc: 0.9765 - val_mDice: 0.7422

Epoch 00232: val_mDice did not improve from 0.74552
Epoch 233/300
 - 15s - loss: 1.4621 - acc: 0.9590 - mDice: 0.6864 - val_loss: 1.1255 - val_acc: 0.9764 - val_mDice: 0.7439

Epoch 00233: val_mDice did not improve from 0.74552
Epoch 234/300
 - 15s - loss: 1.4620 - acc: 0.9590 - mDice: 0.6865 - val_loss: 1.1262 - val_acc: 0.9758 - val_mDice: 0.7423

Epoch 00234: val_mDice did not improve from 0.74552
Epoch 235/300
 - 15s - loss: 1.4574 - acc: 0.9591 - mDice: 0.6872 - val_loss: 1.1247 - val_acc: 0.9766 - val_mDice: 0.7416

Epoch 00235: val_mDice did not improve from 0.74552
Epoch 236/300
 - 15s - loss: 1.4622 - acc: 0.9590 - mDice: 0.6862 - val_loss: 1.1293 - val_acc: 0.9762 - val_mDice: 0.7400

Epoch 00236: val_mDice did not improve from 0.74552
Epoch 237/300
 - 16s - loss: 1.4628 - acc: 0.9591 - mDice: 0.6862 - val_loss: 1.1420 - val_acc: 0.9763 - val_mDice: 0.7426

Epoch 00237: val_mDice did not improve from 0.74552
Epoch 238/300
 - 15s - loss: 1.4571 - acc: 0.9591 - mDice: 0.6874 - val_loss: 1.1225 - val_acc: 0.9765 - val_mDice: 0.7393

Epoch 00238: val_mDice did not improve from 0.74552
Epoch 239/300
 - 15s - loss: 1.4611 - acc: 0.9591 - mDice: 0.6862 - val_loss: 1.1254 - val_acc: 0.9762 - val_mDice: 0.7402

Epoch 00239: val_mDice did not improve from 0.74552
Epoch 240/300
 - 15s - loss: 1.4583 - acc: 0.9591 - mDice: 0.6869 - val_loss: 1.1592 - val_acc: 0.9758 - val_mDice: 0.7396

Epoch 00240: val_mDice did not improve from 0.74552
Epoch 241/300
 - 16s - loss: 1.4635 - acc: 0.9591 - mDice: 0.6866 - val_loss: 1.1419 - val_acc: 0.9762 - val_mDice: 0.7391

Epoch 00241: val_mDice did not improve from 0.74552
Epoch 242/300
 - 15s - loss: 1.4609 - acc: 0.9591 - mDice: 0.6864 - val_loss: 1.1324 - val_acc: 0.9762 - val_mDice: 0.7378

Epoch 00242: val_mDice did not improve from 0.74552
Epoch 243/300
 - 15s - loss: 1.4546 - acc: 0.9591 - mDice: 0.6878 - val_loss: 1.1288 - val_acc: 0.9764 - val_mDice: 0.7421

Epoch 00243: val_mDice did not improve from 0.74552
Epoch 244/300
 - 15s - loss: 1.4586 - acc: 0.9591 - mDice: 0.6872 - val_loss: 1.1345 - val_acc: 0.9760 - val_mDice: 0.7420

Epoch 00244: val_mDice did not improve from 0.74552
Epoch 245/300
 - 16s - loss: 1.4580 - acc: 0.9591 - mDice: 0.6869 - val_loss: 1.1147 - val_acc: 0.9767 - val_mDice: 0.7433

Epoch 00245: val_mDice did not improve from 0.74552
Epoch 246/300
 - 15s - loss: 1.4570 - acc: 0.9591 - mDice: 0.6874 - val_loss: 1.1162 - val_acc: 0.9765 - val_mDice: 0.7419

Epoch 00246: val_mDice did not improve from 0.74552
Epoch 247/300
 - 15s - loss: 1.4640 - acc: 0.9590 - mDice: 0.6860 - val_loss: 1.1194 - val_acc: 0.9762 - val_mDice: 0.7419

Epoch 00247: val_mDice did not improve from 0.74552
Epoch 248/300
 - 16s - loss: 1.4561 - acc: 0.9592 - mDice: 0.6876 - val_loss: 1.1300 - val_acc: 0.9762 - val_mDice: 0.7450

Epoch 00248: val_mDice did not improve from 0.74552
Epoch 249/300
 - 16s - loss: 1.4534 - acc: 0.9592 - mDice: 0.6879 - val_loss: 1.1328 - val_acc: 0.9761 - val_mDice: 0.7429

Epoch 00249: val_mDice did not improve from 0.74552
Epoch 250/300
 - 15s - loss: 1.4539 - acc: 0.9592 - mDice: 0.6880 - val_loss: 1.1287 - val_acc: 0.9761 - val_mDice: 0.7443

Epoch 00250: val_mDice did not improve from 0.74552
Epoch 251/300
 - 15s - loss: 1.4526 - acc: 0.9592 - mDice: 0.6885 - val_loss: 1.1159 - val_acc: 0.9767 - val_mDice: 0.7412

Epoch 00251: val_mDice did not improve from 0.74552
Epoch 252/300
 - 15s - loss: 1.4576 - acc: 0.9591 - mDice: 0.6871 - val_loss: 1.1340 - val_acc: 0.9763 - val_mDice: 0.7402

Epoch 00252: val_mDice did not improve from 0.74552
Epoch 253/300
 - 16s - loss: 1.4568 - acc: 0.9591 - mDice: 0.6873 - val_loss: 1.1212 - val_acc: 0.9764 - val_mDice: 0.7448

Epoch 00253: val_mDice did not improve from 0.74552
Epoch 254/300
 - 15s - loss: 1.4492 - acc: 0.9592 - mDice: 0.6887 - val_loss: 1.1228 - val_acc: 0.9763 - val_mDice: 0.7430

Epoch 00254: val_mDice did not improve from 0.74552
Epoch 255/300
 - 15s - loss: 1.4516 - acc: 0.9592 - mDice: 0.6883 - val_loss: 1.1329 - val_acc: 0.9759 - val_mDice: 0.7431

Epoch 00255: val_mDice did not improve from 0.74552
Epoch 256/300
 - 15s - loss: 1.4505 - acc: 0.9592 - mDice: 0.6884 - val_loss: 1.1274 - val_acc: 0.9766 - val_mDice: 0.7440

Epoch 00256: val_mDice did not improve from 0.74552
Epoch 257/300
 - 15s - loss: 1.4557 - acc: 0.9592 - mDice: 0.6877 - val_loss: 1.1438 - val_acc: 0.9763 - val_mDice: 0.7401

Epoch 00257: val_mDice did not improve from 0.74552
Epoch 258/300
 - 16s - loss: 1.4537 - acc: 0.9592 - mDice: 0.6882 - val_loss: 1.1241 - val_acc: 0.9762 - val_mDice: 0.7424

Epoch 00258: val_mDice did not improve from 0.74552
Epoch 259/300
 - 15s - loss: 1.4538 - acc: 0.9592 - mDice: 0.6882 - val_loss: 1.1256 - val_acc: 0.9757 - val_mDice: 0.7421

Epoch 00259: val_mDice did not improve from 0.74552
Restoring model weights from the end of the best epoch
Epoch 00259: early stopping
{'val_loss': [7.9542657713060425, 6.547854710966087, 5.861528169501226, 5.4528533855096315, 5.127157980612166, 4.844698332734603, 4.555949905425677, 4.385787406579472, 3.970089578460935, 3.7421905960894217, 3.478988237246897, 3.2850666762655356, 3.0540636335818756, 2.623921485693048, 2.5135171115922175, 2.251415280549933, 2.1973416725654804, 2.0041703388016336, 1.918032775025795, 1.8597967829980833, 1.8278442549998815, 1.70687587860389, 1.791862645341977, 1.6284625075613468, 1.605358741195424, 1.5738340597370597, 1.5532062619348401, 1.5261270312726603, 1.5389660806354195, 1.4823664236152527, 1.4913198386428645, 1.4677755348711734, 1.418381016158052, 1.4263248468744314, 1.3998181813630572, 1.3736615809489219, 1.3491613848138149, 1.38338734625932, 1.358597175517694, 1.3294335848836782, 1.3473872252214563, 1.3459075085936616, 1.324986551474184, 1.3197071520850612, 1.286079469799786, 1.3344251258930548, 1.2879602255427356, 1.3009396795648263, 1.2780442843328252, 1.2619691389516283, 1.3217941308901473, 1.2761982557853202, 1.2727614622333976, 1.25458456583727, 1.236133091688575, 1.2514442171908011, 1.2421115561403164, 1.2265579927784698, 1.2363992115525244, 1.2417763768055317, 1.219863842577004, 1.240280136596968, 1.231481585016569, 1.2919139420001495, 1.2284516453323968, 1.235712827928037, 1.22945163612416, 1.2159064626861331, 1.2238622300444673, 1.2248574194673494, 1.2129031793723417, 1.2218262742818553, 1.213445666489995, 1.2236132608147203, 1.1966960750690454, 1.2341473455588303, 1.1851502387720587, 1.1836091900123233, 1.2008873232638688, 1.182104070597248, 1.1808763268961848, 1.1981347636933486, 1.1707215029451676, 1.189302023978141, 1.1664638854497766, 1.1906250833627001, 1.1747037222599104, 1.1736991016852205, 1.1769134099537961, 1.1862685153480155, 1.175773406699169, 1.1708978985441172, 1.183878872746533, 1.1700838547599337, 1.1764426249192343, 1.1993267345512268, 1.165819089002056, 1.1710388651003107, 1.161557674722219, 1.164237837485563, 1.1957041644253923, 1.1506956370517951, 1.1618910840399865, 1.1483711519643587, 1.1606939103984666, 1.169778252423124, 1.1638102398396377, 1.1626664360294443, 1.1622685551224359, 1.1463995763413306, 1.154659941556793, 1.1499058506819821, 1.1481258044762435, 1.1628594256034006, 1.1646470908330908, 1.1595011472911534, 1.1455688632644963, 1.1678352724688752, 1.1462120595003473, 1.1734852175930472, 1.1525193822614337, 1.1629538398844914, 1.1551141791268267, 1.1558246458561432, 1.1530958797474946, 1.1450872659054707, 1.1474015543037643, 1.1490880298069786, 1.1428921656365554, 1.1626953653375796, 1.153943217282555, 1.1461819875428882, 1.1408205032348633, 1.1639786472639007, 1.156781124419194, 1.149930176500277, 1.1322022387348705, 1.1435535180338239, 1.1461793990881037, 1.140945482023571, 1.16835428803164, 1.1377455306178865, 1.1470307219636042, 1.1577431632890014, 1.145870420027701, 1.155539797877595, 1.144194378060909, 1.172158490899158, 1.1560057415275038, 1.1369098733933822, 1.1522664816392119, 1.153012025963862, 1.1391781322356895, 1.1284756324203236, 1.137154378350464, 1.155710840267121, 1.1480103197960945, 1.1296795426437432, 1.1233812084516448, 1.1350081367107183, 1.1513462660601772, 1.147305994964023, 1.1416003170876594, 1.1732127280981135, 1.1361246237855385, 1.1469962346532852, 1.1439848666869903, 1.1399665342902467, 1.1376888723700243, 1.1621998134732037, 1.1459221888091318, 1.1384856041998772, 1.1450096343648875, 1.1481451744144955, 1.1382939872088038, 1.1460154996186442, 1.141783817895477, 1.1668919851784127, 1.128692193379092, 1.1639165972573895, 1.1336711037557028, 1.1423899272623717, 1.1537432152064278, 1.1334090440260713, 1.142733201515486, 1.1389283601764217, 1.140010683104946, 1.121084578322191, 1.1304797535411921, 1.147210080196233, 1.1349299197037734, 1.126938036539852, 1.1297037764886142, 1.1506202576030538, 1.1470003415075882, 1.129199722634468, 1.1275848069593233, 1.1341257810802159, 1.1470194419155222, 1.1235645511866872, 1.1531961635671726, 1.1315621464449408, 1.1595601187020488, 1.134640936273263, 1.132453421297727, 1.1400184983318844, 1.1530319520375432, 1.14337767406591, 1.1151136634429435, 1.1317139175738518, 1.1463896950644643, 1.1387237140709152, 1.1193811401122484, 1.1326431757745927, 1.124689474361433, 1.1508833155271667, 1.1454735246400214, 1.1288081172899538, 1.1282543155556195, 1.1260127674506712, 1.134170979628035, 1.1113646885213198, 1.129249507283075, 1.1359038802358095, 1.1394332851592601, 1.1430153987110185, 1.1517239606862328, 1.1255657914862272, 1.1324900154070192, 1.1216968573459631, 1.1282344567335763, 1.137614999169415, 1.125527098853475, 1.126174941842292, 1.1246568772202008, 1.129312945690222, 1.1419985919719422, 1.1225142730350863, 1.125369069027356, 1.1591515716014722, 1.1419068157987142, 1.1323909118431106, 1.1287752818348957, 1.134462779142316, 1.114665859717686, 1.116241626454573, 1.1194174506542762, 1.13002305887705, 1.1328431898973528, 1.1287402363988344, 1.1158888257241206, 1.1339844214895278, 1.1211524662317836, 1.1228302106496948, 1.1329251373798859, 1.1274239405177808, 1.143764380516193, 1.1241331279592062, 1.1255935450429866], 'val_acc': [0.911152898532016, 0.911152898532016, 0.911152898532016, 0.9111521443080399, 0.9107467910317419, 0.9108525271574935, 0.91034366481124, 0.9106477787079836, 0.9256770496628196, 0.9314523372792611, 0.9330241277892476, 0.9331481693499117, 0.9348066424024545, 0.9483231565864308, 0.9507157551173795, 0.9559732335104045, 0.9572252070757123, 0.9607233285694424, 0.9606624241453483, 0.9627654918048838, 0.9637540641694161, 0.9650546034735829, 0.9640757438378091, 0.96653036000649, 0.966022265397392, 0.9684940658260942, 0.9681473651963085, 0.9690757803422495, 0.969377293213809, 0.970156265478352, 0.9703703541537789, 0.9705041106430214, 0.9708627732650676, 0.9711590470035801, 0.9711224398001217, 0.9721046510186891, 0.9720904559996928, 0.9716462309624483, 0.972711770836205, 0.9731638508019003, 0.9725182514199054, 0.9721039163835857, 0.9736009751346703, 0.9736812916078551, 0.9734918700370722, 0.972534306334276, 0.9735527808511194, 0.9734044542212059, 0.974064250193497, 0.9739656210574199, 0.9730958479779047, 0.9736013367431654, 0.9739521793196197, 0.974300367865705, 0.9746747346879728, 0.9746586640606028, 0.9744599117964559, 0.9750505882742535, 0.9748447218972057, 0.9746384842743564, 0.9750221803234416, 0.9750677668775951, 0.9752534622676762, 0.9736088255587277, 0.9744931683599006, 0.9741505485008387, 0.9750412477340765, 0.9751432374617337, 0.9749556818620182, 0.9750894405510807, 0.9752609346369658, 0.974841729304074, 0.9751473501822232, 0.9747098482136987, 0.975399912032176, 0.9748753648650668, 0.9753789947824864, 0.9752859545508252, 0.9753360224524366, 0.9751365069555272, 0.9754443614889322, 0.9749680281942464, 0.9756132420839869, 0.9753117471252468, 0.9755904500639502, 0.9752583051188759, 0.9755329145069491, 0.9756902050888182, 0.9753334113709328, 0.9752239461313861, 0.9759061427652731, 0.9756610659597628, 0.9755781285582611, 0.9754925727425225, 0.9749426162934262, 0.9752743801458859, 0.9754526068330439, 0.9755056369493214, 0.9754933113582523, 0.9755971693615503, 0.975249728335228, 0.9760231055149086, 0.9756065115777806, 0.9754735064841742, 0.9755624330525658, 0.9761116472195657, 0.975531791446498, 0.9757215863581491, 0.97597715138132, 0.9757843630385106, 0.9758904669532038, 0.9756677955231893, 0.9755971872743697, 0.9751873362043411, 0.9749250402559505, 0.9759592092309559, 0.9757619458258885, 0.9760171294421848, 0.9760130098079756, 0.9756864550243572, 0.9754473526155173, 0.9756547222028508, 0.9750726219849344, 0.9760029182911757, 0.9758605814566721, 0.9759285715007614, 0.9759887403469722, 0.9759069055790223, 0.9761303331185728, 0.9759024209842112, 0.975241515569402, 0.9758826155863664, 0.9756752666354389, 0.9757694167286315, 0.9761497496720567, 0.9754675449721633, 0.9761628393339147, 0.9758493652033681, 0.9757428923474465, 0.9759494979687441, 0.9755134926110454, 0.9764729264750422, 0.9758631971473225, 0.9759438977509685, 0.9762412910930721, 0.9758198422581115, 0.97616656059121, 0.9758941991048454, 0.9758792447289599, 0.9760316862791829, 0.9760126301819075, 0.976264076827909, 0.9753689004373467, 0.9761575976867877, 0.9762409194282572, 0.9761736726090443, 0.9762334395167279, 0.97629545935428, 0.976068314957074, 0.9760414069691945, 0.9756117407592613, 0.9759666910280452, 0.9760873487418901, 0.9754432482752943, 0.9761049197512058, 0.9762547437252186, 0.9761680574115424, 0.9758923242297449, 0.976042901903967, 0.9760724187735305, 0.9763238668860785, 0.976170674778246, 0.9763152517627119, 0.9758034127458328, 0.9759674355099616, 0.9761206039435415, 0.9763163848794827, 0.9758807427015791, 0.9761351861308455, 0.9760118957563109, 0.976318629848098, 0.9763533668903559, 0.9759663145445772, 0.9760029332709019, 0.9746837011540083, 0.9760727791249857, 0.9760993209999769, 0.9763006942971939, 0.9761209868169627, 0.97650692710139, 0.9761101579414729, 0.9759745238535433, 0.9766171362990864, 0.9762267069154549, 0.9761239673634614, 0.9751058837860456, 0.9765095356688139, 0.9757996924313175, 0.9753401400963325, 0.976331690598037, 0.9762218671021017, 0.9761258389912087, 0.9756603300676195, 0.9757623131958169, 0.9762539888727225, 0.976117617111843, 0.9762483803794338, 0.9760029271952088, 0.9765891098599023, 0.9762170123090225, 0.9757959434143898, 0.9764426687899289, 0.9764531090305644, 0.9764748025024294, 0.9761355537102805, 0.9766399272715899, 0.9761654431874387, 0.9762483798556671, 0.9763649609889213, 0.9761019323957406, 0.9756621822112473, 0.9764396709591308, 0.9764314639547378, 0.9758261908336767, 0.9762640663525761, 0.9759341864887566, 0.9758396553029495, 0.9761179868910979, 0.9764576171948747, 0.9758306826564674, 0.9764213728569513, 0.9764882598484756, 0.9764191217078895, 0.9757656676069504, 0.9766406863142191, 0.9762140221252174, 0.9762577367373636, 0.9764624663312741, 0.9761871128802978, 0.975815381023293, 0.976213644908476, 0.976214009031051, 0.9764336948864071, 0.9759905985662933, 0.9767348289699253, 0.9765349340564546, 0.9761688077596453, 0.9761561115512949, 0.9760952085937473, 0.9760656858579975, 0.9767195060718248, 0.9762569786375143, 0.976402683082071, 0.9762984536234649, 0.9758915774432553, 0.9765980858584908, 0.9763175132823535, 0.9761747925268954, 0.9757186136681502], 'val_mDice': [0.01678434750212517, 0.025357254343811156, 0.04612682365627406, 0.06055894765760651, 0.07423570320036374, 0.09633273554078724, 0.11730101374624484, 0.13670454631266568, 0.1771102679110998, 0.20887729080364448, 0.23570147599298213, 0.26339647121504867, 0.30245946706493626, 0.3745220178355232, 0.40440190896418055, 0.44936034476191383, 0.47196758961426144, 0.5092992327963322, 0.5278480356732655, 0.5487601363177039, 0.5552847496445024, 0.5780152070920371, 0.5789766118899381, 0.5937212288693929, 0.6071368950322559, 0.6059278250788134, 0.6213773044844294, 0.6212882469743752, 0.6206148003861439, 0.6360207704961405, 0.637872575770782, 0.6383526155525435, 0.6471884569090992, 0.6552620751158005, 0.6601741081381933, 0.6626035101803409, 0.6680586744486133, 0.6685247072436059, 0.6738507747650146, 0.6733322141459621, 0.6794930006162982, 0.6765309327097057, 0.6757020584728262, 0.6812563583595262, 0.6905227291982078, 0.6813703882254281, 0.6872262831102984, 0.6943846157439354, 0.6906591331183596, 0.696949596773761, 0.6882867141641507, 0.6963330089312656, 0.6991980700166028, 0.7011104178973367, 0.7064933725526337, 0.703790114089349, 0.7033790911438176, 0.7071482718305135, 0.7015536111142807, 0.7035703930368742, 0.7119444878951946, 0.7048275687573036, 0.705963216368469, 0.7020247973540965, 0.7087595851433927, 0.7104536212182003, 0.707400139689655, 0.7073117231652272, 0.7093856752652066, 0.7111878117484242, 0.7125434198781769, 0.7154758406020636, 0.7107401248231294, 0.7132874436034888, 0.7143042292871459, 0.7100041690945416, 0.7178213225517206, 0.7195429223702536, 0.7140504109209996, 0.7168220253317553, 0.7215451907189743, 0.714665969981041, 0.7223859895511755, 0.713638204577099, 0.7221455313199853, 0.7180609370995908, 0.7228720875322714, 0.7169504058801017, 0.7180642463620392, 0.7225842073638326, 0.722217058884029, 0.7204269816041621, 0.7184094781406315, 0.7200281635738635, 0.7208385957774881, 0.7198632236943303, 0.7216375286843111, 0.7231359032419737, 0.7230331108943021, 0.7259347608512651, 0.7203821800714637, 0.7255897936795843, 0.7267235504931432, 0.7280127791612555, 0.7265197721642225, 0.7278213692675156, 0.724832348748125, 0.7266651225425238, 0.7261931555761394, 0.7303080399551793, 0.7276082604547377, 0.7294260506470719, 0.7291922819635361, 0.7271061089210644, 0.7275342981090445, 0.7282440916934625, 0.7301566297014903, 0.7257339893619289, 0.7321127343052092, 0.7231064809646673, 0.7322175548868565, 0.7323156735599565, 0.7261877096600189, 0.726399259009973, 0.7320087993082137, 0.7334703890007703, 0.7314035534230183, 0.7314137576544012, 0.7343228398601284, 0.7285837803448441, 0.73349308862837, 0.7329976401555308, 0.7318641798986492, 0.7284034785989718, 0.7298905390637201, 0.7315027779770232, 0.7358959235499739, 0.7318187650352036, 0.7347858494740169, 0.7341178540395726, 0.7310054285455043, 0.737235436121064, 0.7345939927025713, 0.732597960948106, 0.7324703815532275, 0.7355617985574558, 0.7373492990101578, 0.7293029618179442, 0.7358022447420131, 0.7317824890617746, 0.7396220230679101, 0.7373634643839616, 0.7341336764015506, 0.7356633934698121, 0.7387581955778997, 0.7341604018043759, 0.7354922451746694, 0.7399837285646864, 0.7379780942819659, 0.7354818123715204, 0.7354028275226667, 0.7367607952211779, 0.7345030196731246, 0.7329302758030816, 0.7391198152816987, 0.736215152723718, 0.7400197760711026, 0.7385244474260166, 0.7393475591822123, 0.7347152534813369, 0.7363300329771528, 0.7372310811061222, 0.7369402899473958, 0.7371304820836743, 0.7374375066564456, 0.7334595795046466, 0.7406264276412333, 0.7343140440372675, 0.7400247311131187, 0.7338631921996132, 0.7404510709858109, 0.7381311563909159, 0.7376735285841098, 0.7334763475796879, 0.740583734793068, 0.7400466493856299, 0.7339909221250269, 0.7406608051700626, 0.7387645536860179, 0.7360056877555243, 0.7361471506329957, 0.73348909605995, 0.739524508088251, 0.7376040917917798, 0.7409938546601414, 0.7401994311746688, 0.740081743008223, 0.739264674890649, 0.7355064340132192, 0.7392692690574432, 0.7342963704744416, 0.7395347000634943, 0.7344251864823809, 0.7376880008851706, 0.7392516475570223, 0.7386681575766766, 0.7365710307927459, 0.7392135594557375, 0.7392915189789971, 0.7425326058650896, 0.7403389860749873, 0.7401777204185044, 0.7415481010723617, 0.7397930629433563, 0.7429106564429606, 0.7408266928577255, 0.7397103355722813, 0.7380851023226505, 0.7383056913612178, 0.7384628045956992, 0.7388344374608071, 0.7387942620446686, 0.7402428503824244, 0.7383180953077776, 0.7400209047882335, 0.7369647329846669, 0.7416861797887537, 0.7394445292350488, 0.7455169419203157, 0.740815931338627, 0.7370919371531173, 0.7421550459518165, 0.743929765136464, 0.7423349138093959, 0.7416429053501001, 0.7400189932494675, 0.7426301752117271, 0.7393224128310836, 0.7401822916443193, 0.7396396806034765, 0.7391355525839727, 0.7377524672367451, 0.7421458636939211, 0.7419584550631276, 0.743313457090113, 0.7418830744620996, 0.7418940117572859, 0.7449915828101454, 0.7428891570790161, 0.7443081270412317, 0.7411634901076922, 0.7401657161058985, 0.7447859478332037, 0.7430412772460226, 0.743135018382215, 0.7439599284388269, 0.7400645357024691, 0.7424169577697459, 0.7421460956177938], 'loss': [95.02704890265893, 13.222942723570878, 10.22330328450038, 8.732929885372423, 7.844099188059723, 7.236286619122186, 6.732491636700986, 6.295689653038864, 5.901732517077047, 5.5298063673806785, 5.1986932116806, 4.864510638779468, 4.491466729079799, 3.9750955054074995, 3.6874974233851834, 3.4660954328423434, 3.2900780888293903, 3.1331214519039228, 2.993309107599954, 2.8911726429888254, 2.7781491153379743, 2.709105341533225, 2.6306327379114, 2.568527231014379, 2.508766603946182, 2.454361545848208, 2.4136944808019773, 2.361698594482548, 2.3237635491628734, 2.2896156889834023, 2.2442179175413317, 2.219169743593431, 2.1795220203185126, 2.1557021183226333, 2.116489988377371, 2.092793474223304, 2.0648764819829153, 2.0372012914093958, 2.006101232603028, 1.9952821601625599, 1.9683336085566732, 1.9522539016483091, 1.9323114048438201, 1.9193253854123642, 1.911964747165555, 1.8894976167996549, 1.8740207447174673, 1.864599794117903, 1.854265706809476, 1.8379522825330068, 1.8190104082458471, 1.8142462750816855, 1.8016609883765546, 1.7973138620074613, 1.7824204167520135, 1.7757144759714103, 1.7652074148680015, 1.757059281164264, 1.7502506205506607, 1.7456805014861054, 1.7350775320608767, 1.726125811949799, 1.7276324942069854, 1.7172416893910716, 1.715042294874494, 1.7071386699401512, 1.7085009617856113, 1.705096613400344, 1.6993559036741535, 1.6903472646218254, 1.6837578368747272, 1.6835666442156185, 1.67165956257346, 1.6666456711357658, 1.6686376836506147, 1.662930006901947, 1.6610289492392827, 1.65712719099564, 1.6500560037443404, 1.6453147327394204, 1.6488256868409747, 1.6478051963746834, 1.6396331799167685, 1.6387164882622285, 1.6302313554881473, 1.629708885750601, 1.6243386051566149, 1.625477008956121, 1.62128336557618, 1.6189528975211513, 1.6214398595012671, 1.614456808739223, 1.6095947765042993, 1.6095231647646473, 1.6037057154336156, 1.6040517113048114, 1.6023655662382563, 1.5966045451112654, 1.5901338593072292, 1.5907694006306938, 1.5909329438496294, 1.5876729563782233, 1.587377393817417, 1.587147032822824, 1.5865584955344556, 1.5775663484223301, 1.576963531005713, 1.5739354356282742, 1.5726778877080823, 1.575233520904045, 1.5680597312832796, 1.5704909194116294, 1.565200515520734, 1.5648238648706458, 1.5599376460541345, 1.557403160343772, 1.555469226809562, 1.5564540752220644, 1.5549718037830327, 1.5520559479921394, 1.5496263841001083, 1.5459831727060385, 1.546832340015966, 1.54376368444415, 1.543613826650827, 1.5431718761742994, 1.5395810298438306, 1.537427201513903, 1.5448518787378946, 1.5367311846177476, 1.5382937486275747, 1.5360914002750583, 1.536046624039675, 1.533275582888041, 1.5313766587633029, 1.5321181960305308, 1.5280296734370304, 1.5304489334794285, 1.5234603370827569, 1.5231978808781874, 1.5231321440280086, 1.5232332158080286, 1.5237795704654489, 1.5218742706294413, 1.5192549860649622, 1.5192275801096582, 1.5192528989005356, 1.5203068516921652, 1.5163671027059662, 1.518072792492793, 1.5171633055823108, 1.507922598084736, 1.510721937404606, 1.5095904800673627, 1.5077457172954265, 1.510406466340522, 1.5087509910987504, 1.505585779541902, 1.506386259788656, 1.5018181192047624, 1.5021994096644542, 1.5049289453312193, 1.5050214363640662, 1.5044727994105722, 1.5028965856221355, 1.4989509060142756, 1.4990310404562373, 1.5011514107142305, 1.4983234692457357, 1.4965024560706424, 1.4996040636973247, 1.4985564881197537, 1.492602264075447, 1.4944472098547297, 1.4949541339564232, 1.4930578378982893, 1.491244195547973, 1.4904332835195193, 1.4933654466504918, 1.4939892204195016, 1.488997323170719, 1.485865634980652, 1.4906058837108718, 1.4869824309100934, 1.4882842322659537, 1.486912425181315, 1.483324201494211, 1.483539182519728, 1.4852810213222793, 1.4827905429410837, 1.4816896931873487, 1.4807830002652123, 1.4808285123574452, 1.4773481376073565, 1.4845166258432563, 1.4860549183893945, 1.4843352416112279, 1.482678405610204, 1.4837588185987116, 1.479310289529986, 1.4767406934203082, 1.4763300292691446, 1.4785641376016292, 1.4768198296441974, 1.4758620463237808, 1.4771135757316323, 1.4734603370491617, 1.477157865453214, 1.4762510831376086, 1.4724846847535715, 1.474964241506733, 1.4713414670218699, 1.471369137258128, 1.4724374218307656, 1.4701517550726073, 1.4711574957334883, 1.4727519509368907, 1.4711213782703407, 1.4725690489536167, 1.4689202599913957, 1.469822180396531, 1.4671149637011505, 1.4695160720220095, 1.4678796162201422, 1.4660080165791287, 1.464987199569611, 1.4646142718476527, 1.468546123330903, 1.4660543673088036, 1.4648169331771128, 1.4647027870158258, 1.463138125646001, 1.4620832132828816, 1.4620172168220944, 1.4573709192660427, 1.4622304246821693, 1.4627502105279366, 1.4571251361072888, 1.4610963368486622, 1.4583303024002017, 1.4634642096333927, 1.4608792668349742, 1.4545730562072343, 1.4585932566639546, 1.457976666054028, 1.4569921378646378, 1.4640017452060496, 1.456080497699993, 1.4534335979055455, 1.45387066316787, 1.452555996363455, 1.4576347708960051, 1.4567865134815308, 1.449176723413178, 1.451619916548697, 1.4505255547265006, 1.4557360766020497, 1.4536658775451876, 1.453816784999372], 'acc': [0.7513449620117113, 0.8919839611453491, 0.8929320415788913, 0.8932397109585951, 0.8932228620084374, 0.8935912993718427, 0.8944626096348285, 0.8963528178963338, 0.9014104633816991, 0.9070616449127199, 0.9107117188378181, 0.9125443930552966, 0.9142659790478345, 0.9189721926380119, 0.9236802821652254, 0.9274544892706561, 0.9297943573390152, 0.9314740155448815, 0.933549092532968, 0.9352160168368397, 0.9370458160113343, 0.9381874146269994, 0.9395375350816135, 0.940525906971204, 0.9416646523118337, 0.9426881516473498, 0.9433680270741747, 0.9443520829017794, 0.9450125615265977, 0.9456657548416519, 0.9464309781116375, 0.9468963625902475, 0.9476145136956869, 0.9479858842669229, 0.9485750375849379, 0.9490037391886582, 0.9494717116984848, 0.9499116440699052, 0.9504568265546377, 0.9505238636175559, 0.9509288572226021, 0.9512282444992697, 0.9516029005279587, 0.9517388528765108, 0.9518875802692904, 0.9521569096863909, 0.9523948161382811, 0.9525946270213095, 0.9527215316811432, 0.9528118558777314, 0.9527765516455872, 0.9529153511470329, 0.9530457970294671, 0.9532147629842576, 0.9533085131536018, 0.9534139136928274, 0.9535120155201675, 0.9535739475896905, 0.9536574784375528, 0.9536410370012707, 0.9538435202516935, 0.9539438800965107, 0.9538540267739165, 0.9539040331751487, 0.9539244646281206, 0.9541001620293144, 0.954045450790594, 0.9540441234106627, 0.9541176663815997, 0.9542191708569654, 0.9544317423652423, 0.9543200637750576, 0.9545192875623835, 0.9545987215545324, 0.9544994536016567, 0.9546193486850747, 0.9546443394419352, 0.954758437621948, 0.954833002198679, 0.9548974802663105, 0.9548496267922865, 0.9549582358767466, 0.9550392231681579, 0.955076002182571, 0.955171431990468, 0.9551503666813483, 0.9552204607657413, 0.9552431068412731, 0.9553625899984338, 0.9554414339102987, 0.9553799514747471, 0.9555063313311464, 0.9555902677881515, 0.9556195396948914, 0.9557038204196379, 0.9557240176474429, 0.9558289684207155, 0.9558076225639224, 0.955925295144885, 0.9560339028165431, 0.9560537376651453, 0.956098864440391, 0.9561046723642513, 0.956174266733693, 0.9561912427040454, 0.9563383346750742, 0.9563364391597826, 0.9563817134513498, 0.956460282487746, 0.9564629137998955, 0.9565285383188205, 0.9565680669697966, 0.9566156654405663, 0.9567007792325356, 0.9567105478042025, 0.9567858520832438, 0.9568770647972821, 0.9568282706161239, 0.9568997348004598, 0.956978635116418, 0.957042686928801, 0.9570923684035806, 0.9571066418937597, 0.9570909769016377, 0.9571137112188001, 0.957170077706492, 0.957204571467246, 0.9572643221285532, 0.957216607073126, 0.9572887033708538, 0.9573580151919023, 0.9573702238705214, 0.9573351856830837, 0.9573733571593052, 0.9575071608100562, 0.9574715143978779, 0.9575227607263004, 0.9574938070149566, 0.9575667383446318, 0.9576258688518706, 0.9576409335201445, 0.9576234832232818, 0.957665782969813, 0.9576535743571847, 0.9576652891010422, 0.9577078786999427, 0.9577329268117633, 0.9577092822002037, 0.9578017039547616, 0.9577511659950418, 0.9577891238982014, 0.9579646082062659, 0.957878434047936, 0.9579093835685125, 0.9579662658668994, 0.9579250011882751, 0.9579710568823072, 0.9580213164120566, 0.9579681896562275, 0.9581041227894732, 0.9581003491144573, 0.9580921528623866, 0.9580546978237369, 0.9580774524960458, 0.9580945273295399, 0.9581788927684114, 0.9582138642931937, 0.9581395242559875, 0.9581867874517551, 0.9582567347337265, 0.9581466817956621, 0.9582599212740455, 0.9583183230814315, 0.9582982723740877, 0.9583474010143622, 0.9583054225797905, 0.9583334624092398, 0.9584244607523279, 0.9583164925640287, 0.9583379667008399, 0.9584051510761084, 0.9585004617827005, 0.9584430447582701, 0.9585004083811862, 0.9584436321899256, 0.9584796758064248, 0.9585034787567889, 0.9585072228199558, 0.9584765505175197, 0.9585582749585994, 0.9585394025280878, 0.9586155430419079, 0.95860051178895, 0.9585992913830232, 0.958548410283404, 0.9585989803446239, 0.9585444755819602, 0.9586370881307732, 0.9585797992153848, 0.9586445353278443, 0.9587508971573188, 0.9587407320052435, 0.958714502347049, 0.9587142438582833, 0.9587289754082527, 0.9587588582213575, 0.9587999051005124, 0.9587263769984912, 0.9587599582781544, 0.9587772664309424, 0.9587889191284974, 0.9588635737556053, 0.9588314242502397, 0.9588320239051816, 0.9588705461702303, 0.958832739409884, 0.958812573092746, 0.9588763440874933, 0.958855769782546, 0.9589223360974526, 0.9589244031466989, 0.9589740107058399, 0.9588704385453168, 0.9588667081902285, 0.9589852197055914, 0.9589567326268362, 0.9589875306266524, 0.9589732505793924, 0.9590136885888971, 0.9590380913321838, 0.9590052042062158, 0.9590054535762598, 0.9590440389194664, 0.9590431398914898, 0.9591162308857837, 0.9590119969130319, 0.959056331502324, 0.9590941820205365, 0.9590582904017306, 0.9591228855399515, 0.9590575391840344, 0.9590968038810109, 0.9591360331289566, 0.959120854446665, 0.9591104333194715, 0.9591304399179984, 0.9590304206364421, 0.9591544900397146, 0.9591895690724268, 0.9591582115400444, 0.9591806020723976, 0.9591091803441104, 0.9591126144457269, 0.9592132862948081, 0.9592367365906064, 0.9592488463040981, 0.9592002883368281, 0.9591620005219147, 0.9592328632635593], 'mDice': [0.015621262517535605, 0.02607466328341146, 0.035152488836980085, 0.04397541109723165, 0.0540913881779101, 0.0653121864552419, 0.0783985510283259, 0.09426107679045523, 0.1149546940174552, 0.13774727435712156, 0.16005744011366374, 0.184589320998188, 0.21676356341703135, 0.26502236581569794, 0.3021878507973461, 0.33262763873727264, 0.3574399675533328, 0.3798518195800072, 0.40200649429313084, 0.4193479267515778, 0.4384480064249166, 0.4512489308520104, 0.4653943325062714, 0.4759626921245133, 0.4868567980599808, 0.4965641600791344, 0.5035661427233565, 0.512064575327102, 0.5188099505268905, 0.5256913012859776, 0.5327869543917172, 0.5379967514428703, 0.5449852540486892, 0.5498686360668557, 0.5573813808017718, 0.5621366290164026, 0.5671359335768476, 0.5721132999505306, 0.5775932483005749, 0.5805203610185012, 0.5850941825630525, 0.58862801650637, 0.5924616560387171, 0.5946864117902224, 0.5969065794208787, 0.6010467231702591, 0.6033422073332391, 0.6056643498813157, 0.6077056239808033, 0.610582211307248, 0.6146433550526443, 0.6155400414464698, 0.618004247110507, 0.6187976909196105, 0.6213906251288139, 0.623260976197849, 0.6252010347654676, 0.6265105307957084, 0.6280757503212923, 0.628234199294326, 0.6305607645844341, 0.633288287953278, 0.6324952947590085, 0.6345513538369763, 0.6351124132311247, 0.6364743211574015, 0.6365711146551399, 0.6371029265651249, 0.6378860671365095, 0.6394041471419892, 0.6413813708898892, 0.6407979294581968, 0.6434334213867694, 0.644611658213585, 0.6443240023134436, 0.6451322673869405, 0.6456314532003671, 0.6465194378529802, 0.6480776046561965, 0.6485746300254684, 0.6482700753911477, 0.6484304997012837, 0.6498905831065765, 0.6503094812565197, 0.6521023427598412, 0.6522971017630289, 0.6531684321561929, 0.6530738328418796, 0.6541096264040228, 0.6543407848188354, 0.6538209419660591, 0.6554406351036733, 0.655893569911446, 0.656176506210169, 0.6571978548111694, 0.657087552632043, 0.6581211357650384, 0.6584809865320906, 0.6598370709516916, 0.659868988403586, 0.6599261979770963, 0.6601614833933965, 0.6607706445937583, 0.6609080010544858, 0.6609717851214005, 0.6623626842057722, 0.6630089655177329, 0.663387202672429, 0.6636365611380056, 0.6633599732693761, 0.6648466603924287, 0.6643692778536756, 0.6656091284376344, 0.6656289252395701, 0.666517789278913, 0.6671929238642967, 0.6672546665403685, 0.6673260703731643, 0.6674818478966748, 0.6684079137531634, 0.6689301248069714, 0.6691867348280438, 0.6693167461913879, 0.6698026895091131, 0.6695540667054134, 0.6700797621937824, 0.6711028866748446, 0.671396537657611, 0.6698457435269126, 0.6711172570934366, 0.6713312238681287, 0.6715748473866153, 0.6716530005013143, 0.6720036170627935, 0.672503254846864, 0.6722054139734256, 0.673205102187723, 0.6726185915292668, 0.6737691783309913, 0.6744345061090679, 0.6740516600884308, 0.6742440492768129, 0.673883031865298, 0.6744973378634754, 0.6746796463799402, 0.6746063230922085, 0.6750990320185484, 0.6746762384616533, 0.67538644879221, 0.6748076054939631, 0.6754913446971739, 0.6770721178674882, 0.676380179586603, 0.6770901354238339, 0.6773531334994454, 0.6764749025075895, 0.6767959226408192, 0.6772636901863771, 0.6775440853253134, 0.6784099065767345, 0.6781397155994054, 0.6776499068465268, 0.6778339315514207, 0.6776591854218802, 0.6780800325246241, 0.6785781898872163, 0.6789719143269448, 0.6784754444618037, 0.6791237134901316, 0.6794802107392712, 0.6791600995116335, 0.6787655807682483, 0.6798330087689579, 0.6797647953429392, 0.679955405189745, 0.6801580987346891, 0.6804326671963938, 0.6806908775351379, 0.6798030492416068, 0.6799711012995049, 0.6810442300400996, 0.6813158671597123, 0.6806057895035575, 0.6812824423231192, 0.6808561453293268, 0.6811702540263839, 0.6821884343845004, 0.6818758295806315, 0.6812750987705194, 0.6821319628815293, 0.6822474948305601, 0.6822282423203272, 0.6823705418781439, 0.6830184096617173, 0.6815606319162232, 0.6815262629049821, 0.6822119468615823, 0.682097045994088, 0.6819835916348261, 0.6827263949135878, 0.6833061743626074, 0.6834452405596684, 0.6832506476911924, 0.6834848417734729, 0.6833342301288229, 0.683432898894691, 0.6838722623868555, 0.6833761395760651, 0.6836060184422416, 0.6844524267089246, 0.6837751809594784, 0.6841168324016648, 0.6842208697099845, 0.6841054692205261, 0.6846476566221015, 0.6843476447249915, 0.6844320246368342, 0.6847883164477717, 0.6843001935995673, 0.6847724736015808, 0.6849629361630326, 0.6853727559295599, 0.6844229315344817, 0.6851333328362686, 0.6854953359066165, 0.6855062491898207, 0.685738549346587, 0.6851845522044704, 0.6854248426381802, 0.6858826317164938, 0.6860147695477551, 0.6860696508326388, 0.6864462591166657, 0.6864582229156114, 0.6871757811721549, 0.6862281494997065, 0.6862450243182382, 0.6874149269153245, 0.6862243121886108, 0.6869192905163503, 0.6866018573493088, 0.6864229766503415, 0.6877561155163907, 0.687170663592524, 0.6869387516623372, 0.6873843612809359, 0.6860424297924137, 0.6875753996686699, 0.6878726244044768, 0.6880043008914082, 0.6885488569919455, 0.6871197817640068, 0.6873431812504458, 0.6886817115307406, 0.6883354848419004, 0.6884479396722738, 0.6876782045907424, 0.6881764422220826, 0.6882332897073495]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:01<00:27,  1.96s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:24,  1.87s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:22,  1.88s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:20,  1.88s/it]predicting test subjects:  33%|███▎      | 5/15 [00:09<00:19,  1.98s/it]predicting test subjects:  40%|████      | 6/15 [00:11<00:18,  2.08s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:15,  1.88s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:15<00:14,  2.04s/it]predicting test subjects:  60%|██████    | 9/15 [00:17<00:12,  2.01s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:19<00:09,  1.86s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:20<00:07,  1.78s/it]predicting test subjects:  80%|████████  | 12/15 [00:22<00:05,  1.82s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:24<00:03,  1.91s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:26<00:01,  1.87s/it]predicting test subjects: 100%|██████████| 15/15 [00:28<00:00,  1.87s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:01,  2.26s/it]predicting train subjects:   0%|          | 2/532 [00:03<18:11,  2.06s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:20,  1.97s/it]predicting train subjects:   1%|          | 4/532 [00:07<16:46,  1.91s/it]predicting train subjects:   1%|          | 5/532 [00:09<16:30,  1.88s/it]predicting train subjects:   1%|          | 6/532 [00:10<15:49,  1.81s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<15:34,  1.78s/it]predicting train subjects:   2%|▏         | 8/532 [00:14<15:00,  1.72s/it]predicting train subjects:   2%|▏         | 9/532 [00:16<15:41,  1.80s/it]predicting train subjects:   2%|▏         | 10/532 [00:17<15:20,  1.76s/it]predicting train subjects:   2%|▏         | 11/532 [00:19<14:52,  1.71s/it]predicting train subjects:   2%|▏         | 12/532 [00:21<16:12,  1.87s/it]predicting train subjects:   2%|▏         | 13/532 [00:23<15:10,  1.75s/it]predicting train subjects:   3%|▎         | 14/532 [00:24<14:27,  1.67s/it]predicting train subjects:   3%|▎         | 15/532 [00:26<14:31,  1.68s/it]predicting train subjects:   3%|▎         | 16/532 [00:28<14:51,  1.73s/it]predicting train subjects:   3%|▎         | 17/532 [00:29<14:16,  1.66s/it]predicting train subjects:   3%|▎         | 18/532 [00:31<15:10,  1.77s/it]predicting train subjects:   4%|▎         | 19/532 [00:33<14:16,  1.67s/it]predicting train subjects:   4%|▍         | 20/532 [00:34<14:31,  1.70s/it]predicting train subjects:   4%|▍         | 21/532 [00:36<15:26,  1.81s/it]predicting train subjects:   4%|▍         | 22/532 [00:38<14:52,  1.75s/it]predicting train subjects:   4%|▍         | 23/532 [00:40<14:59,  1.77s/it]predicting train subjects:   5%|▍         | 24/532 [00:41<14:15,  1.68s/it]predicting train subjects:   5%|▍         | 25/532 [00:44<15:31,  1.84s/it]predicting train subjects:   5%|▍         | 26/532 [00:45<15:01,  1.78s/it]predicting train subjects:   5%|▌         | 27/532 [00:48<16:34,  1.97s/it]predicting train subjects:   5%|▌         | 28/532 [00:49<15:49,  1.88s/it]predicting train subjects:   5%|▌         | 29/532 [00:51<16:12,  1.93s/it]predicting train subjects:   6%|▌         | 30/532 [00:53<15:11,  1.82s/it]predicting train subjects:   6%|▌         | 31/532 [00:55<14:59,  1.80s/it]predicting train subjects:   6%|▌         | 32/532 [00:56<14:46,  1.77s/it]predicting train subjects:   6%|▌         | 33/532 [00:58<14:17,  1.72s/it]predicting train subjects:   6%|▋         | 34/532 [01:00<15:16,  1.84s/it]predicting train subjects:   7%|▋         | 35/532 [01:02<14:58,  1.81s/it]predicting train subjects:   7%|▋         | 36/532 [01:04<15:02,  1.82s/it]predicting train subjects:   7%|▋         | 37/532 [01:05<14:57,  1.81s/it]predicting train subjects:   7%|▋         | 38/532 [01:07<15:26,  1.87s/it]predicting train subjects:   7%|▋         | 39/532 [01:09<15:03,  1.83s/it]predicting train subjects:   8%|▊         | 40/532 [01:11<14:27,  1.76s/it]predicting train subjects:   8%|▊         | 41/532 [01:13<14:43,  1.80s/it]predicting train subjects:   8%|▊         | 42/532 [01:15<14:46,  1.81s/it]predicting train subjects:   8%|▊         | 43/532 [01:16<14:03,  1.73s/it]predicting train subjects:   8%|▊         | 44/532 [01:17<13:15,  1.63s/it]predicting train subjects:   8%|▊         | 45/532 [01:19<13:00,  1.60s/it]predicting train subjects:   9%|▊         | 46/532 [01:21<13:33,  1.67s/it]predicting train subjects:   9%|▉         | 47/532 [01:23<14:24,  1.78s/it]predicting train subjects:   9%|▉         | 48/532 [01:25<14:40,  1.82s/it]predicting train subjects:   9%|▉         | 49/532 [01:26<13:53,  1.73s/it]predicting train subjects:   9%|▉         | 50/532 [01:28<14:28,  1.80s/it]predicting train subjects:  10%|▉         | 51/532 [01:30<14:07,  1.76s/it]predicting train subjects:  10%|▉         | 52/532 [01:32<13:50,  1.73s/it]predicting train subjects:  10%|▉         | 53/532 [01:33<13:29,  1.69s/it]predicting train subjects:  10%|█         | 54/532 [01:35<14:05,  1.77s/it]predicting train subjects:  10%|█         | 55/532 [01:37<14:07,  1.78s/it]predicting train subjects:  11%|█         | 56/532 [01:39<14:14,  1.79s/it]predicting train subjects:  11%|█         | 57/532 [01:41<14:08,  1.79s/it]predicting train subjects:  11%|█         | 58/532 [01:42<14:15,  1.81s/it]predicting train subjects:  11%|█         | 59/532 [01:45<15:23,  1.95s/it]predicting train subjects:  11%|█▏        | 60/532 [01:46<13:59,  1.78s/it]predicting train subjects:  11%|█▏        | 61/532 [01:47<13:06,  1.67s/it]predicting train subjects:  12%|█▏        | 62/532 [01:49<13:50,  1.77s/it]predicting train subjects:  12%|█▏        | 63/532 [01:51<14:18,  1.83s/it]predicting train subjects:  12%|█▏        | 64/532 [01:53<13:30,  1.73s/it]predicting train subjects:  12%|█▏        | 65/532 [01:55<13:27,  1.73s/it]predicting train subjects:  12%|█▏        | 66/532 [01:57<14:33,  1.87s/it]predicting train subjects:  13%|█▎        | 67/532 [01:59<14:58,  1.93s/it]predicting train subjects:  13%|█▎        | 68/532 [02:01<14:40,  1.90s/it]predicting train subjects:  13%|█▎        | 69/532 [02:02<14:00,  1.82s/it]predicting train subjects:  13%|█▎        | 70/532 [02:04<13:32,  1.76s/it]predicting train subjects:  13%|█▎        | 71/532 [02:05<12:54,  1.68s/it]predicting train subjects:  14%|█▎        | 72/532 [02:07<12:33,  1.64s/it]predicting train subjects:  14%|█▎        | 73/532 [02:09<13:11,  1.72s/it]predicting train subjects:  14%|█▍        | 74/532 [02:11<14:29,  1.90s/it]predicting train subjects:  14%|█▍        | 75/532 [02:14<16:30,  2.17s/it]predicting train subjects:  14%|█▍        | 76/532 [02:16<15:20,  2.02s/it]predicting train subjects:  14%|█▍        | 77/532 [02:17<14:43,  1.94s/it]predicting train subjects:  15%|█▍        | 78/532 [02:19<14:31,  1.92s/it]predicting train subjects:  15%|█▍        | 79/532 [02:21<14:09,  1.88s/it]predicting train subjects:  15%|█▌        | 80/532 [02:23<13:47,  1.83s/it]predicting train subjects:  15%|█▌        | 81/532 [02:25<13:38,  1.82s/it]predicting train subjects:  15%|█▌        | 82/532 [02:26<13:32,  1.81s/it]predicting train subjects:  16%|█▌        | 83/532 [02:28<12:57,  1.73s/it]predicting train subjects:  16%|█▌        | 84/532 [02:30<12:28,  1.67s/it]predicting train subjects:  16%|█▌        | 85/532 [02:31<12:00,  1.61s/it]predicting train subjects:  16%|█▌        | 86/532 [02:33<11:46,  1.58s/it]predicting train subjects:  16%|█▋        | 87/532 [02:34<11:33,  1.56s/it]predicting train subjects:  17%|█▋        | 88/532 [02:36<11:24,  1.54s/it]predicting train subjects:  17%|█▋        | 89/532 [02:37<11:47,  1.60s/it]predicting train subjects:  17%|█▋        | 90/532 [02:39<12:04,  1.64s/it]predicting train subjects:  17%|█▋        | 91/532 [02:41<12:12,  1.66s/it]predicting train subjects:  17%|█▋        | 92/532 [02:42<12:28,  1.70s/it]predicting train subjects:  17%|█▋        | 93/532 [02:44<12:22,  1.69s/it]predicting train subjects:  18%|█▊        | 94/532 [02:46<12:28,  1.71s/it]predicting train subjects:  18%|█▊        | 95/532 [02:48<13:02,  1.79s/it]predicting train subjects:  18%|█▊        | 96/532 [02:50<13:32,  1.86s/it]predicting train subjects:  18%|█▊        | 97/532 [02:52<13:44,  1.90s/it]predicting train subjects:  18%|█▊        | 98/532 [02:54<14:00,  1.94s/it]predicting train subjects:  19%|█▊        | 99/532 [02:56<14:11,  1.97s/it]predicting train subjects:  19%|█▉        | 100/532 [02:58<14:17,  1.98s/it]predicting train subjects:  19%|█▉        | 101/532 [03:00<13:24,  1.87s/it]predicting train subjects:  19%|█▉        | 102/532 [03:01<12:45,  1.78s/it]predicting train subjects:  19%|█▉        | 103/532 [03:03<12:06,  1.69s/it]predicting train subjects:  20%|█▉        | 104/532 [03:04<11:37,  1.63s/it]predicting train subjects:  20%|█▉        | 105/532 [03:06<11:17,  1.59s/it]predicting train subjects:  20%|█▉        | 106/532 [03:07<11:08,  1.57s/it]predicting train subjects:  20%|██        | 107/532 [03:09<10:59,  1.55s/it]predicting train subjects:  20%|██        | 108/532 [03:10<11:02,  1.56s/it]predicting train subjects:  20%|██        | 109/532 [03:12<10:56,  1.55s/it]predicting train subjects:  21%|██        | 110/532 [03:13<10:51,  1.54s/it]predicting train subjects:  21%|██        | 111/532 [03:15<10:46,  1.54s/it]predicting train subjects:  21%|██        | 112/532 [03:16<10:47,  1.54s/it]predicting train subjects:  21%|██        | 113/532 [03:18<11:36,  1.66s/it]predicting train subjects:  21%|██▏       | 114/532 [03:20<11:49,  1.70s/it]predicting train subjects:  22%|██▏       | 115/532 [03:22<12:06,  1.74s/it]predicting train subjects:  22%|██▏       | 116/532 [03:24<12:14,  1.76s/it]predicting train subjects:  22%|██▏       | 117/532 [03:26<12:14,  1.77s/it]predicting train subjects:  22%|██▏       | 118/532 [03:27<12:16,  1.78s/it]predicting train subjects:  22%|██▏       | 119/532 [03:29<12:05,  1.76s/it]predicting train subjects:  23%|██▎       | 120/532 [03:31<11:56,  1.74s/it]predicting train subjects:  23%|██▎       | 121/532 [03:32<11:49,  1.73s/it]predicting train subjects:  23%|██▎       | 122/532 [03:34<11:42,  1.71s/it]predicting train subjects:  23%|██▎       | 123/532 [03:36<11:42,  1.72s/it]predicting train subjects:  23%|██▎       | 124/532 [03:37<11:29,  1.69s/it]predicting train subjects:  23%|██▎       | 125/532 [03:39<11:43,  1.73s/it]predicting train subjects:  24%|██▎       | 126/532 [03:41<11:58,  1.77s/it]predicting train subjects:  24%|██▍       | 127/532 [03:43<12:08,  1.80s/it]predicting train subjects:  24%|██▍       | 128/532 [03:45<12:05,  1.80s/it]predicting train subjects:  24%|██▍       | 129/532 [03:47<12:10,  1.81s/it]predicting train subjects:  24%|██▍       | 130/532 [03:49<12:25,  1.85s/it]predicting train subjects:  25%|██▍       | 131/532 [03:51<13:02,  1.95s/it]predicting train subjects:  25%|██▍       | 132/532 [03:53<13:32,  2.03s/it]predicting train subjects:  25%|██▌       | 133/532 [03:55<13:41,  2.06s/it]predicting train subjects:  25%|██▌       | 134/532 [03:57<13:57,  2.10s/it]predicting train subjects:  25%|██▌       | 135/532 [04:00<14:09,  2.14s/it]predicting train subjects:  26%|██▌       | 136/532 [04:02<14:05,  2.13s/it]predicting train subjects:  26%|██▌       | 137/532 [04:04<14:09,  2.15s/it]predicting train subjects:  26%|██▌       | 138/532 [04:06<14:07,  2.15s/it]predicting train subjects:  26%|██▌       | 139/532 [04:08<14:19,  2.19s/it]predicting train subjects:  26%|██▋       | 140/532 [04:11<14:24,  2.21s/it]predicting train subjects:  27%|██▋       | 141/532 [04:13<14:18,  2.19s/it]predicting train subjects:  27%|██▋       | 142/532 [04:15<14:26,  2.22s/it]predicting train subjects:  27%|██▋       | 143/532 [04:17<13:11,  2.04s/it]predicting train subjects:  27%|██▋       | 144/532 [04:18<12:13,  1.89s/it]predicting train subjects:  27%|██▋       | 145/532 [04:20<11:37,  1.80s/it]predicting train subjects:  27%|██▋       | 146/532 [04:21<11:07,  1.73s/it]predicting train subjects:  28%|██▊       | 147/532 [04:23<10:56,  1.70s/it]predicting train subjects:  28%|██▊       | 148/532 [04:25<10:42,  1.67s/it]predicting train subjects:  28%|██▊       | 149/532 [04:26<10:42,  1.68s/it]predicting train subjects:  28%|██▊       | 150/532 [04:28<10:44,  1.69s/it]predicting train subjects:  28%|██▊       | 151/532 [04:30<10:39,  1.68s/it]predicting train subjects:  29%|██▊       | 152/532 [04:31<10:39,  1.68s/it]predicting train subjects:  29%|██▉       | 153/532 [04:33<10:52,  1.72s/it]predicting train subjects:  29%|██▉       | 154/532 [04:35<10:50,  1.72s/it]predicting train subjects:  29%|██▉       | 155/532 [04:37<11:57,  1.90s/it]predicting train subjects:  29%|██▉       | 156/532 [04:39<12:35,  2.01s/it]predicting train subjects:  30%|██▉       | 157/532 [04:42<13:16,  2.12s/it]predicting train subjects:  30%|██▉       | 158/532 [04:44<13:33,  2.18s/it]predicting train subjects:  30%|██▉       | 159/532 [04:46<13:52,  2.23s/it]predicting train subjects:  30%|███       | 160/532 [04:49<14:05,  2.27s/it]predicting train subjects:  30%|███       | 161/532 [04:50<12:50,  2.08s/it]predicting train subjects:  30%|███       | 162/532 [04:52<11:52,  1.93s/it]predicting train subjects:  31%|███       | 163/532 [04:54<11:09,  1.82s/it]predicting train subjects:  31%|███       | 164/532 [04:55<10:43,  1.75s/it]predicting train subjects:  31%|███       | 165/532 [04:57<10:24,  1.70s/it]predicting train subjects:  31%|███       | 166/532 [04:58<10:09,  1.66s/it]predicting train subjects:  31%|███▏      | 167/532 [05:00<10:16,  1.69s/it]predicting train subjects:  32%|███▏      | 168/532 [05:02<10:24,  1.71s/it]predicting train subjects:  32%|███▏      | 169/532 [05:04<10:27,  1.73s/it]predicting train subjects:  32%|███▏      | 170/532 [05:05<10:33,  1.75s/it]predicting train subjects:  32%|███▏      | 171/532 [05:07<10:34,  1.76s/it]predicting train subjects:  32%|███▏      | 172/532 [05:09<10:31,  1.75s/it]predicting train subjects:  33%|███▎      | 173/532 [05:11<10:13,  1.71s/it]predicting train subjects:  33%|███▎      | 174/532 [05:12<10:09,  1.70s/it]predicting train subjects:  33%|███▎      | 175/532 [05:14<10:01,  1.68s/it]predicting train subjects:  33%|███▎      | 176/532 [05:16<09:54,  1.67s/it]predicting train subjects:  33%|███▎      | 177/532 [05:17<09:47,  1.66s/it]predicting train subjects:  33%|███▎      | 178/532 [05:19<09:41,  1.64s/it]predicting train subjects:  34%|███▎      | 179/532 [05:20<09:44,  1.66s/it]predicting train subjects:  34%|███▍      | 180/532 [05:22<09:39,  1.65s/it]predicting train subjects:  34%|███▍      | 181/532 [05:24<09:43,  1.66s/it]predicting train subjects:  34%|███▍      | 182/532 [05:25<09:43,  1.67s/it]predicting train subjects:  34%|███▍      | 183/532 [05:27<09:37,  1.66s/it]predicting train subjects:  35%|███▍      | 184/532 [05:29<09:49,  1.69s/it]predicting train subjects:  35%|███▍      | 185/532 [05:30<09:38,  1.67s/it]predicting train subjects:  35%|███▍      | 186/532 [05:32<09:27,  1.64s/it]predicting train subjects:  35%|███▌      | 187/532 [05:34<09:20,  1.62s/it]predicting train subjects:  35%|███▌      | 188/532 [05:35<09:12,  1.61s/it]predicting train subjects:  36%|███▌      | 189/532 [05:37<09:01,  1.58s/it]predicting train subjects:  36%|███▌      | 190/532 [05:38<08:50,  1.55s/it]predicting train subjects:  36%|███▌      | 191/532 [05:41<10:11,  1.79s/it]predicting train subjects:  36%|███▌      | 192/532 [05:43<11:07,  1.96s/it]predicting train subjects:  36%|███▋      | 193/532 [05:45<11:34,  2.05s/it]predicting train subjects:  36%|███▋      | 194/532 [05:47<11:58,  2.12s/it]predicting train subjects:  37%|███▋      | 195/532 [05:50<12:23,  2.21s/it]predicting train subjects:  37%|███▋      | 196/532 [05:52<12:32,  2.24s/it]predicting train subjects:  37%|███▋      | 197/532 [05:54<12:08,  2.17s/it]predicting train subjects:  37%|███▋      | 198/532 [05:56<11:48,  2.12s/it]predicting train subjects:  37%|███▋      | 199/532 [05:58<11:38,  2.10s/it]predicting train subjects:  38%|███▊      | 200/532 [06:00<11:22,  2.06s/it]predicting train subjects:  38%|███▊      | 201/532 [06:02<11:27,  2.08s/it]predicting train subjects:  38%|███▊      | 202/532 [06:04<11:19,  2.06s/it]predicting train subjects:  38%|███▊      | 203/532 [06:06<10:54,  1.99s/it]predicting train subjects:  38%|███▊      | 204/532 [06:08<10:29,  1.92s/it]predicting train subjects:  39%|███▊      | 205/532 [06:10<10:10,  1.87s/it]predicting train subjects:  39%|███▊      | 206/532 [06:11<09:51,  1.81s/it]predicting train subjects:  39%|███▉      | 207/532 [06:13<09:39,  1.78s/it]predicting train subjects:  39%|███▉      | 208/532 [06:15<09:30,  1.76s/it]predicting train subjects:  39%|███▉      | 209/532 [06:16<09:12,  1.71s/it]predicting train subjects:  39%|███▉      | 210/532 [06:18<08:51,  1.65s/it]predicting train subjects:  40%|███▉      | 211/532 [06:19<08:33,  1.60s/it]predicting train subjects:  40%|███▉      | 212/532 [06:21<08:24,  1.58s/it]predicting train subjects:  40%|████      | 213/532 [06:22<08:16,  1.56s/it]predicting train subjects:  40%|████      | 214/532 [06:24<08:02,  1.52s/it]predicting train subjects:  40%|████      | 215/532 [06:26<09:07,  1.73s/it]predicting train subjects:  41%|████      | 216/532 [06:28<09:49,  1.87s/it]predicting train subjects:  41%|████      | 217/532 [06:30<10:10,  1.94s/it]predicting train subjects:  41%|████      | 218/532 [06:32<10:27,  2.00s/it]predicting train subjects:  41%|████      | 219/532 [06:35<10:37,  2.04s/it]predicting train subjects:  41%|████▏     | 220/532 [06:37<10:46,  2.07s/it]predicting train subjects:  42%|████▏     | 221/532 [06:38<09:48,  1.89s/it]predicting train subjects:  42%|████▏     | 222/532 [06:40<09:07,  1.76s/it]predicting train subjects:  42%|████▏     | 223/532 [06:41<08:40,  1.68s/it]predicting train subjects:  42%|████▏     | 224/532 [06:43<08:17,  1.62s/it]predicting train subjects:  42%|████▏     | 225/532 [06:44<07:59,  1.56s/it]predicting train subjects:  42%|████▏     | 226/532 [06:46<07:47,  1.53s/it]predicting train subjects:  43%|████▎     | 227/532 [06:47<07:43,  1.52s/it]predicting train subjects:  43%|████▎     | 228/532 [06:49<07:46,  1.54s/it]predicting train subjects:  43%|████▎     | 229/532 [06:50<07:37,  1.51s/it]predicting train subjects:  43%|████▎     | 230/532 [06:51<07:19,  1.46s/it]predicting train subjects:  43%|████▎     | 231/532 [06:53<07:15,  1.45s/it]predicting train subjects:  44%|████▎     | 232/532 [06:54<07:09,  1.43s/it]predicting train subjects:  44%|████▍     | 233/532 [06:56<07:27,  1.50s/it]predicting train subjects:  44%|████▍     | 234/532 [06:57<07:38,  1.54s/it]predicting train subjects:  44%|████▍     | 235/532 [06:59<07:49,  1.58s/it]predicting train subjects:  44%|████▍     | 236/532 [07:01<07:52,  1.60s/it]predicting train subjects:  45%|████▍     | 237/532 [07:02<07:55,  1.61s/it]predicting train subjects:  45%|████▍     | 238/532 [07:04<07:58,  1.63s/it]predicting train subjects:  45%|████▍     | 239/532 [07:06<08:10,  1.67s/it]predicting train subjects:  45%|████▌     | 240/532 [07:08<08:08,  1.67s/it]predicting train subjects:  45%|████▌     | 241/532 [07:09<08:11,  1.69s/it]predicting train subjects:  45%|████▌     | 242/532 [07:11<08:13,  1.70s/it]predicting train subjects:  46%|████▌     | 243/532 [07:13<08:17,  1.72s/it]predicting train subjects:  46%|████▌     | 244/532 [07:15<08:18,  1.73s/it]predicting train subjects:  46%|████▌     | 245/532 [07:16<07:50,  1.64s/it]predicting train subjects:  46%|████▌     | 246/532 [07:17<07:25,  1.56s/it]predicting train subjects:  46%|████▋     | 247/532 [07:19<07:03,  1.49s/it]predicting train subjects:  47%|████▋     | 248/532 [07:20<06:51,  1.45s/it]predicting train subjects:  47%|████▋     | 249/532 [07:21<06:44,  1.43s/it]predicting train subjects:  47%|████▋     | 250/532 [07:23<06:38,  1.41s/it]predicting train subjects:  47%|████▋     | 251/532 [07:24<06:37,  1.42s/it]predicting train subjects:  47%|████▋     | 252/532 [07:26<06:32,  1.40s/it]predicting train subjects:  48%|████▊     | 253/532 [07:27<06:33,  1.41s/it]predicting train subjects:  48%|████▊     | 254/532 [07:29<06:42,  1.45s/it]predicting train subjects:  48%|████▊     | 255/532 [07:30<06:43,  1.46s/it]predicting train subjects:  48%|████▊     | 256/532 [07:32<06:45,  1.47s/it]predicting train subjects:  48%|████▊     | 257/532 [07:33<07:25,  1.62s/it]predicting train subjects:  48%|████▊     | 258/532 [07:35<07:46,  1.70s/it]predicting train subjects:  49%|████▊     | 259/532 [07:37<08:05,  1.78s/it]predicting train subjects:  49%|████▉     | 260/532 [07:39<08:15,  1.82s/it]predicting train subjects:  49%|████▉     | 261/532 [07:41<08:16,  1.83s/it]predicting train subjects:  49%|████▉     | 262/532 [07:43<08:21,  1.86s/it]predicting train subjects:  49%|████▉     | 263/532 [07:44<07:37,  1.70s/it]predicting train subjects:  50%|████▉     | 264/532 [07:46<07:07,  1.59s/it]predicting train subjects:  50%|████▉     | 265/532 [07:47<06:51,  1.54s/it]predicting train subjects:  50%|█████     | 266/532 [07:48<06:31,  1.47s/it]predicting train subjects:  50%|█████     | 267/532 [07:50<06:22,  1.44s/it]predicting train subjects:  50%|█████     | 268/532 [07:51<06:09,  1.40s/it]predicting train subjects:  51%|█████     | 269/532 [07:53<06:28,  1.48s/it]predicting train subjects:  51%|█████     | 270/532 [07:54<06:40,  1.53s/it]predicting train subjects:  51%|█████     | 271/532 [07:56<06:51,  1.58s/it]predicting train subjects:  51%|█████     | 272/532 [07:58<06:55,  1.60s/it]predicting train subjects:  51%|█████▏    | 273/532 [07:59<06:56,  1.61s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:01<06:59,  1.63s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:03<07:25,  1.73s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:05<07:52,  1.85s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:07<08:02,  1.89s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:09<08:10,  1.93s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:11<08:25,  2.00s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:13<08:31,  2.03s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:15<08:27,  2.02s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:17<08:26,  2.03s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:19<08:23,  2.02s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:21<08:19,  2.01s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:24<08:18,  2.02s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:25<08:11,  2.00s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:27<07:29,  1.84s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:28<07:03,  1.73s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:30<06:51,  1.69s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:32<06:37,  1.64s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:33<06:28,  1.61s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:35<06:19,  1.58s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:36<06:29,  1.63s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:38<06:34,  1.66s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:40<06:38,  1.68s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:41<06:34,  1.67s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:43<06:35,  1.68s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:45<06:33,  1.68s/it]predicting train subjects:  56%|█████▌    | 299/532 [08:46<06:14,  1.61s/it]predicting train subjects:  56%|█████▋    | 300/532 [08:48<06:00,  1.55s/it]predicting train subjects:  57%|█████▋    | 301/532 [08:49<05:48,  1.51s/it]predicting train subjects:  57%|█████▋    | 302/532 [08:51<05:39,  1.48s/it]predicting train subjects:  57%|█████▋    | 303/532 [08:52<05:34,  1.46s/it]predicting train subjects:  57%|█████▋    | 304/532 [08:53<05:34,  1.47s/it]predicting train subjects:  57%|█████▋    | 305/532 [08:56<06:17,  1.66s/it]predicting train subjects:  58%|█████▊    | 306/532 [08:58<06:48,  1.81s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:00<07:05,  1.89s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:02<07:13,  1.93s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:04<07:28,  2.01s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:06<07:29,  2.02s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:09<08:13,  2.23s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:11<08:39,  2.36s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:14<09:00,  2.47s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:17<09:14,  2.54s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:20<09:22,  2.59s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:22<09:29,  2.64s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:24<08:15,  2.31s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:25<07:23,  2.07s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:27<06:46,  1.91s/it]predicting train subjects:  60%|██████    | 320/532 [09:28<06:20,  1.80s/it]predicting train subjects:  60%|██████    | 321/532 [09:30<06:04,  1.73s/it]predicting train subjects:  61%|██████    | 322/532 [09:31<05:47,  1.65s/it]predicting train subjects:  61%|██████    | 323/532 [09:34<06:21,  1.82s/it]predicting train subjects:  61%|██████    | 324/532 [09:36<06:40,  1.92s/it]predicting train subjects:  61%|██████    | 325/532 [09:38<06:52,  1.99s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:40<07:01,  2.05s/it]predicting train subjects:  61%|██████▏   | 327/532 [09:42<07:11,  2.11s/it]predicting train subjects:  62%|██████▏   | 328/532 [09:45<07:12,  2.12s/it]predicting train subjects:  62%|██████▏   | 329/532 [09:46<06:45,  2.00s/it]predicting train subjects:  62%|██████▏   | 330/532 [09:48<06:18,  1.87s/it]predicting train subjects:  62%|██████▏   | 331/532 [09:49<06:01,  1.80s/it]predicting train subjects:  62%|██████▏   | 332/532 [09:51<05:44,  1.72s/it]predicting train subjects:  63%|██████▎   | 333/532 [09:53<05:35,  1.68s/it]predicting train subjects:  63%|██████▎   | 334/532 [09:54<05:25,  1.65s/it]predicting train subjects:  63%|██████▎   | 335/532 [09:56<05:39,  1.72s/it]predicting train subjects:  63%|██████▎   | 336/532 [09:58<05:51,  1.79s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:00<06:01,  1.85s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:02<06:06,  1.89s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:04<06:12,  1.93s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:06<06:10,  1.93s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:07<05:39,  1.78s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:09<05:18,  1.68s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:10<05:04,  1.61s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:12<04:52,  1.56s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:13<04:45,  1.53s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:15<04:38,  1.50s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:16<04:43,  1.53s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:18<04:49,  1.57s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:20<04:50,  1.59s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:21<04:46,  1.57s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:23<04:50,  1.61s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:24<04:53,  1.63s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:26<04:54,  1.64s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:28<04:49,  1.63s/it]predicting train subjects:  67%|██████▋   | 355/532 [10:29<04:48,  1.63s/it]predicting train subjects:  67%|██████▋   | 356/532 [10:31<04:45,  1.62s/it]predicting train subjects:  67%|██████▋   | 357/532 [10:33<04:47,  1.64s/it]predicting train subjects:  67%|██████▋   | 358/532 [10:34<04:45,  1.64s/it]predicting train subjects:  67%|██████▋   | 359/532 [10:36<04:35,  1.60s/it]predicting train subjects:  68%|██████▊   | 360/532 [10:37<04:25,  1.54s/it]predicting train subjects:  68%|██████▊   | 361/532 [10:39<04:17,  1.51s/it]predicting train subjects:  68%|██████▊   | 362/532 [10:40<04:15,  1.50s/it]predicting train subjects:  68%|██████▊   | 363/532 [10:42<04:12,  1.49s/it]predicting train subjects:  68%|██████▊   | 364/532 [10:43<04:07,  1.48s/it]predicting train subjects:  69%|██████▊   | 365/532 [10:44<04:04,  1.47s/it]predicting train subjects:  69%|██████▉   | 366/532 [10:46<04:04,  1.47s/it]predicting train subjects:  69%|██████▉   | 367/532 [10:47<04:03,  1.48s/it]predicting train subjects:  69%|██████▉   | 368/532 [10:49<04:01,  1.48s/it]predicting train subjects:  69%|██████▉   | 369/532 [10:50<04:03,  1.49s/it]predicting train subjects:  70%|██████▉   | 370/532 [10:52<03:59,  1.48s/it]predicting train subjects:  70%|██████▉   | 371/532 [10:54<04:24,  1.64s/it]predicting train subjects:  70%|██████▉   | 372/532 [10:56<04:41,  1.76s/it]predicting train subjects:  70%|███████   | 373/532 [10:58<04:53,  1.85s/it]predicting train subjects:  70%|███████   | 374/532 [11:00<05:03,  1.92s/it]predicting train subjects:  70%|███████   | 375/532 [11:02<05:11,  1.98s/it]predicting train subjects:  71%|███████   | 376/532 [11:04<05:12,  2.00s/it]predicting train subjects:  71%|███████   | 377/532 [11:06<04:54,  1.90s/it]predicting train subjects:  71%|███████   | 378/532 [11:07<04:38,  1.81s/it]predicting train subjects:  71%|███████   | 379/532 [11:09<04:29,  1.76s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:11<04:21,  1.72s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:12<04:15,  1.69s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:14<04:11,  1.68s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:16<04:19,  1.74s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:18<04:21,  1.76s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:20<04:24,  1.80s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:21<04:21,  1.79s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:23<04:17,  1.77s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:25<04:07,  1.72s/it]predicting train subjects:  73%|███████▎  | 389/532 [11:26<04:05,  1.72s/it]predicting train subjects:  73%|███████▎  | 390/532 [11:28<04:03,  1.71s/it]predicting train subjects:  73%|███████▎  | 391/532 [11:30<04:03,  1.73s/it]predicting train subjects:  74%|███████▎  | 392/532 [11:32<04:02,  1.73s/it]predicting train subjects:  74%|███████▍  | 393/532 [11:33<04:03,  1.75s/it]predicting train subjects:  74%|███████▍  | 394/532 [11:35<04:02,  1.76s/it]predicting train subjects:  74%|███████▍  | 395/532 [11:37<04:06,  1.80s/it]predicting train subjects:  74%|███████▍  | 396/532 [11:39<04:04,  1.80s/it]predicting train subjects:  75%|███████▍  | 397/532 [11:41<04:02,  1.80s/it]predicting train subjects:  75%|███████▍  | 398/532 [11:42<03:57,  1.77s/it]predicting train subjects:  75%|███████▌  | 399/532 [11:44<03:56,  1.78s/it]predicting train subjects:  75%|███████▌  | 400/532 [11:46<03:53,  1.77s/it]predicting train subjects:  75%|███████▌  | 401/532 [11:48<03:58,  1.82s/it]predicting train subjects:  76%|███████▌  | 402/532 [11:50<04:03,  1.87s/it]predicting train subjects:  76%|███████▌  | 403/532 [11:52<04:06,  1.91s/it]predicting train subjects:  76%|███████▌  | 404/532 [11:54<04:03,  1.90s/it]predicting train subjects:  76%|███████▌  | 405/532 [11:56<04:03,  1.92s/it]predicting train subjects:  76%|███████▋  | 406/532 [11:58<04:03,  1.93s/it]predicting train subjects:  77%|███████▋  | 407/532 [11:59<03:52,  1.86s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:01<03:45,  1.82s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:03<03:40,  1.79s/it]predicting train subjects:  77%|███████▋  | 410/532 [12:05<03:34,  1.76s/it]predicting train subjects:  77%|███████▋  | 411/532 [12:06<03:30,  1.74s/it]predicting train subjects:  77%|███████▋  | 412/532 [12:08<03:25,  1.71s/it]predicting train subjects:  78%|███████▊  | 413/532 [12:09<03:17,  1.66s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:11<03:11,  1.62s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:12<03:06,  1.60s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:14<03:10,  1.64s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:16<03:04,  1.61s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:17<03:02,  1.60s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:19<03:07,  1.66s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:21<03:09,  1.69s/it]predicting train subjects:  79%|███████▉  | 421/532 [12:23<03:13,  1.75s/it]predicting train subjects:  79%|███████▉  | 422/532 [12:25<03:13,  1.76s/it]predicting train subjects:  80%|███████▉  | 423/532 [12:26<03:14,  1.79s/it]predicting train subjects:  80%|███████▉  | 424/532 [12:28<03:10,  1.76s/it]predicting train subjects:  80%|███████▉  | 425/532 [12:30<03:08,  1.76s/it]predicting train subjects:  80%|████████  | 426/532 [12:32<03:04,  1.74s/it]predicting train subjects:  80%|████████  | 427/532 [12:33<03:05,  1.76s/it]predicting train subjects:  80%|████████  | 428/532 [12:35<03:03,  1.77s/it]predicting train subjects:  81%|████████  | 429/532 [12:37<03:01,  1.76s/it]predicting train subjects:  81%|████████  | 430/532 [12:39<03:00,  1.77s/it]predicting train subjects:  81%|████████  | 431/532 [12:41<03:04,  1.82s/it]predicting train subjects:  81%|████████  | 432/532 [12:43<03:09,  1.90s/it]predicting train subjects:  81%|████████▏ | 433/532 [12:45<03:08,  1.90s/it]predicting train subjects:  82%|████████▏ | 434/532 [12:47<03:07,  1.92s/it]predicting train subjects:  82%|████████▏ | 435/532 [12:49<03:10,  1.96s/it]predicting train subjects:  82%|████████▏ | 436/532 [12:51<03:09,  1.98s/it]predicting train subjects:  82%|████████▏ | 437/532 [12:52<02:56,  1.86s/it]predicting train subjects:  82%|████████▏ | 438/532 [12:54<02:43,  1.74s/it]predicting train subjects:  83%|████████▎ | 439/532 [12:55<02:35,  1.67s/it]predicting train subjects:  83%|████████▎ | 440/532 [12:57<02:29,  1.62s/it]predicting train subjects:  83%|████████▎ | 441/532 [12:58<02:23,  1.58s/it]predicting train subjects:  83%|████████▎ | 442/532 [13:00<02:19,  1.54s/it]predicting train subjects:  83%|████████▎ | 443/532 [13:01<02:14,  1.51s/it]predicting train subjects:  83%|████████▎ | 444/532 [13:03<02:11,  1.49s/it]predicting train subjects:  84%|████████▎ | 445/532 [13:04<02:08,  1.48s/it]predicting train subjects:  84%|████████▍ | 446/532 [13:05<02:06,  1.47s/it]predicting train subjects:  84%|████████▍ | 447/532 [13:07<02:03,  1.45s/it]predicting train subjects:  84%|████████▍ | 448/532 [13:08<02:01,  1.44s/it]predicting train subjects:  84%|████████▍ | 449/532 [13:10<02:02,  1.48s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:11<02:04,  1.52s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:13<02:03,  1.53s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:15<02:03,  1.54s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:16<02:02,  1.55s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:18<02:01,  1.56s/it]predicting train subjects:  86%|████████▌ | 455/532 [13:19<02:05,  1.62s/it]predicting train subjects:  86%|████████▌ | 456/532 [13:21<02:08,  1.69s/it]predicting train subjects:  86%|████████▌ | 457/532 [13:23<02:09,  1.73s/it]predicting train subjects:  86%|████████▌ | 458/532 [13:25<02:10,  1.77s/it]predicting train subjects:  86%|████████▋ | 459/532 [13:27<02:11,  1.80s/it]predicting train subjects:  86%|████████▋ | 460/532 [13:29<02:11,  1.83s/it]predicting train subjects:  87%|████████▋ | 461/532 [13:31<02:16,  1.93s/it]predicting train subjects:  87%|████████▋ | 462/532 [13:33<02:18,  1.98s/it]predicting train subjects:  87%|████████▋ | 463/532 [13:35<02:19,  2.02s/it]predicting train subjects:  87%|████████▋ | 464/532 [13:37<02:21,  2.08s/it]predicting train subjects:  87%|████████▋ | 465/532 [13:39<02:19,  2.08s/it]predicting train subjects:  88%|████████▊ | 466/532 [13:42<02:18,  2.10s/it]predicting train subjects:  88%|████████▊ | 467/532 [13:43<02:09,  2.00s/it]predicting train subjects:  88%|████████▊ | 468/532 [13:45<02:01,  1.90s/it]predicting train subjects:  88%|████████▊ | 469/532 [13:47<01:55,  1.83s/it]predicting train subjects:  88%|████████▊ | 470/532 [13:48<01:51,  1.80s/it]predicting train subjects:  89%|████████▊ | 471/532 [13:50<01:47,  1.76s/it]predicting train subjects:  89%|████████▊ | 472/532 [13:52<01:44,  1.74s/it]predicting train subjects:  89%|████████▉ | 473/532 [13:54<01:45,  1.79s/it]predicting train subjects:  89%|████████▉ | 474/532 [13:56<01:45,  1.82s/it]predicting train subjects:  89%|████████▉ | 475/532 [13:57<01:44,  1.83s/it]predicting train subjects:  89%|████████▉ | 476/532 [13:59<01:42,  1.83s/it]predicting train subjects:  90%|████████▉ | 477/532 [14:01<01:41,  1.84s/it]predicting train subjects:  90%|████████▉ | 478/532 [14:03<01:40,  1.86s/it]predicting train subjects:  90%|█████████ | 479/532 [14:05<01:35,  1.79s/it]predicting train subjects:  90%|█████████ | 480/532 [14:06<01:31,  1.76s/it]predicting train subjects:  90%|█████████ | 481/532 [14:08<01:27,  1.72s/it]predicting train subjects:  91%|█████████ | 482/532 [14:10<01:24,  1.69s/it]predicting train subjects:  91%|█████████ | 483/532 [14:11<01:21,  1.67s/it]predicting train subjects:  91%|█████████ | 484/532 [14:13<01:19,  1.65s/it]predicting train subjects:  91%|█████████ | 485/532 [14:15<01:23,  1.77s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:17<01:26,  1.89s/it]predicting train subjects:  92%|█████████▏| 487/532 [14:19<01:27,  1.94s/it]predicting train subjects:  92%|█████████▏| 488/532 [14:21<01:27,  1.99s/it]predicting train subjects:  92%|█████████▏| 489/532 [14:23<01:27,  2.02s/it]predicting train subjects:  92%|█████████▏| 490/532 [14:25<01:24,  2.02s/it]predicting train subjects:  92%|█████████▏| 491/532 [14:27<01:19,  1.93s/it]predicting train subjects:  92%|█████████▏| 492/532 [14:29<01:13,  1.85s/it]predicting train subjects:  93%|█████████▎| 493/532 [14:30<01:09,  1.79s/it]predicting train subjects:  93%|█████████▎| 494/532 [14:32<01:07,  1.77s/it]predicting train subjects:  93%|█████████▎| 495/532 [14:34<01:04,  1.74s/it]predicting train subjects:  93%|█████████▎| 496/532 [14:35<01:01,  1.71s/it]predicting train subjects:  93%|█████████▎| 497/532 [14:37<00:59,  1.71s/it]predicting train subjects:  94%|█████████▎| 498/532 [14:39<00:58,  1.71s/it]predicting train subjects:  94%|█████████▍| 499/532 [14:40<00:56,  1.71s/it]predicting train subjects:  94%|█████████▍| 500/532 [14:42<00:54,  1.72s/it]predicting train subjects:  94%|█████████▍| 501/532 [14:44<00:52,  1.70s/it]predicting train subjects:  94%|█████████▍| 502/532 [14:46<00:50,  1.68s/it]predicting train subjects:  95%|█████████▍| 503/532 [14:47<00:47,  1.65s/it]predicting train subjects:  95%|█████████▍| 504/532 [14:49<00:45,  1.63s/it]predicting train subjects:  95%|█████████▍| 505/532 [14:50<00:43,  1.60s/it]predicting train subjects:  95%|█████████▌| 506/532 [14:52<00:41,  1.59s/it]predicting train subjects:  95%|█████████▌| 507/532 [14:53<00:39,  1.58s/it]predicting train subjects:  95%|█████████▌| 508/532 [14:55<00:37,  1.57s/it]predicting train subjects:  96%|█████████▌| 509/532 [14:57<00:38,  1.68s/it]predicting train subjects:  96%|█████████▌| 510/532 [14:59<00:38,  1.75s/it]predicting train subjects:  96%|█████████▌| 511/532 [15:01<00:37,  1.80s/it]predicting train subjects:  96%|█████████▌| 512/532 [15:03<00:37,  1.85s/it]predicting train subjects:  96%|█████████▋| 513/532 [15:05<00:35,  1.88s/it]predicting train subjects:  97%|█████████▋| 514/532 [15:07<00:34,  1.92s/it]predicting train subjects:  97%|█████████▋| 515/532 [15:08<00:31,  1.85s/it]predicting train subjects:  97%|█████████▋| 516/532 [15:10<00:28,  1.80s/it]predicting train subjects:  97%|█████████▋| 517/532 [15:12<00:26,  1.78s/it]predicting train subjects:  97%|█████████▋| 518/532 [15:13<00:24,  1.77s/it]predicting train subjects:  98%|█████████▊| 519/532 [15:15<00:22,  1.74s/it]predicting train subjects:  98%|█████████▊| 520/532 [15:17<00:20,  1.72s/it]predicting train subjects:  98%|█████████▊| 521/532 [15:19<00:19,  1.76s/it]predicting train subjects:  98%|█████████▊| 522/532 [15:20<00:17,  1.78s/it]predicting train subjects:  98%|█████████▊| 523/532 [15:22<00:16,  1.84s/it]predicting train subjects:  98%|█████████▊| 524/532 [15:24<00:14,  1.83s/it]predicting train subjects:  99%|█████████▊| 525/532 [15:26<00:12,  1.82s/it]predicting train subjects:  99%|█████████▉| 526/532 [15:28<00:10,  1.80s/it]predicting train subjects:  99%|█████████▉| 527/532 [15:29<00:08,  1.75s/it]predicting train subjects:  99%|█████████▉| 528/532 [15:31<00:06,  1.71s/it]predicting train subjects:  99%|█████████▉| 529/532 [15:33<00:05,  1.67s/it]predicting train subjects: 100%|█████████▉| 530/532 [15:34<00:03,  1.65s/it]predicting train subjects: 100%|█████████▉| 531/532 [15:36<00:01,  1.64s/it]predicting train subjects: 100%|██████████| 532/532 [15:37<00:00,  1.60s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists

Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<13:31,  1.53s/it]Loading train:   0%|          | 2/532 [00:02<11:58,  1.36s/it]Loading train:   1%|          | 3/532 [00:03<10:55,  1.24s/it]Loading train:   1%|          | 4/532 [00:04<10:09,  1.15s/it]Loading train:   1%|          | 5/532 [00:05<09:46,  1.11s/it]Loading train:   1%|          | 6/532 [00:06<09:21,  1.07s/it]Loading train:   1%|▏         | 7/532 [00:07<08:50,  1.01s/it]Loading train:   2%|▏         | 8/532 [00:08<08:41,  1.01it/s]Loading train:   2%|▏         | 9/532 [00:09<08:49,  1.01s/it]Loading train:   2%|▏         | 10/532 [00:10<08:27,  1.03it/s]Loading train:   2%|▏         | 11/532 [00:10<08:01,  1.08it/s]Loading train:   2%|▏         | 12/532 [00:12<09:00,  1.04s/it]Loading train:   2%|▏         | 13/532 [00:13<08:15,  1.05it/s]Loading train:   3%|▎         | 14/532 [00:13<07:48,  1.11it/s]Loading train:   3%|▎         | 15/532 [00:14<07:33,  1.14it/s]Loading train:   3%|▎         | 16/532 [00:15<07:46,  1.11it/s]Loading train:   3%|▎         | 17/532 [00:16<07:47,  1.10it/s]Loading train:   3%|▎         | 18/532 [00:17<07:59,  1.07it/s]Loading train:   4%|▎         | 19/532 [00:18<07:36,  1.12it/s]Loading train:   4%|▍         | 20/532 [00:19<07:44,  1.10it/s]Loading train:   4%|▍         | 21/532 [00:20<08:27,  1.01it/s]Loading train:   4%|▍         | 22/532 [00:21<08:03,  1.05it/s]Loading train:   4%|▍         | 23/532 [00:22<07:58,  1.06it/s]Loading train:   5%|▍         | 24/532 [00:22<07:35,  1.11it/s]Loading train:   5%|▍         | 25/532 [00:24<08:11,  1.03it/s]Loading train:   5%|▍         | 26/532 [00:24<07:47,  1.08it/s]Loading train:   5%|▌         | 27/532 [00:26<08:24,  1.00it/s]Loading train:   5%|▌         | 28/532 [00:27<08:25,  1.00s/it]Loading train:   5%|▌         | 29/532 [00:28<08:43,  1.04s/it]Loading train:   6%|▌         | 30/532 [00:29<08:06,  1.03it/s]Loading train:   6%|▌         | 31/532 [00:29<07:53,  1.06it/s]Loading train:   6%|▌         | 32/532 [00:30<07:47,  1.07it/s]Loading train:   6%|▌         | 33/532 [00:31<07:32,  1.10it/s]Loading train:   6%|▋         | 34/532 [00:32<08:01,  1.03it/s]Loading train:   7%|▋         | 35/532 [00:33<07:53,  1.05it/s]Loading train:   7%|▋         | 36/532 [00:34<08:02,  1.03it/s]Loading train:   7%|▋         | 37/532 [00:35<07:54,  1.04it/s]Loading train:   7%|▋         | 38/532 [00:36<07:56,  1.04it/s]Loading train:   7%|▋         | 39/532 [00:37<07:50,  1.05it/s]Loading train:   8%|▊         | 40/532 [00:38<07:38,  1.07it/s]Loading train:   8%|▊         | 41/532 [00:39<07:45,  1.06it/s]Loading train:   8%|▊         | 42/532 [00:40<07:58,  1.02it/s]Loading train:   8%|▊         | 43/532 [00:41<07:31,  1.08it/s]Loading train:   8%|▊         | 44/532 [00:42<07:07,  1.14it/s]Loading train:   8%|▊         | 45/532 [00:42<07:13,  1.12it/s]Loading train:   9%|▊         | 46/532 [00:44<07:33,  1.07it/s]Loading train:   9%|▉         | 47/532 [00:45<07:48,  1.03it/s]Loading train:   9%|▉         | 48/532 [00:45<07:43,  1.04it/s]Loading train:   9%|▉         | 49/532 [00:46<07:11,  1.12it/s]Loading train:   9%|▉         | 50/532 [00:47<07:34,  1.06it/s]Loading train:  10%|▉         | 51/532 [00:48<07:08,  1.12it/s]Loading train:  10%|▉         | 52/532 [00:49<06:57,  1.15it/s]Loading train:  10%|▉         | 53/532 [00:50<06:40,  1.20it/s]Loading train:  10%|█         | 54/532 [00:51<07:04,  1.13it/s]Loading train:  10%|█         | 55/532 [00:52<07:04,  1.12it/s]Loading train:  11%|█         | 56/532 [00:52<07:08,  1.11it/s]Loading train:  11%|█         | 57/532 [00:53<07:09,  1.11it/s]Loading train:  11%|█         | 58/532 [00:54<07:12,  1.10it/s]Loading train:  11%|█         | 59/532 [00:55<07:40,  1.03it/s]Loading train:  11%|█▏        | 60/532 [00:56<07:04,  1.11it/s]Loading train:  11%|█▏        | 61/532 [00:57<06:42,  1.17it/s]Loading train:  12%|█▏        | 62/532 [00:58<07:12,  1.09it/s]Loading train:  12%|█▏        | 63/532 [00:59<07:20,  1.07it/s]Loading train:  12%|█▏        | 64/532 [01:00<07:13,  1.08it/s]Loading train:  12%|█▏        | 65/532 [01:01<07:26,  1.05it/s]Loading train:  12%|█▏        | 66/532 [01:02<08:01,  1.03s/it]Loading train:  13%|█▎        | 67/532 [01:03<08:10,  1.06s/it]Loading train:  13%|█▎        | 68/532 [01:04<07:46,  1.00s/it]Loading train:  13%|█▎        | 69/532 [01:05<07:22,  1.05it/s]Loading train:  13%|█▎        | 70/532 [01:06<07:12,  1.07it/s]Loading train:  13%|█▎        | 71/532 [01:07<07:20,  1.05it/s]Loading train:  14%|█▎        | 72/532 [01:08<06:56,  1.11it/s]Loading train:  14%|█▎        | 73/532 [01:09<07:10,  1.07it/s]Loading train:  14%|█▍        | 74/532 [01:10<07:37,  1.00it/s]Loading train:  14%|█▍        | 75/532 [01:11<08:44,  1.15s/it]Loading train:  14%|█▍        | 76/532 [01:12<08:21,  1.10s/it]Loading train:  14%|█▍        | 77/532 [01:13<07:55,  1.05s/it]Loading train:  15%|█▍        | 78/532 [01:14<07:47,  1.03s/it]Loading train:  15%|█▍        | 79/532 [01:15<07:37,  1.01s/it]Loading train:  15%|█▌        | 80/532 [01:16<07:22,  1.02it/s]Loading train:  15%|█▌        | 81/532 [01:17<07:40,  1.02s/it]Loading train:  15%|█▌        | 82/532 [01:18<07:35,  1.01s/it]Loading train:  16%|█▌        | 83/532 [01:19<07:12,  1.04it/s]Loading train:  16%|█▌        | 84/532 [01:20<06:51,  1.09it/s]Loading train:  16%|█▌        | 85/532 [01:21<06:31,  1.14it/s]Loading train:  16%|█▌        | 86/532 [01:21<06:15,  1.19it/s]Loading train:  16%|█▋        | 87/532 [01:22<06:04,  1.22it/s]Loading train:  17%|█▋        | 88/532 [01:23<06:00,  1.23it/s]Loading train:  17%|█▋        | 89/532 [01:24<06:05,  1.21it/s]Loading train:  17%|█▋        | 90/532 [01:25<06:13,  1.18it/s]Loading train:  17%|█▋        | 91/532 [01:26<06:31,  1.13it/s]Loading train:  17%|█▋        | 92/532 [01:26<06:20,  1.16it/s]Loading train:  17%|█▋        | 93/532 [01:27<06:16,  1.17it/s]Loading train:  18%|█▊        | 94/532 [01:28<06:12,  1.18it/s]Loading train:  18%|█▊        | 95/532 [01:29<06:41,  1.09it/s]Loading train:  18%|█▊        | 96/532 [01:30<06:56,  1.05it/s]Loading train:  18%|█▊        | 97/532 [01:31<07:00,  1.03it/s]Loading train:  18%|█▊        | 98/532 [01:32<07:02,  1.03it/s]Loading train:  19%|█▊        | 99/532 [01:33<07:10,  1.01it/s]Loading train:  19%|█▉        | 100/532 [01:34<07:13,  1.00s/it]Loading train:  19%|█▉        | 101/532 [01:35<06:48,  1.05it/s]Loading train:  19%|█▉        | 102/532 [01:36<06:24,  1.12it/s]Loading train:  19%|█▉        | 103/532 [01:37<06:07,  1.17it/s]Loading train:  20%|█▉        | 104/532 [01:37<06:01,  1.18it/s]Loading train:  20%|█▉        | 105/532 [01:38<05:59,  1.19it/s]Loading train:  20%|█▉        | 106/532 [01:39<05:53,  1.21it/s]Loading train:  20%|██        | 107/532 [01:40<05:43,  1.24it/s]Loading train:  20%|██        | 108/532 [01:41<05:28,  1.29it/s]Loading train:  20%|██        | 109/532 [01:41<05:22,  1.31it/s]Loading train:  21%|██        | 110/532 [01:42<05:20,  1.32it/s]Loading train:  21%|██        | 111/532 [01:43<05:20,  1.31it/s]Loading train:  21%|██        | 112/532 [01:44<05:14,  1.34it/s]Loading train:  21%|██        | 113/532 [01:45<05:50,  1.20it/s]Loading train:  21%|██▏       | 114/532 [01:45<05:54,  1.18it/s]Loading train:  22%|██▏       | 115/532 [01:46<05:52,  1.18it/s]Loading train:  22%|██▏       | 116/532 [01:47<05:54,  1.17it/s]Loading train:  22%|██▏       | 117/532 [01:48<05:46,  1.20it/s]Loading train:  22%|██▏       | 118/532 [01:49<05:41,  1.21it/s]Loading train:  22%|██▏       | 119/532 [01:50<05:48,  1.18it/s]Loading train:  23%|██▎       | 120/532 [01:51<05:50,  1.17it/s]Loading train:  23%|██▎       | 121/532 [01:51<05:50,  1.17it/s]Loading train:  23%|██▎       | 122/532 [01:52<06:01,  1.13it/s]Loading train:  23%|██▎       | 123/532 [01:53<06:11,  1.10it/s]Loading train:  23%|██▎       | 124/532 [01:54<06:16,  1.08it/s]Loading train:  23%|██▎       | 125/532 [01:55<06:33,  1.04it/s]Loading train:  24%|██▎       | 126/532 [01:56<06:26,  1.05it/s]Loading train:  24%|██▍       | 127/532 [01:57<06:22,  1.06it/s]Loading train:  24%|██▍       | 128/532 [01:58<06:14,  1.08it/s]Loading train:  24%|██▍       | 129/532 [01:59<06:14,  1.08it/s]Loading train:  24%|██▍       | 130/532 [02:00<06:26,  1.04it/s]Loading train:  25%|██▍       | 131/532 [02:01<06:39,  1.00it/s]Loading train:  25%|██▍       | 132/532 [02:02<06:56,  1.04s/it]Loading train:  25%|██▌       | 133/532 [02:03<07:00,  1.05s/it]Loading train:  25%|██▌       | 134/532 [02:04<07:07,  1.07s/it]Loading train:  25%|██▌       | 135/532 [02:05<07:00,  1.06s/it]Loading train:  26%|██▌       | 136/532 [02:07<07:09,  1.09s/it]Loading train:  26%|██▌       | 137/532 [02:08<07:17,  1.11s/it]Loading train:  26%|██▌       | 138/532 [02:09<07:20,  1.12s/it]Loading train:  26%|██▌       | 139/532 [02:10<07:18,  1.12s/it]Loading train:  26%|██▋       | 140/532 [02:11<07:15,  1.11s/it]Loading train:  27%|██▋       | 141/532 [02:12<07:14,  1.11s/it]Loading train:  27%|██▋       | 142/532 [02:13<07:09,  1.10s/it]Loading train:  27%|██▋       | 143/532 [02:14<06:35,  1.02s/it]Loading train:  27%|██▋       | 144/532 [02:15<06:06,  1.06it/s]Loading train:  27%|██▋       | 145/532 [02:16<05:43,  1.13it/s]Loading train:  27%|██▋       | 146/532 [02:16<05:33,  1.16it/s]Loading train:  28%|██▊       | 147/532 [02:17<05:31,  1.16it/s]Loading train:  28%|██▊       | 148/532 [02:18<05:29,  1.16it/s]Loading train:  28%|██▊       | 149/532 [02:19<05:44,  1.11it/s]Loading train:  28%|██▊       | 150/532 [02:20<05:33,  1.15it/s]Loading train:  28%|██▊       | 151/532 [02:21<05:25,  1.17it/s]Loading train:  29%|██▊       | 152/532 [02:22<05:08,  1.23it/s]Loading train:  29%|██▉       | 153/532 [02:22<05:03,  1.25it/s]Loading train:  29%|██▉       | 154/532 [02:23<05:03,  1.24it/s]Loading train:  29%|██▉       | 155/532 [02:24<05:47,  1.08it/s]Loading train:  29%|██▉       | 156/532 [02:25<06:07,  1.02it/s]Loading train:  30%|██▉       | 157/532 [02:26<06:19,  1.01s/it]Loading train:  30%|██▉       | 158/532 [02:28<06:30,  1.04s/it]Loading train:  30%|██▉       | 159/532 [02:29<06:32,  1.05s/it]Loading train:  30%|███       | 160/532 [02:30<06:45,  1.09s/it]Loading train:  30%|███       | 161/532 [02:31<06:28,  1.05s/it]Loading train:  30%|███       | 162/532 [02:32<06:09,  1.00it/s]Loading train:  31%|███       | 163/532 [02:33<05:52,  1.05it/s]Loading train:  31%|███       | 164/532 [02:33<05:40,  1.08it/s]Loading train:  31%|███       | 165/532 [02:34<05:55,  1.03it/s]Loading train:  31%|███       | 166/532 [02:35<05:36,  1.09it/s]Loading train:  31%|███▏      | 167/532 [02:36<05:51,  1.04it/s]Loading train:  32%|███▏      | 168/532 [02:37<05:48,  1.05it/s]Loading train:  32%|███▏      | 169/532 [02:38<05:53,  1.03it/s]Loading train:  32%|███▏      | 170/532 [02:39<05:57,  1.01it/s]Loading train:  32%|███▏      | 171/532 [02:40<05:45,  1.04it/s]Loading train:  32%|███▏      | 172/532 [02:41<05:36,  1.07it/s]Loading train:  33%|███▎      | 173/532 [02:42<05:37,  1.06it/s]Loading train:  33%|███▎      | 174/532 [02:43<05:23,  1.11it/s]Loading train:  33%|███▎      | 175/532 [02:44<05:16,  1.13it/s]Loading train:  33%|███▎      | 176/532 [02:45<05:08,  1.15it/s]Loading train:  33%|███▎      | 177/532 [02:45<05:02,  1.17it/s]Loading train:  33%|███▎      | 178/532 [02:46<05:01,  1.17it/s]Loading train:  34%|███▎      | 179/532 [02:47<05:10,  1.14it/s]Loading train:  34%|███▍      | 180/532 [02:48<05:04,  1.15it/s]Loading train:  34%|███▍      | 181/532 [02:49<04:59,  1.17it/s]Loading train:  34%|███▍      | 182/532 [02:50<04:55,  1.19it/s]Loading train:  34%|███▍      | 183/532 [02:50<04:49,  1.20it/s]Loading train:  35%|███▍      | 184/532 [02:51<04:51,  1.19it/s]Loading train:  35%|███▍      | 185/532 [02:52<04:54,  1.18it/s]Loading train:  35%|███▍      | 186/532 [02:53<04:49,  1.20it/s]Loading train:  35%|███▌      | 187/532 [02:54<04:48,  1.20it/s]Loading train:  35%|███▌      | 188/532 [02:55<04:43,  1.21it/s]Loading train:  36%|███▌      | 189/532 [02:55<04:47,  1.19it/s]Loading train:  36%|███▌      | 190/532 [02:56<04:42,  1.21it/s]Loading train:  36%|███▌      | 191/532 [02:57<05:08,  1.11it/s]Loading train:  36%|███▌      | 192/532 [02:58<05:25,  1.04it/s]Loading train:  36%|███▋      | 193/532 [03:00<05:43,  1.01s/it]Loading train:  36%|███▋      | 194/532 [03:01<05:59,  1.06s/it]Loading train:  37%|███▋      | 195/532 [03:02<06:07,  1.09s/it]Loading train:  37%|███▋      | 196/532 [03:03<06:04,  1.08s/it]Loading train:  37%|███▋      | 197/532 [03:04<05:52,  1.05s/it]Loading train:  37%|███▋      | 198/532 [03:05<05:41,  1.02s/it]Loading train:  37%|███▋      | 199/532 [03:06<05:31,  1.00it/s]Loading train:  38%|███▊      | 200/532 [03:07<05:31,  1.00it/s]Loading train:  38%|███▊      | 201/532 [03:08<05:23,  1.02it/s]Loading train:  38%|███▊      | 202/532 [03:09<05:17,  1.04it/s]Loading train:  38%|███▊      | 203/532 [03:10<05:16,  1.04it/s]Loading train:  38%|███▊      | 204/532 [03:10<05:00,  1.09it/s]Loading train:  39%|███▊      | 205/532 [03:11<04:49,  1.13it/s]Loading train:  39%|███▊      | 206/532 [03:12<04:37,  1.18it/s]Loading train:  39%|███▉      | 207/532 [03:13<04:34,  1.18it/s]Loading train:  39%|███▉      | 208/532 [03:14<04:26,  1.22it/s]Loading train:  39%|███▉      | 209/532 [03:15<04:31,  1.19it/s]Loading train:  39%|███▉      | 210/532 [03:15<04:22,  1.23it/s]Loading train:  40%|███▉      | 211/532 [03:16<04:15,  1.26it/s]Loading train:  40%|███▉      | 212/532 [03:17<04:11,  1.27it/s]Loading train:  40%|████      | 213/532 [03:18<04:06,  1.29it/s]Loading train:  40%|████      | 214/532 [03:18<04:02,  1.31it/s]Loading train:  40%|████      | 215/532 [03:19<04:29,  1.18it/s]Loading train:  41%|████      | 216/532 [03:20<04:57,  1.06it/s]Loading train:  41%|████      | 217/532 [03:22<05:12,  1.01it/s]Loading train:  41%|████      | 218/532 [03:23<05:18,  1.01s/it]Loading train:  41%|████      | 219/532 [03:24<05:27,  1.05s/it]Loading train:  41%|████▏     | 220/532 [03:25<05:33,  1.07s/it]Loading train:  42%|████▏     | 221/532 [03:26<05:03,  1.02it/s]Loading train:  42%|████▏     | 222/532 [03:26<04:43,  1.09it/s]Loading train:  42%|████▏     | 223/532 [03:27<04:35,  1.12it/s]Loading train:  42%|████▏     | 224/532 [03:28<04:27,  1.15it/s]Loading train:  42%|████▏     | 225/532 [03:29<04:15,  1.20it/s]Loading train:  42%|████▏     | 226/532 [03:30<04:08,  1.23it/s]Loading train:  43%|████▎     | 227/532 [03:30<04:06,  1.24it/s]Loading train:  43%|████▎     | 228/532 [03:31<04:02,  1.25it/s]Loading train:  43%|████▎     | 229/532 [03:32<03:59,  1.26it/s]Loading train:  43%|████▎     | 230/532 [03:33<03:57,  1.27it/s]Loading train:  43%|████▎     | 231/532 [03:34<03:58,  1.26it/s]Loading train:  44%|████▎     | 232/532 [03:34<03:48,  1.31it/s]Loading train:  44%|████▍     | 233/532 [03:35<03:55,  1.27it/s]Loading train:  44%|████▍     | 234/532 [03:36<03:55,  1.27it/s]Loading train:  44%|████▍     | 235/532 [03:37<03:58,  1.24it/s]Loading train:  44%|████▍     | 236/532 [03:38<04:02,  1.22it/s]Loading train:  45%|████▍     | 237/532 [03:38<04:06,  1.20it/s]Loading train:  45%|████▍     | 238/532 [03:39<04:06,  1.19it/s]Loading train:  45%|████▍     | 239/532 [03:40<04:18,  1.13it/s]Loading train:  45%|████▌     | 240/532 [03:41<04:20,  1.12it/s]Loading train:  45%|████▌     | 241/532 [03:42<04:27,  1.09it/s]Loading train:  45%|████▌     | 242/532 [03:43<04:26,  1.09it/s]Loading train:  46%|████▌     | 243/532 [03:44<04:28,  1.08it/s]Loading train:  46%|████▌     | 244/532 [03:45<04:30,  1.06it/s]Loading train:  46%|████▌     | 245/532 [03:46<04:14,  1.13it/s]Loading train:  46%|████▌     | 246/532 [03:47<04:08,  1.15it/s]Loading train:  46%|████▋     | 247/532 [03:47<04:01,  1.18it/s]Loading train:  47%|████▋     | 248/532 [03:48<03:47,  1.25it/s]Loading train:  47%|████▋     | 249/532 [03:49<03:42,  1.27it/s]Loading train:  47%|████▋     | 250/532 [03:50<03:38,  1.29it/s]Loading train:  47%|████▋     | 251/532 [03:51<03:57,  1.18it/s]Loading train:  47%|████▋     | 252/532 [03:51<04:02,  1.16it/s]Loading train:  48%|████▊     | 253/532 [03:52<03:56,  1.18it/s]Loading train:  48%|████▊     | 254/532 [03:53<03:50,  1.21it/s]Loading train:  48%|████▊     | 255/532 [03:54<03:57,  1.17it/s]Loading train:  48%|████▊     | 256/532 [03:55<03:54,  1.18it/s]Loading train:  48%|████▊     | 257/532 [03:56<04:20,  1.06it/s]Loading train:  48%|████▊     | 258/532 [03:57<04:24,  1.03it/s]Loading train:  49%|████▊     | 259/532 [03:58<04:22,  1.04it/s]Loading train:  49%|████▉     | 260/532 [03:59<04:27,  1.02it/s]Loading train:  49%|████▉     | 261/532 [04:00<04:26,  1.02it/s]Loading train:  49%|████▉     | 262/532 [04:01<04:36,  1.02s/it]Loading train:  49%|████▉     | 263/532 [04:02<04:15,  1.05it/s]Loading train:  50%|████▉     | 264/532 [04:03<04:02,  1.10it/s]Loading train:  50%|████▉     | 265/532 [04:03<03:47,  1.17it/s]Loading train:  50%|█████     | 266/532 [04:04<03:37,  1.22it/s]Loading train:  50%|█████     | 267/532 [04:05<03:32,  1.25it/s]Loading train:  50%|█████     | 268/532 [04:06<03:26,  1.28it/s]Loading train:  51%|█████     | 269/532 [04:07<03:36,  1.21it/s]Loading train:  51%|█████     | 270/532 [04:08<03:43,  1.17it/s]Loading train:  51%|█████     | 271/532 [04:08<03:48,  1.14it/s]Loading train:  51%|█████     | 272/532 [04:09<03:48,  1.14it/s]Loading train:  51%|█████▏    | 273/532 [04:10<03:42,  1.16it/s]Loading train:  52%|█████▏    | 274/532 [04:11<03:42,  1.16it/s]Loading train:  52%|█████▏    | 275/532 [04:12<04:02,  1.06it/s]Loading train:  52%|█████▏    | 276/532 [04:13<04:13,  1.01it/s]Loading train:  52%|█████▏    | 277/532 [04:14<04:19,  1.02s/it]Loading train:  52%|█████▏    | 278/532 [04:15<04:26,  1.05s/it]Loading train:  52%|█████▏    | 279/532 [04:17<04:29,  1.06s/it]Loading train:  53%|█████▎    | 280/532 [04:18<04:28,  1.06s/it]Loading train:  53%|█████▎    | 281/532 [04:19<04:38,  1.11s/it]Loading train:  53%|█████▎    | 282/532 [04:20<04:58,  1.20s/it]Loading train:  53%|█████▎    | 283/532 [04:22<05:34,  1.34s/it]Loading train:  53%|█████▎    | 284/532 [04:23<05:18,  1.28s/it]Loading train:  54%|█████▎    | 285/532 [04:24<05:16,  1.28s/it]Loading train:  54%|█████▍    | 286/532 [04:25<05:03,  1.23s/it]Loading train:  54%|█████▍    | 287/532 [04:26<04:30,  1.11s/it]Loading train:  54%|█████▍    | 288/532 [04:27<04:07,  1.01s/it]Loading train:  54%|█████▍    | 289/532 [04:28<03:50,  1.05it/s]Loading train:  55%|█████▍    | 290/532 [04:29<03:36,  1.12it/s]Loading train:  55%|█████▍    | 291/532 [04:29<03:22,  1.19it/s]Loading train:  55%|█████▍    | 292/532 [04:30<03:17,  1.22it/s]Loading train:  55%|█████▌    | 293/532 [04:31<03:21,  1.19it/s]Loading train:  55%|█████▌    | 294/532 [04:32<03:26,  1.15it/s]Loading train:  55%|█████▌    | 295/532 [04:33<03:27,  1.14it/s]Loading train:  56%|█████▌    | 296/532 [04:34<03:33,  1.10it/s]Loading train:  56%|█████▌    | 297/532 [04:35<03:34,  1.10it/s]Loading train:  56%|█████▌    | 298/532 [04:36<03:29,  1.12it/s]Loading train:  56%|█████▌    | 299/532 [04:36<03:23,  1.15it/s]Loading train:  56%|█████▋    | 300/532 [04:37<03:13,  1.20it/s]Loading train:  57%|█████▋    | 301/532 [04:38<03:08,  1.23it/s]Loading train:  57%|█████▋    | 302/532 [04:39<03:03,  1.26it/s]Loading train:  57%|█████▋    | 303/532 [04:39<02:58,  1.28it/s]Loading train:  57%|█████▋    | 304/532 [04:40<02:58,  1.28it/s]Loading train:  57%|█████▋    | 305/532 [04:41<03:21,  1.13it/s]Loading train:  58%|█████▊    | 306/532 [04:42<03:30,  1.08it/s]Loading train:  58%|█████▊    | 307/532 [04:43<03:35,  1.04it/s]Loading train:  58%|█████▊    | 308/532 [04:44<03:39,  1.02it/s]Loading train:  58%|█████▊    | 309/532 [04:46<03:47,  1.02s/it]Loading train:  58%|█████▊    | 310/532 [04:47<03:50,  1.04s/it]Loading train:  58%|█████▊    | 311/532 [04:48<04:14,  1.15s/it]Loading train:  59%|█████▊    | 312/532 [04:50<04:34,  1.25s/it]Loading train:  59%|█████▉    | 313/532 [04:51<04:45,  1.31s/it]Loading train:  59%|█████▉    | 314/532 [04:52<04:49,  1.33s/it]Loading train:  59%|█████▉    | 315/532 [04:54<04:52,  1.35s/it]Loading train:  59%|█████▉    | 316/532 [04:55<05:01,  1.40s/it]Loading train:  60%|█████▉    | 317/532 [04:56<04:23,  1.22s/it]Loading train:  60%|█████▉    | 318/532 [04:57<03:55,  1.10s/it]Loading train:  60%|█████▉    | 319/532 [04:58<03:36,  1.02s/it]Loading train:  60%|██████    | 320/532 [04:58<03:20,  1.06it/s]Loading train:  60%|██████    | 321/532 [04:59<03:08,  1.12it/s]Loading train:  61%|██████    | 322/532 [05:00<03:01,  1.16it/s]Loading train:  61%|██████    | 323/532 [05:01<03:18,  1.05it/s]Loading train:  61%|██████    | 324/532 [05:02<03:27,  1.00it/s]Loading train:  61%|██████    | 325/532 [05:03<03:32,  1.03s/it]Loading train:  61%|██████▏   | 326/532 [05:04<03:36,  1.05s/it]Loading train:  61%|██████▏   | 327/532 [05:06<03:42,  1.08s/it]Loading train:  62%|██████▏   | 328/532 [05:07<03:42,  1.09s/it]Loading train:  62%|██████▏   | 329/532 [05:08<03:27,  1.02s/it]Loading train:  62%|██████▏   | 330/532 [05:08<03:14,  1.04it/s]Loading train:  62%|██████▏   | 331/532 [05:09<03:05,  1.09it/s]Loading train:  62%|██████▏   | 332/532 [05:10<02:57,  1.13it/s]Loading train:  63%|██████▎   | 333/532 [05:11<02:51,  1.16it/s]Loading train:  63%|██████▎   | 334/532 [05:12<02:46,  1.19it/s]Loading train:  63%|██████▎   | 335/532 [05:13<02:57,  1.11it/s]Loading train:  63%|██████▎   | 336/532 [05:14<03:05,  1.06it/s]Loading train:  63%|██████▎   | 337/532 [05:15<03:05,  1.05it/s]Loading train:  64%|██████▎   | 338/532 [05:16<03:08,  1.03it/s]Loading train:  64%|██████▎   | 339/532 [05:17<03:15,  1.01s/it]Loading train:  64%|██████▍   | 340/532 [05:18<03:19,  1.04s/it]Loading train:  64%|██████▍   | 341/532 [05:19<03:01,  1.05it/s]Loading train:  64%|██████▍   | 342/532 [05:19<02:48,  1.12it/s]Loading train:  64%|██████▍   | 343/532 [05:20<02:40,  1.18it/s]Loading train:  65%|██████▍   | 344/532 [05:21<02:32,  1.23it/s]Loading train:  65%|██████▍   | 345/532 [05:22<02:27,  1.26it/s]Loading train:  65%|██████▌   | 346/532 [05:22<02:26,  1.27it/s]Loading train:  65%|██████▌   | 347/532 [05:23<02:32,  1.21it/s]Loading train:  65%|██████▌   | 348/532 [05:24<02:33,  1.20it/s]Loading train:  66%|██████▌   | 349/532 [05:25<02:36,  1.17it/s]Loading train:  66%|██████▌   | 350/532 [05:26<02:36,  1.17it/s]Loading train:  66%|██████▌   | 351/532 [05:27<02:36,  1.15it/s]Loading train:  66%|██████▌   | 352/532 [05:28<02:37,  1.15it/s]Loading train:  66%|██████▋   | 353/532 [05:29<02:32,  1.18it/s]Loading train:  67%|██████▋   | 354/532 [05:29<02:35,  1.15it/s]Loading train:  67%|██████▋   | 355/532 [05:30<02:30,  1.18it/s]Loading train:  67%|██████▋   | 356/532 [05:31<02:26,  1.20it/s]Loading train:  67%|██████▋   | 357/532 [05:32<02:23,  1.22it/s]Loading train:  67%|██████▋   | 358/532 [05:33<02:21,  1.23it/s]Loading train:  67%|██████▋   | 359/532 [05:33<02:21,  1.23it/s]Loading train:  68%|██████▊   | 360/532 [05:34<02:18,  1.24it/s]Loading train:  68%|██████▊   | 361/532 [05:35<02:15,  1.26it/s]Loading train:  68%|██████▊   | 362/532 [05:36<02:13,  1.28it/s]Loading train:  68%|██████▊   | 363/532 [05:37<02:15,  1.25it/s]Loading train:  68%|██████▊   | 364/532 [05:37<02:14,  1.25it/s]Loading train:  69%|██████▊   | 365/532 [05:38<02:13,  1.25it/s]Loading train:  69%|██████▉   | 366/532 [05:39<02:09,  1.28it/s]Loading train:  69%|██████▉   | 367/532 [05:40<02:08,  1.28it/s]Loading train:  69%|██████▉   | 368/532 [05:41<02:08,  1.27it/s]Loading train:  69%|██████▉   | 369/532 [05:41<02:11,  1.24it/s]Loading train:  70%|██████▉   | 370/532 [05:42<02:05,  1.29it/s]Loading train:  70%|██████▉   | 371/532 [05:43<02:15,  1.19it/s]Loading train:  70%|██████▉   | 372/532 [05:44<02:22,  1.12it/s]Loading train:  70%|███████   | 373/532 [05:45<02:27,  1.08it/s]Loading train:  70%|███████   | 374/532 [05:46<02:27,  1.07it/s]Loading train:  70%|███████   | 375/532 [05:47<02:30,  1.04it/s]Loading train:  71%|███████   | 376/532 [05:48<02:32,  1.03it/s]Loading train:  71%|███████   | 377/532 [05:49<02:31,  1.02it/s]Loading train:  71%|███████   | 378/532 [05:50<02:22,  1.08it/s]Loading train:  71%|███████   | 379/532 [05:51<02:18,  1.10it/s]Loading train:  71%|███████▏  | 380/532 [05:52<02:12,  1.15it/s]Loading train:  72%|███████▏  | 381/532 [05:52<02:06,  1.19it/s]Loading train:  72%|███████▏  | 382/532 [05:53<02:04,  1.20it/s]Loading train:  72%|███████▏  | 383/532 [05:54<02:06,  1.18it/s]Loading train:  72%|███████▏  | 384/532 [05:55<02:03,  1.19it/s]Loading train:  72%|███████▏  | 385/532 [05:56<02:01,  1.21it/s]Loading train:  73%|███████▎  | 386/532 [05:56<01:59,  1.22it/s]Loading train:  73%|███████▎  | 387/532 [05:57<02:00,  1.20it/s]Loading train:  73%|███████▎  | 388/532 [05:58<02:01,  1.19it/s]Loading train:  73%|███████▎  | 389/532 [05:59<02:08,  1.11it/s]Loading train:  73%|███████▎  | 390/532 [06:00<02:09,  1.10it/s]Loading train:  73%|███████▎  | 391/532 [06:01<02:11,  1.07it/s]Loading train:  74%|███████▎  | 392/532 [06:02<02:11,  1.06it/s]Loading train:  74%|███████▍  | 393/532 [06:03<02:13,  1.04it/s]Loading train:  74%|███████▍  | 394/532 [06:04<02:08,  1.08it/s]Loading train:  74%|███████▍  | 395/532 [06:05<02:08,  1.07it/s]Loading train:  74%|███████▍  | 396/532 [06:06<02:06,  1.07it/s]Loading train:  75%|███████▍  | 397/532 [06:07<02:03,  1.10it/s]Loading train:  75%|███████▍  | 398/532 [06:08<02:02,  1.10it/s]Loading train:  75%|███████▌  | 399/532 [06:08<02:01,  1.10it/s]Loading train:  75%|███████▌  | 400/532 [06:09<02:00,  1.10it/s]Loading train:  75%|███████▌  | 401/532 [06:10<02:00,  1.08it/s]Loading train:  76%|███████▌  | 402/532 [06:11<01:59,  1.09it/s]Loading train:  76%|███████▌  | 403/532 [06:12<01:55,  1.11it/s]Loading train:  76%|███████▌  | 404/532 [06:13<01:57,  1.09it/s]Loading train:  76%|███████▌  | 405/532 [06:14<01:59,  1.07it/s]Loading train:  76%|███████▋  | 406/532 [06:15<01:58,  1.06it/s]Loading train:  77%|███████▋  | 407/532 [06:16<01:54,  1.09it/s]Loading train:  77%|███████▋  | 408/532 [06:17<01:50,  1.12it/s]Loading train:  77%|███████▋  | 409/532 [06:17<01:46,  1.15it/s]Loading train:  77%|███████▋  | 410/532 [06:18<01:44,  1.16it/s]Loading train:  77%|███████▋  | 411/532 [06:19<01:43,  1.17it/s]Loading train:  77%|███████▋  | 412/532 [06:20<01:41,  1.18it/s]Loading train:  78%|███████▊  | 413/532 [06:21<01:40,  1.18it/s]Loading train:  78%|███████▊  | 414/532 [06:22<01:40,  1.18it/s]Loading train:  78%|███████▊  | 415/532 [06:22<01:37,  1.20it/s]Loading train:  78%|███████▊  | 416/532 [06:23<01:37,  1.19it/s]Loading train:  78%|███████▊  | 417/532 [06:24<01:35,  1.20it/s]Loading train:  79%|███████▊  | 418/532 [06:25<01:33,  1.21it/s]Loading train:  79%|███████▉  | 419/532 [06:26<01:37,  1.16it/s]Loading train:  79%|███████▉  | 420/532 [06:27<01:39,  1.13it/s]Loading train:  79%|███████▉  | 421/532 [06:28<01:40,  1.11it/s]Loading train:  79%|███████▉  | 422/532 [06:29<01:39,  1.10it/s]Loading train:  80%|███████▉  | 423/532 [06:30<01:38,  1.10it/s]Loading train:  80%|███████▉  | 424/532 [06:31<01:41,  1.07it/s]Loading train:  80%|███████▉  | 425/532 [06:32<01:41,  1.05it/s]Loading train:  80%|████████  | 426/532 [06:32<01:37,  1.09it/s]Loading train:  80%|████████  | 427/532 [06:33<01:38,  1.06it/s]Loading train:  80%|████████  | 428/532 [06:34<01:37,  1.07it/s]Loading train:  81%|████████  | 429/532 [06:35<01:37,  1.06it/s]Loading train:  81%|████████  | 430/532 [06:36<01:35,  1.06it/s]Loading train:  81%|████████  | 431/532 [06:37<01:38,  1.03it/s]Loading train:  81%|████████  | 432/532 [06:38<01:37,  1.03it/s]Loading train:  81%|████████▏ | 433/532 [06:39<01:35,  1.03it/s]Loading train:  82%|████████▏ | 434/532 [06:40<01:34,  1.03it/s]Loading train:  82%|████████▏ | 435/532 [06:41<01:31,  1.06it/s]Loading train:  82%|████████▏ | 436/532 [06:42<01:31,  1.05it/s]Loading train:  82%|████████▏ | 437/532 [06:43<01:26,  1.09it/s]Loading train:  82%|████████▏ | 438/532 [06:44<01:25,  1.10it/s]Loading train:  83%|████████▎ | 439/532 [06:45<01:19,  1.16it/s]Loading train:  83%|████████▎ | 440/532 [06:45<01:14,  1.24it/s]Loading train:  83%|████████▎ | 441/532 [06:46<01:11,  1.28it/s]Loading train:  83%|████████▎ | 442/532 [06:47<01:08,  1.31it/s]Loading train:  83%|████████▎ | 443/532 [06:47<01:08,  1.31it/s]Loading train:  83%|████████▎ | 444/532 [06:48<01:10,  1.24it/s]Loading train:  84%|████████▎ | 445/532 [06:49<01:11,  1.22it/s]Loading train:  84%|████████▍ | 446/532 [06:50<01:14,  1.15it/s]Loading train:  84%|████████▍ | 447/532 [06:51<01:11,  1.19it/s]Loading train:  84%|████████▍ | 448/532 [06:52<01:06,  1.26it/s]Loading train:  84%|████████▍ | 449/532 [06:52<01:07,  1.24it/s]Loading train:  85%|████████▍ | 450/532 [06:53<01:09,  1.19it/s]Loading train:  85%|████████▍ | 451/532 [06:54<01:08,  1.18it/s]Loading train:  85%|████████▍ | 452/532 [06:55<01:12,  1.10it/s]Loading train:  85%|████████▌ | 453/532 [06:56<01:15,  1.05it/s]Loading train:  85%|████████▌ | 454/532 [06:57<01:14,  1.05it/s]Loading train:  86%|████████▌ | 455/532 [06:59<01:22,  1.07s/it]Loading train:  86%|████████▌ | 456/532 [07:00<01:22,  1.09s/it]Loading train:  86%|████████▌ | 457/532 [07:01<01:20,  1.07s/it]Loading train:  86%|████████▌ | 458/532 [07:02<01:21,  1.10s/it]Loading train:  86%|████████▋ | 459/532 [07:03<01:21,  1.11s/it]Loading train:  86%|████████▋ | 460/532 [07:04<01:20,  1.12s/it]Loading train:  87%|████████▋ | 461/532 [07:06<01:23,  1.18s/it]Loading train:  87%|████████▋ | 462/532 [07:07<01:24,  1.21s/it]Loading train:  87%|████████▋ | 463/532 [07:08<01:26,  1.25s/it]Loading train:  87%|████████▋ | 464/532 [07:09<01:25,  1.25s/it]Loading train:  87%|████████▋ | 465/532 [07:11<01:24,  1.26s/it]Loading train:  88%|████████▊ | 466/532 [07:12<01:30,  1.37s/it]Loading train:  88%|████████▊ | 467/532 [07:14<01:25,  1.31s/it]Loading train:  88%|████████▊ | 468/532 [07:15<01:19,  1.24s/it]Loading train:  88%|████████▊ | 469/532 [07:16<01:17,  1.23s/it]Loading train:  88%|████████▊ | 470/532 [07:17<01:13,  1.18s/it]Loading train:  89%|████████▊ | 471/532 [07:18<01:15,  1.24s/it]Loading train:  89%|████████▊ | 472/532 [07:19<01:09,  1.16s/it]Loading train:  89%|████████▉ | 473/532 [07:21<01:10,  1.19s/it]Loading train:  89%|████████▉ | 474/532 [07:22<01:11,  1.23s/it]Loading train:  89%|████████▉ | 475/532 [07:23<01:09,  1.22s/it]Loading train:  89%|████████▉ | 476/532 [07:24<01:08,  1.22s/it]Loading train:  90%|████████▉ | 477/532 [07:25<01:04,  1.17s/it]Loading train:  90%|████████▉ | 478/532 [07:26<01:02,  1.15s/it]Loading train:  90%|█████████ | 479/532 [07:28<00:59,  1.13s/it]Loading train:  90%|█████████ | 480/532 [07:29<00:56,  1.09s/it]Loading train:  90%|█████████ | 481/532 [07:30<00:56,  1.10s/it]Loading train:  91%|█████████ | 482/532 [07:31<00:55,  1.11s/it]Loading train:  91%|█████████ | 483/532 [07:32<00:55,  1.13s/it]Loading train:  91%|█████████ | 484/532 [07:33<00:51,  1.08s/it]Loading train:  91%|█████████ | 485/532 [07:34<00:56,  1.20s/it]Loading train:  91%|█████████▏| 486/532 [07:36<00:55,  1.21s/it]Loading train:  92%|█████████▏| 487/532 [07:37<00:54,  1.22s/it]Loading train:  92%|█████████▏| 488/532 [07:38<00:54,  1.24s/it]Loading train:  92%|█████████▏| 489/532 [07:39<00:53,  1.24s/it]Loading train:  92%|█████████▏| 490/532 [07:40<00:49,  1.19s/it]Loading train:  92%|█████████▏| 491/532 [07:41<00:45,  1.12s/it]Loading train:  92%|█████████▏| 492/532 [07:42<00:43,  1.09s/it]Loading train:  93%|█████████▎| 493/532 [07:43<00:41,  1.07s/it]Loading train:  93%|█████████▎| 494/532 [07:44<00:39,  1.03s/it]Loading train:  93%|█████████▎| 495/532 [07:45<00:38,  1.04s/it]Loading train:  93%|█████████▎| 496/532 [07:46<00:35,  1.02it/s]Loading train:  93%|█████████▎| 497/532 [07:47<00:35,  1.01s/it]Loading train:  94%|█████████▎| 498/532 [07:48<00:33,  1.01it/s]Loading train:  94%|█████████▍| 499/532 [07:49<00:32,  1.02it/s]Loading train:  94%|█████████▍| 500/532 [07:50<00:33,  1.04s/it]Loading train:  94%|█████████▍| 501/532 [07:51<00:31,  1.01s/it]Loading train:  94%|█████████▍| 502/532 [07:53<00:34,  1.15s/it]Loading train:  95%|█████████▍| 503/532 [07:54<00:31,  1.08s/it]Loading train:  95%|█████████▍| 504/532 [07:55<00:30,  1.08s/it]Loading train:  95%|█████████▍| 505/532 [07:56<00:29,  1.08s/it]Loading train:  95%|█████████▌| 506/532 [07:57<00:27,  1.05s/it]Loading train:  95%|█████████▌| 507/532 [07:58<00:26,  1.06s/it]Loading train:  95%|█████████▌| 508/532 [07:59<00:24,  1.02s/it]Loading train:  96%|█████████▌| 509/532 [08:00<00:24,  1.05s/it]Loading train:  96%|█████████▌| 510/532 [08:01<00:24,  1.11s/it]Loading train:  96%|█████████▌| 511/532 [08:02<00:23,  1.13s/it]Loading train:  96%|█████████▌| 512/532 [08:04<00:22,  1.13s/it]Loading train:  96%|█████████▋| 513/532 [08:05<00:21,  1.16s/it]Loading train:  97%|█████████▋| 514/532 [08:06<00:20,  1.12s/it]Loading train:  97%|█████████▋| 515/532 [08:07<00:18,  1.09s/it]Loading train:  97%|█████████▋| 516/532 [08:08<00:16,  1.05s/it]Loading train:  97%|█████████▋| 517/532 [08:09<00:15,  1.02s/it]Loading train:  97%|█████████▋| 518/532 [08:10<00:14,  1.01s/it]Loading train:  98%|█████████▊| 519/532 [08:11<00:12,  1.02it/s]Loading train:  98%|█████████▊| 520/532 [08:12<00:11,  1.01it/s]Loading train:  98%|█████████▊| 521/532 [08:13<00:11,  1.03s/it]Loading train:  98%|█████████▊| 522/532 [08:14<00:10,  1.02s/it]Loading train:  98%|█████████▊| 523/532 [08:15<00:08,  1.01it/s]Loading train:  98%|█████████▊| 524/532 [08:16<00:08,  1.01s/it]Loading train:  99%|█████████▊| 525/532 [08:17<00:07,  1.05s/it]Loading train:  99%|█████████▉| 526/532 [08:18<00:06,  1.07s/it]Loading train:  99%|█████████▉| 527/532 [08:19<00:05,  1.07s/it]Loading train:  99%|█████████▉| 528/532 [08:20<00:04,  1.04s/it]Loading train:  99%|█████████▉| 529/532 [08:21<00:03,  1.00s/it]Loading train: 100%|█████████▉| 530/532 [08:22<00:02,  1.03s/it]Loading train: 100%|█████████▉| 531/532 [08:23<00:01,  1.01s/it]Loading train: 100%|██████████| 532/532 [08:24<00:00,  1.00it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   1%|          | 5/532 [00:00<00:11, 44.36it/s]concatenating: train:   2%|▏         | 13/532 [00:00<00:10, 50.05it/s]concatenating: train:   4%|▍         | 20/532 [00:00<00:09, 53.72it/s]concatenating: train:   6%|▌         | 30/532 [00:00<00:08, 62.38it/s]concatenating: train:   8%|▊         | 43/532 [00:00<00:06, 73.41it/s]concatenating: train:  10%|█         | 55/532 [00:00<00:05, 82.41it/s]concatenating: train:  12%|█▏        | 66/532 [00:00<00:05, 86.59it/s]concatenating: train:  14%|█▍        | 76/532 [00:00<00:05, 77.53it/s]concatenating: train:  18%|█▊        | 95/532 [00:01<00:04, 94.01it/s]concatenating: train:  22%|██▏       | 117/532 [00:01<00:03, 112.44it/s]concatenating: train:  25%|██▍       | 132/532 [00:01<00:04, 92.72it/s] concatenating: train:  27%|██▋       | 144/532 [00:01<00:04, 96.41it/s]concatenating: train:  32%|███▏      | 171/532 [00:01<00:03, 118.72it/s]concatenating: train:  35%|███▌      | 187/532 [00:01<00:02, 123.14it/s]concatenating: train:  38%|███▊      | 203/532 [00:01<00:02, 123.70it/s]concatenating: train:  42%|████▏     | 223/532 [00:01<00:02, 138.53it/s]concatenating: train:  45%|████▌     | 241/532 [00:02<00:01, 147.58it/s]concatenating: train:  48%|████▊     | 258/532 [00:02<00:01, 150.19it/s]concatenating: train:  52%|█████▏    | 275/532 [00:02<00:01, 153.29it/s]concatenating: train:  55%|█████▍    | 292/532 [00:02<00:01, 156.91it/s]concatenating: train:  58%|█████▊    | 309/532 [00:02<00:01, 148.75it/s]concatenating: train:  61%|██████    | 325/532 [00:02<00:01, 150.15it/s]concatenating: train:  66%|██████▋   | 353/532 [00:02<00:01, 173.30it/s]concatenating: train:  70%|██████▉   | 372/532 [00:02<00:01, 129.78it/s]concatenating: train:  74%|███████▍  | 394/532 [00:03<00:00, 146.00it/s]concatenating: train:  80%|███████▉  | 425/532 [00:03<00:00, 173.01it/s]concatenating: train:  84%|████████▍ | 449/532 [00:03<00:00, 187.99it/s]concatenating: train:  89%|████████▉ | 473/532 [00:03<00:00, 200.42it/s]concatenating: train:  94%|█████████▍| 500/532 [00:03<00:00, 216.29it/s]concatenating: train:  99%|█████████▉| 529/532 [00:03<00:00, 233.91it/s]concatenating: train: 100%|██████████| 532/532 [00:03<00:00, 150.76it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:01<00:15,  1.09s/it]Loading test:  13%|█▎        | 2/15 [00:02<00:13,  1.06s/it]Loading test:  20%|██        | 3/15 [00:03<00:13,  1.12s/it]Loading test:  27%|██▋       | 4/15 [00:04<00:12,  1.15s/it]Loading test:  33%|███▎      | 5/15 [00:06<00:12,  1.27s/it]Loading test:  40%|████      | 6/15 [00:07<00:12,  1.43s/it]Loading test:  47%|████▋     | 7/15 [00:08<00:10,  1.33s/it]Loading test:  53%|█████▎    | 8/15 [00:10<00:09,  1.35s/it]Loading test:  60%|██████    | 9/15 [00:11<00:07,  1.30s/it]Loading test:  67%|██████▋   | 10/15 [00:12<00:06,  1.25s/it]Loading test:  73%|███████▎  | 11/15 [00:13<00:04,  1.23s/it]Loading test:  80%|████████  | 12/15 [00:15<00:04,  1.35s/it]Loading test:  87%|████████▋ | 13/15 [00:17<00:02,  1.48s/it]Loading test:  93%|█████████▎| 14/15 [00:18<00:01,  1.36s/it]Loading test: 100%|██████████| 15/15 [00:19<00:00,  1.39s/it]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 4/15 [00:00<00:00, 36.18it/s]concatenating: validation:  60%|██████    | 9/15 [00:00<00:00, 38.72it/s]concatenating: validation:  93%|█████████▎| 14/15 [00:00<00:00, 40.33it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 42.27it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 84, 15)   150         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 84, 15)   60          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 84, 15)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 84, 15)   2040        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 84, 15)   60          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 84, 15)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 84, 16)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 42, 16)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 42, 16)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 42, 30)   4350        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 42, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 42, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 42, 30)   8130        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 42, 30)   120         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 42, 30)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 42, 46)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 21, 46)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 21, 46)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 21, 60)   24900       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 21, 60)   240         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 21, 60)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 21, 60)   32460       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 21, 60)   240         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 21, 60)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 21, 106)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 21, 106)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 42, 30)   12750       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 42, 76)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 42, 30)   20550       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 42, 30)   120         conv2d_7[0][0]                   
__________________________________________________________________________________________________2019-07-06 02:35:37.016167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 02:35:37.016271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 02:35:37.016285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 02:35:37.016295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 02:35:37.016735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

activation_7 (Activation)       (None, 28, 42, 30)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 42, 30)   8130        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 42, 30)   120         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 42, 30)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 42, 106)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 28, 42, 106)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 84, 15)   6375        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 84, 31)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 84, 15)   4200        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 84, 15)   60          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 84, 15)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 84, 15)   2040        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 84, 15)   60          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 84, 15)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 84, 46)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 84, 46)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 84, 13)   611         dropout_5[0][0]                  
==================================================================================================
Total params: 127,886
Trainable params: 127,286
Non-trainable params: 600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53383120e-02 2.88786827e-02 1.16652967e-01 1.00133292e-02
 3.03165963e-02 5.79538965e-03 6.85058777e-02 1.28145429e-01
 7.55135052e-02 1.22435092e-02 2.73464902e-01 1.84941443e-01
 1.90056842e-04]
Train on 20749 samples, validate on 584 samples
Epoch 1/300
 - 27s - loss: 35.5169 - acc: 0.7451 - mDice: 0.0219 - val_loss: 4.3078 - val_acc: 0.9134 - val_mDice: 0.0372

Epoch 00001: val_mDice improved from -inf to 0.03722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 19s - loss: 5.3021 - acc: 0.8945 - mDice: 0.0426 - val_loss: 3.7218 - val_acc: 0.9134 - val_mDice: 0.0478

Epoch 00002: val_mDice improved from 0.03722 to 0.04781, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 18s - loss: 4.1617 - acc: 0.9015 - mDice: 0.0772 - val_loss: 3.0073 - val_acc: 0.9153 - val_mDice: 0.1111

Epoch 00003: val_mDice improved from 0.04781 to 0.11110, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 18s - loss: 3.4626 - acc: 0.9103 - mDice: 0.1345 - val_loss: 2.3568 - val_acc: 0.9336 - val_mDice: 0.2068

Epoch 00004: val_mDice improved from 0.11110 to 0.20679, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 19s - loss: 2.8226 - acc: 0.9222 - mDice: 0.2289 - val_loss: 1.7801 - val_acc: 0.9551 - val_mDice: 0.3745

Epoch 00005: val_mDice improved from 0.20679 to 0.37446, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 18s - loss: 2.3874 - acc: 0.9320 - mDice: 0.3204 - val_loss: 1.4109 - val_acc: 0.9623 - val_mDice: 0.4848

Epoch 00006: val_mDice improved from 0.37446 to 0.48478, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 18s - loss: 2.0979 - acc: 0.9381 - mDice: 0.3841 - val_loss: 1.2416 - val_acc: 0.9681 - val_mDice: 0.5584

Epoch 00007: val_mDice improved from 0.48478 to 0.55841, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300
 - 18s - loss: 1.9062 - acc: 0.9424 - mDice: 0.4312 - val_loss: 1.1369 - val_acc: 0.9695 - val_mDice: 0.5982

Epoch 00008: val_mDice improved from 0.55841 to 0.59817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300
 - 19s - loss: 1.7522 - acc: 0.9461 - mDice: 0.4694 - val_loss: 1.0340 - val_acc: 0.9702 - val_mDice: 0.6288

Epoch 00009: val_mDice improved from 0.59817 to 0.62880, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300
 - 18s - loss: 1.6369 - acc: 0.9487 - mDice: 0.4984 - val_loss: 0.9736 - val_acc: 0.9724 - val_mDice: 0.6518

Epoch 00010: val_mDice improved from 0.62880 to 0.65176, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 18s - loss: 1.5451 - acc: 0.9507 - mDice: 0.5217 - val_loss: 0.9318 - val_acc: 0.9723 - val_mDice: 0.6707

Epoch 00011: val_mDice improved from 0.65176 to 0.67074, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 18s - loss: 1.4735 - acc: 0.9522 - mDice: 0.5401 - val_loss: 0.9203 - val_acc: 0.9745 - val_mDice: 0.6755

Epoch 00012: val_mDice improved from 0.67074 to 0.67550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300
 - 18s - loss: 1.4141 - acc: 0.9534 - mDice: 0.5551 - val_loss: 0.8916 - val_acc: 0.9735 - val_mDice: 0.6847

Epoch 00013: val_mDice improved from 0.67550 to 0.68466, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 19s - loss: 1.3570 - acc: 0.9545 - mDice: 0.5693 - val_loss: 0.8784 - val_acc: 0.9747 - val_mDice: 0.6917

Epoch 00014: val_mDice improved from 0.68466 to 0.69173, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 18s - loss: 1.3109 - acc: 0.9554 - mDice: 0.5822 - val_loss: 0.8679 - val_acc: 0.9748 - val_mDice: 0.6994

Epoch 00015: val_mDice improved from 0.69173 to 0.69943, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300
 - 18s - loss: 1.2681 - acc: 0.9563 - mDice: 0.5933 - val_loss: 0.8362 - val_acc: 0.9745 - val_mDice: 0.7091

Epoch 00016: val_mDice improved from 0.69943 to 0.70906, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 17/300
 - 18s - loss: 1.2307 - acc: 0.9570 - mDice: 0.6028 - val_loss: 0.8285 - val_acc: 0.9750 - val_mDice: 0.7154

Epoch 00017: val_mDice improved from 0.70906 to 0.71536, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 18/300
 - 18s - loss: 1.2011 - acc: 0.9578 - mDice: 0.6112 - val_loss: 0.8223 - val_acc: 0.9759 - val_mDice: 0.7129

Epoch 00018: val_mDice did not improve from 0.71536
Epoch 19/300
 - 19s - loss: 1.1724 - acc: 0.9583 - mDice: 0.6186 - val_loss: 0.8260 - val_acc: 0.9754 - val_mDice: 0.7144

Epoch 00019: val_mDice did not improve from 0.71536
Epoch 20/300
 - 18s - loss: 1.1441 - acc: 0.9589 - mDice: 0.6269 - val_loss: 0.8199 - val_acc: 0.9761 - val_mDice: 0.7158

Epoch 00020: val_mDice improved from 0.71536 to 0.71578, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300
 - 17s - loss: 1.1211 - acc: 0.9592 - mDice: 0.6337 - val_loss: 0.8196 - val_acc: 0.9749 - val_mDice: 0.7201

Epoch 00021: val_mDice improved from 0.71578 to 0.72013, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300
 - 17s - loss: 1.0994 - acc: 0.9598 - mDice: 0.6398 - val_loss: 0.8007 - val_acc: 0.9759 - val_mDice: 0.7302

Epoch 00022: val_mDice improved from 0.72013 to 0.73020, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300
 - 18s - loss: 1.0786 - acc: 0.9601 - mDice: 0.6462 - val_loss: 0.7870 - val_acc: 0.9761 - val_mDice: 0.7287

Epoch 00023: val_mDice did not improve from 0.73020
Epoch 24/300
 - 18s - loss: 1.0633 - acc: 0.9605 - mDice: 0.6504 - val_loss: 0.8189 - val_acc: 0.9765 - val_mDice: 0.7197

Epoch 00024: val_mDice did not improve from 0.73020
Epoch 25/300
 - 17s - loss: 1.0451 - acc: 0.9607 - mDice: 0.6561 - val_loss: 0.7837 - val_acc: 0.9765 - val_mDice: 0.7302

Epoch 00025: val_mDice improved from 0.73020 to 0.73022, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300
 - 17s - loss: 1.0227 - acc: 0.9611 - mDice: 0.6624 - val_loss: 0.7903 - val_acc: 0.9761 - val_mDice: 0.7295

Epoch 00026: val_mDice did not improve from 0.73022
Epoch 27/300
 - 17s - loss: 1.0120 - acc: 0.9613 - mDice: 0.6661 - val_loss: 0.7684 - val_acc: 0.9769 - val_mDice: 0.7348

Epoch 00027: val_mDice improved from 0.73022 to 0.73482, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300
 - 18s - loss: 0.9996 - acc: 0.9615 - mDice: 0.6695 - val_loss: 0.7806 - val_acc: 0.9765 - val_mDice: 0.7278

Epoch 00028: val_mDice did not improve from 0.73482
Epoch 29/300
 - 19s - loss: 0.9876 - acc: 0.9617 - mDice: 0.6729 - val_loss: 0.8020 - val_acc: 0.9766 - val_mDice: 0.7267

Epoch 00029: val_mDice did not improve from 0.73482
Epoch 30/300
 - 18s - loss: 0.9711 - acc: 0.9622 - mDice: 0.6776 - val_loss: 0.7578 - val_acc: 0.9768 - val_mDice: 0.7343

Epoch 00030: val_mDice did not improve from 0.73482
Epoch 31/300
 - 18s - loss: 0.9630 - acc: 0.9622 - mDice: 0.6802 - val_loss: 0.7764 - val_acc: 0.9769 - val_mDice: 0.7364

Epoch 00031: val_mDice improved from 0.73482 to 0.73637, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 32/300
 - 18s - loss: 0.9538 - acc: 0.9624 - mDice: 0.6827 - val_loss: 0.7580 - val_acc: 0.9757 - val_mDice: 0.7415

Epoch 00032: val_mDice improved from 0.73637 to 0.74151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 33/300
 - 18s - loss: 0.9444 - acc: 0.9627 - mDice: 0.6852 - val_loss: 0.7636 - val_acc: 0.9768 - val_mDice: 0.7372

Epoch 00033: val_mDice did not improve from 0.74151
Epoch 34/300
 - 19s - loss: 0.9356 - acc: 0.9628 - mDice: 0.6880 - val_loss: 0.7566 - val_acc: 0.9774 - val_mDice: 0.7385

Epoch 00034: val_mDice did not improve from 0.74151
Epoch 35/300
 - 18s - loss: 0.9270 - acc: 0.9630 - mDice: 0.6906 - val_loss: 0.7476 - val_acc: 0.9772 - val_mDice: 0.7430

Epoch 00035: val_mDice improved from 0.74151 to 0.74298, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 36/300
 - 18s - loss: 0.9202 - acc: 0.9631 - mDice: 0.6927 - val_loss: 0.7677 - val_acc: 0.9771 - val_mDice: 0.7410

Epoch 00036: val_mDice did not improve from 0.74298
Epoch 37/300
 - 18s - loss: 0.9105 - acc: 0.9633 - mDice: 0.6957 - val_loss: 0.7540 - val_acc: 0.9776 - val_mDice: 0.7467

Epoch 00037: val_mDice improved from 0.74298 to 0.74667, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 38/300
 - 17s - loss: 0.9076 - acc: 0.9634 - mDice: 0.6967 - val_loss: 0.7413 - val_acc: 0.9762 - val_mDice: 0.7485

Epoch 00038: val_mDice improved from 0.74667 to 0.74846, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 39/300
 - 19s - loss: 0.8976 - acc: 0.9636 - mDice: 0.6997 - val_loss: 0.7534 - val_acc: 0.9777 - val_mDice: 0.7448

Epoch 00039: val_mDice did not improve from 0.74846
Epoch 40/300
 - 18s - loss: 0.8927 - acc: 0.9636 - mDice: 0.7011 - val_loss: 0.7576 - val_acc: 0.9757 - val_mDice: 0.7459

Epoch 00040: val_mDice did not improve from 0.74846
Epoch 41/300
 - 17s - loss: 0.8864 - acc: 0.9638 - mDice: 0.7028 - val_loss: 0.7483 - val_acc: 0.9779 - val_mDice: 0.7457

Epoch 00041: val_mDice did not improve from 0.74846
Epoch 42/300
 - 17s - loss: 0.8776 - acc: 0.9640 - mDice: 0.7061 - val_loss: 0.7391 - val_acc: 0.9776 - val_mDice: 0.7444

Epoch 00042: val_mDice did not improve from 0.74846
Epoch 43/300
 - 17s - loss: 0.8758 - acc: 0.9644 - mDice: 0.7065 - val_loss: 0.7525 - val_acc: 0.9779 - val_mDice: 0.7416

Epoch 00043: val_mDice did not improve from 0.74846
Epoch 44/300
 - 18s - loss: 0.8692 - acc: 0.9646 - mDice: 0.7088 - val_loss: 0.7504 - val_acc: 0.9766 - val_mDice: 0.7433

Epoch 00044: val_mDice did not improve from 0.74846
Epoch 45/300
 - 18s - loss: 0.8618 - acc: 0.9645 - mDice: 0.7108 - val_loss: 0.7378 - val_acc: 0.9776 - val_mDice: 0.7451

Epoch 00045: val_mDice did not improve from 0.74846
Epoch 46/300
 - 17s - loss: 0.8574 - acc: 0.9644 - mDice: 0.7126 - val_loss: 0.7251 - val_acc: 0.9776 - val_mDice: 0.7509

Epoch 00046: val_mDice improved from 0.74846 to 0.75086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 47/300
 - 17s - loss: 0.8502 - acc: 0.9646 - mDice: 0.7149 - val_loss: 0.7245 - val_acc: 0.9775 - val_mDice: 0.7501

Epoch 00047: val_mDice did not improve from 0.75086
Epoch 48/300
 - 17s - loss: 0.8484 - acc: 0.9646 - mDice: 0.7153 - val_loss: 0.7289 - val_acc: 0.9780 - val_mDice: 0.7496

Epoch 00048: val_mDice did not improve from 0.75086
Epoch 49/300
 - 18s - loss: 0.8422 - acc: 0.9647 - mDice: 0.7171 - val_loss: 0.7327 - val_acc: 0.9767 - val_mDice: 0.7500

Epoch 00049: val_mDice did not improve from 0.75086
Epoch 50/300
 - 19s - loss: 0.8374 - acc: 0.9647 - mDice: 0.7186 - val_loss: 0.7492 - val_acc: 0.9784 - val_mDice: 0.7483

Epoch 00050: val_mDice did not improve from 0.75086
Epoch 51/300
 - 18s - loss: 0.8342 - acc: 0.9647 - mDice: 0.7196 - val_loss: 0.7385 - val_acc: 0.9780 - val_mDice: 0.7483

Epoch 00051: val_mDice did not improve from 0.75086
Epoch 52/300
 - 17s - loss: 0.8293 - acc: 0.9649 - mDice: 0.7215 - val_loss: 0.7181 - val_acc: 0.9779 - val_mDice: 0.7510

Epoch 00052: val_mDice improved from 0.75086 to 0.75104, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 53/300
 - 17s - loss: 0.8274 - acc: 0.9649 - mDice: 0.7219 - val_loss: 0.7293 - val_acc: 0.9770 - val_mDice: 0.7519

Epoch 00053: val_mDice improved from 0.75104 to 0.75193, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 54/300
 - 17s - loss: 0.8213 - acc: 0.9650 - mDice: 0.7237 - val_loss: 0.7296 - val_acc: 0.9781 - val_mDice: 0.7515

Epoch 00054: val_mDice did not improve from 0.75193
Epoch 55/300
 - 19s - loss: 0.8184 - acc: 0.9650 - mDice: 0.7247 - val_loss: 0.7212 - val_acc: 0.9779 - val_mDice: 0.7509

Epoch 00055: val_mDice did not improve from 0.75193
Epoch 56/300
 - 18s - loss: 0.8161 - acc: 0.9651 - mDice: 0.7256 - val_loss: 0.7202 - val_acc: 0.9788 - val_mDice: 0.7482

Epoch 00056: val_mDice did not improve from 0.75193
Epoch 57/300
 - 17s - loss: 0.8106 - acc: 0.9652 - mDice: 0.7270 - val_loss: 0.7149 - val_acc: 0.9780 - val_mDice: 0.7581

Epoch 00057: val_mDice improved from 0.75193 to 0.75814, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 58/300
 - 18s - loss: 0.8085 - acc: 0.9653 - mDice: 0.7279 - val_loss: 0.7244 - val_acc: 0.9784 - val_mDice: 0.7517

Epoch 00058: val_mDice did not improve from 0.75814
Epoch 59/300
 - 18s - loss: 0.8074 - acc: 0.9653 - mDice: 0.7280 - val_loss: 0.7151 - val_acc: 0.9781 - val_mDice: 0.7558

Epoch 00059: val_mDice did not improve from 0.75814
Epoch 60/300
 - 19s - loss: 0.8048 - acc: 0.9653 - mDice: 0.7288 - val_loss: 0.7135 - val_acc: 0.9789 - val_mDice: 0.7529

Epoch 00060: val_mDice did not improve from 0.75814
Epoch 61/300
 - 18s - loss: 0.7997 - acc: 0.9655 - mDice: 0.7307 - val_loss: 0.7234 - val_acc: 0.9782 - val_mDice: 0.7519

Epoch 00061: val_mDice did not improve from 0.75814
Epoch 62/300
 - 18s - loss: 0.7977 - acc: 0.9655 - mDice: 0.7312 - val_loss: 0.7079 - val_acc: 0.9780 - val_mDice: 0.7551

Epoch 00062: val_mDice did not improve from 0.75814
Epoch 63/300
 - 18s - loss: 0.7966 - acc: 0.9655 - mDice: 0.7313 - val_loss: 0.7211 - val_acc: 0.9783 - val_mDice: 0.7557

Epoch 00063: val_mDice did not improve from 0.75814
Epoch 64/300
 - 17s - loss: 0.7935 - acc: 0.9656 - mDice: 0.7325 - val_loss: 0.7072 - val_acc: 0.9784 - val_mDice: 0.7589

Epoch 00064: val_mDice improved from 0.75814 to 0.75895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 65/300
 - 17s - loss: 0.7909 - acc: 0.9656 - mDice: 0.7333 - val_loss: 0.7311 - val_acc: 0.9781 - val_mDice: 0.7516

Epoch 00065: val_mDice did not improve from 0.75895
Epoch 66/300
 - 19s - loss: 0.7864 - acc: 0.9657 - mDice: 0.7344 - val_loss: 0.7102 - val_acc: 0.9786 - val_mDice: 0.7583

Epoch 00066: val_mDice did not improve from 0.75895
Epoch 67/300
 - 18s - loss: 0.7880 - acc: 0.9657 - mDice: 0.7341 - val_loss: 0.7034 - val_acc: 0.9784 - val_mDice: 0.7574

Epoch 00067: val_mDice did not improve from 0.75895
Epoch 68/300
 - 17s - loss: 0.7844 - acc: 0.9657 - mDice: 0.7353 - val_loss: 0.7291 - val_acc: 0.9780 - val_mDice: 0.7522

Epoch 00068: val_mDice did not improve from 0.75895
Epoch 69/300
 - 17s - loss: 0.7820 - acc: 0.9658 - mDice: 0.7361 - val_loss: 0.7113 - val_acc: 0.9784 - val_mDice: 0.7592

Epoch 00069: val_mDice improved from 0.75895 to 0.75925, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 70/300
 - 17s - loss: 0.7797 - acc: 0.9659 - mDice: 0.7369 - val_loss: 0.7112 - val_acc: 0.9787 - val_mDice: 0.7538

Epoch 00070: val_mDice did not improve from 0.75925
Epoch 71/300
 - 18s - loss: 0.7802 - acc: 0.9658 - mDice: 0.7364 - val_loss: 0.7026 - val_acc: 0.9778 - val_mDice: 0.7587

Epoch 00071: val_mDice did not improve from 0.75925
Epoch 72/300
 - 19s - loss: 0.7759 - acc: 0.9659 - mDice: 0.7382 - val_loss: 0.7258 - val_acc: 0.9783 - val_mDice: 0.7519

Epoch 00072: val_mDice did not improve from 0.75925
Epoch 73/300
 - 18s - loss: 0.7781 - acc: 0.9659 - mDice: 0.7374 - val_loss: 0.7110 - val_acc: 0.9789 - val_mDice: 0.7570

Epoch 00073: val_mDice did not improve from 0.75925
Epoch 74/300
 - 17s - loss: 0.7719 - acc: 0.9660 - mDice: 0.7392 - val_loss: 0.7270 - val_acc: 0.9784 - val_mDice: 0.7559

Epoch 00074: val_mDice did not improve from 0.75925
Epoch 75/300
 - 17s - loss: 0.7684 - acc: 0.9661 - mDice: 0.7405 - val_loss: 0.7046 - val_acc: 0.9787 - val_mDice: 0.7573

Epoch 00075: val_mDice did not improve from 0.75925
Epoch 76/300
 - 17s - loss: 0.7707 - acc: 0.9660 - mDice: 0.7399 - val_loss: 0.6991 - val_acc: 0.9786 - val_mDice: 0.7608

Epoch 00076: val_mDice improved from 0.75925 to 0.76076, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 77/300
 - 18s - loss: 0.7682 - acc: 0.9660 - mDice: 0.7402 - val_loss: 0.7011 - val_acc: 0.9786 - val_mDice: 0.7605

Epoch 00077: val_mDice did not improve from 0.76076
Epoch 78/300
 - 19s - loss: 0.7647 - acc: 0.9661 - mDice: 0.7415 - val_loss: 0.7060 - val_acc: 0.9790 - val_mDice: 0.7559

Epoch 00078: val_mDice did not improve from 0.76076
Epoch 79/300
 - 17s - loss: 0.7638 - acc: 0.9661 - mDice: 0.7417 - val_loss: 0.7030 - val_acc: 0.9786 - val_mDice: 0.7592

Epoch 00079: val_mDice did not improve from 0.76076
Epoch 80/300
 - 17s - loss: 0.7640 - acc: 0.9660 - mDice: 0.7416 - val_loss: 0.7113 - val_acc: 0.9785 - val_mDice: 0.7534

Epoch 00080: val_mDice did not improve from 0.76076
Epoch 81/300
 - 17s - loss: 0.7612 - acc: 0.9661 - mDice: 0.7424 - val_loss: 0.7019 - val_acc: 0.9787 - val_mDice: 0.7592

Epoch 00081: val_mDice did not improve from 0.76076
Epoch 82/300
 - 17s - loss: 0.7601 - acc: 0.9661 - mDice: 0.7428 - val_loss: 0.7076 - val_acc: 0.9779 - val_mDice: 0.7591

Epoch 00082: val_mDice did not improve from 0.76076
Epoch 83/300
 - 19s - loss: 0.7582 - acc: 0.9661 - mDice: 0.7435 - val_loss: 0.7034 - val_acc: 0.9788 - val_mDice: 0.7603

Epoch 00083: val_mDice did not improve from 0.76076
Epoch 84/300
 - 18s - loss: 0.7559 - acc: 0.9662 - mDice: 0.7444 - val_loss: 0.7038 - val_acc: 0.9792 - val_mDice: 0.7565

Epoch 00084: val_mDice did not improve from 0.76076
Epoch 85/300
 - 17s - loss: 0.7552 - acc: 0.9661 - mDice: 0.7448 - val_loss: 0.6991 - val_acc: 0.9789 - val_mDice: 0.7577

Epoch 00085: val_mDice did not improve from 0.76076
Epoch 86/300
 - 17s - loss: 0.7591 - acc: 0.9661 - mDice: 0.7432 - val_loss: 0.7001 - val_acc: 0.9778 - val_mDice: 0.7595

Epoch 00086: val_mDice did not improve from 0.76076
Epoch 87/300
 - 17s - loss: 0.7525 - acc: 0.9662 - mDice: 0.7453 - val_loss: 0.7030 - val_acc: 0.9781 - val_mDice: 0.7603

Epoch 00087: val_mDice did not improve from 0.76076
Epoch 88/300
 - 18s - loss: 0.7514 - acc: 0.9662 - mDice: 0.7453 - val_loss: 0.7017 - val_acc: 0.9788 - val_mDice: 0.7605

Epoch 00088: val_mDice did not improve from 0.76076
Epoch 89/300
 - 19s - loss: 0.7510 - acc: 0.9662 - mDice: 0.7459 - val_loss: 0.6942 - val_acc: 0.9784 - val_mDice: 0.7620

Epoch 00089: val_mDice improved from 0.76076 to 0.76196, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 90/300
 - 18s - loss: 0.7520 - acc: 0.9662 - mDice: 0.7457 - val_loss: 0.7062 - val_acc: 0.9787 - val_mDice: 0.7614

Epoch 00090: val_mDice did not improve from 0.76196
Epoch 91/300
 - 17s - loss: 0.7492 - acc: 0.9662 - mDice: 0.7463 - val_loss: 0.7041 - val_acc: 0.9785 - val_mDice: 0.7616

Epoch 00091: val_mDice did not improve from 0.76196
Epoch 92/300
 - 17s - loss: 0.7495 - acc: 0.9663 - mDice: 0.7464 - val_loss: 0.7091 - val_acc: 0.9782 - val_mDice: 0.7626

Epoch 00092: val_mDice improved from 0.76196 to 0.76257, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 93/300
 - 17s - loss: 0.7462 - acc: 0.9663 - mDice: 0.7472 - val_loss: 0.7052 - val_acc: 0.9785 - val_mDice: 0.7612

Epoch 00093: val_mDice did not improve from 0.76257
Epoch 94/300
 - 19s - loss: 0.7463 - acc: 0.9662 - mDice: 0.7473 - val_loss: 0.7096 - val_acc: 0.9785 - val_mDice: 0.7609

Epoch 00094: val_mDice did not improve from 0.76257
Epoch 95/300
 - 18s - loss: 0.7449 - acc: 0.9663 - mDice: 0.7478 - val_loss: 0.7017 - val_acc: 0.9788 - val_mDice: 0.7608

Epoch 00095: val_mDice did not improve from 0.76257
Epoch 96/300
 - 17s - loss: 0.7420 - acc: 0.9664 - mDice: 0.7488 - val_loss: 0.6854 - val_acc: 0.9792 - val_mDice: 0.7636

Epoch 00096: val_mDice improved from 0.76257 to 0.76360, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 97/300
 - 17s - loss: 0.7419 - acc: 0.9664 - mDice: 0.7486 - val_loss: 0.7047 - val_acc: 0.9785 - val_mDice: 0.7605

Epoch 00097: val_mDice did not improve from 0.76360
Epoch 98/300
 - 17s - loss: 0.7398 - acc: 0.9664 - mDice: 0.7492 - val_loss: 0.6984 - val_acc: 0.9790 - val_mDice: 0.7601

Epoch 00098: val_mDice did not improve from 0.76360
Epoch 99/300
 - 18s - loss: 0.7405 - acc: 0.9664 - mDice: 0.7490 - val_loss: 0.7012 - val_acc: 0.9787 - val_mDice: 0.7633

Epoch 00099: val_mDice did not improve from 0.76360
Epoch 100/300
 - 18s - loss: 0.7392 - acc: 0.9664 - mDice: 0.7494 - val_loss: 0.7028 - val_acc: 0.9788 - val_mDice: 0.7617

Epoch 00100: val_mDice did not improve from 0.76360
Epoch 101/300
 - 17s - loss: 0.7392 - acc: 0.9665 - mDice: 0.7495 - val_loss: 0.7067 - val_acc: 0.9791 - val_mDice: 0.7621

Epoch 00101: val_mDice did not improve from 0.76360
Epoch 102/300
 - 17s - loss: 0.7392 - acc: 0.9665 - mDice: 0.7497 - val_loss: 0.6897 - val_acc: 0.9783 - val_mDice: 0.7620

Epoch 00102: val_mDice did not improve from 0.76360
Epoch 103/300
 - 17s - loss: 0.7373 - acc: 0.9664 - mDice: 0.7501 - val_loss: 0.6908 - val_acc: 0.9788 - val_mDice: 0.7650

Epoch 00103: val_mDice improved from 0.76360 to 0.76504, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 104/300
 - 18s - loss: 0.7369 - acc: 0.9664 - mDice: 0.7502 - val_loss: 0.6997 - val_acc: 0.9793 - val_mDice: 0.7622

Epoch 00104: val_mDice did not improve from 0.76504
Epoch 105/300
 - 19s - loss: 0.7342 - acc: 0.9665 - mDice: 0.7509 - val_loss: 0.7066 - val_acc: 0.9788 - val_mDice: 0.7647

Epoch 00105: val_mDice did not improve from 0.76504
Epoch 106/300
 - 18s - loss: 0.7319 - acc: 0.9666 - mDice: 0.7519 - val_loss: 0.7068 - val_acc: 0.9780 - val_mDice: 0.7614

Epoch 00106: val_mDice did not improve from 0.76504
Epoch 107/300
 - 18s - loss: 0.7333 - acc: 0.9665 - mDice: 0.7514 - val_loss: 0.7059 - val_acc: 0.9786 - val_mDice: 0.7664

Epoch 00107: val_mDice improved from 0.76504 to 0.76640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 108/300
 - 18s - loss: 0.7330 - acc: 0.9665 - mDice: 0.7515 - val_loss: 0.6978 - val_acc: 0.9786 - val_mDice: 0.7665

Epoch 00108: val_mDice improved from 0.76640 to 0.76652, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 109/300
 - 18s - loss: 0.7333 - acc: 0.9665 - mDice: 0.7513 - val_loss: 0.6877 - val_acc: 0.9792 - val_mDice: 0.7621

Epoch 00109: val_mDice did not improve from 0.76652
Epoch 110/300
 - 18s - loss: 0.7316 - acc: 0.9666 - mDice: 0.7518 - val_loss: 0.7053 - val_acc: 0.9788 - val_mDice: 0.7635

Epoch 00110: val_mDice did not improve from 0.76652
Epoch 111/300
 - 19s - loss: 0.7287 - acc: 0.9666 - mDice: 0.7528 - val_loss: 0.6860 - val_acc: 0.9786 - val_mDice: 0.7635

Epoch 00111: val_mDice did not improve from 0.76652
Epoch 112/300
 - 17s - loss: 0.7303 - acc: 0.9666 - mDice: 0.7519 - val_loss: 0.7073 - val_acc: 0.9784 - val_mDice: 0.7629

Epoch 00112: val_mDice did not improve from 0.76652
Epoch 113/300
 - 17s - loss: 0.7290 - acc: 0.9666 - mDice: 0.7526 - val_loss: 0.6926 - val_acc: 0.9789 - val_mDice: 0.7667

Epoch 00113: val_mDice improved from 0.76652 to 0.76670, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 114/300
 - 17s - loss: 0.7286 - acc: 0.9666 - mDice: 0.7525 - val_loss: 0.6927 - val_acc: 0.9788 - val_mDice: 0.7636

Epoch 00114: val_mDice did not improve from 0.76670
Epoch 115/300
 - 17s - loss: 0.7262 - acc: 0.9667 - mDice: 0.7535 - val_loss: 0.7070 - val_acc: 0.9790 - val_mDice: 0.7584

Epoch 00115: val_mDice did not improve from 0.76670
Epoch 116/300
 - 18s - loss: 0.7270 - acc: 0.9667 - mDice: 0.7533 - val_loss: 0.6968 - val_acc: 0.9786 - val_mDice: 0.7617

Epoch 00116: val_mDice did not improve from 0.76670
Epoch 117/300
 - 19s - loss: 0.7259 - acc: 0.9667 - mDice: 0.7533 - val_loss: 0.6901 - val_acc: 0.9787 - val_mDice: 0.7631

Epoch 00117: val_mDice did not improve from 0.76670
Epoch 118/300
 - 17s - loss: 0.7235 - acc: 0.9667 - mDice: 0.7543 - val_loss: 0.6883 - val_acc: 0.9789 - val_mDice: 0.7668

Epoch 00118: val_mDice improved from 0.76670 to 0.76685, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 119/300
 - 17s - loss: 0.7227 - acc: 0.9667 - mDice: 0.7545 - val_loss: 0.6913 - val_acc: 0.9789 - val_mDice: 0.7612

Epoch 00119: val_mDice did not improve from 0.76685
Epoch 120/300
 - 17s - loss: 0.7212 - acc: 0.9668 - mDice: 0.7551 - val_loss: 0.7068 - val_acc: 0.9793 - val_mDice: 0.7634

Epoch 00120: val_mDice did not improve from 0.76685
Epoch 121/300
 - 17s - loss: 0.7216 - acc: 0.9668 - mDice: 0.7549 - val_loss: 0.6854 - val_acc: 0.9786 - val_mDice: 0.7656

Epoch 00121: val_mDice did not improve from 0.76685
Epoch 122/300
 - 18s - loss: 0.7211 - acc: 0.9668 - mDice: 0.7550 - val_loss: 0.6867 - val_acc: 0.9787 - val_mDice: 0.7681

Epoch 00122: val_mDice improved from 0.76685 to 0.76807, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 123/300
 - 18s - loss: 0.7202 - acc: 0.9668 - mDice: 0.7553 - val_loss: 0.6922 - val_acc: 0.9787 - val_mDice: 0.7623

Epoch 00123: val_mDice did not improve from 0.76807
Epoch 124/300
 - 17s - loss: 0.7201 - acc: 0.9668 - mDice: 0.7554 - val_loss: 0.6949 - val_acc: 0.9790 - val_mDice: 0.7656

Epoch 00124: val_mDice did not improve from 0.76807
Epoch 125/300
 - 18s - loss: 0.7199 - acc: 0.9668 - mDice: 0.7555 - val_loss: 0.6963 - val_acc: 0.9790 - val_mDice: 0.7671

Epoch 00125: val_mDice did not improve from 0.76807
Epoch 126/300
 - 17s - loss: 0.7188 - acc: 0.9668 - mDice: 0.7560 - val_loss: 0.6864 - val_acc: 0.9795 - val_mDice: 0.7693

Epoch 00126: val_mDice improved from 0.76807 to 0.76935, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 127/300
 - 18s - loss: 0.7165 - acc: 0.9668 - mDice: 0.7561 - val_loss: 0.7214 - val_acc: 0.9788 - val_mDice: 0.7610

Epoch 00127: val_mDice did not improve from 0.76935
Epoch 128/300
 - 19s - loss: 0.7181 - acc: 0.9668 - mDice: 0.7561 - val_loss: 0.6944 - val_acc: 0.9787 - val_mDice: 0.7664

Epoch 00128: val_mDice did not improve from 0.76935
Epoch 129/300
 - 17s - loss: 0.7139 - acc: 0.9669 - mDice: 0.7573 - val_loss: 0.6843 - val_acc: 0.9795 - val_mDice: 0.7661

Epoch 00129: val_mDice did not improve from 0.76935
Epoch 130/300
 - 17s - loss: 0.7159 - acc: 0.9668 - mDice: 0.7568 - val_loss: 0.7012 - val_acc: 0.9789 - val_mDice: 0.7661

Epoch 00130: val_mDice did not improve from 0.76935
Epoch 131/300
 - 17s - loss: 0.7137 - acc: 0.9669 - mDice: 0.7574 - val_loss: 0.6924 - val_acc: 0.9791 - val_mDice: 0.7697

Epoch 00131: val_mDice improved from 0.76935 to 0.76972, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 132/300
 - 18s - loss: 0.7143 - acc: 0.9669 - mDice: 0.7575 - val_loss: 0.6988 - val_acc: 0.9789 - val_mDice: 0.7657

Epoch 00132: val_mDice did not improve from 0.76972
Epoch 133/300
 - 19s - loss: 0.7144 - acc: 0.9669 - mDice: 0.7572 - val_loss: 0.6759 - val_acc: 0.9790 - val_mDice: 0.7681

Epoch 00133: val_mDice did not improve from 0.76972
Epoch 134/300
 - 17s - loss: 0.7145 - acc: 0.9669 - mDice: 0.7572 - val_loss: 0.6855 - val_acc: 0.9782 - val_mDice: 0.7710

Epoch 00134: val_mDice improved from 0.76972 to 0.77103, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 135/300
 - 17s - loss: 0.7132 - acc: 0.9669 - mDice: 0.7577 - val_loss: 0.6929 - val_acc: 0.9783 - val_mDice: 0.7724

Epoch 00135: val_mDice improved from 0.77103 to 0.77241, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 136/300
 - 18s - loss: 0.7128 - acc: 0.9669 - mDice: 0.7575 - val_loss: 0.6958 - val_acc: 0.9791 - val_mDice: 0.7708

Epoch 00136: val_mDice did not improve from 0.77241
Epoch 137/300
 - 19s - loss: 0.7108 - acc: 0.9670 - mDice: 0.7586 - val_loss: 0.6886 - val_acc: 0.9787 - val_mDice: 0.7714

Epoch 00137: val_mDice did not improve from 0.77241
Epoch 138/300
 - 17s - loss: 0.7125 - acc: 0.9669 - mDice: 0.7578 - val_loss: 0.7047 - val_acc: 0.9792 - val_mDice: 0.7657

Epoch 00138: val_mDice did not improve from 0.77241
Epoch 139/300
 - 17s - loss: 0.7104 - acc: 0.9670 - mDice: 0.7584 - val_loss: 0.6814 - val_acc: 0.9790 - val_mDice: 0.7723

Epoch 00139: val_mDice did not improve from 0.77241
Epoch 140/300
 - 17s - loss: 0.7118 - acc: 0.9670 - mDice: 0.7580 - val_loss: 0.6817 - val_acc: 0.9789 - val_mDice: 0.7706

Epoch 00140: val_mDice did not improve from 0.77241
Epoch 141/300
 - 17s - loss: 0.7083 - acc: 0.9670 - mDice: 0.7593 - val_loss: 0.6899 - val_acc: 0.9796 - val_mDice: 0.7685

Epoch 00141: val_mDice did not improve from 0.77241
Epoch 142/300
 - 19s - loss: 0.7102 - acc: 0.9670 - mDice: 0.7585 - val_loss: 0.6877 - val_acc: 0.9790 - val_mDice: 0.7681

Epoch 00142: val_mDice did not improve from 0.77241
Epoch 143/300
 - 18s - loss: 0.7100 - acc: 0.9670 - mDice: 0.7587 - val_loss: 0.6809 - val_acc: 0.9784 - val_mDice: 0.7693

Epoch 00143: val_mDice did not improve from 0.77241
Epoch 144/300
 - 18s - loss: 0.7057 - acc: 0.9671 - mDice: 0.7600 - val_loss: 0.6879 - val_acc: 0.9794 - val_mDice: 0.7670

Epoch 00144: val_mDice did not improve from 0.77241
Epoch 145/300
 - 19s - loss: 0.7079 - acc: 0.9670 - mDice: 0.7593 - val_loss: 0.6918 - val_acc: 0.9786 - val_mDice: 0.7698

Epoch 00145: val_mDice did not improve from 0.77241
Epoch 146/300
 - 18s - loss: 0.7081 - acc: 0.9670 - mDice: 0.7592 - val_loss: 0.6953 - val_acc: 0.9787 - val_mDice: 0.7702

Epoch 00146: val_mDice did not improve from 0.77241
Epoch 147/300
 - 18s - loss: 0.7051 - acc: 0.9671 - mDice: 0.7602 - val_loss: 0.6819 - val_acc: 0.9792 - val_mDice: 0.7710

Epoch 00147: val_mDice did not improve from 0.77241
Epoch 148/300
 - 18s - loss: 0.7049 - acc: 0.9671 - mDice: 0.7602 - val_loss: 0.6874 - val_acc: 0.9790 - val_mDice: 0.7694

Epoch 00148: val_mDice did not improve from 0.77241
Epoch 149/300
 - 19s - loss: 0.7064 - acc: 0.9671 - mDice: 0.7597 - val_loss: 0.6987 - val_acc: 0.9783 - val_mDice: 0.7684

Epoch 00149: val_mDice did not improve from 0.77241
Epoch 150/300
 - 18s - loss: 0.7045 - acc: 0.9671 - mDice: 0.7605 - val_loss: 0.6870 - val_acc: 0.9795 - val_mDice: 0.7674

Epoch 00150: val_mDice did not improve from 0.77241
Epoch 151/300
 - 18s - loss: 0.7057 - acc: 0.9671 - mDice: 0.7601 - val_loss: 0.6784 - val_acc: 0.9795 - val_mDice: 0.7683

Epoch 00151: val_mDice did not improve from 0.77241
Epoch 152/300
 - 18s - loss: 0.7044 - acc: 0.9671 - mDice: 0.7602 - val_loss: 0.6855 - val_acc: 0.9792 - val_mDice: 0.7719

Epoch 00152: val_mDice did not improve from 0.77241
Epoch 153/300
 - 18s - loss: 0.7048 - acc: 0.9672 - mDice: 0.7607 - val_loss: 0.6864 - val_acc: 0.9787 - val_mDice: 0.7682

Epoch 00153: val_mDice did not improve from 0.77241
Epoch 154/300
 - 19s - loss: 0.7025 - acc: 0.9671 - mDice: 0.7611 - val_loss: 0.6907 - val_acc: 0.9789 - val_mDice: 0.7697

Epoch 00154: val_mDice did not improve from 0.77241
Epoch 155/300
 - 18s - loss: 0.7027 - acc: 0.9671 - mDice: 0.7610 - val_loss: 0.6986 - val_acc: 0.9793 - val_mDice: 0.7688

Epoch 00155: val_mDice did not improve from 0.77241
Epoch 156/300
 - 18s - loss: 0.7006 - acc: 0.9672 - mDice: 0.7619 - val_loss: 0.6751 - val_acc: 0.9796 - val_mDice: 0.7707

Epoch 00156: val_mDice did not improve from 0.77241
Epoch 157/300
 - 18s - loss: 0.7010 - acc: 0.9673 - mDice: 0.7616 - val_loss: 0.6804 - val_acc: 0.9796 - val_mDice: 0.7724

Epoch 00157: val_mDice improved from 0.77241 to 0.77243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 158/300
 - 18s - loss: 0.7010 - acc: 0.9672 - mDice: 0.7616 - val_loss: 0.6882 - val_acc: 0.9790 - val_mDice: 0.7702

Epoch 00158: val_mDice did not improve from 0.77243
Epoch 159/300
 - 19s - loss: 0.7020 - acc: 0.9672 - mDice: 0.7615 - val_loss: 0.6877 - val_acc: 0.9792 - val_mDice: 0.7695

Epoch 00159: val_mDice did not improve from 0.77243
Epoch 160/300
 - 18s - loss: 0.6998 - acc: 0.9672 - mDice: 0.7621 - val_loss: 0.6934 - val_acc: 0.9791 - val_mDice: 0.7719

Epoch 00160: val_mDice did not improve from 0.77243
Epoch 161/300
 - 18s - loss: 0.6996 - acc: 0.9673 - mDice: 0.7621 - val_loss: 0.6804 - val_acc: 0.9789 - val_mDice: 0.7690

Epoch 00161: val_mDice did not improve from 0.77243
Epoch 162/300
 - 18s - loss: 0.7004 - acc: 0.9672 - mDice: 0.7617 - val_loss: 0.6981 - val_acc: 0.9792 - val_mDice: 0.7659

Epoch 00162: val_mDice did not improve from 0.77243
Epoch 163/300
 - 18s - loss: 0.6995 - acc: 0.9672 - mDice: 0.7622 - val_loss: 0.6964 - val_acc: 0.9794 - val_mDice: 0.7696

Epoch 00163: val_mDice did not improve from 0.77243
Epoch 164/300
 - 18s - loss: 0.6988 - acc: 0.9673 - mDice: 0.7624 - val_loss: 0.7013 - val_acc: 0.9792 - val_mDice: 0.7701

Epoch 00164: val_mDice did not improve from 0.77243
Epoch 165/300
 - 19s - loss: 0.6976 - acc: 0.9673 - mDice: 0.7630 - val_loss: 0.6859 - val_acc: 0.9788 - val_mDice: 0.7736

Epoch 00165: val_mDice improved from 0.77243 to 0.77357, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 166/300
 - 18s - loss: 0.6972 - acc: 0.9673 - mDice: 0.7628 - val_loss: 0.6936 - val_acc: 0.9787 - val_mDice: 0.7684

Epoch 00166: val_mDice did not improve from 0.77357
Epoch 167/300
 - 17s - loss: 0.6963 - acc: 0.9674 - mDice: 0.7631 - val_loss: 0.6914 - val_acc: 0.9791 - val_mDice: 0.7671

Epoch 00167: val_mDice did not improve from 0.77357
Epoch 168/300
 - 17s - loss: 0.6969 - acc: 0.9673 - mDice: 0.7631 - val_loss: 0.6862 - val_acc: 0.9792 - val_mDice: 0.7679

Epoch 00168: val_mDice did not improve from 0.77357
Epoch 169/300
 - 17s - loss: 0.6973 - acc: 0.9673 - mDice: 0.7629 - val_loss: 0.6854 - val_acc: 0.9797 - val_mDice: 0.7683

Epoch 00169: val_mDice did not improve from 0.77357
Epoch 170/300
 - 18s - loss: 0.6960 - acc: 0.9674 - mDice: 0.7631 - val_loss: 0.6847 - val_acc: 0.9793 - val_mDice: 0.7708

Epoch 00170: val_mDice did not improve from 0.77357
Epoch 171/300
 - 19s - loss: 0.6962 - acc: 0.9673 - mDice: 0.7632 - val_loss: 0.6835 - val_acc: 0.9794 - val_mDice: 0.7692

Epoch 00171: val_mDice did not improve from 0.77357
Epoch 172/300
 - 18s - loss: 0.6962 - acc: 0.9673 - mDice: 0.7630 - val_loss: 0.6977 - val_acc: 0.9790 - val_mDice: 0.7688

Epoch 00172: val_mDice did not improve from 0.77357
Epoch 173/300
 - 17s - loss: 0.6967 - acc: 0.9674 - mDice: 0.7631 - val_loss: 0.6783 - val_acc: 0.9791 - val_mDice: 0.7714

Epoch 00173: val_mDice did not improve from 0.77357
Epoch 174/300
 - 17s - loss: 0.6959 - acc: 0.9674 - mDice: 0.7634 - val_loss: 0.6820 - val_acc: 0.9787 - val_mDice: 0.7738

Epoch 00174: val_mDice improved from 0.77357 to 0.77378, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 175/300
 - 18s - loss: 0.6937 - acc: 0.9674 - mDice: 0.7641 - val_loss: 0.6900 - val_acc: 0.9791 - val_mDice: 0.7731

Epoch 00175: val_mDice did not improve from 0.77378
Epoch 176/300
 - 19s - loss: 0.6934 - acc: 0.9674 - mDice: 0.7641 - val_loss: 0.6790 - val_acc: 0.9787 - val_mDice: 0.7736

Epoch 00176: val_mDice did not improve from 0.77378
Epoch 177/300
 - 18s - loss: 0.6942 - acc: 0.9674 - mDice: 0.7636 - val_loss: 0.7027 - val_acc: 0.9789 - val_mDice: 0.7691

Epoch 00177: val_mDice did not improve from 0.77378
Epoch 178/300
 - 17s - loss: 0.6931 - acc: 0.9674 - mDice: 0.7645 - val_loss: 0.6853 - val_acc: 0.9792 - val_mDice: 0.7684

Epoch 00178: val_mDice did not improve from 0.77378
Epoch 179/300
 - 17s - loss: 0.6939 - acc: 0.9674 - mDice: 0.7638 - val_loss: 0.6826 - val_acc: 0.9794 - val_mDice: 0.7747

Epoch 00179: val_mDice improved from 0.77378 to 0.77466, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 180/300
 - 17s - loss: 0.6910 - acc: 0.9675 - mDice: 0.7648 - val_loss: 0.6780 - val_acc: 0.9794 - val_mDice: 0.7724

Epoch 00180: val_mDice did not improve from 0.77466
Epoch 181/300
 - 18s - loss: 0.6916 - acc: 0.9674 - mDice: 0.7648 - val_loss: 0.6859 - val_acc: 0.9796 - val_mDice: 0.7710

Epoch 00181: val_mDice did not improve from 0.77466
Epoch 182/300
 - 18s - loss: 0.6921 - acc: 0.9675 - mDice: 0.7644 - val_loss: 0.6916 - val_acc: 0.9789 - val_mDice: 0.7695

Epoch 00182: val_mDice did not improve from 0.77466
Epoch 183/300
 - 17s - loss: 0.6910 - acc: 0.9676 - mDice: 0.7649 - val_loss: 0.6852 - val_acc: 0.9790 - val_mDice: 0.7719

Epoch 00183: val_mDice did not improve from 0.77466
Epoch 184/300
 - 17s - loss: 0.6917 - acc: 0.9675 - mDice: 0.7648 - val_loss: 0.6814 - val_acc: 0.9790 - val_mDice: 0.7740

Epoch 00184: val_mDice did not improve from 0.77466
Epoch 185/300
 - 17s - loss: 0.6889 - acc: 0.9676 - mDice: 0.7658 - val_loss: 0.6855 - val_acc: 0.9795 - val_mDice: 0.7741

Epoch 00185: val_mDice did not improve from 0.77466
Epoch 186/300
 - 18s - loss: 0.6905 - acc: 0.9675 - mDice: 0.7651 - val_loss: 0.6863 - val_acc: 0.9792 - val_mDice: 0.7725

Epoch 00186: val_mDice did not improve from 0.77466
Epoch 187/300
 - 19s - loss: 0.6903 - acc: 0.9676 - mDice: 0.7651 - val_loss: 0.6933 - val_acc: 0.9796 - val_mDice: 0.7698

Epoch 00187: val_mDice did not improve from 0.77466
Epoch 188/300
 - 18s - loss: 0.6878 - acc: 0.9676 - mDice: 0.7662 - val_loss: 0.6952 - val_acc: 0.9792 - val_mDice: 0.7699

Epoch 00188: val_mDice did not improve from 0.77466
Epoch 189/300
 - 17s - loss: 0.6887 - acc: 0.9676 - mDice: 0.7655 - val_loss: 0.6844 - val_acc: 0.9790 - val_mDice: 0.7734

Epoch 00189: val_mDice did not improve from 0.77466
Epoch 190/300
 - 17s - loss: 0.6875 - acc: 0.9676 - mDice: 0.7662 - val_loss: 0.6958 - val_acc: 0.9792 - val_mDice: 0.7709

Epoch 00190: val_mDice did not improve from 0.77466
Epoch 191/300
 - 17s - loss: 0.6893 - acc: 0.9676 - mDice: 0.7657 - val_loss: 0.6767 - val_acc: 0.9798 - val_mDice: 0.7706

Epoch 00191: val_mDice did not improve from 0.77466
Epoch 192/300
 - 19s - loss: 0.6877 - acc: 0.9676 - mDice: 0.7662 - val_loss: 0.6891 - val_acc: 0.9791 - val_mDice: 0.7718

Epoch 00192: val_mDice did not improve from 0.77466
Epoch 193/300
 - 18s - loss: 0.6898 - acc: 0.9676 - mDice: 0.7653 - val_loss: 0.6994 - val_acc: 0.9788 - val_mDice: 0.7719

Epoch 00193: val_mDice did not improve from 0.77466
Epoch 194/300
 - 18s - loss: 0.6867 - acc: 0.9676 - mDice: 0.7662 - val_loss: 0.6879 - val_acc: 0.9792 - val_mDice: 0.7726

Epoch 00194: val_mDice did not improve from 0.77466
Epoch 195/300
 - 18s - loss: 0.6886 - acc: 0.9676 - mDice: 0.7658 - val_loss: 0.6838 - val_acc: 0.9789 - val_mDice: 0.7721

Epoch 00195: val_mDice did not improve from 0.77466
Epoch 196/300
 - 18s - loss: 0.6870 - acc: 0.9677 - mDice: 0.7662 - val_loss: 0.6777 - val_acc: 0.9794 - val_mDice: 0.7741

Epoch 00196: val_mDice did not improve from 0.77466
Epoch 197/300
 - 18s - loss: 0.6876 - acc: 0.9676 - mDice: 0.7661 - val_loss: 0.6909 - val_acc: 0.9796 - val_mDice: 0.7710

Epoch 00197: val_mDice did not improve from 0.77466
Epoch 198/300
 - 19s - loss: 0.6870 - acc: 0.9676 - mDice: 0.7661 - val_loss: 0.6901 - val_acc: 0.9793 - val_mDice: 0.7687

Epoch 00198: val_mDice did not improve from 0.77466
Epoch 199/300
 - 17s - loss: 0.6874 - acc: 0.9677 - mDice: 0.7661 - val_loss: 0.6827 - val_acc: 0.9793 - val_mDice: 0.7700

Epoch 00199: val_mDice did not improve from 0.77466
Epoch 200/300
 - 17s - loss: 0.6863 - acc: 0.9677 - mDice: 0.7665 - val_loss: 0.6861 - val_acc: 0.9793 - val_mDice: 0.7705

Epoch 00200: val_mDice did not improve from 0.77466
Epoch 201/300
 - 17s - loss: 0.6864 - acc: 0.9677 - mDice: 0.7665 - val_loss: 0.6861 - val_acc: 0.9791 - val_mDice: 0.7734

Epoch 00201: val_mDice did not improve from 0.77466
Epoch 202/300
 - 18s - loss: 0.6861 - acc: 0.9677 - mDice: 0.7665 - val_loss: 0.6780 - val_acc: 0.9794 - val_mDice: 0.7741

Epoch 00202: val_mDice did not improve from 0.77466
Epoch 203/300
 - 19s - loss: 0.6859 - acc: 0.9677 - mDice: 0.7668 - val_loss: 0.6864 - val_acc: 0.9791 - val_mDice: 0.7698

Epoch 00203: val_mDice did not improve from 0.77466
Epoch 204/300
 - 18s - loss: 0.6857 - acc: 0.9677 - mDice: 0.7668 - val_loss: 0.6815 - val_acc: 0.9791 - val_mDice: 0.7705

Epoch 00204: val_mDice did not improve from 0.77466
Epoch 205/300
 - 17s - loss: 0.6857 - acc: 0.9677 - mDice: 0.7667 - val_loss: 0.6840 - val_acc: 0.9792 - val_mDice: 0.7706

Epoch 00205: val_mDice did not improve from 0.77466
Epoch 206/300
 - 17s - loss: 0.6835 - acc: 0.9678 - mDice: 0.7675 - val_loss: 0.6801 - val_acc: 0.9791 - val_mDice: 0.7716

Epoch 00206: val_mDice did not improve from 0.77466
Epoch 207/300
 - 17s - loss: 0.6851 - acc: 0.9677 - mDice: 0.7670 - val_loss: 0.6826 - val_acc: 0.9794 - val_mDice: 0.7709

Epoch 00207: val_mDice did not improve from 0.77466
Epoch 208/300
 - 18s - loss: 0.6829 - acc: 0.9678 - mDice: 0.7677 - val_loss: 0.6834 - val_acc: 0.9795 - val_mDice: 0.7701

Epoch 00208: val_mDice did not improve from 0.77466
Epoch 209/300
 - 18s - loss: 0.6841 - acc: 0.9677 - mDice: 0.7673 - val_loss: 0.6910 - val_acc: 0.9794 - val_mDice: 0.7687

Epoch 00209: val_mDice did not improve from 0.77466
Restoring model weights from the end of the best epoch
Epoch 00209: early stopping
{'val_loss': [4.3078495917254935, 3.7218411491341787, 3.0072799545444853, 2.3567756806334406, 1.7800862315582902, 1.41088156585824, 1.2416241111820692, 1.1369069447256115, 1.033953418062158, 0.9736237534104961, 0.9317902840980111, 0.9202689512135231, 0.8916043451387589, 0.8784249469841996, 0.8679142582089934, 0.8362244657457691, 0.8284859649122578, 0.8222580470450936, 0.8259889340563996, 0.8198690361356082, 0.8195522190773323, 0.8007111990288512, 0.7869604189101964, 0.8189176708868106, 0.7836788370184702, 0.7902895293823661, 0.768366179645878, 0.7806254715135653, 0.8020198851415555, 0.7577526936792347, 0.7763547689130862, 0.7580470418276852, 0.7636254850315721, 0.7566443379611185, 0.7475961263049139, 0.7676519959756772, 0.754007148416075, 0.741335882307732, 0.7533531242037472, 0.7575614758550304, 0.7483404005227023, 0.7391146392854926, 0.7525429052032836, 0.7503619055225424, 0.7378199039256736, 0.7251049872130564, 0.7244524743458997, 0.7289471479311381, 0.7327187812491639, 0.7491508779460436, 0.7384807202097488, 0.718085967106362, 0.7293014836637941, 0.7295566666616152, 0.7212168010946822, 0.7201519975923512, 0.7148532695966224, 0.7243894983644354, 0.7150974155288853, 0.7135438780262046, 0.7234007773334032, 0.7078986200567794, 0.72111075340885, 0.7072499284189041, 0.7311437848496111, 0.7102026355593172, 0.7034265864385317, 0.729058858466475, 0.7112770950141019, 0.711200334846157, 0.7026130397842355, 0.7258344386538415, 0.7110370017894326, 0.7269812720279171, 0.7046136100814767, 0.6990620155040532, 0.7011036921853888, 0.70603726008167, 0.7029671954782042, 0.7113087961118515, 0.7019198038806654, 0.7076248355107765, 0.7033990944085056, 0.7038364708423615, 0.6991372802486159, 0.7001403631412819, 0.7029958436750385, 0.7017468042569618, 0.6941869369108383, 0.706161230394285, 0.7041206796691842, 0.7090927223636679, 0.7051931101165406, 0.7096349470419426, 0.7017012120109715, 0.6854435050324218, 0.704698140490545, 0.6984129993066396, 0.7012446632940476, 0.7028069528814864, 0.7067423495527816, 0.6897489779616055, 0.6908316044774774, 0.6997034839571339, 0.7066498480431022, 0.7068409450250129, 0.7058885089338642, 0.6978111381400122, 0.6877078724234071, 0.7052629022565606, 0.6860127930771814, 0.7072940921946748, 0.6926288118917648, 0.6926818379800613, 0.7069801655534196, 0.6967788154948248, 0.6901087291436653, 0.6882841317620996, 0.6912626224021389, 0.7067710581707628, 0.6853643631281918, 0.686720509235173, 0.6921642861545902, 0.6949263602903445, 0.6963387246001257, 0.6864416448220815, 0.7214197843042138, 0.694428318983888, 0.6842764738487871, 0.7012442942351511, 0.6924132790467511, 0.6988365311328679, 0.6759387583357014, 0.6854884902091876, 0.6928515438347647, 0.6957880505552031, 0.6885663903739354, 0.7046940457330991, 0.6813542348881291, 0.6817416842669657, 0.6898634362302415, 0.6877237150113876, 0.680912729811995, 0.6878978036446114, 0.6917529763424233, 0.6952989795436598, 0.6818697913868786, 0.6873892255841869, 0.6986762775133734, 0.6870137665369739, 0.678395002672117, 0.6854833039927156, 0.6863563960953935, 0.6906913388265322, 0.6986124446130779, 0.6751087795789927, 0.6804385050518872, 0.6882197840573037, 0.6876502739240046, 0.69340237954708, 0.6803627508143856, 0.6981373523196129, 0.6964499366610017, 0.701250728270779, 0.6858928003539778, 0.6935582691675997, 0.6913760684124411, 0.6861692446551911, 0.6854461831589268, 0.6847043637543508, 0.6835362400910626, 0.6977413909076011, 0.6782756357976835, 0.6820395315346652, 0.6899833946603618, 0.6789578423924643, 0.7027245327217938, 0.685277864132842, 0.6826247058502616, 0.6780183854168409, 0.6858505314751847, 0.6916268088229715, 0.6852214556850799, 0.6813511542261463, 0.6855495792545684, 0.6863121827171274, 0.6933314473661658, 0.6951666074256374, 0.6844384915208164, 0.6958337948746878, 0.6766901889892474, 0.6891367053740645, 0.6994384857889724, 0.6879086149473713, 0.6837659672106782, 0.6776681742439531, 0.6908610116129053, 0.6900539185902844, 0.6826848379553181, 0.6860663670383088, 0.6861485220389824, 0.6780354341823761, 0.6864347837559165, 0.6815032803848998, 0.6840444839980504, 0.6801227474049346, 0.6825583115832446, 0.6834324165566327, 0.6909967758884169], 'val_acc': [0.9134473159705123, 0.9134276713410469, 0.9153107139345718, 0.9335984759134789, 0.955120519415973, 0.9622548687947939, 0.9681147990161425, 0.9694885705431847, 0.9701565475496527, 0.9723522948892149, 0.9722707520609033, 0.9744923523027603, 0.9734523529059267, 0.9746663468341304, 0.9747992072203387, 0.9745003499396859, 0.974951738364076, 0.9759265511003259, 0.9754198495655844, 0.976098017741556, 0.9748883867916995, 0.9759196369615319, 0.9760885483598056, 0.9764693077296427, 0.9765104386087966, 0.9761467972030379, 0.9769279658794403, 0.9764776789162257, 0.9765883516775419, 0.9767801900432534, 0.9768671850635581, 0.9756986915248714, 0.97681331022145, 0.9773709647459526, 0.9772071715903609, 0.9770637481179956, 0.977590847505282, 0.9761759136637597, 0.9776632887859867, 0.9756914038364202, 0.9779111742973328, 0.9775831813681616, 0.9778751367575502, 0.976589071832291, 0.9775762884584192, 0.9776163195094018, 0.9775431654224657, 0.978036052151902, 0.9766859036602386, 0.9783771278923505, 0.9780061824681008, 0.9779246514790678, 0.9770054923344965, 0.9780560560422401, 0.977854749519531, 0.9788015615450193, 0.97803128786283, 0.9783949533553973, 0.9781328574435352, 0.9789020121097565, 0.9781914745291619, 0.97803274940138, 0.9782624469227987, 0.9784284430007412, 0.9781372224631375, 0.9786151827198185, 0.978443738124142, 0.977958130918137, 0.9784168009072134, 0.9786606802515787, 0.9778023353994709, 0.9782941218924849, 0.9789275086089356, 0.9784324540667337, 0.9787171172769102, 0.9786348485783355, 0.9785591172028895, 0.9790112257820286, 0.9785904439344798, 0.9784914132666914, 0.9787203808353372, 0.9778554774310491, 0.9787706128538471, 0.979159026113275, 0.9789355021633513, 0.9778311019890928, 0.9780993735137051, 0.9787997382960908, 0.9784411849224404, 0.9786970884832618, 0.9784706720750626, 0.9782304253480206, 0.9784688476013811, 0.97846266749787, 0.9788092011458254, 0.97915284437676, 0.9784568360407059, 0.9789671995868422, 0.9787065594980161, 0.9787793579983385, 0.9791022477901146, 0.9783283374080919, 0.9788099286490923, 0.9792667802882521, 0.9787866281319971, 0.9779908975509748, 0.9785503810399199, 0.9785740489829077, 0.9791779526292461, 0.9788357848173952, 0.9786392119649339, 0.9784277101902112, 0.9789151365626348, 0.9788241345588475, 0.9790050342474899, 0.9785696819220504, 0.978696002943875, 0.9788634446385789, 0.9788958320062454, 0.9792543951779196, 0.9785718611658436, 0.9786930757842652, 0.9787363965217382, 0.9789726489210782, 0.9789639082673478, 0.9794993788412173, 0.9787633459861964, 0.9786683223018907, 0.9794880784537694, 0.9789347746600844, 0.9790945914510178, 0.9788576168556736, 0.9790206923060221, 0.9781812723368815, 0.9782555282932438, 0.9790614822955981, 0.9787422259376474, 0.9792351179743466, 0.9790494601203971, 0.9789136929871285, 0.9796191323293398, 0.9790043177669996, 0.9784291615224865, 0.9793861687999882, 0.9786119142623797, 0.9787171136026513, 0.9791659402520689, 0.9789722778209268, 0.9782795730518968, 0.9794789903784451, 0.9795066624471586, 0.9791502964823213, 0.9787134585315234, 0.9788772676089038, 0.9793129857272318, 0.9795940387738894, 0.9795918325855307, 0.9789591860281278, 0.9792449376354478, 0.979082218996466, 0.9789428008745794, 0.9791823172405975, 0.9793603301864781, 0.9792107257940997, 0.9788492464855926, 0.9787185698339383, 0.9790782156872423, 0.9791557360185336, 0.9796963040142843, 0.9793410354281125, 0.9793956279754639, 0.9790436115166913, 0.9791379097389848, 0.9786919845293646, 0.9791288102326328, 0.9786552215275699, 0.9788630817034473, 0.979212172227363, 0.979401458207875, 0.9793723241923606, 0.9795554325188676, 0.9789340418495543, 0.9789970378353171, 0.9790436127414442, 0.9795365092689043, 0.9792005133955446, 0.9795539746545765, 0.9791732185507473, 0.9790265262126923, 0.9791965072285639, 0.9797745681788823, 0.9791200797851771, 0.9788022861905294, 0.9791739395219986, 0.9788925508930258, 0.9794345967573662, 0.979561254178008, 0.9792605907949683, 0.97929589103346, 0.9792653154836942, 0.9790840495939124, 0.9793905281040767, 0.9791488194302337, 0.9791171616070891, 0.9792249145573133, 0.9790567241302909, 0.979445511755878, 0.9794975527345318, 0.9794338594560754], 'val_mDice': [0.03722249415435203, 0.047808273835745575, 0.11109580099582672, 0.20678834423218687, 0.374459686752868, 0.48478492135054446, 0.5584114251071459, 0.5981697783078233, 0.6287982055585678, 0.651762359354594, 0.6707422961927441, 0.6755040888100454, 0.6846578043617614, 0.691730942628155, 0.6994277377651162, 0.7090586595339318, 0.7153647125583805, 0.7129331831246206, 0.7143808815577258, 0.715780109167099, 0.7201263831086355, 0.7301983425061996, 0.7286975963474953, 0.7196986030225885, 0.7302244652623999, 0.7294598103386082, 0.7348235796575677, 0.727751472224928, 0.7266998838071954, 0.734338788545295, 0.7363716265926622, 0.7415097306852472, 0.737219222604412, 0.7385217028937928, 0.7429775763047884, 0.7409823034724144, 0.7466715962102969, 0.748462927259811, 0.7447980536173467, 0.7458558997062787, 0.7456754490937272, 0.7444127487809691, 0.741607419432026, 0.7432990388510978, 0.7450890953410162, 0.7508586481825946, 0.7501080648539817, 0.749614470217326, 0.7500196087033781, 0.7483478772313628, 0.7482627928256989, 0.7510389248802237, 0.7519297060901171, 0.7515132574185933, 0.7508748492149457, 0.7482190246451391, 0.7581410236554603, 0.7517167052177534, 0.755801892035628, 0.7529361199026239, 0.7519291704648161, 0.7551100719464968, 0.7557191371101223, 0.7589458472108188, 0.751579566769404, 0.7582911244810444, 0.7573822838802861, 0.752182420394192, 0.759245466696073, 0.7538093369301051, 0.7586541306482603, 0.7519122884698111, 0.7570064663887024, 0.7558571622796255, 0.7572522890077878, 0.7607630344286357, 0.7605463405178018, 0.7559168632716349, 0.7591666207738119, 0.7533747402772512, 0.7591931481067449, 0.7591021575339852, 0.7603116419217358, 0.7564711321706641, 0.7576782042033052, 0.7595142011773096, 0.7602676953354927, 0.760540295545369, 0.7619595515401396, 0.7613578810267252, 0.7616073015617998, 0.7625664917573537, 0.7612407681060164, 0.7609231863936333, 0.7608273478403483, 0.7635967951931365, 0.7604504750199514, 0.760084915242783, 0.763345528138827, 0.7616925884599555, 0.7621052820388585, 0.7620228663699268, 0.7650423102999386, 0.7621868758985441, 0.7647041262012638, 0.7614026906555647, 0.7663987409578611, 0.7665184838314579, 0.7621447701160222, 0.7634652799939456, 0.763496120090354, 0.762883239821212, 0.7666954120544538, 0.7635655656252822, 0.7584184840933917, 0.7616513889946349, 0.7631205708196719, 0.7668484224848551, 0.7612131119590916, 0.7634267786594287, 0.7656408262579408, 0.7680705471398079, 0.7623259094479966, 0.7655553829996553, 0.7671375184842985, 0.7693466793184411, 0.7609858076049857, 0.7664166243925487, 0.7661174331625847, 0.766124814340513, 0.7697188437801518, 0.7656687812445915, 0.7680771567233621, 0.7710277597381644, 0.7724074611108597, 0.7707785176087733, 0.7714348508887094, 0.7656889892604253, 0.7723325801222292, 0.7705913884182499, 0.7685464383804634, 0.7680563138772364, 0.7693303211094582, 0.7670378860545485, 0.7698167280791557, 0.770249020971664, 0.7710212416028324, 0.7693660304154435, 0.7684345371919136, 0.7674034560379916, 0.768286598871832, 0.7718610588001878, 0.7682119416047449, 0.7697085819832267, 0.768823406059448, 0.7707088589668274, 0.7724348865959743, 0.7702384015468702, 0.7695002147596176, 0.7718808022263932, 0.7689620525869605, 0.7658930449453119, 0.7696056974260774, 0.7700736649232368, 0.7735661096768837, 0.7684121597303103, 0.7670658422659521, 0.767913965737983, 0.7683319718870398, 0.770767532391091, 0.7691923061462298, 0.7687887761690845, 0.7714131931736045, 0.773780205886658, 0.7730721345503037, 0.7736295824998045, 0.7690762256922787, 0.7684035819687255, 0.7746616332498315, 0.7723742339709033, 0.7709661824245976, 0.7695479919649151, 0.7718623570383412, 0.7740358363275659, 0.7740883892529631, 0.7725435507624117, 0.7697933361138383, 0.7698561610424355, 0.7734465962403441, 0.770887118496307, 0.7705778712279177, 0.7718474342398447, 0.7718937772594087, 0.7725605270633958, 0.7721488263509045, 0.7741206896631685, 0.7709535278686105, 0.7686831448992638, 0.7700262220755015, 0.7704507761622128, 0.7734073910811176, 0.7741165667364042, 0.7697785709002246, 0.7705103840729962, 0.7705836700250025, 0.7715788683662675, 0.7709034199584021, 0.770062117543939, 0.7687279406475694], 'loss': [35.51690156883007, 5.3021033967315185, 4.161663432823123, 3.4626008683879426, 2.822642161683109, 2.3873772360777656, 2.097933727690407, 1.9062370675156746, 1.752214206986338, 1.6368520137194962, 1.5451369460413062, 1.4735448876256407, 1.4141117811541977, 1.3570088151449342, 1.3109283405956897, 1.2681357452677406, 1.230735064644464, 1.2010912499810893, 1.1723743950500793, 1.1440669590435555, 1.1210758005764359, 1.0993739577312172, 1.0786258608405863, 1.0632792518301937, 1.0450846265698577, 1.0227035452925537, 1.0119946886300524, 0.9995633540722173, 0.9876294001262558, 0.9710958062872794, 0.9629541710575688, 0.9537520175299258, 0.9443910264932677, 0.9356240267276511, 0.9270226233459058, 0.9201551318093951, 0.9104997971611279, 0.9075865419757033, 0.89763648781215, 0.8927441306505222, 0.8863843247094059, 0.8776063894922748, 0.8757523104661206, 0.8691919670201961, 0.861847615737824, 0.8574456502511775, 0.8502064961554935, 0.8483973929316597, 0.8421713068454351, 0.8374006553778414, 0.8342304998946205, 0.8293397524318463, 0.8273597408908897, 0.8212937855942863, 0.8183680592436314, 0.8161429061927682, 0.8105750480191243, 0.808490077892472, 0.8073727199083766, 0.8048492454222538, 0.7997356952816683, 0.797729296653298, 0.7965777167998611, 0.7934687643951667, 0.7909459486956355, 0.7864446519320198, 0.7879954673343753, 0.7844315153057877, 0.7819942943653064, 0.7797103534607067, 0.7801877873591029, 0.7759456578389875, 0.7781309561460207, 0.7719445022076111, 0.7683938062488617, 0.7706551951203577, 0.7681934956002106, 0.7647192984987462, 0.763750305428115, 0.7639688714678715, 0.7611683848059443, 0.7600858754620816, 0.7582327404624439, 0.7559062909437056, 0.7551950804641077, 0.759084290251961, 0.7524626302031978, 0.7514274403243888, 0.7510338513266523, 0.751990448909183, 0.7492076891293853, 0.7495026420246199, 0.7461675864279118, 0.7463344125536483, 0.744894562074837, 0.742049229517633, 0.7419324493162879, 0.739822722907882, 0.7404738483736214, 0.739217351681296, 0.7391586531368565, 0.7391693831219766, 0.7372523030362489, 0.7368642860917886, 0.7341886192274367, 0.7318909366212182, 0.7332862408481522, 0.7329584085037578, 0.7333240987286579, 0.7315674144648099, 0.7286651914315291, 0.7302881684600431, 0.7290434911527532, 0.7285850723784622, 0.7262412691493799, 0.727037384459022, 0.7258848535195277, 0.7234861183909946, 0.7227017946548132, 0.7212026059437938, 0.7215653748645904, 0.7211425295091087, 0.7202232760261918, 0.7200621251135173, 0.7199157134924368, 0.718804711495269, 0.7164881860996144, 0.7181132749592726, 0.7139034990293617, 0.7159307338575656, 0.7137493793174636, 0.7142680552047249, 0.7144213737642985, 0.7144796237456517, 0.7131777004802868, 0.7128279499878015, 0.7107806343866593, 0.7125030758524638, 0.7104115877974676, 0.7117652420357248, 0.7082782521785739, 0.7102411102401739, 0.7100110788667993, 0.7056918570427005, 0.7079319211955679, 0.7081296688157671, 0.7051077301780645, 0.7049329678887856, 0.7064073147905999, 0.7045369991133418, 0.7056920617262974, 0.7043672393339733, 0.7048412021171433, 0.7025143778366068, 0.7027053120575891, 0.7005994240716771, 0.7009601675864846, 0.7009898383012191, 0.7020262263514541, 0.6998150795762801, 0.6995999700069726, 0.7004000598338433, 0.6994816060468164, 0.6988089198209883, 0.6975540407560343, 0.6971676620530923, 0.6963029342163867, 0.6969211066184007, 0.6972879880241293, 0.6959596255528357, 0.696153814980562, 0.6962050446542328, 0.6966556822473994, 0.6958567319079947, 0.6936925552777138, 0.6933586392464584, 0.6941555564682411, 0.6931025233203931, 0.6939302240617534, 0.6910430020473395, 0.6915674248878235, 0.6921332446320831, 0.6909564288588994, 0.6917027485167252, 0.6888519178741472, 0.6904547648235335, 0.6903471263413073, 0.6878231667519719, 0.6887159364497186, 0.687461099542361, 0.6892531543152839, 0.6876954147816164, 0.6897514843269683, 0.6866915897206401, 0.6885508420817955, 0.6869905188265315, 0.6876047130070329, 0.6869675025485373, 0.6874340146591855, 0.6862956767091062, 0.6863631996466594, 0.686142327693682, 0.6858917854899033, 0.6857246654631977, 0.6857255386846015, 0.683503171725011, 0.6851220040819698, 0.6829314545163925, 0.6840887215374303], 'acc': [0.7450849799216411, 0.8945455904311229, 0.9015074547322217, 0.9102565979944941, 0.9221621163150271, 0.9320478373576063, 0.9381291390763771, 0.9424385006867051, 0.9460851305926862, 0.9487429696816445, 0.9506890854207434, 0.9521634814980415, 0.9533570348753965, 0.9544865165516764, 0.955418564801136, 0.9562704336632303, 0.9570388994284024, 0.9577765581555996, 0.9583259228022852, 0.9589330273896242, 0.9592231813382112, 0.9597628347425209, 0.9601493476212666, 0.9604943798354657, 0.9607347176186326, 0.9610805977993203, 0.9612874937285009, 0.9614986984682311, 0.9616650347824791, 0.9621633786547068, 0.96224681856753, 0.9624249873072149, 0.9626793244975613, 0.9628036518522995, 0.9629907462273892, 0.9630689208918557, 0.9632510964230081, 0.963380694952383, 0.9635501440401347, 0.9636299367772337, 0.9637576288111532, 0.9640271680815501, 0.9643743594064271, 0.964581020918632, 0.9644812085644712, 0.9643997989382719, 0.9645700908246124, 0.9645546508631687, 0.9646753533310853, 0.9647263977187381, 0.9646968074066449, 0.9648819548508375, 0.9648508307813041, 0.9649878008416535, 0.965048436682117, 0.9651295271165239, 0.9652297391886424, 0.9653133231879165, 0.9653384443064575, 0.9653117647084382, 0.9654613495263727, 0.9655431619931016, 0.9655369846836573, 0.9656277396942174, 0.9656293474425407, 0.9657331222344309, 0.9656670614233177, 0.9657269164426475, 0.9658349235865965, 0.9658996752615038, 0.9658188083768322, 0.9659388044511412, 0.965901828155764, 0.9659909001688962, 0.9661234302448132, 0.966013380898482, 0.9660469069872266, 0.9661048940707013, 0.9661108998656187, 0.9660064449821175, 0.9660789732663245, 0.966088861087712, 0.9661133252940665, 0.9661647578860899, 0.966148735497439, 0.9660593512414581, 0.9661825149138236, 0.9662011328263596, 0.9661840928641335, 0.9661963566469108, 0.9661816129414879, 0.9662508345022565, 0.9662566023726906, 0.9662468151257029, 0.966277112991955, 0.9664200588337594, 0.9663662993710864, 0.9664173536866226, 0.9664080404874485, 0.9664265847681827, 0.9664614720024874, 0.9664752414338106, 0.9664412649822635, 0.9664459078333407, 0.9665003732156338, 0.9665560370250187, 0.9665495204094766, 0.9665039475696439, 0.9665451152013625, 0.9665701676261896, 0.9666207899782558, 0.9665812837100269, 0.9666239558297403, 0.9666136387164487, 0.9666642818234215, 0.9666744337563825, 0.9666528689745817, 0.9667285733685114, 0.9667467357618355, 0.9667627423020684, 0.9667831410746487, 0.9667526796942039, 0.9667974432276912, 0.9668274044680293, 0.9667621064693981, 0.9668165944709486, 0.9668275740492619, 0.9668295732739058, 0.9669102357751725, 0.9668482926398727, 0.9669478275591026, 0.9669205428399295, 0.9669446315648866, 0.9668640166376004, 0.9668664967297325, 0.9669400713772273, 0.9669843304961024, 0.966911743948779, 0.9670012575659995, 0.9669862593034416, 0.9669876310835575, 0.9669862475686605, 0.9669635149592508, 0.9670834585042556, 0.9670125286581085, 0.967023059459323, 0.967067226027114, 0.9670581286042804, 0.9670628036979975, 0.967109523811618, 0.9671173511404515, 0.9671069090955486, 0.9671758833813537, 0.9671428195682972, 0.9671272776621992, 0.967196003549834, 0.9672653685133625, 0.967175606199216, 0.9672062702825647, 0.9672076351711897, 0.9672657467151609, 0.967188710235413, 0.9672313822373476, 0.9672711667534679, 0.9673272828991668, 0.9673289006931522, 0.9673614619243484, 0.9673336030799307, 0.9673382237254122, 0.9673598917014709, 0.9673465136861427, 0.9673128043595806, 0.9673544550592384, 0.967369818935636, 0.9673976174687368, 0.9674189580602011, 0.9674357932253604, 0.9674444719506284, 0.9674369924970119, 0.9674801351857579, 0.9674454544462567, 0.9675050931894048, 0.9675569260078452, 0.9675256642406375, 0.9675638614732034, 0.9675221135228035, 0.9675506972923033, 0.9675909329181155, 0.9675853169707254, 0.9676001223491022, 0.9676342753509343, 0.9676329377496252, 0.9675615958704639, 0.967646737628181, 0.9676077107681332, 0.9676728654932738, 0.9676472095760864, 0.9676328258282524, 0.9676621355799699, 0.9676840725568548, 0.9676709289096672, 0.9676876177362169, 0.9676678446673377, 0.9676829962777749, 0.9677488764479474, 0.9677871835249592, 0.9677037032628738, 0.9677801019967657, 0.9677363646002274], 'mDice': [0.02185057803142461, 0.04262071338594549, 0.07715498448988117, 0.13450881118481173, 0.2288829662913707, 0.32039691438800866, 0.3841478138037421, 0.4311958915931965, 0.4693806410737828, 0.498364875477948, 0.521651031341982, 0.5401228580757982, 0.5551434932998809, 0.5693226901328594, 0.582246606077273, 0.5933184200806516, 0.6027815663628971, 0.6112479528890609, 0.6186205092220572, 0.6269366505134651, 0.6337129264126296, 0.6398258921818433, 0.6461765262970173, 0.6503896352681627, 0.6560587368388826, 0.6623782448190868, 0.6661103581932919, 0.6695437886248532, 0.6728630666733018, 0.6776001462879407, 0.680226761252916, 0.6827290082840789, 0.6852428138505994, 0.6879824984705047, 0.6906321915903864, 0.6927111068974519, 0.6957377808755539, 0.696701932831887, 0.6997305174478078, 0.70114469447764, 0.7028347440234517, 0.7060669006465504, 0.706526960586719, 0.7087699711434818, 0.71075664003692, 0.7125890348048272, 0.7149210753880487, 0.7152557753242293, 0.7170884950953028, 0.7185968457603978, 0.7195914956660286, 0.7215157888603128, 0.7218637374620724, 0.7237040307632244, 0.7246561245468072, 0.7255604714306011, 0.7270261475227385, 0.7279031132955402, 0.7280478413791345, 0.7288141797792999, 0.73070391666298, 0.7312378851733924, 0.7313332437888658, 0.7325346125821687, 0.7333214740631603, 0.7344030760168667, 0.7340846076385792, 0.7352708883989667, 0.7360799358783788, 0.7368726151794679, 0.7364308146104633, 0.7381683833467293, 0.7374053149144272, 0.7391952182035617, 0.740479510131184, 0.7398936452334275, 0.7402256257540117, 0.7415217052247646, 0.7417368718190759, 0.7416484708314666, 0.742428820008766, 0.742818906175768, 0.7434804374565303, 0.7443644441985596, 0.7447610526172458, 0.7432355093113733, 0.7453061807551461, 0.7453404877368407, 0.7459341061937418, 0.7457333572962593, 0.7462778545481951, 0.7464339106857039, 0.7471641634589442, 0.7472925330680321, 0.747818645600658, 0.7487955991230998, 0.7485776616036791, 0.7492498094630279, 0.748953944186405, 0.749427457781687, 0.7494639531837222, 0.7496802477131316, 0.7500952469009314, 0.7502124902914676, 0.7509251810153184, 0.7518770576551785, 0.7513852256298915, 0.7514936099414843, 0.7512954954520967, 0.7517978679048531, 0.7527898953957088, 0.7519427901768122, 0.7525563862210118, 0.7525364583995269, 0.7535166870606044, 0.7532945454962323, 0.7533476894186447, 0.7543475799999718, 0.7545353205923666, 0.7551150119266163, 0.7549047799356082, 0.7549727064476536, 0.7553078758813679, 0.7554313833497853, 0.755525141010756, 0.7559902497019054, 0.7560573301979924, 0.7561217061817987, 0.7572613579375789, 0.7568128672563758, 0.7574325006314603, 0.7575341806830461, 0.7571869401582678, 0.7571870695884512, 0.7577475409368944, 0.7574628696245621, 0.7585920606833135, 0.7577827768105416, 0.7583944602692648, 0.7580212447676488, 0.7593310931928646, 0.7585194448119641, 0.7586930239767653, 0.7599977692573454, 0.759267786105424, 0.759155638241252, 0.7601641386214484, 0.7602415118046718, 0.7597036081840632, 0.760516779730938, 0.7600757110503227, 0.7602327543429004, 0.7606921688069079, 0.7611023661653543, 0.761031994018631, 0.7618767982839395, 0.7616102348448793, 0.7616222572243928, 0.76151999015223, 0.7620629790151106, 0.7621160099733777, 0.7617223231934589, 0.7621621753894954, 0.762356832099941, 0.7629705926953606, 0.7627946152400497, 0.7630838065114307, 0.7630567220763889, 0.7628619357375468, 0.7631439734375088, 0.7632250174039059, 0.7629819019621347, 0.7631188930186934, 0.7634230349470525, 0.7640686079940886, 0.7641204317866581, 0.7636135652660605, 0.7644525475258184, 0.7638371493984254, 0.7648337432761222, 0.7648117586103507, 0.7643972876830425, 0.7648921090669819, 0.7648133840197351, 0.7657895837957828, 0.7650561551225232, 0.7650759899304271, 0.7661920496857743, 0.7655155898645772, 0.7662309849771846, 0.7657110598082829, 0.7661608792516466, 0.7652678082745256, 0.7662044374751691, 0.7657728472286388, 0.7662101179911456, 0.7660902317842637, 0.7661456629950614, 0.7661274136551961, 0.7664850628446674, 0.7664927255849366, 0.7664806367667406, 0.7667657677894834, 0.7668348295389069, 0.7667309617851434, 0.7674981710956644, 0.766974920284042, 0.7676718413982054, 0.7673202166959943]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:01<00:26,  1.89s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:23,  1.81s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:21,  1.82s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:20,  1.84s/it]predicting test subjects:  33%|███▎      | 5/15 [00:09<00:19,  1.95s/it]predicting test subjects:  40%|████      | 6/15 [00:11<00:18,  2.02s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:14,  1.83s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:15<00:13,  1.99s/it]predicting test subjects:  60%|██████    | 9/15 [00:17<00:11,  1.95s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:18<00:09,  1.81s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:20<00:07,  1.77s/it]predicting test subjects:  80%|████████  | 12/15 [00:22<00:05,  1.83s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:24<00:03,  1.89s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:26<00:01,  1.84s/it]predicting test subjects: 100%|██████████| 15/15 [00:28<00:00,  1.86s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<19:39,  2.22s/it]predicting train subjects:   0%|          | 2/532 [00:03<17:51,  2.02s/it]predicting train subjects:   1%|          | 3/532 [00:05<16:56,  1.92s/it]predicting train subjects:   1%|          | 4/532 [00:07<16:20,  1.86s/it]predicting train subjects:   1%|          | 5/532 [00:08<15:58,  1.82s/it]predicting train subjects:   1%|          | 6/532 [00:10<15:09,  1.73s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<15:05,  1.73s/it]predicting train subjects:   2%|▏         | 8/532 [00:13<14:31,  1.66s/it]predicting train subjects:   2%|▏         | 9/532 [00:15<15:08,  1.74s/it]predicting train subjects:   2%|▏         | 10/532 [00:17<14:47,  1.70s/it]predicting train subjects:   2%|▏         | 11/532 [00:18<14:07,  1.63s/it]predicting train subjects:   2%|▏         | 12/532 [00:20<15:31,  1.79s/it]predicting train subjects:   2%|▏         | 13/532 [00:22<14:30,  1.68s/it]predicting train subjects:   3%|▎         | 14/532 [00:23<13:47,  1.60s/it]predicting train subjects:   3%|▎         | 15/532 [00:25<13:46,  1.60s/it]predicting train subjects:   3%|▎         | 16/532 [00:27<14:20,  1.67s/it]predicting train subjects:   3%|▎         | 17/532 [00:28<13:57,  1.63s/it]predicting train subjects:   3%|▎         | 18/532 [00:30<14:44,  1.72s/it]predicting train subjects:   4%|▎         | 19/532 [00:31<13:51,  1.62s/it]predicting train subjects:   4%|▍         | 20/532 [00:33<14:05,  1.65s/it]predicting train subjects:   4%|▍         | 21/532 [00:35<15:04,  1.77s/it]predicting train subjects:   4%|▍         | 22/532 [00:37<14:31,  1.71s/it]predicting train subjects:   4%|▍         | 23/532 [00:39<14:36,  1.72s/it]predicting train subjects:   5%|▍         | 24/532 [00:40<13:45,  1.63s/it]predicting train subjects:   5%|▍         | 25/532 [00:42<15:05,  1.79s/it]predicting train subjects:   5%|▍         | 26/532 [00:44<14:36,  1.73s/it]predicting train subjects:   5%|▌         | 27/532 [00:46<16:06,  1.91s/it]predicting train subjects:   5%|▌         | 28/532 [00:48<15:37,  1.86s/it]predicting train subjects:   5%|▌         | 29/532 [00:50<16:13,  1.94s/it]predicting train subjects:   6%|▌         | 30/532 [00:51<15:05,  1.80s/it]predicting train subjects:   6%|▌         | 31/532 [00:53<14:41,  1.76s/it]predicting train subjects:   6%|▌         | 32/532 [00:55<14:39,  1.76s/it]predicting train subjects:   6%|▌         | 33/532 [00:56<13:58,  1.68s/it]predicting train subjects:   6%|▋         | 34/532 [00:58<15:09,  1.83s/it]predicting train subjects:   7%|▋         | 35/532 [01:00<14:45,  1.78s/it]predicting train subjects:   7%|▋         | 36/532 [01:02<15:06,  1.83s/it]predicting train subjects:   7%|▋         | 37/532 [01:04<14:59,  1.82s/it]predicting train subjects:   7%|▋         | 38/532 [01:06<15:24,  1.87s/it]predicting train subjects:   7%|▋         | 39/532 [01:08<14:57,  1.82s/it]predicting train subjects:   8%|▊         | 40/532 [01:09<14:19,  1.75s/it]predicting train subjects:   8%|▊         | 41/532 [01:11<14:48,  1.81s/it]predicting train subjects:   8%|▊         | 42/532 [01:13<14:56,  1.83s/it]predicting train subjects:   8%|▊         | 43/532 [01:14<14:00,  1.72s/it]predicting train subjects:   8%|▊         | 44/532 [01:16<13:12,  1.62s/it]predicting train subjects:   8%|▊         | 45/532 [01:17<13:09,  1.62s/it]predicting train subjects:   9%|▊         | 46/532 [01:19<13:29,  1.67s/it]predicting train subjects:   9%|▉         | 47/532 [01:21<14:34,  1.80s/it]predicting train subjects:   9%|▉         | 48/532 [01:23<14:45,  1.83s/it]predicting train subjects:   9%|▉         | 49/532 [01:25<14:20,  1.78s/it]predicting train subjects:   9%|▉         | 50/532 [01:27<15:05,  1.88s/it]predicting train subjects:  10%|▉         | 51/532 [01:29<14:36,  1.82s/it]predicting train subjects:  10%|▉         | 52/532 [01:31<14:38,  1.83s/it]predicting train subjects:  10%|▉         | 53/532 [01:32<14:02,  1.76s/it]predicting train subjects:  10%|█         | 54/532 [01:34<14:37,  1.84s/it]predicting train subjects:  10%|█         | 55/532 [01:36<14:27,  1.82s/it]predicting train subjects:  11%|█         | 56/532 [01:38<14:30,  1.83s/it]predicting train subjects:  11%|█         | 57/532 [01:39<14:11,  1.79s/it]predicting train subjects:  11%|█         | 58/532 [01:41<14:22,  1.82s/it]predicting train subjects:  11%|█         | 59/532 [01:44<15:24,  1.95s/it]predicting train subjects:  11%|█▏        | 60/532 [01:45<14:03,  1.79s/it]predicting train subjects:  11%|█▏        | 61/532 [01:47<13:21,  1.70s/it]predicting train subjects:  12%|█▏        | 62/532 [01:49<14:04,  1.80s/it]predicting train subjects:  12%|█▏        | 63/532 [01:51<14:36,  1.87s/it]predicting train subjects:  12%|█▏        | 64/532 [01:52<13:52,  1.78s/it]predicting train subjects:  12%|█▏        | 65/532 [01:54<13:53,  1.79s/it]predicting train subjects:  12%|█▏        | 66/532 [01:56<15:02,  1.94s/it]predicting train subjects:  13%|█▎        | 67/532 [01:58<15:31,  2.00s/it]predicting train subjects:  13%|█▎        | 68/532 [02:00<15:02,  1.95s/it]predicting train subjects:  13%|█▎        | 69/532 [02:02<14:22,  1.86s/it]predicting train subjects:  13%|█▎        | 70/532 [02:04<13:48,  1.79s/it]predicting train subjects:  13%|█▎        | 71/532 [02:05<13:08,  1.71s/it]predicting train subjects:  14%|█▎        | 72/532 [02:06<12:35,  1.64s/it]predicting train subjects:  14%|█▎        | 73/532 [02:08<12:57,  1.69s/it]predicting train subjects:  14%|█▍        | 74/532 [02:11<14:18,  1.87s/it]predicting train subjects:  14%|█▍        | 75/532 [02:13<16:28,  2.16s/it]predicting train subjects:  14%|█▍        | 76/532 [02:15<15:30,  2.04s/it]predicting train subjects:  14%|█▍        | 77/532 [02:17<14:52,  1.96s/it]predicting train subjects:  15%|█▍        | 78/532 [02:19<14:33,  1.92s/it]predicting train subjects:  15%|█▍        | 79/532 [02:21<14:17,  1.89s/it]predicting train subjects:  15%|█▌        | 80/532 [02:22<14:10,  1.88s/it]predicting train subjects:  15%|█▌        | 81/532 [02:24<13:59,  1.86s/it]predicting train subjects:  15%|█▌        | 82/532 [02:26<14:21,  1.91s/it]predicting train subjects:  16%|█▌        | 83/532 [02:28<13:27,  1.80s/it]predicting train subjects:  16%|█▌        | 84/532 [02:29<12:51,  1.72s/it]predicting train subjects:  16%|█▌        | 85/532 [02:31<12:22,  1.66s/it]predicting train subjects:  16%|█▌        | 86/532 [02:32<11:59,  1.61s/it]predicting train subjects:  16%|█▋        | 87/532 [02:34<11:48,  1.59s/it]predicting train subjects:  17%|█▋        | 88/532 [02:36<11:44,  1.59s/it]predicting train subjects:  17%|█▋        | 89/532 [02:37<12:04,  1.64s/it]predicting train subjects:  17%|█▋        | 90/532 [02:39<12:12,  1.66s/it]predicting train subjects:  17%|█▋        | 91/532 [02:41<12:19,  1.68s/it]predicting train subjects:  17%|█▋        | 92/532 [02:42<12:22,  1.69s/it]predicting train subjects:  17%|█▋        | 93/532 [02:44<12:31,  1.71s/it]predicting train subjects:  18%|█▊        | 94/532 [02:46<12:35,  1.72s/it]predicting train subjects:  18%|█▊        | 95/532 [02:48<13:16,  1.82s/it]predicting train subjects:  18%|█▊        | 96/532 [02:50<13:55,  1.92s/it]predicting train subjects:  18%|█▊        | 97/532 [02:52<14:20,  1.98s/it]predicting train subjects:  18%|█▊        | 98/532 [02:54<14:27,  2.00s/it]predicting train subjects:  19%|█▊        | 99/532 [02:56<14:21,  1.99s/it]predicting train subjects:  19%|█▉        | 100/532 [02:58<14:22,  2.00s/it]predicting train subjects:  19%|█▉        | 101/532 [03:00<13:13,  1.84s/it]predicting train subjects:  19%|█▉        | 102/532 [03:01<12:25,  1.73s/it]predicting train subjects:  19%|█▉        | 103/532 [03:03<11:53,  1.66s/it]predicting train subjects:  20%|█▉        | 104/532 [03:04<11:29,  1.61s/it]predicting train subjects:  20%|█▉        | 105/532 [03:06<11:23,  1.60s/it]predicting train subjects:  20%|█▉        | 106/532 [03:07<11:14,  1.58s/it]predicting train subjects:  20%|██        | 107/532 [03:09<11:13,  1.59s/it]predicting train subjects:  20%|██        | 108/532 [03:11<11:06,  1.57s/it]predicting train subjects:  20%|██        | 109/532 [03:12<10:53,  1.54s/it]predicting train subjects:  21%|██        | 110/532 [03:13<10:41,  1.52s/it]predicting train subjects:  21%|██        | 111/532 [03:15<10:35,  1.51s/it]predicting train subjects:  21%|██        | 112/532 [03:16<10:28,  1.50s/it]predicting train subjects:  21%|██        | 113/532 [03:18<11:09,  1.60s/it]predicting train subjects:  21%|██▏       | 114/532 [03:20<11:39,  1.67s/it]predicting train subjects:  22%|██▏       | 115/532 [03:22<11:57,  1.72s/it]predicting train subjects:  22%|██▏       | 116/532 [03:24<12:00,  1.73s/it]predicting train subjects:  22%|██▏       | 117/532 [03:25<12:04,  1.74s/it]predicting train subjects:  22%|██▏       | 118/532 [03:27<12:26,  1.80s/it]predicting train subjects:  22%|██▏       | 119/532 [03:29<12:20,  1.79s/it]predicting train subjects:  23%|██▎       | 120/532 [03:31<12:21,  1.80s/it]predicting train subjects:  23%|██▎       | 121/532 [03:33<12:19,  1.80s/it]predicting train subjects:  23%|██▎       | 122/532 [03:35<12:18,  1.80s/it]predicting train subjects:  23%|██▎       | 123/532 [03:36<12:11,  1.79s/it]predicting train subjects:  23%|██▎       | 124/532 [03:38<12:03,  1.77s/it]predicting train subjects:  23%|██▎       | 125/532 [03:40<12:17,  1.81s/it]predicting train subjects:  24%|██▎       | 126/532 [03:42<12:23,  1.83s/it]predicting train subjects:  24%|██▍       | 127/532 [03:44<12:18,  1.82s/it]predicting train subjects:  24%|██▍       | 128/532 [03:46<12:25,  1.85s/it]predicting train subjects:  24%|██▍       | 129/532 [03:47<12:17,  1.83s/it]predicting train subjects:  24%|██▍       | 130/532 [03:49<12:24,  1.85s/it]predicting train subjects:  25%|██▍       | 131/532 [03:51<13:01,  1.95s/it]predicting train subjects:  25%|██▍       | 132/532 [03:54<13:20,  2.00s/it]predicting train subjects:  25%|██▌       | 133/532 [03:56<13:44,  2.07s/it]predicting train subjects:  25%|██▌       | 134/532 [03:58<13:53,  2.09s/it]predicting train subjects:  25%|██▌       | 135/532 [04:00<13:55,  2.10s/it]predicting train subjects:  26%|██▌       | 136/532 [04:02<13:57,  2.12s/it]predicting train subjects:  26%|██▌       | 137/532 [04:04<14:16,  2.17s/it]predicting train subjects:  26%|██▌       | 138/532 [04:07<14:27,  2.20s/it]predicting train subjects:  26%|██▌       | 139/532 [04:09<14:36,  2.23s/it]predicting train subjects:  26%|██▋       | 140/532 [04:11<14:48,  2.27s/it]predicting train subjects:  27%|██▋       | 141/532 [04:14<14:37,  2.25s/it]predicting train subjects:  27%|██▋       | 142/532 [04:16<14:40,  2.26s/it]predicting train subjects:  27%|██▋       | 143/532 [04:18<13:27,  2.08s/it]predicting train subjects:  27%|██▋       | 144/532 [04:19<12:34,  1.94s/it]predicting train subjects:  27%|██▋       | 145/532 [04:21<12:09,  1.88s/it]predicting train subjects:  27%|██▋       | 146/532 [04:23<11:50,  1.84s/it]predicting train subjects:  28%|██▊       | 147/532 [04:24<11:27,  1.79s/it]predicting train subjects:  28%|██▊       | 148/532 [04:26<11:14,  1.76s/it]predicting train subjects:  28%|██▊       | 149/532 [04:28<11:19,  1.77s/it]predicting train subjects:  28%|██▊       | 150/532 [04:30<11:14,  1.77s/it]predicting train subjects:  28%|██▊       | 151/532 [04:31<11:10,  1.76s/it]predicting train subjects:  29%|██▊       | 152/532 [04:33<11:08,  1.76s/it]predicting train subjects:  29%|██▉       | 153/532 [04:35<11:09,  1.77s/it]predicting train subjects:  29%|██▉       | 154/532 [04:37<11:01,  1.75s/it]predicting train subjects:  29%|██▉       | 155/532 [04:39<12:01,  1.91s/it]predicting train subjects:  29%|██▉       | 156/532 [04:41<12:50,  2.05s/it]predicting train subjects:  30%|██▉       | 157/532 [04:44<13:24,  2.15s/it]predicting train subjects:  30%|██▉       | 158/532 [04:46<13:38,  2.19s/it]predicting train subjects:  30%|██▉       | 159/532 [04:48<13:47,  2.22s/it]predicting train subjects:  30%|███       | 160/532 [04:51<14:02,  2.26s/it]predicting train subjects:  30%|███       | 161/532 [04:52<13:04,  2.11s/it]predicting train subjects:  30%|███       | 162/532 [04:54<12:19,  2.00s/it]predicting train subjects:  31%|███       | 163/532 [04:56<11:48,  1.92s/it]predicting train subjects:  31%|███       | 164/532 [04:58<11:30,  1.88s/it]predicting train subjects:  31%|███       | 165/532 [04:59<11:24,  1.87s/it]predicting train subjects:  31%|███       | 166/532 [05:01<11:11,  1.84s/it]predicting train subjects:  31%|███▏      | 167/532 [05:03<11:14,  1.85s/it]predicting train subjects:  32%|███▏      | 168/532 [05:05<11:15,  1.85s/it]predicting train subjects:  32%|███▏      | 169/532 [05:07<11:23,  1.88s/it]predicting train subjects:  32%|███▏      | 170/532 [05:09<11:14,  1.86s/it]predicting train subjects:  32%|███▏      | 171/532 [05:11<11:14,  1.87s/it]predicting train subjects:  32%|███▏      | 172/532 [05:12<11:08,  1.86s/it]predicting train subjects:  33%|███▎      | 173/532 [05:14<10:48,  1.81s/it]predicting train subjects:  33%|███▎      | 174/532 [05:16<10:27,  1.75s/it]predicting train subjects:  33%|███▎      | 175/532 [05:17<10:17,  1.73s/it]predicting train subjects:  33%|███▎      | 176/532 [05:19<10:07,  1.71s/it]predicting train subjects:  33%|███▎      | 177/532 [05:21<10:03,  1.70s/it]predicting train subjects:  33%|███▎      | 178/532 [05:22<10:00,  1.70s/it]predicting train subjects:  34%|███▎      | 179/532 [05:24<10:05,  1.71s/it]predicting train subjects:  34%|███▍      | 180/532 [05:26<10:04,  1.72s/it]predicting train subjects:  34%|███▍      | 181/532 [05:28<10:07,  1.73s/it]predicting train subjects:  34%|███▍      | 182/532 [05:29<10:05,  1.73s/it]predicting train subjects:  34%|███▍      | 183/532 [05:31<09:58,  1.72s/it]predicting train subjects:  35%|███▍      | 184/532 [05:33<09:57,  1.72s/it]predicting train subjects:  35%|███▍      | 185/532 [05:34<09:46,  1.69s/it]predicting train subjects:  35%|███▍      | 186/532 [05:36<09:38,  1.67s/it]predicting train subjects:  35%|███▌      | 187/532 [05:38<09:27,  1.64s/it]predicting train subjects:  35%|███▌      | 188/532 [05:39<09:27,  1.65s/it]predicting train subjects:  36%|███▌      | 189/532 [05:41<09:31,  1.67s/it]predicting train subjects:  36%|███▌      | 190/532 [05:43<09:30,  1.67s/it]predicting train subjects:  36%|███▌      | 191/532 [05:45<10:36,  1.87s/it]predicting train subjects:  36%|███▌      | 192/532 [05:47<11:30,  2.03s/it]predicting train subjects:  36%|███▋      | 193/532 [05:50<12:01,  2.13s/it]predicting train subjects:  36%|███▋      | 194/532 [05:52<12:28,  2.21s/it]predicting train subjects:  37%|███▋      | 195/532 [05:55<12:46,  2.28s/it]predicting train subjects:  37%|███▋      | 196/532 [05:57<12:56,  2.31s/it]predicting train subjects:  37%|███▋      | 197/532 [05:59<12:20,  2.21s/it]predicting train subjects:  37%|███▋      | 198/532 [06:01<12:18,  2.21s/it]predicting train subjects:  37%|███▋      | 199/532 [06:03<11:58,  2.16s/it]predicting train subjects:  38%|███▊      | 200/532 [06:05<11:40,  2.11s/it]predicting train subjects:  38%|███▊      | 201/532 [06:07<11:30,  2.09s/it]predicting train subjects:  38%|███▊      | 202/532 [06:09<11:27,  2.08s/it]predicting train subjects:  38%|███▊      | 203/532 [06:11<10:56,  1.99s/it]predicting train subjects:  38%|███▊      | 204/532 [06:13<10:24,  1.90s/it]predicting train subjects:  39%|███▊      | 205/532 [06:14<09:58,  1.83s/it]predicting train subjects:  39%|███▊      | 206/532 [06:16<09:40,  1.78s/it]predicting train subjects:  39%|███▉      | 207/532 [06:18<09:32,  1.76s/it]predicting train subjects:  39%|███▉      | 208/532 [06:20<09:22,  1.74s/it]predicting train subjects:  39%|███▉      | 209/532 [06:21<08:56,  1.66s/it]predicting train subjects:  39%|███▉      | 210/532 [06:22<08:34,  1.60s/it]predicting train subjects:  40%|███▉      | 211/532 [06:24<08:17,  1.55s/it]predicting train subjects:  40%|███▉      | 212/532 [06:25<08:08,  1.53s/it]predicting train subjects:  40%|████      | 213/532 [06:27<08:02,  1.51s/it]predicting train subjects:  40%|████      | 214/532 [06:28<07:59,  1.51s/it]predicting train subjects:  40%|████      | 215/532 [06:30<08:58,  1.70s/it]predicting train subjects:  41%|████      | 216/532 [06:33<09:37,  1.83s/it]predicting train subjects:  41%|████      | 217/532 [06:35<10:00,  1.91s/it]predicting train subjects:  41%|████      | 218/532 [06:37<10:21,  1.98s/it]predicting train subjects:  41%|████      | 219/532 [06:39<10:30,  2.02s/it]predicting train subjects:  41%|████▏     | 220/532 [06:41<10:35,  2.04s/it]predicting train subjects:  42%|████▏     | 221/532 [06:42<09:36,  1.85s/it]predicting train subjects:  42%|████▏     | 222/532 [06:44<08:51,  1.72s/it]predicting train subjects:  42%|████▏     | 223/532 [06:45<08:24,  1.63s/it]predicting train subjects:  42%|████▏     | 224/532 [06:47<08:06,  1.58s/it]predicting train subjects:  42%|████▏     | 225/532 [06:48<07:52,  1.54s/it]predicting train subjects:  42%|████▏     | 226/532 [06:50<07:40,  1.50s/it]predicting train subjects:  43%|████▎     | 227/532 [06:51<07:30,  1.48s/it]predicting train subjects:  43%|████▎     | 228/532 [06:52<07:21,  1.45s/it]predicting train subjects:  43%|████▎     | 229/532 [06:54<07:10,  1.42s/it]predicting train subjects:  43%|████▎     | 230/532 [06:55<07:04,  1.40s/it]predicting train subjects:  43%|████▎     | 231/532 [06:57<07:01,  1.40s/it]predicting train subjects:  44%|████▎     | 232/532 [06:58<07:00,  1.40s/it]predicting train subjects:  44%|████▍     | 233/532 [06:59<07:13,  1.45s/it]predicting train subjects:  44%|████▍     | 234/532 [07:01<07:25,  1.50s/it]predicting train subjects:  44%|████▍     | 235/532 [07:03<07:29,  1.51s/it]predicting train subjects:  44%|████▍     | 236/532 [07:04<07:26,  1.51s/it]predicting train subjects:  45%|████▍     | 237/532 [07:06<07:30,  1.53s/it]predicting train subjects:  45%|████▍     | 238/532 [07:07<07:34,  1.55s/it]predicting train subjects:  45%|████▍     | 239/532 [07:09<07:57,  1.63s/it]predicting train subjects:  45%|████▌     | 240/532 [07:11<08:14,  1.69s/it]predicting train subjects:  45%|████▌     | 241/532 [07:13<08:25,  1.74s/it]predicting train subjects:  45%|████▌     | 242/532 [07:15<08:26,  1.75s/it]predicting train subjects:  46%|████▌     | 243/532 [07:16<08:35,  1.79s/it]predicting train subjects:  46%|████▌     | 244/532 [07:18<08:35,  1.79s/it]predicting train subjects:  46%|████▌     | 245/532 [07:20<08:04,  1.69s/it]predicting train subjects:  46%|████▌     | 246/532 [07:21<07:36,  1.60s/it]predicting train subjects:  46%|████▋     | 247/532 [07:22<07:13,  1.52s/it]predicting train subjects:  47%|████▋     | 248/532 [07:24<06:57,  1.47s/it]predicting train subjects:  47%|████▋     | 249/532 [07:25<06:49,  1.45s/it]predicting train subjects:  47%|████▋     | 250/532 [07:27<06:43,  1.43s/it]predicting train subjects:  47%|████▋     | 251/532 [07:28<06:49,  1.46s/it]predicting train subjects:  47%|████▋     | 252/532 [07:30<06:55,  1.48s/it]predicting train subjects:  48%|████▊     | 253/532 [07:31<06:57,  1.50s/it]predicting train subjects:  48%|████▊     | 254/532 [07:33<06:55,  1.49s/it]predicting train subjects:  48%|████▊     | 255/532 [07:34<06:55,  1.50s/it]predicting train subjects:  48%|████▊     | 256/532 [07:36<07:07,  1.55s/it]predicting train subjects:  48%|████▊     | 257/532 [07:38<07:43,  1.69s/it]predicting train subjects:  48%|████▊     | 258/532 [07:40<08:08,  1.78s/it]predicting train subjects:  49%|████▊     | 259/532 [07:42<08:24,  1.85s/it]predicting train subjects:  49%|████▉     | 260/532 [07:44<08:39,  1.91s/it]predicting train subjects:  49%|████▉     | 261/532 [07:46<08:48,  1.95s/it]predicting train subjects:  49%|████▉     | 262/532 [07:48<08:47,  1.95s/it]predicting train subjects:  49%|████▉     | 263/532 [07:49<07:59,  1.78s/it]predicting train subjects:  50%|████▉     | 264/532 [07:51<07:29,  1.68s/it]predicting train subjects:  50%|████▉     | 265/532 [07:52<06:59,  1.57s/it]predicting train subjects:  50%|█████     | 266/532 [07:53<06:43,  1.52s/it]predicting train subjects:  50%|█████     | 267/532 [07:55<06:28,  1.47s/it]predicting train subjects:  50%|█████     | 268/532 [07:56<06:19,  1.44s/it]predicting train subjects:  51%|█████     | 269/532 [07:58<06:42,  1.53s/it]predicting train subjects:  51%|█████     | 270/532 [08:00<06:50,  1.57s/it]predicting train subjects:  51%|█████     | 271/532 [08:01<06:55,  1.59s/it]predicting train subjects:  51%|█████     | 272/532 [08:03<07:00,  1.62s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:05<07:02,  1.63s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:06<07:09,  1.66s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:08<07:37,  1.78s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:10<08:04,  1.89s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:13<08:21,  1.97s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:15<08:27,  2.00s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:17<08:43,  2.07s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:19<08:50,  2.11s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:21<08:48,  2.11s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:23<08:44,  2.10s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:25<08:39,  2.08s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:27<08:32,  2.07s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:29<08:30,  2.07s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:32<08:32,  2.08s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:33<07:49,  1.92s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:35<07:20,  1.80s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:36<06:57,  1.72s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:38<06:41,  1.66s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:39<06:27,  1.61s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:41<06:20,  1.59s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:42<06:32,  1.64s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:44<06:43,  1.69s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:46<06:50,  1.73s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:48<06:55,  1.76s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:50<06:54,  1.76s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:52<06:56,  1.78s/it]predicting train subjects:  56%|█████▌    | 299/532 [08:53<06:32,  1.68s/it]predicting train subjects:  56%|█████▋    | 300/532 [08:54<06:14,  1.61s/it]predicting train subjects:  57%|█████▋    | 301/532 [08:56<05:59,  1.56s/it]predicting train subjects:  57%|█████▋    | 302/532 [08:57<05:47,  1.51s/it]predicting train subjects:  57%|█████▋    | 303/532 [08:59<05:41,  1.49s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:00<05:40,  1.49s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:02<06:25,  1.70s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:05<06:52,  1.83s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:07<07:15,  1.93s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:09<07:27,  2.00s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:11<07:35,  2.04s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:13<07:39,  2.07s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:16<08:24,  2.28s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:19<09:02,  2.47s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:22<09:23,  2.57s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:24<09:36,  2.64s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:27<09:47,  2.71s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:30<09:45,  2.71s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:32<08:30,  2.38s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:33<07:40,  2.15s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:35<07:01,  1.98s/it]predicting train subjects:  60%|██████    | 320/532 [09:36<06:32,  1.85s/it]predicting train subjects:  60%|██████    | 321/532 [09:38<06:17,  1.79s/it]predicting train subjects:  61%|██████    | 322/532 [09:40<06:01,  1.72s/it]predicting train subjects:  61%|██████    | 323/532 [09:42<06:32,  1.88s/it]predicting train subjects:  61%|██████    | 324/532 [09:44<06:55,  2.00s/it]predicting train subjects:  61%|██████    | 325/532 [09:46<07:07,  2.07s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:49<07:20,  2.14s/it]predicting train subjects:  61%|██████▏   | 327/532 [09:51<07:26,  2.18s/it]predicting train subjects:  62%|██████▏   | 328/532 [09:53<07:36,  2.24s/it]predicting train subjects:  62%|██████▏   | 329/532 [09:55<07:02,  2.08s/it]predicting train subjects:  62%|██████▏   | 330/532 [09:57<06:37,  1.97s/it]predicting train subjects:  62%|██████▏   | 331/532 [09:58<06:24,  1.91s/it]predicting train subjects:  62%|██████▏   | 332/532 [10:00<06:10,  1.85s/it]predicting train subjects:  63%|██████▎   | 333/532 [10:02<06:01,  1.81s/it]predicting train subjects:  63%|██████▎   | 334/532 [10:04<05:51,  1.78s/it]predicting train subjects:  63%|██████▎   | 335/532 [10:06<06:03,  1.85s/it]predicting train subjects:  63%|██████▎   | 336/532 [10:08<06:11,  1.90s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:10<06:16,  1.93s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:12<06:20,  1.96s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:14<06:23,  1.99s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:16<06:21,  1.99s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:17<05:52,  1.85s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:19<05:29,  1.73s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:20<05:15,  1.67s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:22<05:00,  1.60s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:23<04:50,  1.56s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:25<04:46,  1.54s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:26<04:53,  1.59s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:28<04:58,  1.62s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:30<05:06,  1.67s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:31<05:05,  1.68s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:33<05:06,  1.69s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:35<05:08,  1.71s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:37<05:07,  1.72s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:38<05:02,  1.70s/it]predicting train subjects:  67%|██████▋   | 355/532 [10:40<04:58,  1.69s/it]predicting train subjects:  67%|██████▋   | 356/532 [10:42<04:56,  1.68s/it]predicting train subjects:  67%|██████▋   | 357/532 [10:43<04:58,  1.70s/it]predicting train subjects:  67%|██████▋   | 358/532 [10:45<04:55,  1.70s/it]predicting train subjects:  67%|██████▋   | 359/532 [10:47<04:48,  1.67s/it]predicting train subjects:  68%|██████▊   | 360/532 [10:48<04:39,  1.63s/it]predicting train subjects:  68%|██████▊   | 361/532 [10:50<04:37,  1.62s/it]predicting train subjects:  68%|██████▊   | 362/532 [10:51<04:28,  1.58s/it]predicting train subjects:  68%|██████▊   | 363/532 [10:53<04:23,  1.56s/it]predicting train subjects:  68%|██████▊   | 364/532 [10:54<04:21,  1.55s/it]predicting train subjects:  69%|██████▊   | 365/532 [10:56<04:16,  1.53s/it]predicting train subjects:  69%|██████▉   | 366/532 [10:57<04:12,  1.52s/it]predicting train subjects:  69%|██████▉   | 367/532 [10:59<04:11,  1.52s/it]predicting train subjects:  69%|██████▉   | 368/532 [11:00<04:09,  1.52s/it]predicting train subjects:  69%|██████▉   | 369/532 [11:02<04:07,  1.52s/it]predicting train subjects:  70%|██████▉   | 370/532 [11:03<04:05,  1.52s/it]predicting train subjects:  70%|██████▉   | 371/532 [11:06<04:34,  1.71s/it]predicting train subjects:  70%|██████▉   | 372/532 [11:08<04:53,  1.83s/it]predicting train subjects:  70%|███████   | 373/532 [11:10<05:06,  1.93s/it]predicting train subjects:  70%|███████   | 374/532 [11:12<05:17,  2.01s/it]predicting train subjects:  70%|███████   | 375/532 [11:14<05:21,  2.05s/it]predicting train subjects:  71%|███████   | 376/532 [11:16<05:25,  2.09s/it]predicting train subjects:  71%|███████   | 377/532 [11:18<05:07,  1.98s/it]predicting train subjects:  71%|███████   | 378/532 [11:20<04:52,  1.90s/it]predicting train subjects:  71%|███████   | 379/532 [11:22<04:43,  1.85s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:23<04:35,  1.81s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:25<04:29,  1.78s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:27<04:22,  1.75s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:28<04:21,  1.76s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:30<04:23,  1.78s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:32<04:21,  1.78s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:34<04:18,  1.77s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:36<04:16,  1.77s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:37<04:09,  1.73s/it]predicting train subjects:  73%|███████▎  | 389/532 [11:39<04:14,  1.78s/it]predicting train subjects:  73%|███████▎  | 390/532 [11:41<04:17,  1.81s/it]predicting train subjects:  73%|███████▎  | 391/532 [11:43<04:21,  1.85s/it]predicting train subjects:  74%|███████▎  | 392/532 [11:45<04:19,  1.85s/it]predicting train subjects:  74%|███████▍  | 393/532 [11:47<04:19,  1.87s/it]predicting train subjects:  74%|███████▍  | 394/532 [11:49<04:19,  1.88s/it]predicting train subjects:  74%|███████▍  | 395/532 [11:50<04:15,  1.86s/it]predicting train subjects:  74%|███████▍  | 396/532 [11:52<04:13,  1.87s/it]predicting train subjects:  75%|███████▍  | 397/532 [11:54<04:13,  1.88s/it]predicting train subjects:  75%|███████▍  | 398/532 [11:56<04:09,  1.86s/it]predicting train subjects:  75%|███████▌  | 399/532 [11:58<04:05,  1.84s/it]predicting train subjects:  75%|███████▌  | 400/532 [12:00<04:02,  1.84s/it]predicting train subjects:  75%|███████▌  | 401/532 [12:02<04:06,  1.88s/it]predicting train subjects:  76%|███████▌  | 402/532 [12:04<04:10,  1.93s/it]predicting train subjects:  76%|███████▌  | 403/532 [12:06<04:10,  1.94s/it]predicting train subjects:  76%|███████▌  | 404/532 [12:08<04:12,  1.97s/it]predicting train subjects:  76%|███████▌  | 405/532 [12:10<04:13,  1.99s/it]predicting train subjects:  76%|███████▋  | 406/532 [12:12<04:08,  1.97s/it]predicting train subjects:  77%|███████▋  | 407/532 [12:13<03:56,  1.89s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:15<03:46,  1.82s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:17<03:39,  1.78s/it]predicting train subjects:  77%|███████▋  | 410/532 [12:18<03:33,  1.75s/it]predicting train subjects:  77%|███████▋  | 411/532 [12:20<03:27,  1.71s/it]predicting train subjects:  77%|███████▋  | 412/532 [12:22<03:24,  1.71s/it]predicting train subjects:  78%|███████▊  | 413/532 [12:23<03:18,  1.67s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:25<03:14,  1.65s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:27<03:12,  1.65s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:28<03:09,  1.63s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:30<03:05,  1.62s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:31<03:03,  1.61s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:33<03:10,  1.68s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:35<03:13,  1.73s/it]predicting train subjects:  79%|███████▉  | 421/532 [12:37<03:17,  1.78s/it]predicting train subjects:  79%|███████▉  | 422/532 [12:39<03:22,  1.84s/it]predicting train subjects:  80%|███████▉  | 423/532 [12:41<03:21,  1.85s/it]predicting train subjects:  80%|███████▉  | 424/532 [12:43<03:19,  1.84s/it]predicting train subjects:  80%|███████▉  | 425/532 [12:45<03:21,  1.88s/it]predicting train subjects:  80%|████████  | 426/532 [12:46<03:19,  1.88s/it]predicting train subjects:  80%|████████  | 427/532 [12:48<03:16,  1.87s/it]predicting train subjects:  80%|████████  | 428/532 [12:50<03:13,  1.86s/it]predicting train subjects:  81%|████████  | 429/532 [12:52<03:12,  1.87s/it]predicting train subjects:  81%|████████  | 430/532 [12:54<03:11,  1.88s/it]predicting train subjects:  81%|████████  | 431/532 [12:56<03:11,  1.90s/it]predicting train subjects:  81%|████████  | 432/532 [12:58<03:12,  1.92s/it]predicting train subjects:  81%|████████▏ | 433/532 [13:00<03:11,  1.93s/it]predicting train subjects:  82%|████████▏ | 434/532 [13:02<03:11,  1.95s/it]predicting train subjects:  82%|████████▏ | 435/532 [13:04<03:10,  1.96s/it]predicting train subjects:  82%|████████▏ | 436/532 [13:06<03:09,  1.98s/it]predicting train subjects:  82%|████████▏ | 437/532 [13:07<02:53,  1.83s/it]predicting train subjects:  82%|████████▏ | 438/532 [13:09<02:42,  1.73s/it]predicting train subjects:  83%|████████▎ | 439/532 [13:10<02:33,  1.65s/it]predicting train subjects:  83%|████████▎ | 440/532 [13:12<02:26,  1.59s/it]predicting train subjects:  83%|████████▎ | 441/532 [13:13<02:20,  1.55s/it]predicting train subjects:  83%|████████▎ | 442/532 [13:15<02:16,  1.52s/it]predicting train subjects:  83%|████████▎ | 443/532 [13:16<02:12,  1.48s/it]predicting train subjects:  83%|████████▎ | 444/532 [13:17<02:08,  1.46s/it]predicting train subjects:  84%|████████▎ | 445/532 [13:19<02:05,  1.44s/it]predicting train subjects:  84%|████████▍ | 446/532 [13:20<02:02,  1.43s/it]predicting train subjects:  84%|████████▍ | 447/532 [13:22<02:00,  1.42s/it]predicting train subjects:  84%|████████▍ | 448/532 [13:23<01:58,  1.41s/it]predicting train subjects:  84%|████████▍ | 449/532 [13:25<02:01,  1.46s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:26<02:02,  1.49s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:28<02:04,  1.54s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:29<02:04,  1.55s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:31<02:03,  1.56s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:32<02:00,  1.55s/it]predicting train subjects:  86%|████████▌ | 455/532 [13:34<02:04,  1.62s/it]predicting train subjects:  86%|████████▌ | 456/532 [13:36<02:07,  1.68s/it]predicting train subjects:  86%|████████▌ | 457/532 [13:38<02:09,  1.73s/it]predicting train subjects:  86%|████████▌ | 458/532 [13:40<02:08,  1.74s/it]predicting train subjects:  86%|████████▋ | 459/532 [13:42<02:09,  1.78s/it]predicting train subjects:  86%|████████▋ | 460/532 [13:43<02:08,  1.79s/it]predicting train subjects:  87%|████████▋ | 461/532 [13:45<02:14,  1.89s/it]predicting train subjects:  87%|████████▋ | 462/532 [13:48<02:16,  1.95s/it]predicting train subjects:  87%|████████▋ | 463/532 [13:50<02:19,  2.02s/it]predicting train subjects:  87%|████████▋ | 464/532 [13:52<02:20,  2.07s/it]predicting train subjects:  87%|████████▋ | 465/532 [13:54<02:21,  2.11s/it]predicting train subjects:  88%|████████▊ | 466/532 [13:56<02:19,  2.11s/it]predicting train subjects:  88%|████████▊ | 467/532 [13:58<02:09,  1.99s/it]predicting train subjects:  88%|████████▊ | 468/532 [14:00<02:02,  1.92s/it]predicting train subjects:  88%|████████▊ | 469/532 [14:01<01:57,  1.86s/it]predicting train subjects:  88%|████████▊ | 470/532 [14:03<01:51,  1.80s/it]predicting train subjects:  89%|████████▊ | 471/532 [14:05<01:48,  1.77s/it]predicting train subjects:  89%|████████▊ | 472/532 [14:06<01:44,  1.74s/it]predicting train subjects:  89%|████████▉ | 473/532 [14:08<01:44,  1.76s/it]predicting train subjects:  89%|████████▉ | 474/532 [14:10<01:42,  1.77s/it]predicting train subjects:  89%|████████▉ | 475/532 [14:12<01:41,  1.78s/it]predicting train subjects:  89%|████████▉ | 476/532 [14:14<01:39,  1.78s/it]predicting train subjects:  90%|████████▉ | 477/532 [14:15<01:38,  1.79s/it]predicting train subjects:  90%|████████▉ | 478/532 [14:17<01:38,  1.83s/it]predicting train subjects:  90%|█████████ | 479/532 [14:19<01:32,  1.75s/it]predicting train subjects:  90%|█████████ | 480/532 [14:21<01:29,  1.73s/it]predicting train subjects:  90%|█████████ | 481/532 [14:22<01:26,  1.69s/it]predicting train subjects:  91%|█████████ | 482/532 [14:24<01:22,  1.66s/it]predicting train subjects:  91%|█████████ | 483/532 [14:25<01:20,  1.64s/it]predicting train subjects:  91%|█████████ | 484/532 [14:27<01:17,  1.62s/it]predicting train subjects:  91%|█████████ | 485/532 [14:29<01:22,  1.76s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:31<01:25,  1.85s/it]predicting train subjects:  92%|█████████▏| 487/532 [14:33<01:27,  1.93s/it]predicting train subjects:  92%|█████████▏| 488/532 [14:35<01:27,  1.99s/it]predicting train subjects:  92%|█████████▏| 489/532 [14:38<01:28,  2.05s/it]predicting train subjects:  92%|█████████▏| 490/532 [14:40<01:26,  2.07s/it]predicting train subjects:  92%|█████████▏| 491/532 [14:41<01:20,  1.96s/it]predicting train subjects:  92%|█████████▏| 492/532 [14:43<01:15,  1.89s/it]predicting train subjects:  93%|█████████▎| 493/532 [14:45<01:12,  1.85s/it]predicting train subjects:  93%|█████████▎| 494/532 [14:47<01:09,  1.84s/it]predicting train subjects:  93%|█████████▎| 495/532 [14:48<01:06,  1.80s/it]predicting train subjects:  93%|█████████▎| 496/532 [14:50<01:03,  1.77s/it]predicting train subjects:  93%|█████████▎| 497/532 [14:52<01:02,  1.80s/it]predicting train subjects:  94%|█████████▎| 498/532 [14:54<01:00,  1.79s/it]predicting train subjects:  94%|█████████▍| 499/532 [14:56<01:00,  1.82s/it]predicting train subjects:  94%|█████████▍| 500/532 [14:58<00:58,  1.83s/it]predicting train subjects:  94%|█████████▍| 501/532 [14:59<00:56,  1.82s/it]predicting train subjects:  94%|█████████▍| 502/532 [15:01<00:55,  1.85s/it]predicting train subjects:  95%|█████████▍| 503/532 [15:03<00:52,  1.80s/it]predicting train subjects:  95%|█████████▍| 504/532 [15:04<00:48,  1.74s/it]predicting train subjects:  95%|█████████▍| 505/532 [15:06<00:46,  1.71s/it]predicting train subjects:  95%|█████████▌| 506/532 [15:08<00:43,  1.67s/it]predicting train subjects:  95%|█████████▌| 507/532 [15:09<00:41,  1.66s/it]predicting train subjects:  95%|█████████▌| 508/532 [15:11<00:39,  1.65s/it]predicting train subjects:  96%|█████████▌| 509/532 [15:13<00:40,  1.78s/it]predicting train subjects:  96%|█████████▌| 510/532 [15:15<00:41,  1.88s/it]predicting train subjects:  96%|█████████▌| 511/532 [15:17<00:40,  1.94s/it]predicting train subjects:  96%|█████████▌| 512/532 [15:19<00:39,  1.99s/it]predicting train subjects:  96%|█████████▋| 513/532 [15:21<00:38,  2.02s/it]predicting train subjects:  97%|█████████▋| 514/532 [15:24<00:36,  2.05s/it]predicting train subjects:  97%|█████████▋| 515/532 [15:25<00:33,  1.94s/it]predicting train subjects:  97%|█████████▋| 516/532 [15:27<00:29,  1.85s/it]predicting train subjects:  97%|█████████▋| 517/532 [15:29<00:27,  1.80s/it]predicting train subjects:  97%|█████████▋| 518/532 [15:30<00:24,  1.76s/it]predicting train subjects:  98%|█████████▊| 519/532 [15:32<00:22,  1.74s/it]predicting train subjects:  98%|█████████▊| 520/532 [15:34<00:20,  1.73s/it]predicting train subjects:  98%|█████████▊| 521/532 [15:36<00:19,  1.77s/it]predicting train subjects:  98%|█████████▊| 522/532 [15:37<00:17,  1.76s/it]predicting train subjects:  98%|█████████▊| 523/532 [15:39<00:16,  1.78s/it]predicting train subjects:  98%|█████████▊| 524/532 [15:41<00:14,  1.79s/it]predicting train subjects:  99%|█████████▊| 525/532 [15:43<00:12,  1.80s/it]predicting train subjects:  99%|█████████▉| 526/532 [15:45<00:10,  1.80s/it]predicting train subjects:  99%|█████████▉| 527/532 [15:46<00:08,  1.76s/it]predicting train subjects:  99%|█████████▉| 528/532 [15:48<00:06,  1.72s/it]predicting train subjects:  99%|█████████▉| 529/532 [15:49<00:05,  1.70s/it]predicting train subjects: 100%|█████████▉| 530/532 [15:51<00:03,  1.68s/it]predicting train subjects: 100%|█████████▉| 531/532 [15:53<00:01,  1.66s/it]predicting train subjects: 100%|██████████| 532/532 [15:54<00:00,  1.66s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<12:20,  1.39s/it]Loading train:   0%|          | 2/532 [00:02<11:36,  1.31s/it]Loading train:   1%|          | 3/532 [00:03<11:12,  1.27s/it]Loading train:   1%|          | 4/532 [00:04<11:00,  1.25s/it]Loading train:   1%|          | 5/532 [00:06<10:41,  1.22s/it]Loading train:   1%|          | 6/532 [00:07<10:13,  1.17s/it]Loading train:   1%|▏         | 7/532 [00:08<09:46,  1.12s/it]Loading train:   2%|▏         | 8/532 [00:09<09:36,  1.10s/it]Loading train:   2%|▏         | 9/532 [00:10<10:06,  1.16s/it]Loading train:   2%|▏         | 10/532 [00:11<09:45,  1.12s/it]Loading train:   2%|▏         | 11/532 [00:12<09:28,  1.09s/it]Loading train:   2%|▏         | 12/532 [00:13<10:11,  1.18s/it]Loading train:   2%|▏         | 13/532 [00:14<09:50,  1.14s/it]Loading train:   3%|▎         | 14/532 [00:15<09:15,  1.07s/it]Loading train:   3%|▎         | 15/532 [00:17<09:34,  1.11s/it]Loading train:   3%|▎         | 16/532 [00:18<09:36,  1.12s/it]Loading train:   3%|▎         | 17/532 [00:19<09:24,  1.10s/it]Loading train:   3%|▎         | 18/532 [00:20<09:57,  1.16s/it]Loading train:   4%|▎         | 19/532 [00:21<09:14,  1.08s/it]Loading train:   4%|▍         | 20/532 [00:22<09:04,  1.06s/it]Loading train:   4%|▍         | 21/532 [00:23<09:27,  1.11s/it]Loading train:   4%|▍         | 22/532 [00:24<09:14,  1.09s/it]Loading train:   4%|▍         | 23/532 [00:25<09:07,  1.07s/it]Loading train:   5%|▍         | 24/532 [00:26<08:46,  1.04s/it]Loading train:   5%|▍         | 25/532 [00:28<09:48,  1.16s/it]Loading train:   5%|▍         | 26/532 [00:29<09:29,  1.13s/it]Loading train:   5%|▌         | 27/532 [00:30<10:01,  1.19s/it]Loading train:   5%|▌         | 28/532 [00:31<09:45,  1.16s/it]Loading train:   5%|▌         | 29/532 [00:32<09:36,  1.15s/it]Loading train:   6%|▌         | 30/532 [00:33<09:12,  1.10s/it]Loading train:   6%|▌         | 31/532 [00:34<08:57,  1.07s/it]Loading train:   6%|▌         | 32/532 [00:35<09:00,  1.08s/it]Loading train:   6%|▌         | 33/532 [00:36<08:44,  1.05s/it]Loading train:   6%|▋         | 34/532 [00:38<09:12,  1.11s/it]Loading train:   7%|▋         | 35/532 [00:39<09:01,  1.09s/it]Loading train:   7%|▋         | 36/532 [00:40<09:54,  1.20s/it]Loading train:   7%|▋         | 37/532 [00:42<11:18,  1.37s/it]Loading train:   7%|▋         | 38/532 [00:43<11:10,  1.36s/it]Loading train:   7%|▋         | 39/532 [00:44<10:56,  1.33s/it]Loading train:   8%|▊         | 40/532 [00:46<10:29,  1.28s/it]Loading train:   8%|▊         | 41/532 [00:47<10:17,  1.26s/it]Loading train:   8%|▊         | 42/532 [00:48<09:56,  1.22s/it]Loading train:   8%|▊         | 43/532 [00:49<09:08,  1.12s/it]Loading train:   8%|▊         | 44/532 [00:50<08:43,  1.07s/it]Loading train:   8%|▊         | 45/532 [00:51<08:08,  1.00s/it]Loading train:   9%|▊         | 46/532 [00:52<08:12,  1.01s/it]Loading train:   9%|▉         | 47/532 [00:53<08:44,  1.08s/it]Loading train:   9%|▉         | 48/532 [00:54<08:44,  1.08s/it]Loading train:   9%|▉         | 49/532 [00:55<08:25,  1.05s/it]Loading train:   9%|▉         | 50/532 [00:56<08:39,  1.08s/it]Loading train:  10%|▉         | 51/532 [00:57<08:30,  1.06s/it]Loading train:  10%|▉         | 52/532 [00:58<08:26,  1.06s/it]Loading train:  10%|▉         | 53/532 [00:59<08:09,  1.02s/it]Loading train:  10%|█         | 54/532 [01:00<08:44,  1.10s/it]Loading train:  10%|█         | 55/532 [01:02<08:54,  1.12s/it]Loading train:  11%|█         | 56/532 [01:03<08:35,  1.08s/it]Loading train:  11%|█         | 57/532 [01:04<08:29,  1.07s/it]Loading train:  11%|█         | 58/532 [01:05<08:35,  1.09s/it]Loading train:  11%|█         | 59/532 [01:06<08:50,  1.12s/it]Loading train:  11%|█▏        | 60/532 [01:07<08:28,  1.08s/it]Loading train:  11%|█▏        | 61/532 [01:08<08:14,  1.05s/it]Loading train:  12%|█▏        | 62/532 [01:09<08:27,  1.08s/it]Loading train:  12%|█▏        | 63/532 [01:10<08:38,  1.11s/it]Loading train:  12%|█▏        | 64/532 [01:11<08:25,  1.08s/it]Loading train:  12%|█▏        | 65/532 [01:12<08:16,  1.06s/it]Loading train:  12%|█▏        | 66/532 [01:14<08:55,  1.15s/it]Loading train:  13%|█▎        | 67/532 [01:15<08:52,  1.15s/it]Loading train:  13%|█▎        | 68/532 [01:16<08:26,  1.09s/it]Loading train:  13%|█▎        | 69/532 [01:17<08:01,  1.04s/it]Loading train:  13%|█▎        | 70/532 [01:18<07:43,  1.00s/it]Loading train:  13%|█▎        | 71/532 [01:19<07:42,  1.00s/it]Loading train:  14%|█▎        | 72/532 [01:19<07:33,  1.01it/s]Loading train:  14%|█▎        | 73/532 [01:21<07:53,  1.03s/it]Loading train:  14%|█▍        | 74/532 [01:22<08:31,  1.12s/it]Loading train:  14%|█▍        | 75/532 [01:24<09:40,  1.27s/it]Loading train:  14%|█▍        | 76/532 [01:25<09:02,  1.19s/it]Loading train:  14%|█▍        | 77/532 [01:26<08:39,  1.14s/it]Loading train:  15%|█▍        | 78/532 [01:27<08:27,  1.12s/it]Loading train:  15%|█▍        | 79/532 [01:28<08:12,  1.09s/it]Loading train:  15%|█▌        | 80/532 [01:29<08:06,  1.08s/it]Loading train:  15%|█▌        | 81/532 [01:30<07:58,  1.06s/it]Loading train:  15%|█▌        | 82/532 [01:31<08:13,  1.10s/it]Loading train:  16%|█▌        | 83/532 [01:32<07:51,  1.05s/it]Loading train:  16%|█▌        | 84/532 [01:33<07:27,  1.00it/s]Loading train:  16%|█▌        | 85/532 [01:34<07:06,  1.05it/s]Loading train:  16%|█▌        | 86/532 [01:34<06:45,  1.10it/s]Loading train:  16%|█▋        | 87/532 [01:35<06:47,  1.09it/s]Loading train:  17%|█▋        | 88/532 [01:36<07:00,  1.06it/s]Loading train:  17%|█▋        | 89/532 [01:37<07:15,  1.02it/s]Loading train:  17%|█▋        | 90/532 [01:38<07:18,  1.01it/s]Loading train:  17%|█▋        | 91/532 [01:39<07:23,  1.01s/it]Loading train:  17%|█▋        | 92/532 [01:40<07:23,  1.01s/it]Loading train:  17%|█▋        | 93/532 [01:41<07:19,  1.00s/it]Loading train:  18%|█▊        | 94/532 [01:42<07:10,  1.02it/s]Loading train:  18%|█▊        | 95/532 [01:44<07:26,  1.02s/it]Loading train:  18%|█▊        | 96/532 [01:45<07:36,  1.05s/it]Loading train:  18%|█▊        | 97/532 [01:46<07:52,  1.09s/it]Loading train:  18%|█▊        | 98/532 [01:47<07:56,  1.10s/it]Loading train:  19%|█▊        | 99/532 [01:48<08:07,  1.13s/it]Loading train:  19%|█▉        | 100/532 [01:49<08:07,  1.13s/it]Loading train:  19%|█▉        | 101/532 [01:50<07:51,  1.09s/it]Loading train:  19%|█▉        | 102/532 [01:51<07:32,  1.05s/it]Loading train:  19%|█▉        | 103/532 [01:52<07:36,  1.06s/it]Loading train:  20%|█▉        | 104/532 [01:53<07:10,  1.01s/it]Loading train:  20%|█▉        | 105/532 [01:54<07:07,  1.00s/it]Loading train:  20%|█▉        | 106/532 [01:55<06:51,  1.04it/s]Loading train:  20%|██        | 107/532 [01:56<06:35,  1.07it/s]Loading train:  20%|██        | 108/532 [01:57<06:30,  1.09it/s]Loading train:  20%|██        | 109/532 [01:58<06:20,  1.11it/s]Loading train:  21%|██        | 110/532 [01:59<06:13,  1.13it/s]Loading train:  21%|██        | 111/532 [01:59<06:13,  1.13it/s]Loading train:  21%|██        | 112/532 [02:00<06:10,  1.13it/s]Loading train:  21%|██        | 113/532 [02:02<06:58,  1.00it/s]Loading train:  21%|██▏       | 114/532 [02:03<07:05,  1.02s/it]Loading train:  22%|██▏       | 115/532 [02:04<07:02,  1.01s/it]Loading train:  22%|██▏       | 116/532 [02:05<07:10,  1.04s/it]Loading train:  22%|██▏       | 117/532 [02:06<07:01,  1.02s/it]Loading train:  22%|██▏       | 118/532 [02:07<07:01,  1.02s/it]Loading train:  22%|██▏       | 119/532 [02:08<07:09,  1.04s/it]Loading train:  23%|██▎       | 120/532 [02:09<07:10,  1.04s/it]Loading train:  23%|██▎       | 121/532 [02:10<07:01,  1.02s/it]Loading train:  23%|██▎       | 122/532 [02:11<06:52,  1.01s/it]Loading train:  23%|██▎       | 123/532 [02:12<06:54,  1.01s/it]Loading train:  23%|██▎       | 124/532 [02:13<06:52,  1.01s/it]Loading train:  23%|██▎       | 125/532 [02:14<07:04,  1.04s/it]Loading train:  24%|██▎       | 126/532 [02:15<07:29,  1.11s/it]Loading train:  24%|██▍       | 127/532 [02:16<07:24,  1.10s/it]Loading train:  24%|██▍       | 128/532 [02:17<07:15,  1.08s/it]Loading train:  24%|██▍       | 129/532 [02:18<07:14,  1.08s/it]Loading train:  24%|██▍       | 130/532 [02:20<07:20,  1.10s/it]Loading train:  25%|██▍       | 131/532 [02:21<07:39,  1.15s/it]Loading train:  25%|██▍       | 132/532 [02:22<07:46,  1.17s/it]Loading train:  25%|██▌       | 133/532 [02:23<08:01,  1.21s/it]Loading train:  25%|██▌       | 134/532 [02:25<08:04,  1.22s/it]Loading train:  25%|██▌       | 135/532 [02:26<08:01,  1.21s/it]Loading train:  26%|██▌       | 136/532 [02:27<08:04,  1.22s/it]Loading train:  26%|██▌       | 137/532 [02:28<08:14,  1.25s/it]Loading train:  26%|██▌       | 138/532 [02:30<08:13,  1.25s/it]Loading train:  26%|██▌       | 139/532 [02:31<08:15,  1.26s/it]Loading train:  26%|██▋       | 140/532 [02:32<08:10,  1.25s/it]Loading train:  27%|██▋       | 141/532 [02:33<08:03,  1.24s/it]Loading train:  27%|██▋       | 142/532 [02:34<07:59,  1.23s/it]Loading train:  27%|██▋       | 143/532 [02:35<07:32,  1.16s/it]Loading train:  27%|██▋       | 144/532 [02:36<07:08,  1.11s/it]Loading train:  27%|██▋       | 145/532 [02:37<06:44,  1.04s/it]Loading train:  27%|██▋       | 146/532 [02:38<06:18,  1.02it/s]Loading train:  28%|██▊       | 147/532 [02:39<06:01,  1.06it/s]Loading train:  28%|██▊       | 148/532 [02:40<05:52,  1.09it/s]Loading train:  28%|██▊       | 149/532 [02:41<05:54,  1.08it/s]Loading train:  28%|██▊       | 150/532 [02:42<05:46,  1.10it/s]Loading train:  28%|██▊       | 151/532 [02:43<05:52,  1.08it/s]Loading train:  29%|██▊       | 152/532 [02:44<05:48,  1.09it/s]Loading train:  29%|██▉       | 153/532 [02:44<05:41,  1.11it/s]Loading train:  29%|██▉       | 154/532 [02:45<05:35,  1.13it/s]Loading train:  29%|██▉       | 155/532 [02:47<06:13,  1.01it/s]Loading train:  29%|██▉       | 156/532 [02:48<06:45,  1.08s/it]Loading train:  30%|██▉       | 157/532 [02:49<07:03,  1.13s/it]Loading train:  30%|██▉       | 158/532 [02:50<07:15,  1.16s/it]Loading train:  30%|██▉       | 159/532 [02:52<07:23,  1.19s/it]Loading train:  30%|███       | 160/532 [02:53<07:18,  1.18s/it]Loading train:  30%|███       | 161/532 [02:54<07:10,  1.16s/it]Loading train:  30%|███       | 162/532 [02:55<06:51,  1.11s/it]Loading train:  31%|███       | 163/532 [02:56<06:36,  1.08s/it]Loading train:  31%|███       | 164/532 [02:57<06:24,  1.04s/it]Loading train:  31%|███       | 165/532 [02:58<06:21,  1.04s/it]Loading train:  31%|███       | 166/532 [02:59<06:11,  1.02s/it]Loading train:  31%|███▏      | 167/532 [03:00<06:19,  1.04s/it]Loading train:  32%|███▏      | 168/532 [03:01<06:17,  1.04s/it]Loading train:  32%|███▏      | 169/532 [03:02<06:14,  1.03s/it]Loading train:  32%|███▏      | 170/532 [03:03<06:08,  1.02s/it]Loading train:  32%|███▏      | 171/532 [03:04<05:58,  1.01it/s]Loading train:  32%|███▏      | 172/532 [03:05<06:02,  1.01s/it]Loading train:  33%|███▎      | 173/532 [03:06<05:59,  1.00s/it]Loading train:  33%|███▎      | 174/532 [03:07<05:54,  1.01it/s]Loading train:  33%|███▎      | 175/532 [03:08<05:43,  1.04it/s]Loading train:  33%|███▎      | 176/532 [03:09<05:30,  1.08it/s]Loading train:  33%|███▎      | 177/532 [03:09<05:21,  1.10it/s]Loading train:  33%|███▎      | 178/532 [03:10<05:27,  1.08it/s]Loading train:  34%|███▎      | 179/532 [03:11<05:39,  1.04it/s]Loading train:  34%|███▍      | 180/532 [03:13<05:50,  1.01it/s]Loading train:  34%|███▍      | 181/532 [03:13<05:44,  1.02it/s]Loading train:  34%|███▍      | 182/532 [03:14<05:36,  1.04it/s]Loading train:  34%|███▍      | 183/532 [03:15<05:41,  1.02it/s]Loading train:  35%|███▍      | 184/532 [03:16<05:34,  1.04it/s]Loading train:  35%|███▍      | 185/532 [03:17<05:25,  1.07it/s]Loading train:  35%|███▍      | 186/532 [03:18<05:28,  1.05it/s]Loading train:  35%|███▌      | 187/532 [03:19<05:26,  1.06it/s]Loading train:  35%|███▌      | 188/532 [03:20<05:23,  1.06it/s]Loading train:  36%|███▌      | 189/532 [03:21<05:17,  1.08it/s]Loading train:  36%|███▌      | 190/532 [03:22<05:14,  1.09it/s]Loading train:  36%|███▌      | 191/532 [03:23<05:45,  1.01s/it]Loading train:  36%|███▌      | 192/532 [03:24<06:09,  1.09s/it]Loading train:  36%|███▋      | 193/532 [03:26<06:23,  1.13s/it]Loading train:  36%|███▋      | 194/532 [03:27<06:33,  1.17s/it]Loading train:  37%|███▋      | 195/532 [03:28<06:40,  1.19s/it]Loading train:  37%|███▋      | 196/532 [03:29<06:40,  1.19s/it]Loading train:  37%|███▋      | 197/532 [03:30<06:32,  1.17s/it]Loading train:  37%|███▋      | 198/532 [03:31<06:25,  1.15s/it]Loading train:  37%|███▋      | 199/532 [03:33<06:15,  1.13s/it]Loading train:  38%|███▊      | 200/532 [03:34<06:09,  1.11s/it]Loading train:  38%|███▊      | 201/532 [03:35<06:06,  1.11s/it]Loading train:  38%|███▊      | 202/532 [03:36<05:59,  1.09s/it]Loading train:  38%|███▊      | 203/532 [03:37<05:47,  1.05s/it]Loading train:  38%|███▊      | 204/532 [03:38<05:33,  1.02s/it]Loading train:  39%|███▊      | 205/532 [03:39<05:29,  1.01s/it]Loading train:  39%|███▊      | 206/532 [03:40<05:25,  1.00it/s]Loading train:  39%|███▉      | 207/532 [03:41<05:29,  1.01s/it]Loading train:  39%|███▉      | 208/532 [03:42<05:26,  1.01s/it]Loading train:  39%|███▉      | 209/532 [03:43<05:15,  1.02it/s]Loading train:  39%|███▉      | 210/532 [03:43<05:01,  1.07it/s]Loading train:  40%|███▉      | 211/532 [03:44<05:02,  1.06it/s]Loading train:  40%|███▉      | 212/532 [03:45<04:53,  1.09it/s]Loading train:  40%|████      | 213/532 [03:46<04:45,  1.12it/s]Loading train:  40%|████      | 214/532 [03:47<04:41,  1.13it/s]Loading train:  40%|████      | 215/532 [03:48<05:11,  1.02it/s]Loading train:  41%|████      | 216/532 [03:49<05:27,  1.04s/it]Loading train:  41%|████      | 217/532 [03:51<05:40,  1.08s/it]Loading train:  41%|████      | 218/532 [03:52<05:48,  1.11s/it]Loading train:  41%|████      | 219/532 [03:53<05:51,  1.12s/it]Loading train:  41%|████▏     | 220/532 [03:54<05:52,  1.13s/it]Loading train:  42%|████▏     | 221/532 [03:55<05:24,  1.04s/it]Loading train:  42%|████▏     | 222/532 [03:56<05:12,  1.01s/it]Loading train:  42%|████▏     | 223/532 [03:57<05:13,  1.02s/it]Loading train:  42%|████▏     | 224/532 [03:58<05:08,  1.00s/it]Loading train:  42%|████▏     | 225/532 [03:59<04:50,  1.06it/s]Loading train:  42%|████▏     | 226/532 [03:59<04:44,  1.07it/s]Loading train:  43%|████▎     | 227/532 [04:00<04:39,  1.09it/s]Loading train:  43%|████▎     | 228/532 [04:01<04:30,  1.13it/s]Loading train:  43%|████▎     | 229/532 [04:02<04:27,  1.13it/s]Loading train:  43%|████▎     | 230/532 [04:03<04:19,  1.16it/s]Loading train:  43%|████▎     | 231/532 [04:04<04:14,  1.18it/s]Loading train:  44%|████▎     | 232/532 [04:04<04:06,  1.22it/s]Loading train:  44%|████▍     | 233/532 [04:05<04:18,  1.16it/s]Loading train:  44%|████▍     | 234/532 [04:06<04:22,  1.14it/s]Loading train:  44%|████▍     | 235/532 [04:07<04:35,  1.08it/s]Loading train:  44%|████▍     | 236/532 [04:08<04:38,  1.06it/s]Loading train:  45%|████▍     | 237/532 [04:09<04:39,  1.06it/s]Loading train:  45%|████▍     | 238/532 [04:10<04:35,  1.07it/s]Loading train:  45%|████▍     | 239/532 [04:11<04:42,  1.04it/s]Loading train:  45%|████▌     | 240/532 [04:12<04:50,  1.00it/s]Loading train:  45%|████▌     | 241/532 [04:13<04:50,  1.00it/s]Loading train:  45%|████▌     | 242/532 [04:14<04:54,  1.01s/it]Loading train:  46%|████▌     | 243/532 [04:15<04:58,  1.03s/it]Loading train:  46%|████▌     | 244/532 [04:16<04:52,  1.02s/it]Loading train:  46%|████▌     | 245/532 [04:17<04:40,  1.02it/s]Loading train:  46%|████▌     | 246/532 [04:18<04:31,  1.05it/s]Loading train:  46%|████▋     | 247/532 [04:19<04:26,  1.07it/s]Loading train:  47%|████▋     | 248/532 [04:20<04:20,  1.09it/s]Loading train:  47%|████▋     | 249/532 [04:21<04:12,  1.12it/s]Loading train:  47%|████▋     | 250/532 [04:22<04:12,  1.12it/s]Loading train:  47%|████▋     | 251/532 [04:23<04:11,  1.12it/s]Loading train:  47%|████▋     | 252/532 [04:23<04:04,  1.15it/s]Loading train:  48%|████▊     | 253/532 [04:24<04:06,  1.13it/s]Loading train:  48%|████▊     | 254/532 [04:25<04:04,  1.14it/s]Loading train:  48%|████▊     | 255/532 [04:26<04:07,  1.12it/s]Loading train:  48%|████▊     | 256/532 [04:27<04:09,  1.10it/s]Loading train:  48%|████▊     | 257/532 [04:28<04:33,  1.01it/s]Loading train:  48%|████▊     | 258/532 [04:29<04:41,  1.03s/it]Loading train:  49%|████▊     | 259/532 [04:30<04:41,  1.03s/it]Loading train:  49%|████▉     | 260/532 [04:31<04:39,  1.03s/it]Loading train:  49%|████▉     | 261/532 [04:32<04:39,  1.03s/it]Loading train:  49%|████▉     | 262/532 [04:34<04:42,  1.04s/it]Loading train:  49%|████▉     | 263/532 [04:34<04:24,  1.02it/s]Loading train:  50%|████▉     | 264/532 [04:35<04:13,  1.06it/s]Loading train:  50%|████▉     | 265/532 [04:36<04:06,  1.08it/s]Loading train:  50%|█████     | 266/532 [04:37<03:54,  1.13it/s]Loading train:  50%|█████     | 267/532 [04:38<03:52,  1.14it/s]Loading train:  50%|█████     | 268/532 [04:39<03:44,  1.18it/s]Loading train:  51%|█████     | 269/532 [04:40<03:54,  1.12it/s]Loading train:  51%|█████     | 270/532 [04:41<04:03,  1.07it/s]Loading train:  51%|█████     | 271/532 [04:41<04:00,  1.08it/s]Loading train:  51%|█████     | 272/532 [04:42<04:00,  1.08it/s]Loading train:  51%|█████▏    | 273/532 [04:43<04:04,  1.06it/s]Loading train:  52%|█████▏    | 274/532 [04:44<04:03,  1.06it/s]Loading train:  52%|█████▏    | 275/532 [04:46<04:26,  1.04s/it]Loading train:  52%|█████▏    | 276/532 [04:47<04:27,  1.05s/it]Loading train:  52%|█████▏    | 277/532 [04:48<04:35,  1.08s/it]Loading train:  52%|█████▏    | 278/532 [04:49<04:34,  1.08s/it]Loading train:  52%|█████▏    | 279/532 [04:50<04:36,  1.09s/it]Loading train:  53%|█████▎    | 280/532 [04:51<04:39,  1.11s/it]Loading train:  53%|█████▎    | 281/532 [04:52<04:36,  1.10s/it]Loading train:  53%|█████▎    | 282/532 [04:53<04:33,  1.09s/it]Loading train:  53%|█████▎    | 283/532 [04:54<04:28,  1.08s/it]Loading train:  53%|█████▎    | 284/532 [04:55<04:24,  1.07s/it]Loading train:  54%|█████▎    | 285/532 [04:56<04:20,  1.06s/it]Loading train:  54%|█████▍    | 286/532 [04:57<04:21,  1.06s/it]Loading train:  54%|█████▍    | 287/532 [04:58<04:10,  1.02s/it]Loading train:  54%|█████▍    | 288/532 [04:59<03:59,  1.02it/s]Loading train:  54%|█████▍    | 289/532 [05:00<03:54,  1.04it/s]Loading train:  55%|█████▍    | 290/532 [05:01<03:50,  1.05it/s]Loading train:  55%|█████▍    | 291/532 [05:02<03:46,  1.06it/s]Loading train:  55%|█████▍    | 292/532 [05:03<03:39,  1.09it/s]Loading train:  55%|█████▌    | 293/532 [05:04<03:51,  1.03it/s]Loading train:  55%|█████▌    | 294/532 [05:05<03:51,  1.03it/s]Loading train:  55%|█████▌    | 295/532 [05:06<03:57,  1.00s/it]Loading train:  56%|█████▌    | 296/532 [05:07<03:57,  1.00s/it]Loading train:  56%|█████▌    | 297/532 [05:08<03:58,  1.02s/it]Loading train:  56%|█████▌    | 298/532 [05:09<03:55,  1.01s/it]Loading train:  56%|█████▌    | 299/532 [05:10<03:50,  1.01it/s]Loading train:  56%|█████▋    | 300/532 [05:11<03:36,  1.07it/s]Loading train:  57%|█████▋    | 301/532 [05:12<03:22,  1.14it/s]Loading train:  57%|█████▋    | 302/532 [05:12<03:17,  1.16it/s]Loading train:  57%|█████▋    | 303/532 [05:13<03:14,  1.18it/s]Loading train:  57%|█████▋    | 304/532 [05:14<03:11,  1.19it/s]Loading train:  57%|█████▋    | 305/532 [05:15<03:31,  1.07it/s]Loading train:  58%|█████▊    | 306/532 [05:16<03:46,  1.00s/it]Loading train:  58%|█████▊    | 307/532 [05:18<04:05,  1.09s/it]Loading train:  58%|█████▊    | 308/532 [05:19<04:16,  1.14s/it]Loading train:  58%|█████▊    | 309/532 [05:20<04:21,  1.17s/it]Loading train:  58%|█████▊    | 310/532 [05:21<04:21,  1.18s/it]Loading train:  58%|█████▊    | 311/532 [05:23<04:50,  1.31s/it]Loading train:  59%|█████▊    | 312/532 [05:25<05:03,  1.38s/it]Loading train:  59%|█████▉    | 313/532 [05:26<05:16,  1.44s/it]Loading train:  59%|█████▉    | 314/532 [05:28<05:13,  1.44s/it]Loading train:  59%|█████▉    | 315/532 [05:29<05:15,  1.45s/it]Loading train:  59%|█████▉    | 316/532 [05:31<05:19,  1.48s/it]Loading train:  60%|█████▉    | 317/532 [05:32<04:49,  1.35s/it]Loading train:  60%|█████▉    | 318/532 [05:32<04:18,  1.21s/it]Loading train:  60%|█████▉    | 319/532 [05:33<03:57,  1.12s/it]Loading train:  60%|██████    | 320/532 [05:34<03:41,  1.04s/it]Loading train:  60%|██████    | 321/532 [05:35<03:34,  1.02s/it]Loading train:  61%|██████    | 322/532 [05:36<03:25,  1.02it/s]Loading train:  61%|██████    | 323/532 [05:37<03:44,  1.07s/it]Loading train:  61%|██████    | 324/532 [05:39<03:50,  1.11s/it]Loading train:  61%|██████    | 325/532 [05:40<03:55,  1.14s/it]Loading train:  61%|██████▏   | 326/532 [05:41<03:55,  1.15s/it]Loading train:  61%|██████▏   | 327/532 [05:42<03:57,  1.16s/it]Loading train:  62%|██████▏   | 328/532 [05:43<04:02,  1.19s/it]Loading train:  62%|██████▏   | 329/532 [05:44<03:54,  1.15s/it]Loading train:  62%|██████▏   | 330/532 [05:45<03:42,  1.10s/it]Loading train:  62%|██████▏   | 331/532 [05:46<03:33,  1.06s/it]Loading train:  62%|██████▏   | 332/532 [05:47<03:28,  1.04s/it]Loading train:  63%|██████▎   | 333/532 [05:48<03:22,  1.02s/it]Loading train:  63%|██████▎   | 334/532 [05:49<03:19,  1.01s/it]Loading train:  63%|██████▎   | 335/532 [05:50<03:21,  1.02s/it]Loading train:  63%|██████▎   | 336/532 [05:52<03:24,  1.04s/it]Loading train:  63%|██████▎   | 337/532 [05:53<03:26,  1.06s/it]Loading train:  64%|██████▎   | 338/532 [05:54<03:25,  1.06s/it]Loading train:  64%|██████▎   | 339/532 [05:55<03:26,  1.07s/it]Loading train:  64%|██████▍   | 340/532 [05:56<03:20,  1.05s/it]Loading train:  64%|██████▍   | 341/532 [05:57<03:13,  1.02s/it]Loading train:  64%|██████▍   | 342/532 [05:58<03:07,  1.01it/s]Loading train:  64%|██████▍   | 343/532 [05:58<02:59,  1.05it/s]Loading train:  65%|██████▍   | 344/532 [05:59<02:57,  1.06it/s]Loading train:  65%|██████▍   | 345/532 [06:00<02:56,  1.06it/s]Loading train:  65%|██████▌   | 346/532 [06:01<02:53,  1.07it/s]Loading train:  65%|██████▌   | 347/532 [06:02<02:59,  1.03it/s]Loading train:  65%|██████▌   | 348/532 [06:03<02:56,  1.04it/s]Loading train:  66%|██████▌   | 349/532 [06:04<02:53,  1.06it/s]Loading train:  66%|██████▌   | 350/532 [06:05<02:52,  1.05it/s]Loading train:  66%|██████▌   | 351/532 [06:06<02:47,  1.08it/s]Loading train:  66%|██████▌   | 352/532 [06:07<02:44,  1.09it/s]Loading train:  66%|██████▋   | 353/532 [06:08<02:53,  1.03it/s]Loading train:  67%|██████▋   | 354/532 [06:09<02:53,  1.03it/s]Loading train:  67%|██████▋   | 355/532 [06:10<02:49,  1.05it/s]Loading train:  67%|██████▋   | 356/532 [06:11<02:46,  1.06it/s]Loading train:  67%|██████▋   | 357/532 [06:12<02:48,  1.04it/s]Loading train:  67%|██████▋   | 358/532 [06:13<02:50,  1.02it/s]Loading train:  67%|██████▋   | 359/532 [06:14<02:44,  1.05it/s]Loading train:  68%|██████▊   | 360/532 [06:15<02:38,  1.08it/s]Loading train:  68%|██████▊   | 361/532 [06:15<02:33,  1.11it/s]Loading train:  68%|██████▊   | 362/532 [06:16<02:32,  1.12it/s]Loading train:  68%|██████▊   | 363/532 [06:17<02:31,  1.11it/s]Loading train:  68%|██████▊   | 364/532 [06:18<02:28,  1.13it/s]Loading train:  69%|██████▊   | 365/532 [06:19<02:28,  1.12it/s]Loading train:  69%|██████▉   | 366/532 [06:20<02:24,  1.15it/s]Loading train:  69%|██████▉   | 367/532 [06:21<02:20,  1.18it/s]Loading train:  69%|██████▉   | 368/532 [06:21<02:21,  1.16it/s]Loading train:  69%|██████▉   | 369/532 [06:22<02:19,  1.17it/s]Loading train:  70%|██████▉   | 370/532 [06:23<02:20,  1.16it/s]Loading train:  70%|██████▉   | 371/532 [06:24<02:37,  1.02it/s]Loading train:  70%|██████▉   | 372/532 [06:26<02:46,  1.04s/it]Loading train:  70%|███████   | 373/532 [06:27<02:52,  1.09s/it]Loading train:  70%|███████   | 374/532 [06:28<02:55,  1.11s/it]Loading train:  70%|███████   | 375/532 [06:29<02:53,  1.11s/it]Loading train:  71%|███████   | 376/532 [06:30<02:51,  1.10s/it]Loading train:  71%|███████   | 377/532 [06:31<02:42,  1.05s/it]Loading train:  71%|███████   | 378/532 [06:32<02:38,  1.03s/it]Loading train:  71%|███████   | 379/532 [06:33<02:32,  1.00it/s]Loading train:  71%|███████▏  | 380/532 [06:34<02:30,  1.01it/s]Loading train:  72%|███████▏  | 381/532 [06:35<02:25,  1.03it/s]Loading train:  72%|███████▏  | 382/532 [06:36<02:20,  1.07it/s]Loading train:  72%|███████▏  | 383/532 [06:37<02:19,  1.07it/s]Loading train:  72%|███████▏  | 384/532 [06:38<02:21,  1.04it/s]Loading train:  72%|███████▏  | 385/532 [06:39<02:18,  1.06it/s]Loading train:  73%|███████▎  | 386/532 [06:40<02:21,  1.03it/s]Loading train:  73%|███████▎  | 387/532 [06:41<02:21,  1.02it/s]Loading train:  73%|███████▎  | 388/532 [06:42<02:21,  1.02it/s]Loading train:  73%|███████▎  | 389/532 [06:43<02:25,  1.02s/it]Loading train:  73%|███████▎  | 390/532 [06:44<02:25,  1.03s/it]Loading train:  73%|███████▎  | 391/532 [06:45<02:23,  1.02s/it]Loading train:  74%|███████▎  | 392/532 [06:46<02:22,  1.02s/it]Loading train:  74%|███████▍  | 393/532 [06:47<02:19,  1.01s/it]Loading train:  74%|███████▍  | 394/532 [06:48<02:19,  1.01s/it]Loading train:  74%|███████▍  | 395/532 [06:49<02:21,  1.04s/it]Loading train:  74%|███████▍  | 396/532 [06:50<02:21,  1.04s/it]Loading train:  75%|███████▍  | 397/532 [06:51<02:22,  1.05s/it]Loading train:  75%|███████▍  | 398/532 [06:52<02:19,  1.04s/it]Loading train:  75%|███████▌  | 399/532 [06:53<02:20,  1.06s/it]Loading train:  75%|███████▌  | 400/532 [06:54<02:18,  1.05s/it]Loading train:  75%|███████▌  | 401/532 [06:55<02:19,  1.07s/it]Loading train:  76%|███████▌  | 402/532 [06:56<02:16,  1.05s/it]Loading train:  76%|███████▌  | 403/532 [06:57<02:13,  1.03s/it]Loading train:  76%|███████▌  | 404/532 [06:58<02:11,  1.03s/it]Loading train:  76%|███████▌  | 405/532 [06:59<02:11,  1.04s/it]Loading train:  76%|███████▋  | 406/532 [07:00<02:08,  1.02s/it]Loading train:  77%|███████▋  | 407/532 [07:01<02:08,  1.03s/it]Loading train:  77%|███████▋  | 408/532 [07:02<02:06,  1.02s/it]Loading train:  77%|███████▋  | 409/532 [07:03<02:04,  1.01s/it]Loading train:  77%|███████▋  | 410/532 [07:04<02:05,  1.03s/it]Loading train:  77%|███████▋  | 411/532 [07:05<02:01,  1.01s/it]Loading train:  77%|███████▋  | 412/532 [07:07<02:06,  1.06s/it]Loading train:  78%|███████▊  | 413/532 [07:08<02:02,  1.03s/it]Loading train:  78%|███████▊  | 414/532 [07:08<01:55,  1.02it/s]Loading train:  78%|███████▊  | 415/532 [07:09<01:51,  1.05it/s]Loading train:  78%|███████▊  | 416/532 [07:10<01:48,  1.07it/s]Loading train:  78%|███████▊  | 417/532 [07:11<01:43,  1.11it/s]Loading train:  79%|███████▊  | 418/532 [07:12<01:45,  1.08it/s]Loading train:  79%|███████▉  | 419/532 [07:13<01:51,  1.02it/s]Loading train:  79%|███████▉  | 420/532 [07:14<01:51,  1.00it/s]Loading train:  79%|███████▉  | 421/532 [07:15<01:53,  1.02s/it]Loading train:  79%|███████▉  | 422/532 [07:16<01:53,  1.03s/it]Loading train:  80%|███████▉  | 423/532 [07:17<01:52,  1.04s/it]Loading train:  80%|███████▉  | 424/532 [07:18<01:51,  1.03s/it]Loading train:  80%|███████▉  | 425/532 [07:19<01:49,  1.03s/it]Loading train:  80%|████████  | 426/532 [07:20<01:48,  1.02s/it]Loading train:  80%|████████  | 427/532 [07:21<01:44,  1.00it/s]Loading train:  80%|████████  | 428/532 [07:22<01:44,  1.00s/it]Loading train:  81%|████████  | 429/532 [07:23<01:42,  1.00it/s]Loading train:  81%|████████  | 430/532 [07:24<01:42,  1.00s/it]Loading train:  81%|████████  | 431/532 [07:25<01:43,  1.02s/it]Loading train:  81%|████████  | 432/532 [07:27<01:45,  1.05s/it]Loading train:  81%|████████▏ | 433/532 [07:28<01:45,  1.06s/it]Loading train:  82%|████████▏ | 434/532 [07:29<01:46,  1.08s/it]Loading train:  82%|████████▏ | 435/532 [07:30<01:46,  1.10s/it]Loading train:  82%|████████▏ | 436/532 [07:31<01:45,  1.09s/it]Loading train:  82%|████████▏ | 437/532 [07:32<01:39,  1.05s/it]Loading train:  82%|████████▏ | 438/532 [07:33<01:32,  1.01it/s]Loading train:  83%|████████▎ | 439/532 [07:34<01:30,  1.03it/s]Loading train:  83%|████████▎ | 440/532 [07:35<01:25,  1.07it/s]Loading train:  83%|████████▎ | 441/532 [07:35<01:22,  1.10it/s]Loading train:  83%|████████▎ | 442/532 [07:36<01:18,  1.14it/s]Loading train:  83%|████████▎ | 443/532 [07:37<01:18,  1.14it/s]Loading train:  83%|████████▎ | 444/532 [07:38<01:16,  1.15it/s]Loading train:  84%|████████▎ | 445/532 [07:39<01:14,  1.16it/s]Loading train:  84%|████████▍ | 446/532 [07:40<01:13,  1.17it/s]Loading train:  84%|████████▍ | 447/532 [07:40<01:13,  1.16it/s]Loading train:  84%|████████▍ | 448/532 [07:41<01:13,  1.14it/s]Loading train:  84%|████████▍ | 449/532 [07:42<01:13,  1.13it/s]Loading train:  85%|████████▍ | 450/532 [07:43<01:13,  1.12it/s]Loading train:  85%|████████▍ | 451/532 [07:44<01:12,  1.11it/s]Loading train:  85%|████████▍ | 452/532 [07:45<01:11,  1.12it/s]Loading train:  85%|████████▌ | 453/532 [07:46<01:10,  1.11it/s]Loading train:  85%|████████▌ | 454/532 [07:47<01:09,  1.13it/s]Loading train:  86%|████████▌ | 455/532 [07:48<01:11,  1.07it/s]Loading train:  86%|████████▌ | 456/532 [07:49<01:10,  1.08it/s]Loading train:  86%|████████▌ | 457/532 [07:50<01:11,  1.05it/s]Loading train:  86%|████████▌ | 458/532 [07:51<01:12,  1.03it/s]Loading train:  86%|████████▋ | 459/532 [07:52<01:10,  1.03it/s]Loading train:  86%|████████▋ | 460/532 [07:53<01:10,  1.02it/s]Loading train:  87%|████████▋ | 461/532 [07:54<01:14,  1.04s/it]Loading train:  87%|████████▋ | 462/532 [07:55<01:15,  1.08s/it]Loading train:  87%|████████▋ | 463/532 [07:56<01:16,  1.10s/it]Loading train:  87%|████████▋ | 464/532 [07:57<01:16,  1.13s/it]Loading train:  87%|████████▋ | 465/532 [07:59<01:14,  1.12s/it]Loading train:  88%|████████▊ | 466/532 [08:00<01:15,  1.14s/it]Loading train:  88%|████████▊ | 467/532 [08:01<01:11,  1.10s/it]Loading train:  88%|████████▊ | 468/532 [08:02<01:06,  1.04s/it]Loading train:  88%|████████▊ | 469/532 [08:03<01:02,  1.00it/s]Loading train:  88%|████████▊ | 470/532 [08:03<01:00,  1.03it/s]Loading train:  89%|████████▊ | 471/532 [08:04<00:59,  1.02it/s]Loading train:  89%|████████▊ | 472/532 [08:05<00:58,  1.02it/s]Loading train:  89%|████████▉ | 473/532 [08:06<00:57,  1.02it/s]Loading train:  89%|████████▉ | 474/532 [08:07<00:57,  1.01it/s]Loading train:  89%|████████▉ | 475/532 [08:08<00:57,  1.01s/it]Loading train:  89%|████████▉ | 476/532 [08:09<00:56,  1.01s/it]Loading train:  90%|████████▉ | 477/532 [08:11<00:57,  1.05s/it]Loading train:  90%|████████▉ | 478/532 [08:12<00:56,  1.04s/it]Loading train:  90%|█████████ | 479/532 [08:13<00:54,  1.02s/it]Loading train:  90%|█████████ | 480/532 [08:14<00:51,  1.02it/s]Loading train:  90%|█████████ | 481/532 [08:14<00:48,  1.06it/s]Loading train:  91%|█████████ | 482/532 [08:15<00:47,  1.06it/s]Loading train:  91%|█████████ | 483/532 [08:16<00:46,  1.06it/s]Loading train:  91%|█████████ | 484/532 [08:17<00:44,  1.07it/s]Loading train:  91%|█████████ | 485/532 [08:18<00:45,  1.03it/s]Loading train:  91%|█████████▏| 486/532 [08:19<00:46,  1.00s/it]Loading train:  92%|█████████▏| 487/532 [08:20<00:45,  1.01s/it]Loading train:  92%|█████████▏| 488/532 [08:21<00:45,  1.04s/it]Loading train:  92%|█████████▏| 489/532 [08:23<00:45,  1.05s/it]Loading train:  92%|█████████▏| 490/532 [08:24<00:45,  1.09s/it]Loading train:  92%|█████████▏| 491/532 [08:25<00:42,  1.05s/it]Loading train:  92%|█████████▏| 492/532 [08:26<00:40,  1.01s/it]Loading train:  93%|█████████▎| 493/532 [08:27<00:39,  1.01s/it]Loading train:  93%|█████████▎| 494/532 [08:27<00:37,  1.02it/s]Loading train:  93%|█████████▎| 495/532 [08:28<00:35,  1.05it/s]Loading train:  93%|█████████▎| 496/532 [08:29<00:33,  1.06it/s]Loading train:  93%|█████████▎| 497/532 [08:30<00:32,  1.07it/s]Loading train:  94%|█████████▎| 498/532 [08:31<00:32,  1.05it/s]Loading train:  94%|█████████▍| 499/532 [08:32<00:31,  1.05it/s]Loading train:  94%|█████████▍| 500/532 [08:33<00:30,  1.06it/s]Loading train:  94%|█████████▍| 501/532 [08:34<00:29,  1.04it/s]Loading train:  94%|█████████▍| 502/532 [08:35<00:28,  1.04it/s]Loading train:  95%|█████████▍| 503/532 [08:36<00:28,  1.03it/s]Loading train:  95%|█████████▍| 504/532 [08:37<00:26,  1.06it/s]Loading train:  95%|█████████▍| 505/532 [08:38<00:24,  1.09it/s]Loading train:  95%|█████████▌| 506/532 [08:39<00:23,  1.12it/s]Loading train:  95%|█████████▌| 507/532 [08:39<00:22,  1.13it/s]Loading train:  95%|█████████▌| 508/532 [08:40<00:21,  1.12it/s]Loading train:  96%|█████████▌| 509/532 [08:42<00:22,  1.02it/s]Loading train:  96%|█████████▌| 510/532 [08:43<00:22,  1.02s/it]Loading train:  96%|█████████▌| 511/532 [08:44<00:21,  1.03s/it]Loading train:  96%|█████████▌| 512/532 [08:45<00:21,  1.06s/it]Loading train:  96%|█████████▋| 513/532 [08:46<00:20,  1.07s/it]Loading train:  97%|█████████▋| 514/532 [08:47<00:19,  1.08s/it]Loading train:  97%|█████████▋| 515/532 [08:48<00:17,  1.04s/it]Loading train:  97%|█████████▋| 516/532 [08:49<00:15,  1.01it/s]Loading train:  97%|█████████▋| 517/532 [08:50<00:14,  1.04it/s]Loading train:  97%|█████████▋| 518/532 [08:51<00:13,  1.06it/s]Loading train:  98%|█████████▊| 519/532 [08:52<00:12,  1.07it/s]Loading train:  98%|█████████▊| 520/532 [08:52<00:11,  1.08it/s]Loading train:  98%|█████████▊| 521/532 [08:54<00:10,  1.03it/s]Loading train:  98%|█████████▊| 522/532 [08:55<00:09,  1.01it/s]Loading train:  98%|█████████▊| 523/532 [08:56<00:09,  1.01s/it]Loading train:  98%|█████████▊| 524/532 [08:57<00:07,  1.01it/s]Loading train:  99%|█████████▊| 525/532 [08:58<00:07,  1.00s/it]Loading train:  99%|█████████▉| 526/532 [08:59<00:06,  1.01s/it]Loading train:  99%|█████████▉| 527/532 [09:00<00:04,  1.01it/s]Loading train:  99%|█████████▉| 528/532 [09:01<00:03,  1.03it/s]Loading train:  99%|█████████▉| 529/532 [09:01<00:02,  1.07it/s]Loading train: 100%|█████████▉| 530/532 [09:02<00:01,  1.07it/s]Loading train: 100%|█████████▉| 531/532 [09:03<00:00,  1.09it/s]Loading train: 100%|██████████| 532/532 [09:04<00:00,  1.08it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   4%|▍         | 23/532 [00:00<00:02, 229.94it/s]concatenating: train:  10%|▉         | 51/532 [00:00<00:01, 241.22it/s]concatenating: train:  15%|█▍        | 79/532 [00:00<00:01, 250.60it/s]concatenating: train:  20%|██        | 107/532 [00:00<00:01, 258.21it/s]concatenating: train:  25%|██▌       | 134/532 [00:00<00:01, 259.99it/s]concatenating: train:  30%|███       | 162/532 [00:00<00:01, 265.46it/s]concatenating: train:  36%|███▌      | 190/532 [00:00<00:01, 265.81it/s]concatenating: train:  41%|████      | 216/532 [00:00<00:01, 261.31it/s]concatenating: train:  45%|████▌     | 241/532 [00:00<00:01, 239.12it/s]concatenating: train:  51%|█████     | 270/532 [00:01<00:01, 251.20it/s]concatenating: train:  56%|█████▌    | 297/532 [00:01<00:00, 254.30it/s]concatenating: train:  61%|██████    | 324/532 [00:01<00:00, 258.55it/s]concatenating: train:  66%|██████▌   | 352/532 [00:01<00:00, 264.36it/s]concatenating: train:  71%|███████   | 379/532 [00:01<00:00, 261.98it/s]concatenating: train:  76%|███████▋  | 406/532 [00:01<00:00, 258.68it/s]concatenating: train:  81%|████████  | 432/532 [00:01<00:00, 254.57it/s]concatenating: train:  86%|████████▋ | 459/532 [00:01<00:00, 258.50it/s]concatenating: train:  91%|█████████ | 485/532 [00:01<00:00, 253.84it/s]concatenating: train:  97%|█████████▋| 517/532 [00:01<00:00, 270.11it/s]concatenating: train: 100%|██████████| 532/532 [00:02<00:00, 262.42it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:12,  1.12it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:11,  1.10it/s]Loading test:  20%|██        | 3/15 [00:02<00:11,  1.05it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:10,  1.01it/s]Loading test:  33%|███▎      | 5/15 [00:05<00:10,  1.06s/it]Loading test:  40%|████      | 6/15 [00:06<00:09,  1.10s/it]Loading test:  47%|████▋     | 7/15 [00:07<00:08,  1.07s/it]Loading test:  53%|█████▎    | 8/15 [00:08<00:07,  1.11s/it]Loading test:  60%|██████    | 9/15 [00:09<00:06,  1.10s/it]Loading test:  67%|██████▋   | 10/15 [00:10<00:05,  1.03s/it]Loading test:  73%|███████▎  | 11/15 [00:11<00:04,  1.02s/it]Loading test:  80%|████████  | 12/15 [00:12<00:03,  1.06s/it]Loading test:  87%|████████▋ | 13/15 [00:13<00:02,  1.08s/it]Loading test:  93%|█████████▎| 14/15 [00:14<00:01,  1.07s/it]Loading test: 100%|██████████| 15/15 [00:15<00:00,  1.06s/it]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation:  80%|████████  | 12/15 [00:00<00:00, 117.95it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 115.53it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 56, 15)   150         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 56, 15)   60          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 56, 15)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 56, 15)   2040        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 56, 15)   60          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 56, 15)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 16)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 16)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 28, 16)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 30)   4350        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 30)   8130        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 30)   120         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 30)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 46)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 46)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 14, 46)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 14, 60)   24900       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 14, 60)   240         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 14, 60)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 14, 60)   32460       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 14, 60)   240         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 14, 60)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 14, 106)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 106)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 30)   12750       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 76)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 30)   20550       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 30)   120         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 30)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 30)   8130        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 30)   120         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 30)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 106)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-06 04:04:38.883148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 04:04:38.883250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 04:04:38.883264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 04:04:38.883273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 04:04:38.883671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 28, 28, 106)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 56, 15)   6375        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 56, 31)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 56, 15)   4200        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 56, 15)   60          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 56, 15)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 56, 15)   2040        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 56, 15)   60          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 56, 15)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 56, 46)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 56, 46)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 56, 13)   611         dropout_5[0][0]                  
==================================================================================================
Total params: 127,886
Trainable params: 127,286
Non-trainable params: 600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53974061e-02 2.89048015e-02 1.16758472e-01 1.00223856e-02
 3.03440156e-02 5.80063118e-03 6.84518755e-02 1.28261328e-01
 7.55818021e-02 1.22545826e-02 2.73712232e-01 1.84335085e-01
 1.75382711e-04]
Train on 33496 samples, validate on 969 samples
Epoch 1/300
 - 29s - loss: 31.9697 - acc: 0.8630 - mDice: 0.0269 - val_loss: 3.7784 - val_acc: 0.9217 - val_mDice: 0.0372

Epoch 00001: val_mDice improved from -inf to 0.03724, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 21s - loss: 4.2219 - acc: 0.9066 - mDice: 0.0774 - val_loss: 2.6258 - val_acc: 0.9256 - val_mDice: 0.1302

Epoch 00002: val_mDice improved from 0.03724 to 0.13020, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 21s - loss: 2.9149 - acc: 0.9174 - mDice: 0.1950 - val_loss: 1.6852 - val_acc: 0.9475 - val_mDice: 0.3534

Epoch 00003: val_mDice improved from 0.13020 to 0.35338, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 22s - loss: 2.2181 - acc: 0.9322 - mDice: 0.3197 - val_loss: 1.2570 - val_acc: 0.9630 - val_mDice: 0.4982

Epoch 00004: val_mDice improved from 0.35338 to 0.49821, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300
 - 21s - loss: 1.8383 - acc: 0.9422 - mDice: 0.4076 - val_loss: 1.0288 - val_acc: 0.9689 - val_mDice: 0.5813

Epoch 00005: val_mDice improved from 0.49821 to 0.58135, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 21s - loss: 1.6192 - acc: 0.9476 - mDice: 0.4657 - val_loss: 0.9726 - val_acc: 0.9722 - val_mDice: 0.6186

Epoch 00006: val_mDice improved from 0.58135 to 0.61861, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 21s - loss: 1.4784 - acc: 0.9510 - mDice: 0.5061 - val_loss: 0.8992 - val_acc: 0.9740 - val_mDice: 0.6456

Epoch 00007: val_mDice improved from 0.61861 to 0.64560, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300
 - 22s - loss: 1.3659 - acc: 0.9535 - mDice: 0.5374 - val_loss: 0.8390 - val_acc: 0.9749 - val_mDice: 0.6694

Epoch 00008: val_mDice improved from 0.64560 to 0.66936, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 21s - loss: 1.2860 - acc: 0.9556 - mDice: 0.5607 - val_loss: 0.8296 - val_acc: 0.9756 - val_mDice: 0.6810

Epoch 00009: val_mDice improved from 0.66936 to 0.68105, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 20s - loss: 1.2190 - acc: 0.9572 - mDice: 0.5797 - val_loss: 0.8159 - val_acc: 0.9758 - val_mDice: 0.6875

Epoch 00010: val_mDice improved from 0.68105 to 0.68751, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300
 - 21s - loss: 1.1701 - acc: 0.9584 - mDice: 0.5949 - val_loss: 0.7850 - val_acc: 0.9772 - val_mDice: 0.6970

Epoch 00011: val_mDice improved from 0.68751 to 0.69696, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300
 - 22s - loss: 1.1240 - acc: 0.9594 - mDice: 0.6084 - val_loss: 0.7697 - val_acc: 0.9771 - val_mDice: 0.7026

Epoch 00012: val_mDice improved from 0.69696 to 0.70257, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 13/300
 - 20s - loss: 1.0861 - acc: 0.9602 - mDice: 0.6190 - val_loss: 0.7683 - val_acc: 0.9776 - val_mDice: 0.7053

Epoch 00013: val_mDice improved from 0.70257 to 0.70528, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 14/300
 - 21s - loss: 1.0525 - acc: 0.9609 - mDice: 0.6287 - val_loss: 0.7538 - val_acc: 0.9780 - val_mDice: 0.7101

Epoch 00014: val_mDice improved from 0.70528 to 0.71012, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 15/300
 - 21s - loss: 1.0219 - acc: 0.9615 - mDice: 0.6375 - val_loss: 0.7795 - val_acc: 0.9781 - val_mDice: 0.7024

Epoch 00015: val_mDice did not improve from 0.71012
Epoch 16/300
 - 23s - loss: 0.9993 - acc: 0.9619 - mDice: 0.6450 - val_loss: 0.7102 - val_acc: 0.9785 - val_mDice: 0.7210

Epoch 00016: val_mDice improved from 0.71012 to 0.72104, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 17/300
 - 21s - loss: 0.9694 - acc: 0.9628 - mDice: 0.6542 - val_loss: 0.7112 - val_acc: 0.9783 - val_mDice: 0.7253

Epoch 00017: val_mDice improved from 0.72104 to 0.72529, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300
 - 21s - loss: 0.9489 - acc: 0.9634 - mDice: 0.6605 - val_loss: 0.7287 - val_acc: 0.9785 - val_mDice: 0.7206

Epoch 00018: val_mDice did not improve from 0.72529
Epoch 19/300
 - 21s - loss: 0.9322 - acc: 0.9638 - mDice: 0.6660 - val_loss: 0.7298 - val_acc: 0.9781 - val_mDice: 0.7205

Epoch 00019: val_mDice did not improve from 0.72529
Epoch 20/300
 - 21s - loss: 0.9165 - acc: 0.9642 - mDice: 0.6706 - val_loss: 0.7211 - val_acc: 0.9779 - val_mDice: 0.7240

Epoch 00020: val_mDice did not improve from 0.72529
Epoch 21/300
 - 22s - loss: 0.9018 - acc: 0.9646 - mDice: 0.6753 - val_loss: 0.7045 - val_acc: 0.9783 - val_mDice: 0.7268

Epoch 00021: val_mDice improved from 0.72529 to 0.72680, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300
 - 21s - loss: 0.8872 - acc: 0.9650 - mDice: 0.6801 - val_loss: 0.7007 - val_acc: 0.9779 - val_mDice: 0.7272

Epoch 00022: val_mDice improved from 0.72680 to 0.72716, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 23/300
 - 21s - loss: 0.8767 - acc: 0.9653 - mDice: 0.6837 - val_loss: 0.6829 - val_acc: 0.9794 - val_mDice: 0.7352

Epoch 00023: val_mDice improved from 0.72716 to 0.73521, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 24/300
 - 21s - loss: 0.8652 - acc: 0.9655 - mDice: 0.6875 - val_loss: 0.6894 - val_acc: 0.9793 - val_mDice: 0.7305

Epoch 00024: val_mDice did not improve from 0.73521
Epoch 25/300
 - 22s - loss: 0.8531 - acc: 0.9658 - mDice: 0.6917 - val_loss: 0.6930 - val_acc: 0.9791 - val_mDice: 0.7366

Epoch 00025: val_mDice improved from 0.73521 to 0.73657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 26/300
 - 21s - loss: 0.8445 - acc: 0.9660 - mDice: 0.6949 - val_loss: 0.6852 - val_acc: 0.9788 - val_mDice: 0.7336

Epoch 00026: val_mDice did not improve from 0.73657
Epoch 27/300
 - 20s - loss: 0.8351 - acc: 0.9662 - mDice: 0.6981 - val_loss: 0.7103 - val_acc: 0.9795 - val_mDice: 0.7307

Epoch 00027: val_mDice did not improve from 0.73657
Epoch 28/300
 - 20s - loss: 0.8274 - acc: 0.9664 - mDice: 0.7007 - val_loss: 0.6937 - val_acc: 0.9798 - val_mDice: 0.7392

Epoch 00028: val_mDice improved from 0.73657 to 0.73917, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 29/300
 - 22s - loss: 0.8208 - acc: 0.9666 - mDice: 0.7028 - val_loss: 0.6703 - val_acc: 0.9795 - val_mDice: 0.7440

Epoch 00029: val_mDice improved from 0.73917 to 0.74403, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 30/300
 - 21s - loss: 0.8103 - acc: 0.9668 - mDice: 0.7064 - val_loss: 0.6717 - val_acc: 0.9803 - val_mDice: 0.7427

Epoch 00030: val_mDice did not improve from 0.74403
Epoch 31/300
 - 20s - loss: 0.8023 - acc: 0.9670 - mDice: 0.7089 - val_loss: 0.6804 - val_acc: 0.9795 - val_mDice: 0.7403

Epoch 00031: val_mDice did not improve from 0.74403
Epoch 32/300
 - 21s - loss: 0.7997 - acc: 0.9670 - mDice: 0.7099 - val_loss: 0.6813 - val_acc: 0.9800 - val_mDice: 0.7385

Epoch 00032: val_mDice did not improve from 0.74403
Epoch 33/300
 - 22s - loss: 0.7944 - acc: 0.9671 - mDice: 0.7118 - val_loss: 0.6670 - val_acc: 0.9797 - val_mDice: 0.7451

Epoch 00033: val_mDice improved from 0.74403 to 0.74506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 34/300
 - 21s - loss: 0.7866 - acc: 0.9672 - mDice: 0.7142 - val_loss: 0.6716 - val_acc: 0.9795 - val_mDice: 0.7415

Epoch 00034: val_mDice did not improve from 0.74506
Epoch 35/300
 - 20s - loss: 0.7826 - acc: 0.9674 - mDice: 0.7160 - val_loss: 0.6678 - val_acc: 0.9802 - val_mDice: 0.7394

Epoch 00035: val_mDice did not improve from 0.74506
Epoch 36/300
 - 21s - loss: 0.7772 - acc: 0.9675 - mDice: 0.7176 - val_loss: 0.6552 - val_acc: 0.9793 - val_mDice: 0.7468

Epoch 00036: val_mDice improved from 0.74506 to 0.74678, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 37/300
 - 22s - loss: 0.7737 - acc: 0.9676 - mDice: 0.7189 - val_loss: 0.6640 - val_acc: 0.9796 - val_mDice: 0.7421

Epoch 00037: val_mDice did not improve from 0.74678
Epoch 38/300
 - 21s - loss: 0.7679 - acc: 0.9677 - mDice: 0.7210 - val_loss: 0.6660 - val_acc: 0.9798 - val_mDice: 0.7431

Epoch 00038: val_mDice did not improve from 0.74678
Epoch 39/300
 - 20s - loss: 0.7662 - acc: 0.9677 - mDice: 0.7215 - val_loss: 0.6586 - val_acc: 0.9799 - val_mDice: 0.7484

Epoch 00039: val_mDice improved from 0.74678 to 0.74842, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 40/300
 - 21s - loss: 0.7607 - acc: 0.9678 - mDice: 0.7231 - val_loss: 0.6480 - val_acc: 0.9799 - val_mDice: 0.7495

Epoch 00040: val_mDice improved from 0.74842 to 0.74950, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 41/300
 - 22s - loss: 0.7577 - acc: 0.9679 - mDice: 0.7244 - val_loss: 0.6478 - val_acc: 0.9802 - val_mDice: 0.7501

Epoch 00041: val_mDice improved from 0.74950 to 0.75007, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 42/300
 - 21s - loss: 0.7536 - acc: 0.9679 - mDice: 0.7258 - val_loss: 0.6479 - val_acc: 0.9804 - val_mDice: 0.7510

Epoch 00042: val_mDice improved from 0.75007 to 0.75101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 43/300
 - 20s - loss: 0.7502 - acc: 0.9680 - mDice: 0.7266 - val_loss: 0.6440 - val_acc: 0.9804 - val_mDice: 0.7495

Epoch 00043: val_mDice did not improve from 0.75101
Epoch 44/300
 - 21s - loss: 0.7489 - acc: 0.9681 - mDice: 0.7273 - val_loss: 0.6498 - val_acc: 0.9806 - val_mDice: 0.7499

Epoch 00044: val_mDice did not improve from 0.75101
Epoch 45/300
 - 22s - loss: 0.7458 - acc: 0.9682 - mDice: 0.7276 - val_loss: 0.6631 - val_acc: 0.9802 - val_mDice: 0.7480

Epoch 00045: val_mDice did not improve from 0.75101
Epoch 46/300
 - 21s - loss: 0.7458 - acc: 0.9681 - mDice: 0.7282 - val_loss: 0.6473 - val_acc: 0.9799 - val_mDice: 0.7511

Epoch 00046: val_mDice improved from 0.75101 to 0.75106, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 47/300
 - 20s - loss: 0.7383 - acc: 0.9683 - mDice: 0.7308 - val_loss: 0.6491 - val_acc: 0.9803 - val_mDice: 0.7515

Epoch 00047: val_mDice improved from 0.75106 to 0.75147, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 48/300
 - 20s - loss: 0.7359 - acc: 0.9683 - mDice: 0.7311 - val_loss: 0.6499 - val_acc: 0.9804 - val_mDice: 0.7508

Epoch 00048: val_mDice did not improve from 0.75147
Epoch 49/300
 - 22s - loss: 0.7357 - acc: 0.9684 - mDice: 0.7314 - val_loss: 0.6509 - val_acc: 0.9807 - val_mDice: 0.7512

Epoch 00049: val_mDice did not improve from 0.75147
Epoch 50/300
 - 21s - loss: 0.7310 - acc: 0.9685 - mDice: 0.7330 - val_loss: 0.6428 - val_acc: 0.9810 - val_mDice: 0.7532

Epoch 00050: val_mDice improved from 0.75147 to 0.75317, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 51/300
 - 21s - loss: 0.7296 - acc: 0.9685 - mDice: 0.7333 - val_loss: 0.6494 - val_acc: 0.9802 - val_mDice: 0.7516

Epoch 00051: val_mDice did not improve from 0.75317
Epoch 52/300
 - 21s - loss: 0.7282 - acc: 0.9685 - mDice: 0.7339 - val_loss: 0.6442 - val_acc: 0.9808 - val_mDice: 0.7534

Epoch 00052: val_mDice improved from 0.75317 to 0.75342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 53/300
 - 21s - loss: 0.7264 - acc: 0.9685 - mDice: 0.7346 - val_loss: 0.6580 - val_acc: 0.9803 - val_mDice: 0.7488

Epoch 00053: val_mDice did not improve from 0.75342
Epoch 54/300
 - 22s - loss: 0.7230 - acc: 0.9687 - mDice: 0.7359 - val_loss: 0.6439 - val_acc: 0.9802 - val_mDice: 0.7555

Epoch 00054: val_mDice improved from 0.75342 to 0.75552, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 55/300
 - 21s - loss: 0.7229 - acc: 0.9686 - mDice: 0.7358 - val_loss: 0.6550 - val_acc: 0.9803 - val_mDice: 0.7532

Epoch 00055: val_mDice did not improve from 0.75552
Epoch 56/300
 - 21s - loss: 0.7185 - acc: 0.9687 - mDice: 0.7371 - val_loss: 0.6549 - val_acc: 0.9811 - val_mDice: 0.7509

Epoch 00056: val_mDice did not improve from 0.75552
Epoch 57/300
 - 21s - loss: 0.7177 - acc: 0.9688 - mDice: 0.7377 - val_loss: 0.6450 - val_acc: 0.9804 - val_mDice: 0.7569

Epoch 00057: val_mDice improved from 0.75552 to 0.75687, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 58/300
 - 22s - loss: 0.7168 - acc: 0.9688 - mDice: 0.7378 - val_loss: 0.6402 - val_acc: 0.9809 - val_mDice: 0.7568

Epoch 00058: val_mDice did not improve from 0.75687
Epoch 59/300
 - 21s - loss: 0.7124 - acc: 0.9689 - mDice: 0.7394 - val_loss: 0.6331 - val_acc: 0.9808 - val_mDice: 0.7557

Epoch 00059: val_mDice did not improve from 0.75687
Epoch 60/300
 - 21s - loss: 0.7103 - acc: 0.9690 - mDice: 0.7401 - val_loss: 0.6629 - val_acc: 0.9804 - val_mDice: 0.7548

Epoch 00060: val_mDice did not improve from 0.75687
Epoch 61/300
 - 22s - loss: 0.7099 - acc: 0.9690 - mDice: 0.7403 - val_loss: 0.6429 - val_acc: 0.9804 - val_mDice: 0.7558

Epoch 00061: val_mDice did not improve from 0.75687
Epoch 62/300
 - 20s - loss: 0.7086 - acc: 0.9690 - mDice: 0.7406 - val_loss: 0.6502 - val_acc: 0.9812 - val_mDice: 0.7536

Epoch 00062: val_mDice did not improve from 0.75687
Epoch 63/300
 - 20s - loss: 0.7064 - acc: 0.9690 - mDice: 0.7414 - val_loss: 0.6472 - val_acc: 0.9810 - val_mDice: 0.7549

Epoch 00063: val_mDice did not improve from 0.75687
Epoch 64/300
 - 21s - loss: 0.7060 - acc: 0.9690 - mDice: 0.7416 - val_loss: 0.6465 - val_acc: 0.9813 - val_mDice: 0.7570

Epoch 00064: val_mDice improved from 0.75687 to 0.75700, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 65/300
 - 22s - loss: 0.7041 - acc: 0.9691 - mDice: 0.7422 - val_loss: 0.6458 - val_acc: 0.9811 - val_mDice: 0.7558

Epoch 00065: val_mDice did not improve from 0.75700
Epoch 66/300
 - 20s - loss: 0.7042 - acc: 0.9691 - mDice: 0.7422 - val_loss: 0.6517 - val_acc: 0.9809 - val_mDice: 0.7476

Epoch 00066: val_mDice did not improve from 0.75700
Epoch 67/300
 - 21s - loss: 0.7017 - acc: 0.9692 - mDice: 0.7426 - val_loss: 0.6471 - val_acc: 0.9808 - val_mDice: 0.7535

Epoch 00067: val_mDice did not improve from 0.75700
Epoch 68/300
 - 21s - loss: 0.7010 - acc: 0.9692 - mDice: 0.7434 - val_loss: 0.6458 - val_acc: 0.9813 - val_mDice: 0.7551

Epoch 00068: val_mDice did not improve from 0.75700
Epoch 69/300
 - 22s - loss: 0.6981 - acc: 0.9693 - mDice: 0.7441 - val_loss: 0.6514 - val_acc: 0.9811 - val_mDice: 0.7578

Epoch 00069: val_mDice improved from 0.75700 to 0.75778, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 70/300
 - 21s - loss: 0.6975 - acc: 0.9693 - mDice: 0.7443 - val_loss: 0.6384 - val_acc: 0.9814 - val_mDice: 0.7535

Epoch 00070: val_mDice did not improve from 0.75778
Epoch 71/300
 - 21s - loss: 0.6960 - acc: 0.9693 - mDice: 0.7451 - val_loss: 0.6598 - val_acc: 0.9807 - val_mDice: 0.7530

Epoch 00071: val_mDice did not improve from 0.75778
Epoch 72/300
 - 21s - loss: 0.6943 - acc: 0.9694 - mDice: 0.7454 - val_loss: 0.6415 - val_acc: 0.9813 - val_mDice: 0.7602

Epoch 00072: val_mDice improved from 0.75778 to 0.76019, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 73/300
 - 22s - loss: 0.6944 - acc: 0.9694 - mDice: 0.7457 - val_loss: 0.6433 - val_acc: 0.9812 - val_mDice: 0.7604

Epoch 00073: val_mDice improved from 0.76019 to 0.76043, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 74/300
 - 21s - loss: 0.6918 - acc: 0.9694 - mDice: 0.7463 - val_loss: 0.6431 - val_acc: 0.9812 - val_mDice: 0.7580

Epoch 00074: val_mDice did not improve from 0.76043
Epoch 75/300
 - 20s - loss: 0.6918 - acc: 0.9694 - mDice: 0.7464 - val_loss: 0.6648 - val_acc: 0.9809 - val_mDice: 0.7534

Epoch 00075: val_mDice did not improve from 0.76043
Epoch 76/300
 - 20s - loss: 0.6914 - acc: 0.9694 - mDice: 0.7463 - val_loss: 0.6402 - val_acc: 0.9810 - val_mDice: 0.7587

Epoch 00076: val_mDice did not improve from 0.76043
Epoch 77/300
 - 22s - loss: 0.6900 - acc: 0.9695 - mDice: 0.7471 - val_loss: 0.6402 - val_acc: 0.9814 - val_mDice: 0.7583

Epoch 00077: val_mDice did not improve from 0.76043
Epoch 78/300
 - 21s - loss: 0.6881 - acc: 0.9695 - mDice: 0.7475 - val_loss: 0.6526 - val_acc: 0.9810 - val_mDice: 0.7586

Epoch 00078: val_mDice did not improve from 0.76043
Epoch 79/300
 - 20s - loss: 0.6873 - acc: 0.9695 - mDice: 0.7481 - val_loss: 0.6553 - val_acc: 0.9808 - val_mDice: 0.7580

Epoch 00079: val_mDice did not improve from 0.76043
Epoch 80/300
 - 21s - loss: 0.6879 - acc: 0.9695 - mDice: 0.7475 - val_loss: 0.6452 - val_acc: 0.9810 - val_mDice: 0.7603

Epoch 00080: val_mDice did not improve from 0.76043
Epoch 81/300
 - 21s - loss: 0.6853 - acc: 0.9695 - mDice: 0.7485 - val_loss: 0.6393 - val_acc: 0.9812 - val_mDice: 0.7600

Epoch 00081: val_mDice did not improve from 0.76043
Epoch 82/300
 - 22s - loss: 0.6855 - acc: 0.9695 - mDice: 0.7483 - val_loss: 0.6401 - val_acc: 0.9814 - val_mDice: 0.7584

Epoch 00082: val_mDice did not improve from 0.76043
Epoch 83/300
 - 21s - loss: 0.6842 - acc: 0.9696 - mDice: 0.7488 - val_loss: 0.6491 - val_acc: 0.9811 - val_mDice: 0.7578

Epoch 00083: val_mDice did not improve from 0.76043
Epoch 84/300
 - 20s - loss: 0.6826 - acc: 0.9696 - mDice: 0.7496 - val_loss: 0.6603 - val_acc: 0.9812 - val_mDice: 0.7543

Epoch 00084: val_mDice did not improve from 0.76043
Epoch 85/300
 - 20s - loss: 0.6833 - acc: 0.9696 - mDice: 0.7490 - val_loss: 0.6427 - val_acc: 0.9813 - val_mDice: 0.7579

Epoch 00085: val_mDice did not improve from 0.76043
Epoch 86/300
 - 21s - loss: 0.6801 - acc: 0.9697 - mDice: 0.7504 - val_loss: 0.6363 - val_acc: 0.9814 - val_mDice: 0.7580

Epoch 00086: val_mDice did not improve from 0.76043
Epoch 87/300
 - 22s - loss: 0.6802 - acc: 0.9697 - mDice: 0.7499 - val_loss: 0.6379 - val_acc: 0.9808 - val_mDice: 0.7618

Epoch 00087: val_mDice improved from 0.76043 to 0.76183, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 88/300
 - 20s - loss: 0.6795 - acc: 0.9697 - mDice: 0.7506 - val_loss: 0.6415 - val_acc: 0.9812 - val_mDice: 0.7592

Epoch 00088: val_mDice did not improve from 0.76183
Epoch 89/300
 - 21s - loss: 0.6796 - acc: 0.9697 - mDice: 0.7503 - val_loss: 0.6479 - val_acc: 0.9814 - val_mDice: 0.7612

Epoch 00089: val_mDice did not improve from 0.76183
Epoch 90/300
 - 20s - loss: 0.6778 - acc: 0.9697 - mDice: 0.7510 - val_loss: 0.6578 - val_acc: 0.9806 - val_mDice: 0.7555

Epoch 00090: val_mDice did not improve from 0.76183
Epoch 91/300
 - 22s - loss: 0.6775 - acc: 0.9697 - mDice: 0.7513 - val_loss: 0.6305 - val_acc: 0.9814 - val_mDice: 0.7604

Epoch 00091: val_mDice did not improve from 0.76183
Epoch 92/300
 - 21s - loss: 0.6767 - acc: 0.9698 - mDice: 0.7514 - val_loss: 0.6397 - val_acc: 0.9815 - val_mDice: 0.7595

Epoch 00092: val_mDice did not improve from 0.76183
Epoch 93/300
 - 21s - loss: 0.6757 - acc: 0.9698 - mDice: 0.7515 - val_loss: 0.6546 - val_acc: 0.9809 - val_mDice: 0.7587

Epoch 00093: val_mDice did not improve from 0.76183
Epoch 94/300
 - 21s - loss: 0.6754 - acc: 0.9698 - mDice: 0.7520 - val_loss: 0.6358 - val_acc: 0.9816 - val_mDice: 0.7614

Epoch 00094: val_mDice did not improve from 0.76183
Epoch 95/300
 - 21s - loss: 0.6723 - acc: 0.9698 - mDice: 0.7529 - val_loss: 0.6466 - val_acc: 0.9810 - val_mDice: 0.7586

Epoch 00095: val_mDice did not improve from 0.76183
Epoch 96/300
 - 22s - loss: 0.6736 - acc: 0.9698 - mDice: 0.7527 - val_loss: 0.6505 - val_acc: 0.9814 - val_mDice: 0.7596

Epoch 00096: val_mDice did not improve from 0.76183
Epoch 97/300
 - 21s - loss: 0.6716 - acc: 0.9699 - mDice: 0.7531 - val_loss: 0.6528 - val_acc: 0.9816 - val_mDice: 0.7576

Epoch 00097: val_mDice did not improve from 0.76183
Epoch 98/300
 - 20s - loss: 0.6718 - acc: 0.9699 - mDice: 0.7531 - val_loss: 0.6414 - val_acc: 0.9814 - val_mDice: 0.7596

Epoch 00098: val_mDice did not improve from 0.76183
Epoch 99/300
 - 20s - loss: 0.6730 - acc: 0.9698 - mDice: 0.7526 - val_loss: 0.6660 - val_acc: 0.9810 - val_mDice: 0.7606

Epoch 00099: val_mDice did not improve from 0.76183
Epoch 100/300
 - 22s - loss: 0.6711 - acc: 0.9699 - mDice: 0.7535 - val_loss: 0.6592 - val_acc: 0.9811 - val_mDice: 0.7584

Epoch 00100: val_mDice did not improve from 0.76183
Epoch 101/300
 - 22s - loss: 0.6686 - acc: 0.9699 - mDice: 0.7542 - val_loss: 0.6554 - val_acc: 0.9810 - val_mDice: 0.7588

Epoch 00101: val_mDice did not improve from 0.76183
Epoch 102/300
 - 20s - loss: 0.6692 - acc: 0.9699 - mDice: 0.7540 - val_loss: 0.6404 - val_acc: 0.9813 - val_mDice: 0.7630

Epoch 00102: val_mDice improved from 0.76183 to 0.76299, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 103/300
 - 20s - loss: 0.6679 - acc: 0.9699 - mDice: 0.7545 - val_loss: 0.6428 - val_acc: 0.9815 - val_mDice: 0.7625

Epoch 00103: val_mDice did not improve from 0.76299
Epoch 104/300
 - 21s - loss: 0.6685 - acc: 0.9700 - mDice: 0.7542 - val_loss: 0.6443 - val_acc: 0.9812 - val_mDice: 0.7589

Epoch 00104: val_mDice did not improve from 0.76299
Epoch 105/300
 - 22s - loss: 0.6671 - acc: 0.9700 - mDice: 0.7547 - val_loss: 0.6352 - val_acc: 0.9817 - val_mDice: 0.7626

Epoch 00105: val_mDice did not improve from 0.76299
Epoch 106/300
 - 21s - loss: 0.6653 - acc: 0.9700 - mDice: 0.7553 - val_loss: 0.6438 - val_acc: 0.9811 - val_mDice: 0.7613

Epoch 00106: val_mDice did not improve from 0.76299
Epoch 107/300
 - 21s - loss: 0.6670 - acc: 0.9700 - mDice: 0.7548 - val_loss: 0.6436 - val_acc: 0.9813 - val_mDice: 0.7602

Epoch 00107: val_mDice did not improve from 0.76299
Epoch 108/300
 - 21s - loss: 0.6658 - acc: 0.9700 - mDice: 0.7551 - val_loss: 0.6456 - val_acc: 0.9814 - val_mDice: 0.7648

Epoch 00108: val_mDice improved from 0.76299 to 0.76480, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 109/300
 - 22s - loss: 0.6633 - acc: 0.9700 - mDice: 0.7559 - val_loss: 0.6395 - val_acc: 0.9814 - val_mDice: 0.7637

Epoch 00109: val_mDice did not improve from 0.76480
Epoch 110/300
 - 21s - loss: 0.6631 - acc: 0.9700 - mDice: 0.7565 - val_loss: 0.6463 - val_acc: 0.9815 - val_mDice: 0.7633

Epoch 00110: val_mDice did not improve from 0.76480
Epoch 111/300
 - 21s - loss: 0.6640 - acc: 0.9700 - mDice: 0.7558 - val_loss: 0.6596 - val_acc: 0.9813 - val_mDice: 0.7595

Epoch 00111: val_mDice did not improve from 0.76480
Epoch 112/300
 - 21s - loss: 0.6639 - acc: 0.9700 - mDice: 0.7557 - val_loss: 0.6504 - val_acc: 0.9817 - val_mDice: 0.7615

Epoch 00112: val_mDice did not improve from 0.76480
Epoch 113/300
 - 22s - loss: 0.6623 - acc: 0.9700 - mDice: 0.7562 - val_loss: 0.6437 - val_acc: 0.9815 - val_mDice: 0.7599

Epoch 00113: val_mDice did not improve from 0.76480
Epoch 114/300
 - 20s - loss: 0.6625 - acc: 0.9700 - mDice: 0.7563 - val_loss: 0.6416 - val_acc: 0.9816 - val_mDice: 0.7608

Epoch 00114: val_mDice did not improve from 0.76480
Epoch 115/300
 - 21s - loss: 0.6607 - acc: 0.9701 - mDice: 0.7570 - val_loss: 0.6451 - val_acc: 0.9811 - val_mDice: 0.7590

Epoch 00115: val_mDice did not improve from 0.76480
Epoch 116/300
 - 21s - loss: 0.6611 - acc: 0.9701 - mDice: 0.7569 - val_loss: 0.6539 - val_acc: 0.9809 - val_mDice: 0.7635

Epoch 00116: val_mDice did not improve from 0.76480
Epoch 117/300
 - 22s - loss: 0.6601 - acc: 0.9701 - mDice: 0.7574 - val_loss: 0.6565 - val_acc: 0.9816 - val_mDice: 0.7605

Epoch 00117: val_mDice did not improve from 0.76480
Epoch 118/300
 - 21s - loss: 0.6595 - acc: 0.9701 - mDice: 0.7572 - val_loss: 0.6504 - val_acc: 0.9811 - val_mDice: 0.7633

Epoch 00118: val_mDice did not improve from 0.76480
Epoch 119/300
 - 20s - loss: 0.6602 - acc: 0.9701 - mDice: 0.7570 - val_loss: 0.6362 - val_acc: 0.9816 - val_mDice: 0.7651

Epoch 00119: val_mDice improved from 0.76480 to 0.76506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 120/300
 - 21s - loss: 0.6579 - acc: 0.9701 - mDice: 0.7582 - val_loss: 0.6567 - val_acc: 0.9816 - val_mDice: 0.7615

Epoch 00120: val_mDice did not improve from 0.76506
Epoch 121/300
 - 22s - loss: 0.6583 - acc: 0.9702 - mDice: 0.7575 - val_loss: 0.6477 - val_acc: 0.9811 - val_mDice: 0.7619

Epoch 00121: val_mDice did not improve from 0.76506
Epoch 122/300
 - 21s - loss: 0.6568 - acc: 0.9702 - mDice: 0.7582 - val_loss: 0.6392 - val_acc: 0.9816 - val_mDice: 0.7634

Epoch 00122: val_mDice did not improve from 0.76506
Epoch 123/300
 - 21s - loss: 0.6568 - acc: 0.9702 - mDice: 0.7583 - val_loss: 0.6457 - val_acc: 0.9812 - val_mDice: 0.7644

Epoch 00123: val_mDice did not improve from 0.76506
Epoch 124/300
 - 21s - loss: 0.6578 - acc: 0.9702 - mDice: 0.7578 - val_loss: 0.6548 - val_acc: 0.9812 - val_mDice: 0.7622

Epoch 00124: val_mDice did not improve from 0.76506
Epoch 125/300
 - 23s - loss: 0.6572 - acc: 0.9702 - mDice: 0.7583 - val_loss: 0.6337 - val_acc: 0.9818 - val_mDice: 0.7634

Epoch 00125: val_mDice did not improve from 0.76506
Epoch 126/300
 - 21s - loss: 0.6561 - acc: 0.9702 - mDice: 0.7584 - val_loss: 0.6451 - val_acc: 0.9813 - val_mDice: 0.7623

Epoch 00126: val_mDice did not improve from 0.76506
Epoch 127/300
 - 21s - loss: 0.6548 - acc: 0.9702 - mDice: 0.7591 - val_loss: 0.6396 - val_acc: 0.9816 - val_mDice: 0.7631

Epoch 00127: val_mDice did not improve from 0.76506
Epoch 128/300
 - 21s - loss: 0.6553 - acc: 0.9702 - mDice: 0.7587 - val_loss: 0.6398 - val_acc: 0.9814 - val_mDice: 0.7631

Epoch 00128: val_mDice did not improve from 0.76506
Epoch 129/300
 - 22s - loss: 0.6553 - acc: 0.9703 - mDice: 0.7589 - val_loss: 0.6498 - val_acc: 0.9814 - val_mDice: 0.7639

Epoch 00129: val_mDice did not improve from 0.76506
Epoch 130/300
 - 21s - loss: 0.6532 - acc: 0.9703 - mDice: 0.7597 - val_loss: 0.6463 - val_acc: 0.9813 - val_mDice: 0.7620

Epoch 00130: val_mDice did not improve from 0.76506
Epoch 131/300
 - 20s - loss: 0.6530 - acc: 0.9703 - mDice: 0.7596 - val_loss: 0.6504 - val_acc: 0.9814 - val_mDice: 0.7637

Epoch 00131: val_mDice did not improve from 0.76506
Epoch 132/300
 - 20s - loss: 0.6546 - acc: 0.9703 - mDice: 0.7590 - val_loss: 0.6400 - val_acc: 0.9812 - val_mDice: 0.7630

Epoch 00132: val_mDice did not improve from 0.76506
Epoch 133/300
 - 21s - loss: 0.6537 - acc: 0.9702 - mDice: 0.7593 - val_loss: 0.6478 - val_acc: 0.9817 - val_mDice: 0.7622

Epoch 00133: val_mDice did not improve from 0.76506
Epoch 134/300
 - 22s - loss: 0.6534 - acc: 0.9703 - mDice: 0.7595 - val_loss: 0.6501 - val_acc: 0.9810 - val_mDice: 0.7631

Epoch 00134: val_mDice did not improve from 0.76506
Epoch 135/300
 - 20s - loss: 0.6508 - acc: 0.9703 - mDice: 0.7603 - val_loss: 0.6429 - val_acc: 0.9818 - val_mDice: 0.7641

Epoch 00135: val_mDice did not improve from 0.76506
Epoch 136/300
 - 21s - loss: 0.6499 - acc: 0.9703 - mDice: 0.7605 - val_loss: 0.6594 - val_acc: 0.9812 - val_mDice: 0.7625

Epoch 00136: val_mDice did not improve from 0.76506
Epoch 137/300
 - 20s - loss: 0.6518 - acc: 0.9703 - mDice: 0.7600 - val_loss: 0.6421 - val_acc: 0.9813 - val_mDice: 0.7632

Epoch 00137: val_mDice did not improve from 0.76506
Epoch 138/300
 - 22s - loss: 0.6502 - acc: 0.9704 - mDice: 0.7602 - val_loss: 0.6417 - val_acc: 0.9816 - val_mDice: 0.7663

Epoch 00138: val_mDice improved from 0.76506 to 0.76633, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 139/300
 - 21s - loss: 0.6494 - acc: 0.9704 - mDice: 0.7611 - val_loss: 0.6432 - val_acc: 0.9817 - val_mDice: 0.7630

Epoch 00139: val_mDice did not improve from 0.76633
Epoch 140/300
 - 21s - loss: 0.6494 - acc: 0.9703 - mDice: 0.7609 - val_loss: 0.6429 - val_acc: 0.9817 - val_mDice: 0.7670

Epoch 00140: val_mDice improved from 0.76633 to 0.76697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 141/300
 - 20s - loss: 0.6482 - acc: 0.9704 - mDice: 0.7613 - val_loss: 0.6464 - val_acc: 0.9816 - val_mDice: 0.7642

Epoch 00141: val_mDice did not improve from 0.76697
Epoch 142/300
 - 22s - loss: 0.6483 - acc: 0.9704 - mDice: 0.7611 - val_loss: 0.6402 - val_acc: 0.9816 - val_mDice: 0.7655

Epoch 00142: val_mDice did not improve from 0.76697
Epoch 143/300
 - 21s - loss: 0.6479 - acc: 0.9704 - mDice: 0.7615 - val_loss: 0.6448 - val_acc: 0.9816 - val_mDice: 0.7624

Epoch 00143: val_mDice did not improve from 0.76697
Epoch 144/300
 - 20s - loss: 0.6488 - acc: 0.9704 - mDice: 0.7611 - val_loss: 0.6523 - val_acc: 0.9815 - val_mDice: 0.7633

Epoch 00144: val_mDice did not improve from 0.76697
Epoch 145/300
 - 20s - loss: 0.6483 - acc: 0.9704 - mDice: 0.7617 - val_loss: 0.6501 - val_acc: 0.9815 - val_mDice: 0.7627

Epoch 00145: val_mDice did not improve from 0.76697
Epoch 146/300
 - 21s - loss: 0.6470 - acc: 0.9704 - mDice: 0.7619 - val_loss: 0.6452 - val_acc: 0.9814 - val_mDice: 0.7646

Epoch 00146: val_mDice did not improve from 0.76697
Epoch 147/300
 - 22s - loss: 0.6468 - acc: 0.9704 - mDice: 0.7618 - val_loss: 0.6418 - val_acc: 0.9813 - val_mDice: 0.7638

Epoch 00147: val_mDice did not improve from 0.76697
Epoch 148/300
 - 21s - loss: 0.6471 - acc: 0.9704 - mDice: 0.7617 - val_loss: 0.6415 - val_acc: 0.9814 - val_mDice: 0.7638

Epoch 00148: val_mDice did not improve from 0.76697
Epoch 149/300
 - 21s - loss: 0.6475 - acc: 0.9704 - mDice: 0.7616 - val_loss: 0.6490 - val_acc: 0.9813 - val_mDice: 0.7613

Epoch 00149: val_mDice did not improve from 0.76697
Epoch 150/300
 - 21s - loss: 0.6453 - acc: 0.9704 - mDice: 0.7623 - val_loss: 0.6565 - val_acc: 0.9813 - val_mDice: 0.7630

Epoch 00150: val_mDice did not improve from 0.76697
Epoch 151/300
 - 22s - loss: 0.6462 - acc: 0.9705 - mDice: 0.7622 - val_loss: 0.6393 - val_acc: 0.9816 - val_mDice: 0.7631

Epoch 00151: val_mDice did not improve from 0.76697
Epoch 152/300
 - 21s - loss: 0.6452 - acc: 0.9705 - mDice: 0.7621 - val_loss: 0.6438 - val_acc: 0.9813 - val_mDice: 0.7604

Epoch 00152: val_mDice did not improve from 0.76697
Epoch 153/300
 - 21s - loss: 0.6454 - acc: 0.9705 - mDice: 0.7621 - val_loss: 0.6362 - val_acc: 0.9809 - val_mDice: 0.7682

Epoch 00153: val_mDice improved from 0.76697 to 0.76819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 154/300
 - 20s - loss: 0.6457 - acc: 0.9705 - mDice: 0.7622 - val_loss: 0.6485 - val_acc: 0.9815 - val_mDice: 0.7605

Epoch 00154: val_mDice did not improve from 0.76819
Epoch 155/300
 - 22s - loss: 0.6451 - acc: 0.9705 - mDice: 0.7622 - val_loss: 0.6401 - val_acc: 0.9813 - val_mDice: 0.7664

Epoch 00155: val_mDice did not improve from 0.76819
Epoch 156/300
 - 21s - loss: 0.6455 - acc: 0.9705 - mDice: 0.7624 - val_loss: 0.6424 - val_acc: 0.9816 - val_mDice: 0.7654

Epoch 00156: val_mDice did not improve from 0.76819
Epoch 157/300
 - 20s - loss: 0.6447 - acc: 0.9705 - mDice: 0.7623 - val_loss: 0.6418 - val_acc: 0.9818 - val_mDice: 0.7664

Epoch 00157: val_mDice did not improve from 0.76819
Epoch 158/300
 - 20s - loss: 0.6423 - acc: 0.9705 - mDice: 0.7632 - val_loss: 0.6409 - val_acc: 0.9817 - val_mDice: 0.7626

Epoch 00158: val_mDice did not improve from 0.76819
Epoch 159/300
 - 22s - loss: 0.6421 - acc: 0.9705 - mDice: 0.7633 - val_loss: 0.6355 - val_acc: 0.9815 - val_mDice: 0.7640

Epoch 00159: val_mDice did not improve from 0.76819
Epoch 160/300
 - 21s - loss: 0.6434 - acc: 0.9705 - mDice: 0.7630 - val_loss: 0.6437 - val_acc: 0.9816 - val_mDice: 0.7623

Epoch 00160: val_mDice did not improve from 0.76819
Epoch 161/300
 - 21s - loss: 0.6421 - acc: 0.9705 - mDice: 0.7634 - val_loss: 0.6359 - val_acc: 0.9815 - val_mDice: 0.7679

Epoch 00161: val_mDice did not improve from 0.76819
Epoch 162/300
 - 21s - loss: 0.6415 - acc: 0.9706 - mDice: 0.7636 - val_loss: 0.6509 - val_acc: 0.9810 - val_mDice: 0.7663

Epoch 00162: val_mDice did not improve from 0.76819
Epoch 163/300
 - 22s - loss: 0.6418 - acc: 0.9706 - mDice: 0.7635 - val_loss: 0.6515 - val_acc: 0.9814 - val_mDice: 0.7624

Epoch 00163: val_mDice did not improve from 0.76819
Epoch 164/300
 - 21s - loss: 0.6419 - acc: 0.9705 - mDice: 0.7638 - val_loss: 0.6385 - val_acc: 0.9816 - val_mDice: 0.7679

Epoch 00164: val_mDice did not improve from 0.76819
Epoch 165/300
 - 21s - loss: 0.6409 - acc: 0.9706 - mDice: 0.7639 - val_loss: 0.6455 - val_acc: 0.9815 - val_mDice: 0.7624

Epoch 00165: val_mDice did not improve from 0.76819
Epoch 166/300
 - 21s - loss: 0.6420 - acc: 0.9706 - mDice: 0.7633 - val_loss: 0.6563 - val_acc: 0.9816 - val_mDice: 0.7605

Epoch 00166: val_mDice did not improve from 0.76819
Epoch 167/300
 - 22s - loss: 0.6400 - acc: 0.9706 - mDice: 0.7642 - val_loss: 0.6573 - val_acc: 0.9812 - val_mDice: 0.7632

Epoch 00167: val_mDice did not improve from 0.76819
Epoch 168/300
 - 21s - loss: 0.6403 - acc: 0.9706 - mDice: 0.7639 - val_loss: 0.6440 - val_acc: 0.9815 - val_mDice: 0.7660

Epoch 00168: val_mDice did not improve from 0.76819
Epoch 169/300
 - 21s - loss: 0.6387 - acc: 0.9706 - mDice: 0.7644 - val_loss: 0.6372 - val_acc: 0.9817 - val_mDice: 0.7668

Epoch 00169: val_mDice did not improve from 0.76819
Epoch 170/300
 - 21s - loss: 0.6391 - acc: 0.9706 - mDice: 0.7643 - val_loss: 0.6472 - val_acc: 0.9816 - val_mDice: 0.7624

Epoch 00170: val_mDice did not improve from 0.76819
Epoch 171/300
 - 22s - loss: 0.6398 - acc: 0.9706 - mDice: 0.7641 - val_loss: 0.6392 - val_acc: 0.9817 - val_mDice: 0.7645

Epoch 00171: val_mDice did not improve from 0.76819
Epoch 172/300
 - 20s - loss: 0.6389 - acc: 0.9706 - mDice: 0.7643 - val_loss: 0.6537 - val_acc: 0.9816 - val_mDice: 0.7625

Epoch 00172: val_mDice did not improve from 0.76819
Epoch 173/300
 - 21s - loss: 0.6401 - acc: 0.9706 - mDice: 0.7645 - val_loss: 0.6477 - val_acc: 0.9814 - val_mDice: 0.7617

Epoch 00173: val_mDice did not improve from 0.76819
Epoch 174/300
 - 21s - loss: 0.6379 - acc: 0.9707 - mDice: 0.7646 - val_loss: 0.6520 - val_acc: 0.9811 - val_mDice: 0.7604

Epoch 00174: val_mDice did not improve from 0.76819
Epoch 175/300
 - 22s - loss: 0.6385 - acc: 0.9707 - mDice: 0.7647 - val_loss: 0.6580 - val_acc: 0.9816 - val_mDice: 0.7641

Epoch 00175: val_mDice did not improve from 0.76819
Epoch 176/300
 - 21s - loss: 0.6388 - acc: 0.9706 - mDice: 0.7646 - val_loss: 0.6513 - val_acc: 0.9813 - val_mDice: 0.7628

Epoch 00176: val_mDice did not improve from 0.76819
Epoch 177/300
 - 20s - loss: 0.6366 - acc: 0.9707 - mDice: 0.7654 - val_loss: 0.6495 - val_acc: 0.9813 - val_mDice: 0.7660

Epoch 00177: val_mDice did not improve from 0.76819
Epoch 178/300
 - 20s - loss: 0.6376 - acc: 0.9707 - mDice: 0.7650 - val_loss: 0.6371 - val_acc: 0.9818 - val_mDice: 0.7665

Epoch 00178: val_mDice did not improve from 0.76819
Epoch 179/300
 - 21s - loss: 0.6380 - acc: 0.9707 - mDice: 0.7649 - val_loss: 0.6389 - val_acc: 0.9815 - val_mDice: 0.7675

Epoch 00179: val_mDice did not improve from 0.76819
Epoch 180/300
 - 22s - loss: 0.6377 - acc: 0.9707 - mDice: 0.7650 - val_loss: 0.6440 - val_acc: 0.9814 - val_mDice: 0.7622

Epoch 00180: val_mDice did not improve from 0.76819
Epoch 181/300
 - 21s - loss: 0.6366 - acc: 0.9707 - mDice: 0.7651 - val_loss: 0.6432 - val_acc: 0.9813 - val_mDice: 0.7643

Epoch 00181: val_mDice did not improve from 0.76819
Epoch 182/300
 - 20s - loss: 0.6369 - acc: 0.9707 - mDice: 0.7650 - val_loss: 0.6456 - val_acc: 0.9812 - val_mDice: 0.7654

Epoch 00182: val_mDice did not improve from 0.76819
Epoch 183/300
 - 20s - loss: 0.6357 - acc: 0.9707 - mDice: 0.7657 - val_loss: 0.6386 - val_acc: 0.9814 - val_mDice: 0.7659

Epoch 00183: val_mDice did not improve from 0.76819
Restoring model weights from the end of the best epoch
Epoch 00183: early stopping
{'val_loss': [3.778376854856194, 2.6258289660216607, 1.6852292793084962, 1.2569536309001124, 1.0288413111639465, 0.9725534106432715, 0.8992302912922713, 0.8389747383916834, 0.8295617219833398, 0.8158521149672714, 0.7850358105111787, 0.7696585576170123, 0.7682898163610936, 0.7537514207840458, 0.7794684738812678, 0.7101562942452229, 0.7112056136869425, 0.728732686478287, 0.7298114489537152, 0.7211106513368333, 0.7044993966233497, 0.7007413815910725, 0.6828516615003009, 0.6893557613663629, 0.6930241716720003, 0.6851962110329461, 0.7103351535135255, 0.6936730954361651, 0.6703116669251331, 0.6716581748181453, 0.6803590132479083, 0.6813424053698993, 0.6669866372864804, 0.6715651284614953, 0.6678160065775439, 0.6551631980328614, 0.664006457578533, 0.6660143244315719, 0.6585811955087325, 0.6479642155615783, 0.6478170433895752, 0.6479433488612082, 0.6439963205187929, 0.649780985525395, 0.6631279044906667, 0.647294014138703, 0.6491090490168462, 0.649910978088684, 0.6509056083312098, 0.6428179832250341, 0.6493521906335533, 0.6442355898019576, 0.6579968562374666, 0.6438917275614291, 0.6550123713702979, 0.6549371594430492, 0.6450098180610944, 0.6402428698859594, 0.6331218235325395, 0.6629057820613417, 0.6428901696783347, 0.6501985604425948, 0.6472402048000241, 0.6464677075052901, 0.6457729704977927, 0.6517331757966209, 0.6471187648266339, 0.6458394053796742, 0.6513877362781757, 0.6383873418019652, 0.6597547857138887, 0.6415113962656204, 0.6432688725253007, 0.6430699351955863, 0.6647592086048933, 0.6401811444168859, 0.6402134613294473, 0.652598114420878, 0.655285031187768, 0.6451985302109221, 0.6392806755807501, 0.6401343550475389, 0.6490985728085226, 0.6602628465714961, 0.6427418317711144, 0.6363140797344401, 0.6379384439365536, 0.6414944771581144, 0.6479287510439831, 0.6578286201899758, 0.630472133895315, 0.6397418096523166, 0.6546113338202268, 0.6358177991597399, 0.6465860355810731, 0.650511642174086, 0.6527568785583272, 0.6413906563238709, 0.6660044875122815, 0.6591863230839602, 0.6554337336429009, 0.6404472665767059, 0.6428228328840652, 0.644337152641255, 0.6351922473978824, 0.6438390598762146, 0.6436343877062094, 0.6456383107493413, 0.6395288820852313, 0.6462798557475871, 0.6595534080998939, 0.6504229662096045, 0.6436884898396346, 0.6416280376960373, 0.6450793274108347, 0.653867525581974, 0.6564751494410607, 0.6504190079752505, 0.6362213786726028, 0.6567348540751927, 0.6476888772872949, 0.6391785027011383, 0.6456667887598615, 0.6548377848138996, 0.6336923714331183, 0.6451193256643903, 0.639621146938257, 0.639771234201819, 0.6498103555757072, 0.6463269991463798, 0.6503917585954577, 0.6399766058806172, 0.6477792641019181, 0.6500985925042593, 0.6428683638203624, 0.6594410787794981, 0.6420555471512801, 0.6417026792578899, 0.643232806978826, 0.6428999263558718, 0.6463842822548282, 0.6401728949249098, 0.6448324362990534, 0.6522529644370694, 0.6500733188738886, 0.6452460691286683, 0.6417982994095337, 0.6415098313207597, 0.6489864650833103, 0.6564507913786314, 0.6393148239675075, 0.6437767796349108, 0.6361658470426428, 0.6484923208338065, 0.6400550430404144, 0.6423544737638212, 0.6417612567221048, 0.6409410584575743, 0.635466603456513, 0.6436601145840773, 0.6358856475759217, 0.6509227509100002, 0.6514741777451045, 0.6385346768564238, 0.6455020118731586, 0.6563157047403609, 0.6572875965182873, 0.6439693206296494, 0.6371970387373669, 0.6471895428818446, 0.6391731667014221, 0.6536587879938238, 0.6477107402586961, 0.6519901145660963, 0.6579992173256889, 0.6512766547800956, 0.6495429538536367, 0.6371254888973492, 0.6389298546055892, 0.6440429678580832, 0.6431595966050745, 0.6455720191892579, 0.6386354826736745], 'val_acc': [0.9217381076300969, 0.9255827359001703, 0.9474898486929658, 0.9629947348767882, 0.96888952672297, 0.9722112553776602, 0.9739826863518194, 0.9749152951319274, 0.9755665468603711, 0.9758406629626349, 0.9772185186237496, 0.9771138730432966, 0.9776450025284869, 0.9779773640558815, 0.9780912373457161, 0.9785499848325432, 0.9782893318140838, 0.978462112208269, 0.9781343423914245, 0.977935908441081, 0.9783429845444804, 0.977928008395944, 0.9794253224066782, 0.979280198813715, 0.979113357354982, 0.978753993201182, 0.9794615216791568, 0.9797803941033819, 0.9794908079200485, 0.9803302912397158, 0.9795293144889414, 0.9799660014909365, 0.9797274120328843, 0.9794888279878441, 0.9802049133307671, 0.9792558478251084, 0.9795678109084367, 0.9798113334658715, 0.9799297993889288, 0.9799031455327359, 0.9801608202873245, 0.9804392151911315, 0.980358924536021, 0.9805810503792345, 0.9802381445502841, 0.979868260576506, 0.9802552649234224, 0.9804135410286202, 0.9807462387163696, 0.9809713318744065, 0.980235845127342, 0.9808462803454837, 0.9802569053121396, 0.9801871429040828, 0.9803204255945542, 0.9811125073516578, 0.9803506989105075, 0.9808525404939956, 0.9808485854889956, 0.9804099277815214, 0.9804398739177991, 0.9812260458343908, 0.9810160882578792, 0.9812875765272954, 0.9811052709541085, 0.9809101164894577, 0.9807567709001594, 0.9813099624079693, 0.98110889865641, 0.9813596542282617, 0.9806662765941876, 0.9812632259692693, 0.981228023798227, 0.9812336157473002, 0.9809275700088871, 0.9809825300432211, 0.981352079763501, 0.9809900906678939, 0.980803177821747, 0.9809650875343504, 0.981217486324448, 0.9814494837794387, 0.9811483739699373, 0.9811701056757948, 0.9812694872249883, 0.981391567809909, 0.9808298346920034, 0.9811506997813135, 0.9814435585610515, 0.9805521001756745, 0.9813807141055018, 0.9815176113348135, 0.980859776645499, 0.9815995493294408, 0.9809670460605523, 0.9814300810835556, 0.9815883550358507, 0.981358663339487, 0.9809812033016492, 0.9810624823235629, 0.9810154259635445, 0.9812987890898013, 0.9814929377183825, 0.9812464456317103, 0.9816999156162589, 0.9811243567427369, 0.9813010707974311, 0.9814313904788841, 0.981403089713755, 0.9814860170593694, 0.98126816472771, 0.9817249280380391, 0.9814837073640066, 0.9815956002910563, 0.9811006598674353, 0.980943036706824, 0.9815771717528194, 0.9810526219068789, 0.9816186322885401, 0.9815837493621897, 0.9810743385424185, 0.9816360686462607, 0.9811635104126236, 0.9812253879073727, 0.9818447082158336, 0.9813069971230254, 0.9816229146577502, 0.9814004573905677, 0.9813981501556649, 0.9812859398292684, 0.9814264448311553, 0.9812461198052878, 0.9816752338803097, 0.9809538929332029, 0.9817782364270512, 0.981207290793105, 0.9813086366505814, 0.981550508854436, 0.9816972808326115, 0.9816798449669829, 0.9815834135709041, 0.9815617115135902, 0.9816077713257757, 0.9815103719232006, 0.9814580525161066, 0.9813955207235181, 0.9813250976204257, 0.9813751120685423, 0.9813474815942431, 0.9812668408771786, 0.9815547960215432, 0.9812875830475145, 0.9809088103542387, 0.9814843681205537, 0.98128526959995, 0.9815814404664763, 0.9817677036281464, 0.981740720131818, 0.9814590261201495, 0.981636408312771, 0.9815225388366494, 0.9810200461539199, 0.981423486373988, 0.9816104078317451, 0.9814883220798583, 0.9815564400394389, 0.9812359293793992, 0.981489638241452, 0.9816716184803084, 0.9815827582888805, 0.9817357823575613, 0.9815824362761711, 0.9813751135448184, 0.9811345575640691, 0.9816150116600612, 0.9812747467043968, 0.9812520415175194, 0.9818496476509008, 0.9814679173616186, 0.9814333677660939, 0.9812655359721897, 0.9811750424658674, 0.9813576815544144], 'val_mDice': [0.037238590373158825, 0.13020483417469159, 0.353382198188081, 0.4982144847988959, 0.581348062484257, 0.6186123120034319, 0.645599880700756, 0.6693589756240294, 0.6810462023943693, 0.6875094756007317, 0.69695951424393, 0.7025694013010976, 0.7052796956678417, 0.7101152459288284, 0.7023859409724965, 0.7210352161843464, 0.7252919200896233, 0.7205810228860539, 0.7205351223271451, 0.7239757690144274, 0.7267950438616568, 0.7271564126137733, 0.7352099924879792, 0.7305243334533998, 0.7365693289429042, 0.733604348726932, 0.7306678874573841, 0.7391666506097043, 0.7440343234684199, 0.742688306221898, 0.7403184793681922, 0.7384985548792978, 0.7450585337615234, 0.7415488898077494, 0.7394173070250157, 0.7467780521041468, 0.7421184303959826, 0.7430616951825326, 0.748417873685205, 0.7494950531929024, 0.7500733891261744, 0.7510065160791695, 0.7494935858360385, 0.7499102944560095, 0.7479823260607488, 0.7510607422197813, 0.7514714994174654, 0.7507754985639062, 0.751205560592675, 0.7531688468621119, 0.7516478000533593, 0.7534232781152361, 0.7488297822551707, 0.7555201538330015, 0.753198386899458, 0.750923097072125, 0.7568691101605678, 0.7567851884815346, 0.755730719143146, 0.7547972774973103, 0.7557706546857261, 0.7536489460367414, 0.7549343800520134, 0.7570043632489609, 0.7558457417138713, 0.7476395085623144, 0.7535025506934884, 0.7550773625526389, 0.7577795676033563, 0.7535105255981225, 0.7529665263690692, 0.7601872051463407, 0.760427454000164, 0.7580164984037756, 0.7534142187013223, 0.758699098730727, 0.7583002492370251, 0.7586109764927804, 0.7579590358970335, 0.7603379734037339, 0.7599808799470049, 0.7583945202876663, 0.7577889671758724, 0.7542965922685354, 0.7579077465615406, 0.7580439169709527, 0.7618265318673708, 0.759216333628931, 0.7611714651341039, 0.7555002122963668, 0.7604176255079492, 0.7594602949479047, 0.7586642206637852, 0.7614037413592186, 0.7586287829652044, 0.7595950312166637, 0.7575584811199806, 0.7595556857539159, 0.7606101720079672, 0.7583808537238154, 0.7587767873878204, 0.7629859262329629, 0.7625448850778357, 0.7588952148661894, 0.7626197366399539, 0.7612559740634403, 0.7601880060260879, 0.7648049010457885, 0.7636974100850069, 0.7632773702481706, 0.7594984986341652, 0.7614502251086712, 0.7598752681438891, 0.7608323051710493, 0.7590459901358936, 0.763542069431675, 0.7605175466360322, 0.763309937574054, 0.7650627907830742, 0.7614953118704175, 0.7619154826652401, 0.7633944987266549, 0.7644122811908939, 0.7622337207828636, 0.7633933417568266, 0.762277367068272, 0.7631097154228557, 0.7630803217828828, 0.7638832411898917, 0.7619662225184918, 0.7636909230205666, 0.7630426080357302, 0.7622195904222927, 0.7631331126756343, 0.7641238949124166, 0.762484915731369, 0.7631826031687828, 0.7663314831515214, 0.7629597172279476, 0.7669679091933834, 0.764213244612372, 0.7654609427978626, 0.7624273779960609, 0.763342912229099, 0.7627344721980139, 0.7645807171507639, 0.7637964416952694, 0.7637511981407064, 0.7612700713308234, 0.7630084198202757, 0.7631206583189398, 0.760350087601826, 0.7681927771391145, 0.7605236164064476, 0.7663902541309195, 0.7654439041735095, 0.766443809058029, 0.7625642122375953, 0.7640457239436415, 0.762265867124032, 0.7679368448208237, 0.7663328931796661, 0.7623518944647782, 0.7679293548113545, 0.7623752033255294, 0.760483116495843, 0.7631515554845395, 0.7660262210081237, 0.7667678213217926, 0.7623925514142457, 0.7644709683301156, 0.7625238564484382, 0.7617340882004107, 0.7603627701661905, 0.7641282119741135, 0.7627634683506653, 0.7659870217582143, 0.7665391921751025, 0.7675461242565552, 0.7621556132939578, 0.7642583251614565, 0.7654377001234868, 0.7658596198748263], 'loss': [31.969719849710167, 4.22187959515795, 2.914921652795251, 2.2181366508662457, 1.8383256485816941, 1.619241158874558, 1.4783591356458543, 1.3659093659420072, 1.2860164242597079, 1.2189985733556667, 1.170080398371546, 1.1239776473455905, 1.0860817670494725, 1.0525223500039032, 1.0218763720641877, 0.9992621083845522, 0.9694182001052507, 0.948863366231276, 0.9322305423478281, 0.9165024353377654, 0.9017727113521723, 0.8872060545210568, 0.8767300460185986, 0.8652205770514534, 0.8530623630874334, 0.8444615396289972, 0.8350542069748418, 0.8274331659413136, 0.820780646980364, 0.8103085696654528, 0.8022864585222308, 0.7997014697650922, 0.7944027623114963, 0.7866460401017654, 0.782624055220627, 0.7771899891278327, 0.7737058104081059, 0.7678642003539641, 0.7662144831962509, 0.7607109718627966, 0.7577355104939648, 0.7536277468061573, 0.7501905345036985, 0.7489348112155977, 0.7458288859276149, 0.7458112493588015, 0.7383342608008716, 0.7358692662938557, 0.7357488451665701, 0.730965976124878, 0.7295654599439395, 0.728203156105738, 0.7263512999381659, 0.7230000508366834, 0.7229139707112386, 0.7185253999185983, 0.7177490666241756, 0.7168451532177006, 0.7124390432992356, 0.7102632939189153, 0.7099083551493299, 0.7086360822194853, 0.7063878995010726, 0.7059913716083898, 0.7041480560305012, 0.7041956378048692, 0.7017203584929085, 0.7009557076020944, 0.6980985587626047, 0.6974811009512842, 0.695971110085186, 0.6942796340158397, 0.6944453244864699, 0.6918483997604173, 0.6918033518027029, 0.6913714261250875, 0.6900282643747865, 0.6881406393274931, 0.6873400181581731, 0.6878922287350713, 0.6853217718736181, 0.6854955719996102, 0.6842239238828925, 0.6825833029285205, 0.6833477793615076, 0.6800649141621868, 0.6802026261242984, 0.6794802221786101, 0.6796218698836751, 0.6778438765122323, 0.6774512293417356, 0.676653870163654, 0.6757351415828386, 0.6754485250999855, 0.6722844769056697, 0.6736364600722192, 0.6716330889296583, 0.6717833274655561, 0.672998315829749, 0.6710592980512378, 0.6685690197934392, 0.6692295182974093, 0.6678995609511218, 0.668541388342823, 0.6670502222876916, 0.6653266458358177, 0.666995544773968, 0.6657943738789498, 0.6633016201991127, 0.6630529318087591, 0.6639938408445282, 0.6639324228775538, 0.6623115742906968, 0.6624667134722292, 0.6607010972643628, 0.6610685162493571, 0.6600665916598211, 0.6594570351580831, 0.6601667070776004, 0.6578594367886205, 0.6582838758765754, 0.6568134486112954, 0.6567582837651845, 0.6578402190311501, 0.6571715709443932, 0.6560712827114664, 0.6547658602925739, 0.65525908993812, 0.6553003632825857, 0.6531904202701084, 0.65299327285386, 0.6545682530614394, 0.6536806208480364, 0.653445956906015, 0.6508312219078113, 0.6499386832584252, 0.6518425552976539, 0.6501994132355001, 0.6493926881740223, 0.6493629781519965, 0.6482124090308587, 0.648252888446296, 0.647925970877928, 0.6487856633073252, 0.6482984859461656, 0.6470029516025918, 0.646755425200304, 0.6471387456888112, 0.6474556497693887, 0.6453108359114891, 0.6462350003447999, 0.6451559294720723, 0.645359775079501, 0.6457362318340736, 0.6451000993693675, 0.6455091135311674, 0.6446763231962428, 0.6423171864778465, 0.6420741630626915, 0.6434085933446656, 0.6421105677383396, 0.6415474529386791, 0.6418254521835781, 0.6418519487365607, 0.6409455814892846, 0.6419673718143711, 0.6400374306296004, 0.6403095998619119, 0.6386934689748592, 0.6390680704761411, 0.6397844443628433, 0.6389004485416959, 0.640101888604286, 0.637858617503631, 0.6385363793249258, 0.6388273880238006, 0.6366283036065552, 0.6376165733300105, 0.6379755794759978, 0.6376678623474382, 0.6365969132303707, 0.6368548091610434, 0.6357262805442352], 'acc': [0.8629538920377244, 0.9065613848499104, 0.9173963963273661, 0.9322173098533285, 0.942197217039029, 0.9476135009465994, 0.9509879803996341, 0.9535369435371975, 0.9556003369449687, 0.9572093381442117, 0.958353435406686, 0.9593707489844919, 0.9601913336811243, 0.960850706981935, 0.9614520134363819, 0.9619436746911157, 0.962792265534771, 0.9634032701218871, 0.9638114438610428, 0.9642091550988174, 0.9645560598897285, 0.9649791412957677, 0.9652542477224048, 0.9655005173023202, 0.9658301703921929, 0.9660276507944089, 0.966187537043881, 0.966412101123337, 0.966554079664787, 0.9667601757995377, 0.9669755351537732, 0.9669951357646748, 0.9670894490041111, 0.9672407289308097, 0.9673658095697016, 0.9674524802466352, 0.9675575201939022, 0.9676549573815293, 0.9676846023430994, 0.9678093323865311, 0.9678769723908384, 0.9679323297811631, 0.9680354479306178, 0.9680563918169626, 0.9681546378668094, 0.9681006408774087, 0.9683046616259753, 0.9683408451971474, 0.9683629971701346, 0.9684637688369035, 0.9684629565011865, 0.9684854338156001, 0.9685203238862609, 0.9686661202069937, 0.96862215848592, 0.968740297411574, 0.9687589307815897, 0.9687764361831719, 0.9689075347202503, 0.9689758580497435, 0.9689747364161719, 0.9689814757198942, 0.9690423452301814, 0.9690352814747606, 0.9691141936896893, 0.9691030364817242, 0.9691705024125669, 0.9692047265326461, 0.9692893297028251, 0.9692784381533455, 0.9693126124130789, 0.9693955336845317, 0.9693869659332378, 0.9693952108272255, 0.9694221888190384, 0.9694284910884654, 0.9694609255246339, 0.9694684634836186, 0.9694849349716725, 0.969499414045998, 0.9695269172192189, 0.9695427965613918, 0.9695896426803433, 0.9696405944233155, 0.9696250363511746, 0.9697068431068782, 0.9696600719813034, 0.9697177333466785, 0.969714209127745, 0.9697456165333195, 0.9697481001622072, 0.9697728998654778, 0.9697635700160574, 0.969783059852289, 0.9698359029430292, 0.9698273639050309, 0.9698784384233217, 0.9698638811810705, 0.96981322746844, 0.9699129384800219, 0.9699114029664312, 0.9699390982710322, 0.9699384900246368, 0.9699621926792943, 0.9699608133449628, 0.9700054792891489, 0.9699659539644206, 0.9700024454881788, 0.9700287839721204, 0.970015562410596, 0.9700185792640311, 0.970042398928571, 0.9700488133464077, 0.970026892197283, 0.9700853055890659, 0.9700999278028, 0.9701139967624345, 0.9700989067369411, 0.9701220903457535, 0.9701456710151707, 0.9701776847097394, 0.9701538766258965, 0.9701759431992506, 0.970188157845147, 0.9701584066856668, 0.9702096738264673, 0.9702286257830958, 0.9702070152913297, 0.9702581743052902, 0.9702524636834967, 0.9702891722510133, 0.9702597191005212, 0.9702372123752655, 0.9702556909112905, 0.9703232255362371, 0.9703233867620323, 0.9702731882483799, 0.9703665120037354, 0.9704017716388529, 0.9703401613340817, 0.970391683983353, 0.9703893978099764, 0.9703722149952232, 0.9703718333162327, 0.970422945765399, 0.9704326164161559, 0.970438757644851, 0.970406390116213, 0.9704496206518628, 0.970446726751692, 0.9704636320283141, 0.9704584549727234, 0.970488918419809, 0.9704643097019492, 0.9704729721629879, 0.9704620613099739, 0.9704692495916976, 0.9705486833579449, 0.9705064815326953, 0.9705450179427684, 0.9705450376306651, 0.9705751015472617, 0.9705589841637544, 0.9705235897669053, 0.9705861371685846, 0.9705501784280687, 0.9705815077796027, 0.970578147847128, 0.9706232909701279, 0.9706373897107313, 0.9706398946859685, 0.9706222725378633, 0.9706445221887273, 0.9706596083848323, 0.9706575359311911, 0.9706419977747173, 0.9707105700215777, 0.9706735659007215, 0.970667968181827, 0.9706730901810614, 0.9706547548413874, 0.9706700899975664, 0.970715681885033], 'mDice': [0.026925198917424186, 0.07742756676237567, 0.19498050291689992, 0.3197020264736132, 0.40760296778559885, 0.4657392070135126, 0.5061082440157736, 0.5374219550034697, 0.5606666321696944, 0.5796871666538707, 0.5948759943339373, 0.6084430471778002, 0.6190196713030068, 0.628670711185074, 0.6375361080169109, 0.644955407257777, 0.6542066365288242, 0.6604734666099235, 0.6660479055061413, 0.6705556661851485, 0.6753419623159695, 0.6801005433404078, 0.6837233927359808, 0.6874816912738639, 0.6916911387844987, 0.6948926106383586, 0.6981344326871489, 0.7006955320464417, 0.7027999488456681, 0.7063895944040602, 0.708904081514692, 0.7099159510972137, 0.7118305096997547, 0.7142000153094907, 0.7160211733055479, 0.7175655654756784, 0.7189051253302501, 0.7209804477314691, 0.7215477337067355, 0.7231458228500527, 0.7244150019614486, 0.7257569133167708, 0.7265600220653702, 0.7272605620678724, 0.7275883615344357, 0.7281745521684009, 0.7308325821897252, 0.7311099827830536, 0.7313829860924707, 0.733044877751562, 0.7332513185597446, 0.7338655948681623, 0.7346075385388993, 0.7359240909578808, 0.7358306644425234, 0.7371346045286353, 0.7377244149810066, 0.7378236466713102, 0.7394458718023125, 0.7400839411403787, 0.7403397557414857, 0.7405612440888797, 0.7414426604503375, 0.7416011847543181, 0.7421852523553961, 0.7421819860578751, 0.7426026906165981, 0.7433644862285457, 0.7440679858927799, 0.7442915806244744, 0.7450952919038577, 0.7453897258020797, 0.7456987999007728, 0.7463368145810809, 0.7463620534967004, 0.7462743669000935, 0.7471113139329733, 0.7474908912796834, 0.7480616779067735, 0.7474963172739948, 0.7484647110051177, 0.7483293260584477, 0.7488216931662754, 0.7495865208922692, 0.7490347742906242, 0.7504118416384521, 0.7499279836239438, 0.7505847101055525, 0.7502970690940199, 0.7510231957248836, 0.7513056153937345, 0.7514170574450054, 0.7514717991721639, 0.7520431648910487, 0.7529207476360689, 0.7526567904952605, 0.7530898127161235, 0.7531325871061249, 0.7526415522411156, 0.753469948334998, 0.754179393744053, 0.7540042637810662, 0.7545484575617134, 0.7542166144602447, 0.7547156492380303, 0.7553048705767127, 0.7548271650512947, 0.755101279388728, 0.755864923207322, 0.7565046066824451, 0.7558276948537068, 0.7557172665511733, 0.756240990748689, 0.7563363058145103, 0.7569804248887826, 0.7569229125777264, 0.7573699864547977, 0.7572470878911297, 0.7570386684066162, 0.7581626919792408, 0.7575244722985299, 0.7581918832294577, 0.758299918014735, 0.7577995896439066, 0.7582719900071664, 0.7583791721769697, 0.7590615136301088, 0.7586989071365186, 0.7588641029398453, 0.7596980425717582, 0.7596207851864769, 0.7590455125233482, 0.7593489022792482, 0.7595162985217807, 0.7602584087424441, 0.7605192735595466, 0.7600410154348687, 0.7602456993509483, 0.7611190654766254, 0.7609071519812166, 0.7612987576505863, 0.7610775759247456, 0.7615194066643629, 0.7611071848274273, 0.7616773537271826, 0.7619082942224216, 0.7617958355895198, 0.7616637508212054, 0.7616054561689357, 0.7623309526233932, 0.7621674070088938, 0.7620977668964922, 0.7620894554010404, 0.7621795806276619, 0.762248504046526, 0.7623971514537279, 0.7623392856502579, 0.7631935200088771, 0.7632920630783347, 0.7630261070964044, 0.7634188817043933, 0.7635766464086031, 0.76349197735586, 0.7637718882912635, 0.7638791414542274, 0.7633227149341794, 0.7642039966335552, 0.7639061751857076, 0.7643556192762846, 0.76430938558189, 0.7641062957306082, 0.7642947341352594, 0.7644835131704195, 0.7646444450991804, 0.7646835721034494, 0.7645706387117935, 0.7653926781845571, 0.7650421905025024, 0.7648664459867297, 0.7650074595863505, 0.7651281536963609, 0.7650030475471935, 0.7656620694624657]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:02<00:29,  2.14s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:26,  2.03s/it]predicting test subjects:  20%|██        | 3/15 [00:06<00:24,  2.05s/it]predicting test subjects:  27%|██▋       | 4/15 [00:08<00:22,  2.04s/it]predicting test subjects:  33%|███▎      | 5/15 [00:10<00:21,  2.11s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:19,  2.19s/it]predicting test subjects:  47%|████▋     | 7/15 [00:14<00:15,  2.00s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:16<00:14,  2.12s/it]predicting test subjects:  60%|██████    | 9/15 [00:18<00:12,  2.06s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:20<00:09,  1.91s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:22<00:07,  1.91s/it]predicting test subjects:  80%|████████  | 12/15 [00:24<00:05,  1.97s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:26<00:03,  1.99s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:28<00:01,  1.95s/it]predicting test subjects: 100%|██████████| 15/15 [00:30<00:00,  1.96s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:35,  2.33s/it]predicting train subjects:   0%|          | 2/532 [00:04<19:06,  2.16s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:59,  2.04s/it]predicting train subjects:   1%|          | 4/532 [00:07<17:28,  1.99s/it]predicting train subjects:   1%|          | 5/532 [00:09<17:05,  1.95s/it]predicting train subjects:   1%|          | 6/532 [00:11<16:15,  1.85s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<15:43,  1.80s/it]predicting train subjects:   2%|▏         | 8/532 [00:14<15:23,  1.76s/it]predicting train subjects:   2%|▏         | 9/532 [00:16<16:09,  1.85s/it]predicting train subjects:   2%|▏         | 10/532 [00:18<15:52,  1.82s/it]predicting train subjects:   2%|▏         | 11/532 [00:19<15:10,  1.75s/it]predicting train subjects:   2%|▏         | 12/532 [00:22<16:15,  1.88s/it]predicting train subjects:   2%|▏         | 13/532 [00:23<15:31,  1.79s/it]predicting train subjects:   3%|▎         | 14/532 [00:25<14:39,  1.70s/it]predicting train subjects:   3%|▎         | 15/532 [00:26<14:40,  1.70s/it]predicting train subjects:   3%|▎         | 16/532 [00:28<15:00,  1.75s/it]predicting train subjects:   3%|▎         | 17/532 [00:30<14:36,  1.70s/it]predicting train subjects:   3%|▎         | 18/532 [00:32<15:15,  1.78s/it]predicting train subjects:   4%|▎         | 19/532 [00:33<14:27,  1.69s/it]predicting train subjects:   4%|▍         | 20/532 [00:35<14:31,  1.70s/it]predicting train subjects:   4%|▍         | 21/532 [00:37<15:17,  1.80s/it]predicting train subjects:   4%|▍         | 22/532 [00:39<14:45,  1.74s/it]predicting train subjects:   4%|▍         | 23/532 [00:40<14:51,  1.75s/it]predicting train subjects:   5%|▍         | 24/532 [00:42<14:15,  1.68s/it]predicting train subjects:   5%|▍         | 25/532 [00:44<15:20,  1.82s/it]predicting train subjects:   5%|▍         | 26/532 [00:46<14:44,  1.75s/it]predicting train subjects:   5%|▌         | 27/532 [00:48<15:54,  1.89s/it]predicting train subjects:   5%|▌         | 28/532 [00:50<15:40,  1.87s/it]predicting train subjects:   5%|▌         | 29/532 [00:52<16:09,  1.93s/it]predicting train subjects:   6%|▌         | 30/532 [00:53<15:14,  1.82s/it]predicting train subjects:   6%|▌         | 31/532 [00:55<15:04,  1.81s/it]predicting train subjects:   6%|▌         | 32/532 [00:57<14:58,  1.80s/it]predicting train subjects:   6%|▌         | 33/532 [00:58<14:21,  1.73s/it]predicting train subjects:   6%|▋         | 34/532 [01:01<15:32,  1.87s/it]predicting train subjects:   7%|▋         | 35/532 [01:02<15:15,  1.84s/it]predicting train subjects:   7%|▋         | 36/532 [01:04<15:26,  1.87s/it]predicting train subjects:   7%|▋         | 37/532 [01:06<15:33,  1.89s/it]predicting train subjects:   7%|▋         | 38/532 [01:08<16:01,  1.95s/it]predicting train subjects:   7%|▋         | 39/532 [01:10<15:55,  1.94s/it]predicting train subjects:   8%|▊         | 40/532 [01:12<15:17,  1.87s/it]predicting train subjects:   8%|▊         | 41/532 [01:14<15:29,  1.89s/it]predicting train subjects:   8%|▊         | 42/532 [01:16<15:31,  1.90s/it]predicting train subjects:   8%|▊         | 43/532 [01:17<14:46,  1.81s/it]predicting train subjects:   8%|▊         | 44/532 [01:19<13:53,  1.71s/it]predicting train subjects:   8%|▊         | 45/532 [01:21<13:35,  1.67s/it]predicting train subjects:   9%|▊         | 46/532 [01:22<13:54,  1.72s/it]predicting train subjects:   9%|▉         | 47/532 [01:24<14:54,  1.84s/it]predicting train subjects:   9%|▉         | 48/532 [01:26<15:12,  1.89s/it]predicting train subjects:   9%|▉         | 49/532 [01:28<14:45,  1.83s/it]predicting train subjects:   9%|▉         | 50/532 [01:30<15:32,  1.93s/it]predicting train subjects:  10%|▉         | 51/532 [01:32<15:02,  1.88s/it]predicting train subjects:  10%|▉         | 52/532 [01:34<14:45,  1.85s/it]predicting train subjects:  10%|▉         | 53/532 [01:36<14:17,  1.79s/it]predicting train subjects:  10%|█         | 54/532 [01:38<14:51,  1.87s/it]predicting train subjects:  10%|█         | 55/532 [01:40<14:59,  1.89s/it]predicting train subjects:  11%|█         | 56/532 [01:41<14:52,  1.88s/it]predicting train subjects:  11%|█         | 57/532 [01:43<14:26,  1.82s/it]predicting train subjects:  11%|█         | 58/532 [01:45<14:18,  1.81s/it]predicting train subjects:  11%|█         | 59/532 [01:47<15:03,  1.91s/it]predicting train subjects:  11%|█▏        | 60/532 [01:48<14:04,  1.79s/it]predicting train subjects:  11%|█▏        | 61/532 [01:50<13:32,  1.72s/it]predicting train subjects:  12%|█▏        | 62/532 [01:52<14:18,  1.83s/it]predicting train subjects:  12%|█▏        | 63/532 [01:54<14:51,  1.90s/it]predicting train subjects:  12%|█▏        | 64/532 [01:56<14:01,  1.80s/it]predicting train subjects:  12%|█▏        | 65/532 [01:58<13:53,  1.78s/it]predicting train subjects:  12%|█▏        | 66/532 [02:00<15:04,  1.94s/it]predicting train subjects:  13%|█▎        | 67/532 [02:02<15:37,  2.02s/it]predicting train subjects:  13%|█▎        | 68/532 [02:04<15:33,  2.01s/it]predicting train subjects:  13%|█▎        | 69/532 [02:06<15:05,  1.95s/it]predicting train subjects:  13%|█▎        | 70/532 [02:08<14:35,  1.90s/it]predicting train subjects:  13%|█▎        | 71/532 [02:09<14:00,  1.82s/it]predicting train subjects:  14%|█▎        | 72/532 [02:11<13:28,  1.76s/it]predicting train subjects:  14%|█▎        | 73/532 [02:13<13:52,  1.81s/it]predicting train subjects:  14%|█▍        | 74/532 [02:15<14:53,  1.95s/it]predicting train subjects:  14%|█▍        | 75/532 [02:18<16:50,  2.21s/it]predicting train subjects:  14%|█▍        | 76/532 [02:20<15:41,  2.06s/it]predicting train subjects:  14%|█▍        | 77/532 [02:21<15:13,  2.01s/it]predicting train subjects:  15%|█▍        | 78/532 [02:23<15:01,  1.99s/it]predicting train subjects:  15%|█▍        | 79/532 [02:25<14:42,  1.95s/it]predicting train subjects:  15%|█▌        | 80/532 [02:27<14:34,  1.94s/it]predicting train subjects:  15%|█▌        | 81/532 [02:29<14:23,  1.91s/it]predicting train subjects:  15%|█▌        | 82/532 [02:31<14:17,  1.91s/it]predicting train subjects:  16%|█▌        | 83/532 [02:33<13:42,  1.83s/it]predicting train subjects:  16%|█▌        | 84/532 [02:34<13:15,  1.77s/it]predicting train subjects:  16%|█▌        | 85/532 [02:36<12:52,  1.73s/it]predicting train subjects:  16%|█▌        | 86/532 [02:37<12:33,  1.69s/it]predicting train subjects:  16%|█▋        | 87/532 [02:39<12:17,  1.66s/it]predicting train subjects:  17%|█▋        | 88/532 [02:41<12:06,  1.64s/it]predicting train subjects:  17%|█▋        | 89/532 [02:42<12:24,  1.68s/it]predicting train subjects:  17%|█▋        | 90/532 [02:44<12:27,  1.69s/it]predicting train subjects:  17%|█▋        | 91/532 [02:46<12:34,  1.71s/it]predicting train subjects:  17%|█▋        | 92/532 [02:48<12:45,  1.74s/it]predicting train subjects:  17%|█▋        | 93/532 [02:50<12:55,  1.77s/it]predicting train subjects:  18%|█▊        | 94/532 [02:51<12:57,  1.78s/it]predicting train subjects:  18%|█▊        | 95/532 [02:53<13:42,  1.88s/it]predicting train subjects:  18%|█▊        | 96/532 [02:56<14:08,  1.95s/it]predicting train subjects:  18%|█▊        | 97/532 [02:58<14:25,  1.99s/it]predicting train subjects:  18%|█▊        | 98/532 [03:00<14:39,  2.03s/it]predicting train subjects:  19%|█▊        | 99/532 [03:02<14:55,  2.07s/it]predicting train subjects:  19%|█▉        | 100/532 [03:04<15:02,  2.09s/it]predicting train subjects:  19%|█▉        | 101/532 [03:06<14:08,  1.97s/it]predicting train subjects:  19%|█▉        | 102/532 [03:07<13:08,  1.83s/it]predicting train subjects:  19%|█▉        | 103/532 [03:09<12:36,  1.76s/it]predicting train subjects:  20%|█▉        | 104/532 [03:10<12:19,  1.73s/it]predicting train subjects:  20%|█▉        | 105/532 [03:12<12:01,  1.69s/it]predicting train subjects:  20%|█▉        | 106/532 [03:14<12:03,  1.70s/it]predicting train subjects:  20%|██        | 107/532 [03:15<11:53,  1.68s/it]predicting train subjects:  20%|██        | 108/532 [03:17<11:45,  1.66s/it]predicting train subjects:  20%|██        | 109/532 [03:19<11:33,  1.64s/it]predicting train subjects:  21%|██        | 110/532 [03:20<11:21,  1.62s/it]predicting train subjects:  21%|██        | 111/532 [03:22<11:11,  1.59s/it]predicting train subjects:  21%|██        | 112/532 [03:23<11:11,  1.60s/it]predicting train subjects:  21%|██        | 113/532 [03:25<11:59,  1.72s/it]predicting train subjects:  21%|██▏       | 114/532 [03:27<12:15,  1.76s/it]predicting train subjects:  22%|██▏       | 115/532 [03:29<12:26,  1.79s/it]predicting train subjects:  22%|██▏       | 116/532 [03:31<12:29,  1.80s/it]predicting train subjects:  22%|██▏       | 117/532 [03:33<12:39,  1.83s/it]predicting train subjects:  22%|██▏       | 118/532 [03:35<12:48,  1.86s/it]predicting train subjects:  22%|██▏       | 119/532 [03:37<12:39,  1.84s/it]predicting train subjects:  23%|██▎       | 120/532 [03:38<12:28,  1.82s/it]predicting train subjects:  23%|██▎       | 121/532 [03:40<12:28,  1.82s/it]predicting train subjects:  23%|██▎       | 122/532 [03:42<12:21,  1.81s/it]predicting train subjects:  23%|██▎       | 123/532 [03:44<12:19,  1.81s/it]predicting train subjects:  23%|██▎       | 124/532 [03:45<12:11,  1.79s/it]predicting train subjects:  23%|██▎       | 125/532 [03:47<12:25,  1.83s/it]predicting train subjects:  24%|██▎       | 126/532 [03:49<12:42,  1.88s/it]predicting train subjects:  24%|██▍       | 127/532 [03:51<12:45,  1.89s/it]predicting train subjects:  24%|██▍       | 128/532 [03:53<12:53,  1.91s/it]predicting train subjects:  24%|██▍       | 129/532 [03:55<12:55,  1.92s/it]predicting train subjects:  24%|██▍       | 130/532 [03:57<12:59,  1.94s/it]predicting train subjects:  25%|██▍       | 131/532 [03:59<13:32,  2.03s/it]predicting train subjects:  25%|██▍       | 132/532 [04:02<14:04,  2.11s/it]predicting train subjects:  25%|██▌       | 133/532 [04:04<14:14,  2.14s/it]predicting train subjects:  25%|██▌       | 134/532 [04:06<14:24,  2.17s/it]predicting train subjects:  25%|██▌       | 135/532 [04:08<14:27,  2.18s/it]predicting train subjects:  26%|██▌       | 136/532 [04:11<14:26,  2.19s/it]predicting train subjects:  26%|██▌       | 137/532 [04:13<14:33,  2.21s/it]predicting train subjects:  26%|██▌       | 138/532 [04:15<14:42,  2.24s/it]predicting train subjects:  26%|██▌       | 139/532 [04:17<14:36,  2.23s/it]predicting train subjects:  26%|██▋       | 140/532 [04:20<14:42,  2.25s/it]predicting train subjects:  27%|██▋       | 141/532 [04:22<14:39,  2.25s/it]predicting train subjects:  27%|██▋       | 142/532 [04:24<14:34,  2.24s/it]predicting train subjects:  27%|██▋       | 143/532 [04:26<13:32,  2.09s/it]predicting train subjects:  27%|██▋       | 144/532 [04:28<12:42,  1.97s/it]predicting train subjects:  27%|██▋       | 145/532 [04:29<12:11,  1.89s/it]predicting train subjects:  27%|██▋       | 146/532 [04:31<11:49,  1.84s/it]predicting train subjects:  28%|██▊       | 147/532 [04:33<11:32,  1.80s/it]predicting train subjects:  28%|██▊       | 148/532 [04:34<11:14,  1.76s/it]predicting train subjects:  28%|██▊       | 149/532 [04:36<11:24,  1.79s/it]predicting train subjects:  28%|██▊       | 150/532 [04:38<11:27,  1.80s/it]predicting train subjects:  28%|██▊       | 151/532 [04:40<11:25,  1.80s/it]predicting train subjects:  29%|██▊       | 152/532 [04:42<11:22,  1.80s/it]predicting train subjects:  29%|██▉       | 153/532 [04:43<11:25,  1.81s/it]predicting train subjects:  29%|██▉       | 154/532 [04:45<11:18,  1.80s/it]predicting train subjects:  29%|██▉       | 155/532 [04:47<12:09,  1.94s/it]predicting train subjects:  29%|██▉       | 156/532 [04:50<12:48,  2.04s/it]predicting train subjects:  30%|██▉       | 157/532 [04:52<13:20,  2.13s/it]predicting train subjects:  30%|██▉       | 158/532 [04:54<13:35,  2.18s/it]predicting train subjects:  30%|██▉       | 159/532 [04:57<13:51,  2.23s/it]predicting train subjects:  30%|███       | 160/532 [04:59<14:02,  2.27s/it]predicting train subjects:  30%|███       | 161/532 [05:01<13:09,  2.13s/it]predicting train subjects:  30%|███       | 162/532 [05:03<12:27,  2.02s/it]predicting train subjects:  31%|███       | 163/532 [05:04<11:57,  1.94s/it]predicting train subjects:  31%|███       | 164/532 [05:06<11:39,  1.90s/it]predicting train subjects:  31%|███       | 165/532 [05:08<11:30,  1.88s/it]predicting train subjects:  31%|███       | 166/532 [05:10<11:17,  1.85s/it]predicting train subjects:  31%|███▏      | 167/532 [05:12<11:05,  1.82s/it]predicting train subjects:  32%|███▏      | 168/532 [05:14<11:09,  1.84s/it]predicting train subjects:  32%|███▏      | 169/532 [05:15<11:00,  1.82s/it]predicting train subjects:  32%|███▏      | 170/532 [05:17<11:04,  1.84s/it]predicting train subjects:  32%|███▏      | 171/532 [05:19<11:05,  1.84s/it]predicting train subjects:  32%|███▏      | 172/532 [05:21<10:57,  1.83s/it]predicting train subjects:  33%|███▎      | 173/532 [05:22<10:33,  1.76s/it]predicting train subjects:  33%|███▎      | 174/532 [05:24<10:25,  1.75s/it]predicting train subjects:  33%|███▎      | 175/532 [05:26<10:11,  1.71s/it]predicting train subjects:  33%|███▎      | 176/532 [05:27<09:58,  1.68s/it]predicting train subjects:  33%|███▎      | 177/532 [05:29<09:52,  1.67s/it]predicting train subjects:  33%|███▎      | 178/532 [05:31<09:49,  1.66s/it]predicting train subjects:  34%|███▎      | 179/532 [05:32<09:43,  1.65s/it]predicting train subjects:  34%|███▍      | 180/532 [05:34<09:54,  1.69s/it]predicting train subjects:  34%|███▍      | 181/532 [05:36<09:56,  1.70s/it]predicting train subjects:  34%|███▍      | 182/532 [05:37<09:52,  1.69s/it]predicting train subjects:  34%|███▍      | 183/532 [05:39<09:47,  1.68s/it]predicting train subjects:  35%|███▍      | 184/532 [05:41<09:53,  1.70s/it]predicting train subjects:  35%|███▍      | 185/532 [05:43<09:44,  1.68s/it]predicting train subjects:  35%|███▍      | 186/532 [05:44<09:43,  1.69s/it]predicting train subjects:  35%|███▌      | 187/532 [05:46<09:36,  1.67s/it]predicting train subjects:  35%|███▌      | 188/532 [05:48<09:33,  1.67s/it]predicting train subjects:  36%|███▌      | 189/532 [05:49<09:31,  1.67s/it]predicting train subjects:  36%|███▌      | 190/532 [05:51<09:31,  1.67s/it]predicting train subjects:  36%|███▌      | 191/532 [05:53<10:52,  1.91s/it]predicting train subjects:  36%|███▌      | 192/532 [05:56<11:39,  2.06s/it]predicting train subjects:  36%|███▋      | 193/532 [05:58<12:04,  2.14s/it]predicting train subjects:  36%|███▋      | 194/532 [06:00<12:28,  2.22s/it]predicting train subjects:  37%|███▋      | 195/532 [06:03<12:35,  2.24s/it]predicting train subjects:  37%|███▋      | 196/532 [06:05<12:41,  2.27s/it]predicting train subjects:  37%|███▋      | 197/532 [06:07<12:14,  2.19s/it]predicting train subjects:  37%|███▋      | 198/532 [06:09<11:54,  2.14s/it]predicting train subjects:  37%|███▋      | 199/532 [06:11<11:44,  2.12s/it]predicting train subjects:  38%|███▊      | 200/532 [06:13<11:29,  2.08s/it]predicting train subjects:  38%|███▊      | 201/532 [06:15<11:17,  2.05s/it]predicting train subjects:  38%|███▊      | 202/532 [06:17<11:04,  2.01s/it]predicting train subjects:  38%|███▊      | 203/532 [06:19<10:29,  1.91s/it]predicting train subjects:  38%|███▊      | 204/532 [06:20<10:09,  1.86s/it]predicting train subjects:  39%|███▊      | 205/532 [06:22<09:58,  1.83s/it]predicting train subjects:  39%|███▊      | 206/532 [06:24<09:52,  1.82s/it]predicting train subjects:  39%|███▉      | 207/532 [06:26<09:48,  1.81s/it]predicting train subjects:  39%|███▉      | 208/532 [06:28<09:50,  1.82s/it]predicting train subjects:  39%|███▉      | 209/532 [06:29<09:45,  1.81s/it]predicting train subjects:  39%|███▉      | 210/532 [06:31<09:19,  1.74s/it]predicting train subjects:  40%|███▉      | 211/532 [06:33<09:02,  1.69s/it]predicting train subjects:  40%|███▉      | 212/532 [06:34<08:53,  1.67s/it]predicting train subjects:  40%|████      | 213/532 [06:36<08:44,  1.64s/it]predicting train subjects:  40%|████      | 214/532 [06:37<08:38,  1.63s/it]predicting train subjects:  40%|████      | 215/532 [06:40<09:33,  1.81s/it]predicting train subjects:  41%|████      | 216/532 [06:42<10:12,  1.94s/it]predicting train subjects:  41%|████      | 217/532 [06:44<10:33,  2.01s/it]predicting train subjects:  41%|████      | 218/532 [06:46<11:07,  2.13s/it]predicting train subjects:  41%|████      | 219/532 [06:49<11:19,  2.17s/it]predicting train subjects:  41%|████▏     | 220/532 [06:51<11:31,  2.22s/it]predicting train subjects:  42%|████▏     | 221/532 [06:53<10:37,  2.05s/it]predicting train subjects:  42%|████▏     | 222/532 [06:54<09:58,  1.93s/it]predicting train subjects:  42%|████▏     | 223/532 [06:56<09:34,  1.86s/it]predicting train subjects:  42%|████▏     | 224/532 [06:58<09:12,  1.79s/it]predicting train subjects:  42%|████▏     | 225/532 [06:59<08:54,  1.74s/it]predicting train subjects:  42%|████▏     | 226/532 [07:01<08:40,  1.70s/it]predicting train subjects:  43%|████▎     | 227/532 [07:03<08:28,  1.67s/it]predicting train subjects:  43%|████▎     | 228/532 [07:04<08:16,  1.63s/it]predicting train subjects:  43%|████▎     | 229/532 [07:06<08:13,  1.63s/it]predicting train subjects:  43%|████▎     | 230/532 [07:07<08:02,  1.60s/it]predicting train subjects:  43%|████▎     | 231/532 [07:09<07:53,  1.57s/it]predicting train subjects:  44%|████▎     | 232/532 [07:10<07:57,  1.59s/it]predicting train subjects:  44%|████▍     | 233/532 [07:12<08:21,  1.68s/it]predicting train subjects:  44%|████▍     | 234/532 [07:14<08:28,  1.71s/it]predicting train subjects:  44%|████▍     | 235/532 [07:16<08:34,  1.73s/it]predicting train subjects:  44%|████▍     | 236/532 [07:18<08:34,  1.74s/it]predicting train subjects:  45%|████▍     | 237/532 [07:19<08:31,  1.73s/it]predicting train subjects:  45%|████▍     | 238/532 [07:21<08:36,  1.76s/it]predicting train subjects:  45%|████▍     | 239/532 [07:23<08:49,  1.81s/it]predicting train subjects:  45%|████▌     | 240/532 [07:25<08:56,  1.84s/it]predicting train subjects:  45%|████▌     | 241/532 [07:27<09:05,  1.87s/it]predicting train subjects:  45%|████▌     | 242/532 [07:29<09:05,  1.88s/it]predicting train subjects:  46%|████▌     | 243/532 [07:31<09:08,  1.90s/it]predicting train subjects:  46%|████▌     | 244/532 [07:33<09:06,  1.90s/it]predicting train subjects:  46%|████▌     | 245/532 [07:34<08:36,  1.80s/it]predicting train subjects:  46%|████▌     | 246/532 [07:36<08:11,  1.72s/it]predicting train subjects:  46%|████▋     | 247/532 [07:37<07:50,  1.65s/it]predicting train subjects:  47%|████▋     | 248/532 [07:39<07:36,  1.61s/it]predicting train subjects:  47%|████▋     | 249/532 [07:40<07:26,  1.58s/it]predicting train subjects:  47%|████▋     | 250/532 [07:42<07:17,  1.55s/it]predicting train subjects:  47%|████▋     | 251/532 [07:43<07:22,  1.58s/it]predicting train subjects:  47%|████▋     | 252/532 [07:45<07:27,  1.60s/it]predicting train subjects:  48%|████▊     | 253/532 [07:47<07:26,  1.60s/it]predicting train subjects:  48%|████▊     | 254/532 [07:48<07:29,  1.62s/it]predicting train subjects:  48%|████▊     | 255/532 [07:50<07:31,  1.63s/it]predicting train subjects:  48%|████▊     | 256/532 [07:51<07:25,  1.61s/it]predicting train subjects:  48%|████▊     | 257/532 [07:54<07:59,  1.74s/it]predicting train subjects:  48%|████▊     | 258/532 [07:56<08:16,  1.81s/it]predicting train subjects:  49%|████▊     | 259/532 [07:58<08:33,  1.88s/it]predicting train subjects:  49%|████▉     | 260/532 [08:00<08:40,  1.91s/it]predicting train subjects:  49%|████▉     | 261/532 [08:02<08:43,  1.93s/it]predicting train subjects:  49%|████▉     | 262/532 [08:04<08:53,  1.98s/it]predicting train subjects:  49%|████▉     | 263/532 [08:05<08:16,  1.85s/it]predicting train subjects:  50%|████▉     | 264/532 [08:07<07:47,  1.75s/it]predicting train subjects:  50%|████▉     | 265/532 [08:08<07:23,  1.66s/it]predicting train subjects:  50%|█████     | 266/532 [08:10<07:04,  1.60s/it]predicting train subjects:  50%|█████     | 267/532 [08:11<06:51,  1.55s/it]predicting train subjects:  50%|█████     | 268/532 [08:12<06:38,  1.51s/it]predicting train subjects:  51%|█████     | 269/532 [08:14<07:03,  1.61s/it]predicting train subjects:  51%|█████     | 270/532 [08:16<07:16,  1.67s/it]predicting train subjects:  51%|█████     | 271/532 [08:18<07:26,  1.71s/it]predicting train subjects:  51%|█████     | 272/532 [08:20<07:31,  1.74s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:22<07:39,  1.77s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:23<07:38,  1.78s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:25<08:07,  1.90s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:28<08:25,  1.97s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:30<08:32,  2.01s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:32<08:41,  2.05s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:34<08:47,  2.09s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:36<08:46,  2.09s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:38<08:39,  2.07s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:40<08:39,  2.08s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:42<08:33,  2.06s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:44<08:28,  2.05s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:46<08:23,  2.04s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:48<08:17,  2.02s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:50<07:43,  1.89s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:52<07:24,  1.82s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:53<07:09,  1.77s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:55<06:53,  1.71s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:56<06:46,  1.69s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:58<06:51,  1.72s/it]predicting train subjects:  55%|█████▌    | 293/532 [09:00<06:56,  1.74s/it]predicting train subjects:  55%|█████▌    | 294/532 [09:02<06:58,  1.76s/it]predicting train subjects:  55%|█████▌    | 295/532 [09:04<06:57,  1.76s/it]predicting train subjects:  56%|█████▌    | 296/532 [09:05<06:59,  1.78s/it]predicting train subjects:  56%|█████▌    | 297/532 [09:07<06:59,  1.78s/it]predicting train subjects:  56%|█████▌    | 298/532 [09:09<06:58,  1.79s/it]predicting train subjects:  56%|█████▌    | 299/532 [09:11<06:40,  1.72s/it]predicting train subjects:  56%|█████▋    | 300/532 [09:12<06:29,  1.68s/it]predicting train subjects:  57%|█████▋    | 301/532 [09:14<06:20,  1.65s/it]predicting train subjects:  57%|█████▋    | 302/532 [09:15<06:12,  1.62s/it]predicting train subjects:  57%|█████▋    | 303/532 [09:17<06:08,  1.61s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:18<06:03,  1.59s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:21<06:42,  1.77s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:23<07:07,  1.89s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:25<07:22,  1.97s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:27<07:33,  2.02s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:29<07:38,  2.05s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:31<07:45,  2.09s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:34<08:35,  2.33s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:37<09:07,  2.49s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:40<09:27,  2.59s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:43<09:40,  2.66s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:46<09:55,  2.74s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:49<09:58,  2.77s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:50<08:48,  2.46s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:52<07:59,  2.24s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:54<07:23,  2.08s/it]predicting train subjects:  60%|██████    | 320/532 [09:55<06:56,  1.97s/it]predicting train subjects:  60%|██████    | 321/532 [09:57<06:33,  1.86s/it]predicting train subjects:  61%|██████    | 322/532 [09:59<06:20,  1.81s/it]predicting train subjects:  61%|██████    | 323/532 [10:01<06:48,  1.96s/it]predicting train subjects:  61%|██████    | 324/532 [10:03<07:05,  2.05s/it]predicting train subjects:  61%|██████    | 325/532 [10:06<07:17,  2.11s/it]predicting train subjects:  61%|██████▏   | 326/532 [10:08<07:25,  2.16s/it]predicting train subjects:  61%|██████▏   | 327/532 [10:10<07:30,  2.20s/it]predicting train subjects:  62%|██████▏   | 328/532 [10:12<07:37,  2.24s/it]predicting train subjects:  62%|██████▏   | 329/532 [10:14<07:05,  2.10s/it]predicting train subjects:  62%|██████▏   | 330/532 [10:16<06:44,  2.00s/it]predicting train subjects:  62%|██████▏   | 331/532 [10:18<06:29,  1.94s/it]predicting train subjects:  62%|██████▏   | 332/532 [10:20<06:15,  1.88s/it]predicting train subjects:  63%|██████▎   | 333/532 [10:21<06:04,  1.83s/it]predicting train subjects:  63%|██████▎   | 334/532 [10:23<05:58,  1.81s/it]predicting train subjects:  63%|██████▎   | 335/532 [10:25<06:04,  1.85s/it]predicting train subjects:  63%|██████▎   | 336/532 [10:27<06:08,  1.88s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:29<06:06,  1.88s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:31<06:12,  1.92s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:33<06:11,  1.92s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:35<06:06,  1.91s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:36<05:50,  1.84s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:38<05:34,  1.76s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:39<05:23,  1.71s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:41<05:07,  1.64s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:42<04:55,  1.58s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:44<04:52,  1.57s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:46<04:56,  1.60s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:47<05:03,  1.65s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:49<05:12,  1.71s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:51<05:16,  1.74s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:53<05:17,  1.75s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:55<05:14,  1.75s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:56<05:13,  1.75s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:58<05:12,  1.75s/it]predicting train subjects:  67%|██████▋   | 355/532 [11:00<05:08,  1.74s/it]predicting train subjects:  67%|██████▋   | 356/532 [11:01<05:06,  1.74s/it]predicting train subjects:  67%|██████▋   | 357/532 [11:03<05:10,  1.78s/it]predicting train subjects:  67%|██████▋   | 358/532 [11:05<05:08,  1.77s/it]predicting train subjects:  67%|██████▋   | 359/532 [11:07<04:53,  1.69s/it]predicting train subjects:  68%|██████▊   | 360/532 [11:08<04:45,  1.66s/it]predicting train subjects:  68%|██████▊   | 361/532 [11:10<04:38,  1.63s/it]predicting train subjects:  68%|██████▊   | 362/532 [11:11<04:33,  1.61s/it]predicting train subjects:  68%|██████▊   | 363/532 [11:13<04:27,  1.59s/it]predicting train subjects:  68%|██████▊   | 364/532 [11:14<04:22,  1.56s/it]predicting train subjects:  69%|██████▊   | 365/532 [11:16<04:20,  1.56s/it]predicting train subjects:  69%|██████▉   | 366/532 [11:17<04:19,  1.56s/it]predicting train subjects:  69%|██████▉   | 367/532 [11:19<04:17,  1.56s/it]predicting train subjects:  69%|██████▉   | 368/532 [11:21<04:15,  1.56s/it]predicting train subjects:  69%|██████▉   | 369/532 [11:22<04:15,  1.57s/it]predicting train subjects:  70%|██████▉   | 370/532 [11:24<04:12,  1.56s/it]predicting train subjects:  70%|██████▉   | 371/532 [11:26<04:40,  1.74s/it]predicting train subjects:  70%|██████▉   | 372/532 [11:28<05:00,  1.88s/it]predicting train subjects:  70%|███████   | 373/532 [11:30<05:15,  1.99s/it]predicting train subjects:  70%|███████   | 374/532 [11:32<05:23,  2.05s/it]predicting train subjects:  70%|███████   | 375/532 [11:35<05:28,  2.09s/it]predicting train subjects:  71%|███████   | 376/532 [11:37<05:31,  2.12s/it]predicting train subjects:  71%|███████   | 377/532 [11:39<05:10,  2.00s/it]predicting train subjects:  71%|███████   | 378/532 [11:40<04:59,  1.94s/it]predicting train subjects:  71%|███████   | 379/532 [11:42<04:50,  1.90s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:44<04:44,  1.87s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:46<04:38,  1.84s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:48<04:33,  1.83s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:50<04:37,  1.86s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:51<04:37,  1.87s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:53<04:35,  1.87s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:55<04:32,  1.87s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:57<04:32,  1.88s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:59<04:29,  1.87s/it]predicting train subjects:  73%|███████▎  | 389/532 [12:01<04:25,  1.85s/it]predicting train subjects:  73%|███████▎  | 390/532 [12:03<04:27,  1.88s/it]predicting train subjects:  73%|███████▎  | 391/532 [12:05<04:30,  1.92s/it]predicting train subjects:  74%|███████▎  | 392/532 [12:07<04:30,  1.93s/it]predicting train subjects:  74%|███████▍  | 393/532 [12:09<04:31,  1.95s/it]predicting train subjects:  74%|███████▍  | 394/532 [12:11<04:29,  1.95s/it]predicting train subjects:  74%|███████▍  | 395/532 [12:13<04:25,  1.94s/it]predicting train subjects:  74%|███████▍  | 396/532 [12:14<04:21,  1.92s/it]predicting train subjects:  75%|███████▍  | 397/532 [12:16<04:17,  1.90s/it]predicting train subjects:  75%|███████▍  | 398/532 [12:18<04:13,  1.89s/it]predicting train subjects:  75%|███████▌  | 399/532 [12:20<04:10,  1.88s/it]predicting train subjects:  75%|███████▌  | 400/532 [12:22<04:06,  1.87s/it]predicting train subjects:  75%|███████▌  | 401/532 [12:24<04:12,  1.93s/it]predicting train subjects:  76%|███████▌  | 402/532 [12:26<04:13,  1.95s/it]predicting train subjects:  76%|███████▌  | 403/532 [12:28<04:17,  1.99s/it]predicting train subjects:  76%|███████▌  | 404/532 [12:30<04:19,  2.03s/it]predicting train subjects:  76%|███████▌  | 405/532 [12:32<04:20,  2.05s/it]predicting train subjects:  76%|███████▋  | 406/532 [12:34<04:16,  2.04s/it]predicting train subjects:  77%|███████▋  | 407/532 [12:36<04:09,  1.99s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:38<04:00,  1.94s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:40<03:51,  1.88s/it]predicting train subjects:  77%|███████▋  | 410/532 [12:41<03:46,  1.85s/it]predicting train subjects:  77%|███████▋  | 411/532 [12:43<03:43,  1.84s/it]predicting train subjects:  77%|███████▋  | 412/532 [12:45<03:38,  1.82s/it]predicting train subjects:  78%|███████▊  | 413/532 [12:47<03:34,  1.80s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:48<03:27,  1.76s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:50<03:22,  1.73s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:52<03:18,  1.71s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:53<03:15,  1.70s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:55<03:12,  1.69s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:57<03:18,  1.76s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:59<03:22,  1.81s/it]predicting train subjects:  79%|███████▉  | 421/532 [13:01<03:27,  1.87s/it]predicting train subjects:  79%|███████▉  | 422/532 [13:03<03:28,  1.89s/it]predicting train subjects:  80%|███████▉  | 423/532 [13:05<03:27,  1.90s/it]predicting train subjects:  80%|███████▉  | 424/532 [13:07<03:24,  1.90s/it]predicting train subjects:  80%|███████▉  | 425/532 [13:09<03:23,  1.90s/it]predicting train subjects:  80%|████████  | 426/532 [13:11<03:21,  1.90s/it]predicting train subjects:  80%|████████  | 427/532 [13:12<03:19,  1.90s/it]predicting train subjects:  80%|████████  | 428/532 [13:14<03:19,  1.92s/it]predicting train subjects:  81%|████████  | 429/532 [13:16<03:16,  1.91s/it]predicting train subjects:  81%|████████  | 430/532 [13:18<03:16,  1.93s/it]predicting train subjects:  81%|████████  | 431/532 [13:20<03:19,  1.98s/it]predicting train subjects:  81%|████████  | 432/532 [13:22<03:20,  2.01s/it]predicting train subjects:  81%|████████▏ | 433/532 [13:25<03:20,  2.03s/it]predicting train subjects:  82%|████████▏ | 434/532 [13:27<03:19,  2.03s/it]predicting train subjects:  82%|████████▏ | 435/532 [13:29<03:19,  2.05s/it]predicting train subjects:  82%|████████▏ | 436/532 [13:31<03:17,  2.05s/it]predicting train subjects:  82%|████████▏ | 437/532 [13:32<03:00,  1.90s/it]predicting train subjects:  82%|████████▏ | 438/532 [13:34<02:49,  1.80s/it]predicting train subjects:  83%|████████▎ | 439/532 [13:35<02:40,  1.72s/it]predicting train subjects:  83%|████████▎ | 440/532 [13:37<02:34,  1.67s/it]predicting train subjects:  83%|████████▎ | 441/532 [13:38<02:29,  1.64s/it]predicting train subjects:  83%|████████▎ | 442/532 [13:40<02:25,  1.61s/it]predicting train subjects:  83%|████████▎ | 443/532 [13:42<02:21,  1.59s/it]predicting train subjects:  83%|████████▎ | 444/532 [13:43<02:19,  1.58s/it]predicting train subjects:  84%|████████▎ | 445/532 [13:45<02:15,  1.56s/it]predicting train subjects:  84%|████████▍ | 446/532 [13:46<02:12,  1.54s/it]predicting train subjects:  84%|████████▍ | 447/532 [13:48<02:10,  1.53s/it]predicting train subjects:  84%|████████▍ | 448/532 [13:49<02:07,  1.51s/it]predicting train subjects:  84%|████████▍ | 449/532 [13:51<02:08,  1.54s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:52<02:08,  1.56s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:54<02:07,  1.58s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:56<02:07,  1.60s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:57<02:08,  1.62s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:59<02:07,  1.64s/it]predicting train subjects:  86%|████████▌ | 455/532 [14:01<02:11,  1.70s/it]predicting train subjects:  86%|████████▌ | 456/532 [14:03<02:12,  1.75s/it]predicting train subjects:  86%|████████▌ | 457/532 [14:05<02:14,  1.80s/it]predicting train subjects:  86%|████████▌ | 458/532 [14:06<02:15,  1.83s/it]predicting train subjects:  86%|████████▋ | 459/532 [14:08<02:14,  1.84s/it]predicting train subjects:  86%|████████▋ | 460/532 [14:10<02:13,  1.85s/it]predicting train subjects:  87%|████████▋ | 461/532 [14:12<02:18,  1.95s/it]predicting train subjects:  87%|████████▋ | 462/532 [14:15<02:21,  2.02s/it]predicting train subjects:  87%|████████▋ | 463/532 [14:17<02:23,  2.07s/it]predicting train subjects:  87%|████████▋ | 464/532 [14:19<02:22,  2.09s/it]predicting train subjects:  87%|████████▋ | 465/532 [14:21<02:21,  2.11s/it]predicting train subjects:  88%|████████▊ | 466/532 [14:23<02:19,  2.12s/it]predicting train subjects:  88%|████████▊ | 467/532 [14:25<02:10,  2.01s/it]predicting train subjects:  88%|████████▊ | 468/532 [14:27<02:02,  1.92s/it]predicting train subjects:  88%|████████▊ | 469/532 [14:28<01:57,  1.87s/it]predicting train subjects:  88%|████████▊ | 470/532 [14:30<01:53,  1.83s/it]predicting train subjects:  89%|████████▊ | 471/532 [14:32<01:49,  1.80s/it]predicting train subjects:  89%|████████▊ | 472/532 [14:34<01:46,  1.78s/it]predicting train subjects:  89%|████████▉ | 473/532 [14:35<01:46,  1.81s/it]predicting train subjects:  89%|████████▉ | 474/532 [14:37<01:47,  1.85s/it]predicting train subjects:  89%|████████▉ | 475/532 [14:39<01:47,  1.88s/it]predicting train subjects:  89%|████████▉ | 476/532 [14:41<01:44,  1.87s/it]predicting train subjects:  90%|████████▉ | 477/532 [14:43<01:43,  1.88s/it]predicting train subjects:  90%|████████▉ | 478/532 [14:45<01:42,  1.90s/it]predicting train subjects:  90%|█████████ | 479/532 [14:47<01:37,  1.84s/it]predicting train subjects:  90%|█████████ | 480/532 [14:48<01:32,  1.79s/it]predicting train subjects:  90%|█████████ | 481/532 [14:50<01:28,  1.74s/it]predicting train subjects:  91%|█████████ | 482/532 [14:52<01:24,  1.69s/it]predicting train subjects:  91%|█████████ | 483/532 [14:53<01:22,  1.68s/it]predicting train subjects:  91%|█████████ | 484/532 [14:55<01:20,  1.67s/it]predicting train subjects:  91%|█████████ | 485/532 [14:57<01:24,  1.80s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:59<01:27,  1.91s/it]predicting train subjects:  92%|█████████▏| 487/532 [15:01<01:26,  1.93s/it]predicting train subjects:  92%|█████████▏| 488/532 [15:03<01:25,  1.94s/it]predicting train subjects:  92%|█████████▏| 489/532 [15:05<01:24,  1.97s/it]predicting train subjects:  92%|█████████▏| 490/532 [15:07<01:24,  2.01s/it]predicting train subjects:  92%|█████████▏| 491/532 [15:09<01:19,  1.93s/it]predicting train subjects:  92%|█████████▏| 492/532 [15:11<01:15,  1.88s/it]predicting train subjects:  93%|█████████▎| 493/532 [15:13<01:11,  1.84s/it]predicting train subjects:  93%|█████████▎| 494/532 [15:14<01:09,  1.82s/it]predicting train subjects:  93%|█████████▎| 495/532 [15:16<01:06,  1.80s/it]predicting train subjects:  93%|█████████▎| 496/532 [15:18<01:04,  1.80s/it]predicting train subjects:  93%|█████████▎| 497/532 [15:20<01:03,  1.80s/it]predicting train subjects:  94%|█████████▎| 498/532 [15:22<01:01,  1.81s/it]predicting train subjects:  94%|█████████▍| 499/532 [15:23<00:59,  1.80s/it]predicting train subjects:  94%|█████████▍| 500/532 [15:25<00:57,  1.80s/it]predicting train subjects:  94%|█████████▍| 501/532 [15:27<00:56,  1.81s/it]predicting train subjects:  94%|█████████▍| 502/532 [15:29<00:54,  1.81s/it]predicting train subjects:  95%|█████████▍| 503/532 [15:31<00:52,  1.80s/it]predicting train subjects:  95%|█████████▍| 504/532 [15:32<00:49,  1.78s/it]predicting train subjects:  95%|█████████▍| 505/532 [15:34<00:47,  1.76s/it]predicting train subjects:  95%|█████████▌| 506/532 [15:36<00:45,  1.75s/it]predicting train subjects:  95%|█████████▌| 507/532 [15:37<00:43,  1.75s/it]predicting train subjects:  95%|█████████▌| 508/532 [15:39<00:41,  1.74s/it]predicting train subjects:  96%|█████████▌| 509/532 [15:41<00:42,  1.85s/it]predicting train subjects:  96%|█████████▌| 510/532 [15:43<00:42,  1.95s/it]predicting train subjects:  96%|█████████▌| 511/532 [15:45<00:41,  1.96s/it]predicting train subjects:  96%|█████████▌| 512/532 [15:48<00:40,  2.01s/it]predicting train subjects:  96%|█████████▋| 513/532 [15:50<00:38,  2.03s/it]predicting train subjects:  97%|█████████▋| 514/532 [15:52<00:36,  2.04s/it]predicting train subjects:  97%|█████████▋| 515/532 [15:53<00:33,  1.95s/it]predicting train subjects:  97%|█████████▋| 516/532 [15:55<00:30,  1.89s/it]predicting train subjects:  97%|█████████▋| 517/532 [15:57<00:28,  1.87s/it]predicting train subjects:  97%|█████████▋| 518/532 [15:59<00:25,  1.85s/it]predicting train subjects:  98%|█████████▊| 519/532 [16:01<00:23,  1.84s/it]predicting train subjects:  98%|█████████▊| 520/532 [16:02<00:21,  1.82s/it]predicting train subjects:  98%|█████████▊| 521/532 [16:04<00:20,  1.85s/it]predicting train subjects:  98%|█████████▊| 522/532 [16:06<00:18,  1.86s/it]predicting train subjects:  98%|█████████▊| 523/532 [16:08<00:16,  1.87s/it]predicting train subjects:  98%|█████████▊| 524/532 [16:10<00:14,  1.85s/it]predicting train subjects:  99%|█████████▊| 525/532 [16:12<00:13,  1.88s/it]predicting train subjects:  99%|█████████▉| 526/532 [16:14<00:11,  1.87s/it]predicting train subjects:  99%|█████████▉| 527/532 [16:15<00:09,  1.80s/it]predicting train subjects:  99%|█████████▉| 528/532 [16:17<00:07,  1.76s/it]predicting train subjects:  99%|█████████▉| 529/532 [16:19<00:05,  1.72s/it]predicting train subjects: 100%|█████████▉| 530/532 [16:20<00:03,  1.71s/it]predicting train subjects: 100%|█████████▉| 531/532 [16:22<00:01,  1.71s/it]predicting train subjects: 100%|██████████| 532/532 [16:24<00:00,  1.70s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<11:25,  1.29s/it]Loading train:   0%|          | 2/532 [00:02<10:29,  1.19s/it]Loading train:   1%|          | 3/532 [00:03<09:45,  1.11s/it]Loading train:   1%|          | 4/532 [00:03<08:58,  1.02s/it]Loading train:   1%|          | 5/532 [00:04<08:42,  1.01it/s]Loading train:   1%|          | 6/532 [00:05<08:25,  1.04it/s]Loading train:   1%|▏         | 7/532 [00:06<08:24,  1.04it/s]Loading train:   2%|▏         | 8/532 [00:07<08:02,  1.09it/s]Loading train:   2%|▏         | 9/532 [00:08<08:26,  1.03it/s]Loading train:   2%|▏         | 10/532 [00:09<08:04,  1.08it/s]Loading train:   2%|▏         | 11/532 [00:10<08:09,  1.06it/s]Loading train:   2%|▏         | 12/532 [00:11<08:44,  1.01s/it]Loading train:   2%|▏         | 13/532 [00:12<08:10,  1.06it/s]Loading train:   3%|▎         | 14/532 [00:13<07:36,  1.14it/s]Loading train:   3%|▎         | 15/532 [00:13<07:24,  1.16it/s]Loading train:   3%|▎         | 16/532 [00:14<07:27,  1.15it/s]Loading train:   3%|▎         | 17/532 [00:15<07:06,  1.21it/s]Loading train:   3%|▎         | 18/532 [00:16<07:24,  1.16it/s]Loading train:   4%|▎         | 19/532 [00:17<07:05,  1.20it/s]Loading train:   4%|▍         | 20/532 [00:18<07:10,  1.19it/s]Loading train:   4%|▍         | 21/532 [00:19<08:02,  1.06it/s]Loading train:   4%|▍         | 22/532 [00:20<07:39,  1.11it/s]Loading train:   4%|▍         | 23/532 [00:21<07:44,  1.10it/s]Loading train:   5%|▍         | 24/532 [00:21<07:23,  1.14it/s]Loading train:   5%|▍         | 25/532 [00:22<07:56,  1.06it/s]Loading train:   5%|▍         | 26/532 [00:23<07:35,  1.11it/s]Loading train:   5%|▌         | 27/532 [00:24<08:17,  1.02it/s]Loading train:   5%|▌         | 28/532 [00:25<07:51,  1.07it/s]Loading train:   5%|▌         | 29/532 [00:26<08:04,  1.04it/s]Loading train:   6%|▌         | 30/532 [00:27<07:23,  1.13it/s]Loading train:   6%|▌         | 31/532 [00:28<07:21,  1.13it/s]Loading train:   6%|▌         | 32/532 [00:29<07:36,  1.09it/s]Loading train:   6%|▌         | 33/532 [00:30<07:15,  1.15it/s]Loading train:   6%|▋         | 34/532 [00:31<07:45,  1.07it/s]Loading train:   7%|▋         | 35/532 [00:31<07:10,  1.15it/s]Loading train:   7%|▋         | 36/532 [00:32<07:06,  1.16it/s]Loading train:   7%|▋         | 37/532 [00:33<07:02,  1.17it/s]Loading train:   7%|▋         | 38/532 [00:34<07:08,  1.15it/s]Loading train:   7%|▋         | 39/532 [00:35<07:21,  1.12it/s]Loading train:   8%|▊         | 40/532 [00:36<07:04,  1.16it/s]Loading train:   8%|▊         | 41/532 [00:37<07:26,  1.10it/s]Loading train:   8%|▊         | 42/532 [00:38<07:36,  1.07it/s]Loading train:   8%|▊         | 43/532 [00:39<07:23,  1.10it/s]Loading train:   8%|▊         | 44/532 [00:39<07:02,  1.15it/s]Loading train:   8%|▊         | 45/532 [00:40<06:54,  1.17it/s]Loading train:   9%|▊         | 46/532 [00:41<06:59,  1.16it/s]Loading train:   9%|▉         | 47/532 [00:42<07:21,  1.10it/s]Loading train:   9%|▉         | 48/532 [00:43<07:30,  1.07it/s]Loading train:   9%|▉         | 49/532 [00:44<07:18,  1.10it/s]Loading train:   9%|▉         | 50/532 [00:45<07:48,  1.03it/s]Loading train:  10%|▉         | 51/532 [00:46<07:33,  1.06it/s]Loading train:  10%|▉         | 52/532 [00:47<07:23,  1.08it/s]Loading train:  10%|▉         | 53/532 [00:48<07:22,  1.08it/s]Loading train:  10%|█         | 54/532 [00:49<07:28,  1.07it/s]Loading train:  10%|█         | 55/532 [00:50<07:29,  1.06it/s]Loading train:  11%|█         | 56/532 [00:51<07:27,  1.06it/s]Loading train:  11%|█         | 57/532 [00:52<07:23,  1.07it/s]Loading train:  11%|█         | 58/532 [00:53<07:32,  1.05it/s]Loading train:  11%|█         | 59/532 [00:54<07:58,  1.01s/it]Loading train:  11%|█▏        | 60/532 [00:54<07:18,  1.08it/s]Loading train:  11%|█▏        | 61/532 [00:55<06:48,  1.15it/s]Loading train:  12%|█▏        | 62/532 [00:56<07:10,  1.09it/s]Loading train:  12%|█▏        | 63/532 [00:57<07:33,  1.03it/s]Loading train:  12%|█▏        | 64/532 [00:58<07:07,  1.10it/s]Loading train:  12%|█▏        | 65/532 [00:59<07:06,  1.09it/s]Loading train:  12%|█▏        | 66/532 [01:00<07:57,  1.02s/it]Loading train:  13%|█▎        | 67/532 [01:01<07:56,  1.03s/it]Loading train:  13%|█▎        | 68/532 [01:02<07:47,  1.01s/it]Loading train:  13%|█▎        | 69/532 [01:03<07:25,  1.04it/s]Loading train:  13%|█▎        | 70/532 [01:04<07:13,  1.07it/s]Loading train:  13%|█▎        | 71/532 [01:05<06:46,  1.13it/s]Loading train:  14%|█▎        | 72/532 [01:06<06:38,  1.16it/s]Loading train:  14%|█▎        | 73/532 [01:07<06:52,  1.11it/s]Loading train:  14%|█▍        | 74/532 [01:08<07:40,  1.01s/it]Loading train:  14%|█▍        | 75/532 [01:09<08:38,  1.13s/it]Loading train:  14%|█▍        | 76/532 [01:10<08:03,  1.06s/it]Loading train:  14%|█▍        | 77/532 [01:11<07:38,  1.01s/it]Loading train:  15%|█▍        | 78/532 [01:12<07:28,  1.01it/s]Loading train:  15%|█▍        | 79/532 [01:13<07:10,  1.05it/s]Loading train:  15%|█▌        | 80/532 [01:14<07:03,  1.07it/s]Loading train:  15%|█▌        | 81/532 [01:15<06:56,  1.08it/s]Loading train:  15%|█▌        | 82/532 [01:15<06:49,  1.10it/s]Loading train:  16%|█▌        | 83/532 [01:16<06:27,  1.16it/s]Loading train:  16%|█▌        | 84/532 [01:17<06:17,  1.19it/s]Loading train:  16%|█▌        | 85/532 [01:18<06:07,  1.22it/s]Loading train:  16%|█▌        | 86/532 [01:19<05:54,  1.26it/s]Loading train:  16%|█▋        | 87/532 [01:19<05:47,  1.28it/s]Loading train:  17%|█▋        | 88/532 [01:20<05:42,  1.30it/s]Loading train:  17%|█▋        | 89/532 [01:21<05:52,  1.26it/s]Loading train:  17%|█▋        | 90/532 [01:22<05:59,  1.23it/s]Loading train:  17%|█▋        | 91/532 [01:23<06:07,  1.20it/s]Loading train:  17%|█▋        | 92/532 [01:24<06:20,  1.16it/s]Loading train:  17%|█▋        | 93/532 [01:24<06:13,  1.17it/s]Loading train:  18%|█▊        | 94/532 [01:25<06:16,  1.16it/s]Loading train:  18%|█▊        | 95/532 [01:26<06:41,  1.09it/s]Loading train:  18%|█▊        | 96/532 [01:27<07:07,  1.02it/s]Loading train:  18%|█▊        | 97/532 [01:29<07:27,  1.03s/it]Loading train:  18%|█▊        | 98/532 [01:30<07:21,  1.02s/it]Loading train:  19%|█▊        | 99/532 [01:31<07:26,  1.03s/it]Loading train:  19%|█▉        | 100/532 [01:32<07:29,  1.04s/it]Loading train:  19%|█▉        | 101/532 [01:33<07:11,  1.00s/it]Loading train:  19%|█▉        | 102/532 [01:33<06:35,  1.09it/s]Loading train:  19%|█▉        | 103/532 [01:34<06:13,  1.15it/s]Loading train:  20%|█▉        | 104/532 [01:35<05:53,  1.21it/s]Loading train:  20%|█▉        | 105/532 [01:36<05:40,  1.25it/s]Loading train:  20%|█▉        | 106/532 [01:36<05:32,  1.28it/s]Loading train:  20%|██        | 107/532 [01:37<05:36,  1.26it/s]Loading train:  20%|██        | 108/532 [01:38<05:32,  1.28it/s]Loading train:  20%|██        | 109/532 [01:39<05:29,  1.28it/s]Loading train:  21%|██        | 110/532 [01:39<05:25,  1.30it/s]Loading train:  21%|██        | 111/532 [01:40<05:26,  1.29it/s]Loading train:  21%|██        | 112/532 [01:41<05:22,  1.30it/s]Loading train:  21%|██        | 113/532 [01:42<05:39,  1.24it/s]Loading train:  21%|██▏       | 114/532 [01:43<05:39,  1.23it/s]Loading train:  22%|██▏       | 115/532 [01:43<05:46,  1.21it/s]Loading train:  22%|██▏       | 116/532 [01:44<05:52,  1.18it/s]Loading train:  22%|██▏       | 117/532 [01:45<05:55,  1.17it/s]Loading train:  22%|██▏       | 118/532 [01:46<05:53,  1.17it/s]Loading train:  22%|██▏       | 119/532 [01:47<06:01,  1.14it/s]Loading train:  23%|██▎       | 120/532 [01:48<05:54,  1.16it/s]Loading train:  23%|██▎       | 121/532 [01:49<05:50,  1.17it/s]Loading train:  23%|██▎       | 122/532 [01:50<06:00,  1.14it/s]Loading train:  23%|██▎       | 123/532 [01:51<06:00,  1.13it/s]Loading train:  23%|██▎       | 124/532 [01:51<05:59,  1.14it/s]Loading train:  23%|██▎       | 125/532 [01:53<06:35,  1.03it/s]Loading train:  24%|██▎       | 126/532 [01:54<06:34,  1.03it/s]Loading train:  24%|██▍       | 127/532 [01:55<06:48,  1.01s/it]Loading train:  24%|██▍       | 128/532 [01:56<06:55,  1.03s/it]Loading train:  24%|██▍       | 129/532 [01:57<06:45,  1.01s/it]Loading train:  24%|██▍       | 130/532 [01:58<06:40,  1.00it/s]Loading train:  25%|██▍       | 131/532 [01:59<06:54,  1.03s/it]Loading train:  25%|██▍       | 132/532 [02:00<07:11,  1.08s/it]Loading train:  25%|██▌       | 133/532 [02:01<07:17,  1.10s/it]Loading train:  25%|██▌       | 134/532 [02:02<07:16,  1.10s/it]Loading train:  25%|██▌       | 135/532 [02:03<07:23,  1.12s/it]Loading train:  26%|██▌       | 136/532 [02:04<07:16,  1.10s/it]Loading train:  26%|██▌       | 137/532 [02:06<07:24,  1.13s/it]Loading train:  26%|██▌       | 138/532 [02:07<07:21,  1.12s/it]Loading train:  26%|██▌       | 139/532 [02:08<07:20,  1.12s/it]Loading train:  26%|██▋       | 140/532 [02:09<07:22,  1.13s/it]Loading train:  27%|██▋       | 141/532 [02:10<07:15,  1.11s/it]Loading train:  27%|██▋       | 142/532 [02:11<07:22,  1.13s/it]Loading train:  27%|██▋       | 143/532 [02:12<06:46,  1.05s/it]Loading train:  27%|██▋       | 144/532 [02:13<06:11,  1.04it/s]Loading train:  27%|██▋       | 145/532 [02:14<05:44,  1.12it/s]Loading train:  27%|██▋       | 146/532 [02:14<05:24,  1.19it/s]Loading train:  28%|██▊       | 147/532 [02:15<05:12,  1.23it/s]Loading train:  28%|██▊       | 148/532 [02:16<05:14,  1.22it/s]Loading train:  28%|██▊       | 149/532 [02:17<05:27,  1.17it/s]Loading train:  28%|██▊       | 150/532 [02:18<05:20,  1.19it/s]Loading train:  28%|██▊       | 151/532 [02:18<05:15,  1.21it/s]Loading train:  29%|██▊       | 152/532 [02:19<05:16,  1.20it/s]Loading train:  29%|██▉       | 153/532 [02:20<05:08,  1.23it/s]Loading train:  29%|██▉       | 154/532 [02:21<05:14,  1.20it/s]Loading train:  29%|██▉       | 155/532 [02:22<06:06,  1.03it/s]Loading train:  29%|██▉       | 156/532 [02:23<06:33,  1.05s/it]Loading train:  30%|██▉       | 157/532 [02:25<06:44,  1.08s/it]Loading train:  30%|██▉       | 158/532 [02:26<06:52,  1.10s/it]Loading train:  30%|██▉       | 159/532 [02:27<07:00,  1.13s/it]Loading train:  30%|███       | 160/532 [02:28<07:04,  1.14s/it]Loading train:  30%|███       | 161/532 [02:29<06:40,  1.08s/it]Loading train:  30%|███       | 162/532 [02:30<06:06,  1.01it/s]Loading train:  31%|███       | 163/532 [02:31<05:43,  1.08it/s]Loading train:  31%|███       | 164/532 [02:31<05:25,  1.13it/s]Loading train:  31%|███       | 165/532 [02:32<05:11,  1.18it/s]Loading train:  31%|███       | 166/532 [02:33<04:57,  1.23it/s]Loading train:  31%|███▏      | 167/532 [02:34<05:18,  1.15it/s]Loading train:  32%|███▏      | 168/532 [02:35<05:14,  1.16it/s]Loading train:  32%|███▏      | 169/532 [02:36<05:10,  1.17it/s]Loading train:  32%|███▏      | 170/532 [02:36<05:09,  1.17it/s]Loading train:  32%|███▏      | 171/532 [02:37<05:09,  1.17it/s]Loading train:  32%|███▏      | 172/532 [02:38<05:22,  1.12it/s]Loading train:  33%|███▎      | 173/532 [02:39<05:17,  1.13it/s]Loading train:  33%|███▎      | 174/532 [02:40<05:14,  1.14it/s]Loading train:  33%|███▎      | 175/532 [02:41<05:04,  1.17it/s]Loading train:  33%|███▎      | 176/532 [02:42<04:53,  1.21it/s]Loading train:  33%|███▎      | 177/532 [02:42<04:50,  1.22it/s]Loading train:  33%|███▎      | 178/532 [02:43<04:46,  1.24it/s]Loading train:  34%|███▎      | 179/532 [02:44<05:04,  1.16it/s]Loading train:  34%|███▍      | 180/532 [02:45<05:04,  1.16it/s]Loading train:  34%|███▍      | 181/532 [02:46<05:00,  1.17it/s]Loading train:  34%|███▍      | 182/532 [02:47<04:59,  1.17it/s]Loading train:  34%|███▍      | 183/532 [02:47<04:50,  1.20it/s]Loading train:  35%|███▍      | 184/532 [02:48<04:51,  1.19it/s]Loading train:  35%|███▍      | 185/532 [02:49<04:41,  1.23it/s]Loading train:  35%|███▍      | 186/532 [02:50<04:40,  1.23it/s]Loading train:  35%|███▌      | 187/532 [02:51<04:30,  1.27it/s]Loading train:  35%|███▌      | 188/532 [02:51<04:27,  1.29it/s]Loading train:  36%|███▌      | 189/532 [02:52<04:27,  1.28it/s]Loading train:  36%|███▌      | 190/532 [02:53<04:22,  1.31it/s]Loading train:  36%|███▌      | 191/532 [02:54<05:02,  1.13it/s]Loading train:  36%|███▌      | 192/532 [02:55<05:38,  1.00it/s]Loading train:  36%|███▋      | 193/532 [02:56<05:58,  1.06s/it]Loading train:  36%|███▋      | 194/532 [02:58<06:07,  1.09s/it]Loading train:  37%|███▋      | 195/532 [02:59<06:21,  1.13s/it]Loading train:  37%|███▋      | 196/532 [03:00<06:18,  1.13s/it]Loading train:  37%|███▋      | 197/532 [03:01<05:59,  1.07s/it]Loading train:  37%|███▋      | 198/532 [03:02<05:43,  1.03s/it]Loading train:  37%|███▋      | 199/532 [03:03<05:39,  1.02s/it]Loading train:  38%|███▊      | 200/532 [03:04<05:36,  1.01s/it]Loading train:  38%|███▊      | 201/532 [03:05<05:29,  1.00it/s]Loading train:  38%|███▊      | 202/532 [03:06<05:26,  1.01it/s]Loading train:  38%|███▊      | 203/532 [03:07<05:25,  1.01it/s]Loading train:  38%|███▊      | 204/532 [03:08<05:03,  1.08it/s]Loading train:  39%|███▊      | 205/532 [03:08<04:54,  1.11it/s]Loading train:  39%|███▊      | 206/532 [03:09<04:41,  1.16it/s]Loading train:  39%|███▉      | 207/532 [03:10<04:30,  1.20it/s]Loading train:  39%|███▉      | 208/532 [03:11<04:25,  1.22it/s]Loading train:  39%|███▉      | 209/532 [03:11<04:14,  1.27it/s]Loading train:  39%|███▉      | 210/532 [03:12<04:09,  1.29it/s]Loading train:  40%|███▉      | 211/532 [03:13<04:07,  1.30it/s]Loading train:  40%|███▉      | 212/532 [03:14<04:00,  1.33it/s]Loading train:  40%|████      | 213/532 [03:14<03:58,  1.34it/s]Loading train:  40%|████      | 214/532 [03:15<03:56,  1.34it/s]Loading train:  40%|████      | 215/532 [03:16<04:33,  1.16it/s]Loading train:  41%|████      | 216/532 [03:17<04:50,  1.09it/s]Loading train:  41%|████      | 217/532 [03:18<05:11,  1.01it/s]Loading train:  41%|████      | 218/532 [03:20<05:15,  1.00s/it]Loading train:  41%|████      | 219/532 [03:21<05:19,  1.02s/it]Loading train:  41%|████▏     | 220/532 [03:22<05:21,  1.03s/it]Loading train:  42%|████▏     | 221/532 [03:22<05:03,  1.03it/s]Loading train:  42%|████▏     | 222/532 [03:23<04:40,  1.10it/s]Loading train:  42%|████▏     | 223/532 [03:24<04:22,  1.18it/s]Loading train:  42%|████▏     | 224/532 [03:25<04:13,  1.21it/s]Loading train:  42%|████▏     | 225/532 [03:25<04:05,  1.25it/s]Loading train:  42%|████▏     | 226/532 [03:26<03:59,  1.28it/s]Loading train:  43%|████▎     | 227/532 [03:27<03:57,  1.28it/s]Loading train:  43%|████▎     | 228/532 [03:28<03:55,  1.29it/s]Loading train:  43%|████▎     | 229/532 [03:28<03:46,  1.34it/s]Loading train:  43%|████▎     | 230/532 [03:29<03:43,  1.35it/s]Loading train:  43%|████▎     | 231/532 [03:30<03:37,  1.38it/s]Loading train:  44%|████▎     | 232/532 [03:31<03:34,  1.40it/s]Loading train:  44%|████▍     | 233/532 [03:31<03:49,  1.30it/s]Loading train:  44%|████▍     | 234/532 [03:32<03:54,  1.27it/s]Loading train:  44%|████▍     | 235/532 [03:33<03:52,  1.28it/s]Loading train:  44%|████▍     | 236/532 [03:34<03:48,  1.30it/s]Loading train:  45%|████▍     | 237/532 [03:35<03:46,  1.30it/s]Loading train:  45%|████▍     | 238/532 [03:35<03:51,  1.27it/s]Loading train:  45%|████▍     | 239/532 [03:36<03:56,  1.24it/s]Loading train:  45%|████▌     | 240/532 [03:37<03:58,  1.22it/s]Loading train:  45%|████▌     | 241/532 [03:38<03:53,  1.25it/s]Loading train:  45%|████▌     | 242/532 [03:39<03:53,  1.24it/s]Loading train:  46%|████▌     | 243/532 [03:39<03:51,  1.25it/s]Loading train:  46%|████▌     | 244/532 [03:40<03:53,  1.24it/s]Loading train:  46%|████▌     | 245/532 [03:41<03:46,  1.27it/s]Loading train:  46%|████▌     | 246/532 [03:42<03:43,  1.28it/s]Loading train:  46%|████▋     | 247/532 [03:42<03:38,  1.30it/s]Loading train:  47%|████▋     | 248/532 [03:43<03:35,  1.32it/s]Loading train:  47%|████▋     | 249/532 [03:44<03:31,  1.34it/s]Loading train:  47%|████▋     | 250/532 [03:45<03:27,  1.36it/s]Loading train:  47%|████▋     | 251/532 [03:45<03:30,  1.34it/s]Loading train:  47%|████▋     | 252/532 [03:46<03:30,  1.33it/s]Loading train:  48%|████▊     | 253/532 [03:47<03:30,  1.33it/s]Loading train:  48%|████▊     | 254/532 [03:48<03:22,  1.37it/s]Loading train:  48%|████▊     | 255/532 [03:48<03:23,  1.36it/s]Loading train:  48%|████▊     | 256/532 [03:49<03:25,  1.34it/s]Loading train:  48%|████▊     | 257/532 [03:50<03:56,  1.16it/s]Loading train:  48%|████▊     | 258/532 [03:51<03:57,  1.15it/s]Loading train:  49%|████▊     | 259/532 [03:52<04:06,  1.11it/s]Loading train:  49%|████▉     | 260/532 [03:53<04:09,  1.09it/s]Loading train:  49%|████▉     | 261/532 [03:54<04:10,  1.08it/s]Loading train:  49%|████▉     | 262/532 [03:55<04:12,  1.07it/s]Loading train:  49%|████▉     | 263/532 [03:56<03:54,  1.15it/s]Loading train:  50%|████▉     | 264/532 [03:56<03:43,  1.20it/s]Loading train:  50%|████▉     | 265/532 [03:57<03:34,  1.25it/s]Loading train:  50%|█████     | 266/532 [03:58<03:26,  1.29it/s]Loading train:  50%|█████     | 267/532 [03:59<03:19,  1.33it/s]Loading train:  50%|█████     | 268/532 [03:59<03:10,  1.38it/s]Loading train:  51%|█████     | 269/532 [04:00<03:15,  1.34it/s]Loading train:  51%|█████     | 270/532 [04:01<03:22,  1.29it/s]Loading train:  51%|█████     | 271/532 [04:02<03:24,  1.28it/s]Loading train:  51%|█████     | 272/532 [04:02<03:26,  1.26it/s]Loading train:  51%|█████▏    | 273/532 [04:03<03:28,  1.24it/s]Loading train:  52%|█████▏    | 274/532 [04:04<03:27,  1.24it/s]Loading train:  52%|█████▏    | 275/532 [04:05<03:52,  1.10it/s]Loading train:  52%|█████▏    | 276/532 [04:06<04:03,  1.05it/s]Loading train:  52%|█████▏    | 277/532 [04:07<04:10,  1.02it/s]Loading train:  52%|█████▏    | 278/532 [04:08<04:12,  1.01it/s]Loading train:  52%|█████▏    | 279/532 [04:09<04:15,  1.01s/it]Loading train:  53%|█████▎    | 280/532 [04:10<04:17,  1.02s/it]Loading train:  53%|█████▎    | 281/532 [04:12<04:21,  1.04s/it]Loading train:  53%|█████▎    | 282/532 [04:13<04:19,  1.04s/it]Loading train:  53%|█████▎    | 283/532 [04:14<04:14,  1.02s/it]Loading train:  53%|█████▎    | 284/532 [04:15<04:12,  1.02s/it]Loading train:  54%|█████▎    | 285/532 [04:16<04:11,  1.02s/it]Loading train:  54%|█████▍    | 286/532 [04:17<04:08,  1.01s/it]Loading train:  54%|█████▍    | 287/532 [04:17<03:49,  1.07it/s]Loading train:  54%|█████▍    | 288/532 [04:18<03:33,  1.14it/s]Loading train:  54%|█████▍    | 289/532 [04:19<03:20,  1.21it/s]Loading train:  55%|█████▍    | 290/532 [04:20<03:16,  1.23it/s]Loading train:  55%|█████▍    | 291/532 [04:20<03:12,  1.25it/s]Loading train:  55%|█████▍    | 292/532 [04:21<03:10,  1.26it/s]Loading train:  55%|█████▌    | 293/532 [04:22<03:32,  1.13it/s]Loading train:  55%|█████▌    | 294/532 [04:23<03:27,  1.15it/s]Loading train:  55%|█████▌    | 295/532 [04:24<03:24,  1.16it/s]Loading train:  56%|█████▌    | 296/532 [04:25<03:23,  1.16it/s]Loading train:  56%|█████▌    | 297/532 [04:26<03:22,  1.16it/s]Loading train:  56%|█████▌    | 298/532 [04:26<03:19,  1.17it/s]Loading train:  56%|█████▌    | 299/532 [04:27<03:17,  1.18it/s]Loading train:  56%|█████▋    | 300/532 [04:28<03:04,  1.26it/s]Loading train:  57%|█████▋    | 301/532 [04:29<02:55,  1.31it/s]Loading train:  57%|█████▋    | 302/532 [04:29<02:53,  1.33it/s]Loading train:  57%|█████▋    | 303/532 [04:30<02:48,  1.36it/s]Loading train:  57%|█████▋    | 304/532 [04:31<02:43,  1.40it/s]Loading train:  57%|█████▋    | 305/532 [04:32<03:06,  1.22it/s]Loading train:  58%|█████▊    | 306/532 [04:33<03:22,  1.12it/s]Loading train:  58%|█████▊    | 307/532 [04:34<03:27,  1.08it/s]Loading train:  58%|█████▊    | 308/532 [04:35<03:28,  1.07it/s]Loading train:  58%|█████▊    | 309/532 [04:36<03:36,  1.03it/s]Loading train:  58%|█████▊    | 310/532 [04:37<03:35,  1.03it/s]Loading train:  58%|█████▊    | 311/532 [04:38<04:07,  1.12s/it]Loading train:  59%|█████▊    | 312/532 [04:40<04:23,  1.20s/it]Loading train:  59%|█████▉    | 313/532 [04:41<04:42,  1.29s/it]Loading train:  59%|█████▉    | 314/532 [04:43<04:50,  1.33s/it]Loading train:  59%|█████▉    | 315/532 [04:44<04:55,  1.36s/it]Loading train:  59%|█████▉    | 316/532 [04:46<04:57,  1.38s/it]Loading train:  60%|█████▉    | 317/532 [04:46<04:18,  1.20s/it]Loading train:  60%|█████▉    | 318/532 [04:47<03:48,  1.07s/it]Loading train:  60%|█████▉    | 319/532 [04:48<03:26,  1.03it/s]Loading train:  60%|██████    | 320/532 [04:49<03:13,  1.10it/s]Loading train:  60%|██████    | 321/532 [04:49<03:03,  1.15it/s]Loading train:  61%|██████    | 322/532 [04:50<02:54,  1.20it/s]Loading train:  61%|██████    | 323/532 [04:51<03:16,  1.06it/s]Loading train:  61%|██████    | 324/532 [04:52<03:25,  1.01it/s]Loading train:  61%|██████    | 325/532 [04:53<03:31,  1.02s/it]Loading train:  61%|██████▏   | 326/532 [04:55<03:34,  1.04s/it]Loading train:  61%|██████▏   | 327/532 [04:56<03:36,  1.06s/it]Loading train:  62%|██████▏   | 328/532 [04:57<03:38,  1.07s/it]Loading train:  62%|██████▏   | 329/532 [04:58<03:32,  1.05s/it]Loading train:  62%|██████▏   | 330/532 [04:59<03:13,  1.04it/s]Loading train:  62%|██████▏   | 331/532 [04:59<03:04,  1.09it/s]Loading train:  62%|██████▏   | 332/532 [05:00<02:56,  1.13it/s]Loading train:  63%|██████▎   | 333/532 [05:01<02:50,  1.17it/s]Loading train:  63%|██████▎   | 334/532 [05:02<02:44,  1.20it/s]Loading train:  63%|██████▎   | 335/532 [05:03<03:03,  1.07it/s]Loading train:  63%|██████▎   | 336/532 [05:04<03:04,  1.06it/s]Loading train:  63%|██████▎   | 337/532 [05:05<03:09,  1.03it/s]Loading train:  64%|██████▎   | 338/532 [05:06<03:08,  1.03it/s]Loading train:  64%|██████▎   | 339/532 [05:07<03:05,  1.04it/s]Loading train:  64%|██████▍   | 340/532 [05:08<03:07,  1.03it/s]Loading train:  64%|██████▍   | 341/532 [05:09<02:54,  1.09it/s]Loading train:  64%|██████▍   | 342/532 [05:10<02:56,  1.08it/s]Loading train:  64%|██████▍   | 343/532 [05:10<02:45,  1.14it/s]Loading train:  65%|██████▍   | 344/532 [05:11<02:41,  1.17it/s]Loading train:  65%|██████▍   | 345/532 [05:12<02:35,  1.20it/s]Loading train:  65%|██████▌   | 346/532 [05:13<02:29,  1.24it/s]Loading train:  65%|██████▌   | 347/532 [05:14<02:35,  1.19it/s]Loading train:  65%|██████▌   | 348/532 [05:14<02:31,  1.22it/s]Loading train:  66%|██████▌   | 349/532 [05:15<02:28,  1.23it/s]Loading train:  66%|██████▌   | 350/532 [05:16<02:29,  1.22it/s]Loading train:  66%|██████▌   | 351/532 [05:17<02:28,  1.22it/s]Loading train:  66%|██████▌   | 352/532 [05:18<02:29,  1.20it/s]Loading train:  66%|██████▋   | 353/532 [05:18<02:28,  1.21it/s]Loading train:  67%|██████▋   | 354/532 [05:19<02:27,  1.21it/s]Loading train:  67%|██████▋   | 355/532 [05:20<02:32,  1.16it/s]Loading train:  67%|██████▋   | 356/532 [05:21<02:29,  1.18it/s]Loading train:  67%|██████▋   | 357/532 [05:22<02:26,  1.20it/s]Loading train:  67%|██████▋   | 358/532 [05:23<02:25,  1.20it/s]Loading train:  67%|██████▋   | 359/532 [05:23<02:22,  1.21it/s]Loading train:  68%|██████▊   | 360/532 [05:24<02:18,  1.24it/s]Loading train:  68%|██████▊   | 361/532 [05:25<02:18,  1.24it/s]Loading train:  68%|██████▊   | 362/532 [05:26<02:19,  1.22it/s]Loading train:  68%|██████▊   | 363/532 [05:27<02:15,  1.25it/s]Loading train:  68%|██████▊   | 364/532 [05:27<02:15,  1.24it/s]Loading train:  69%|██████▊   | 365/532 [05:28<02:12,  1.26it/s]Loading train:  69%|██████▉   | 366/532 [05:29<02:11,  1.26it/s]Loading train:  69%|██████▉   | 367/532 [05:30<02:11,  1.25it/s]Loading train:  69%|██████▉   | 368/532 [05:31<02:12,  1.24it/s]Loading train:  69%|██████▉   | 369/532 [05:31<02:11,  1.24it/s]Loading train:  70%|██████▉   | 370/532 [05:32<02:07,  1.27it/s]Loading train:  70%|██████▉   | 371/532 [05:33<02:25,  1.10it/s]Loading train:  70%|██████▉   | 372/532 [05:34<02:34,  1.03it/s]Loading train:  70%|███████   | 373/532 [05:36<02:41,  1.02s/it]Loading train:  70%|███████   | 374/532 [05:37<02:47,  1.06s/it]Loading train:  70%|███████   | 375/532 [05:38<02:49,  1.08s/it]Loading train:  71%|███████   | 376/532 [05:39<02:49,  1.08s/it]Loading train:  71%|███████   | 377/532 [05:40<02:45,  1.07s/it]Loading train:  71%|███████   | 378/532 [05:41<02:34,  1.00s/it]Loading train:  71%|███████   | 379/532 [05:42<02:23,  1.06it/s]Loading train:  71%|███████▏  | 380/532 [05:43<02:20,  1.08it/s]Loading train:  72%|███████▏  | 381/532 [05:43<02:15,  1.12it/s]Loading train:  72%|███████▏  | 382/532 [05:44<02:09,  1.16it/s]Loading train:  72%|███████▏  | 383/532 [05:45<02:10,  1.14it/s]Loading train:  72%|███████▏  | 384/532 [05:46<02:12,  1.12it/s]Loading train:  72%|███████▏  | 385/532 [05:47<02:09,  1.14it/s]Loading train:  73%|███████▎  | 386/532 [05:48<02:10,  1.12it/s]Loading train:  73%|███████▎  | 387/532 [05:49<02:10,  1.11it/s]Loading train:  73%|███████▎  | 388/532 [05:50<02:13,  1.08it/s]Loading train:  73%|███████▎  | 389/532 [05:51<02:29,  1.05s/it]Loading train:  73%|███████▎  | 390/532 [05:52<02:38,  1.12s/it]Loading train:  73%|███████▎  | 391/532 [05:54<02:59,  1.27s/it]Loading train:  74%|███████▎  | 392/532 [05:55<02:51,  1.23s/it]Loading train:  74%|███████▍  | 393/532 [05:56<02:42,  1.17s/it]Loading train:  74%|███████▍  | 394/532 [05:57<02:32,  1.10s/it]Loading train:  74%|███████▍  | 395/532 [05:58<02:30,  1.10s/it]Loading train:  74%|███████▍  | 396/532 [05:59<02:25,  1.07s/it]Loading train:  75%|███████▍  | 397/532 [06:00<02:16,  1.01s/it]Loading train:  75%|███████▍  | 398/532 [06:01<02:08,  1.04it/s]Loading train:  75%|███████▌  | 399/532 [06:02<02:07,  1.04it/s]Loading train:  75%|███████▌  | 400/532 [06:03<02:05,  1.05it/s]Loading train:  75%|███████▌  | 401/532 [06:04<02:09,  1.01it/s]Loading train:  76%|███████▌  | 402/532 [06:05<02:05,  1.03it/s]Loading train:  76%|███████▌  | 403/532 [06:06<02:07,  1.01it/s]Loading train:  76%|███████▌  | 404/532 [06:07<02:07,  1.01it/s]Loading train:  76%|███████▌  | 405/532 [06:08<02:04,  1.02it/s]Loading train:  76%|███████▋  | 406/532 [06:09<02:09,  1.03s/it]Loading train:  77%|███████▋  | 407/532 [06:10<02:05,  1.00s/it]Loading train:  77%|███████▋  | 408/532 [06:11<02:00,  1.03it/s]Loading train:  77%|███████▋  | 409/532 [06:12<01:53,  1.09it/s]Loading train:  77%|███████▋  | 410/532 [06:12<01:50,  1.10it/s]Loading train:  77%|███████▋  | 411/532 [06:13<01:48,  1.11it/s]Loading train:  77%|███████▋  | 412/532 [06:14<01:46,  1.13it/s]Loading train:  78%|███████▊  | 413/532 [06:15<01:46,  1.12it/s]Loading train:  78%|███████▊  | 414/532 [06:16<01:41,  1.16it/s]Loading train:  78%|███████▊  | 415/532 [06:17<01:37,  1.20it/s]Loading train:  78%|███████▊  | 416/532 [06:17<01:33,  1.23it/s]Loading train:  78%|███████▊  | 417/532 [06:18<01:32,  1.24it/s]Loading train:  79%|███████▊  | 418/532 [06:19<01:28,  1.29it/s]Loading train:  79%|███████▉  | 419/532 [06:20<01:34,  1.20it/s]Loading train:  79%|███████▉  | 420/532 [06:21<01:36,  1.16it/s]Loading train:  79%|███████▉  | 421/532 [06:22<01:38,  1.13it/s]Loading train:  79%|███████▉  | 422/532 [06:23<01:38,  1.12it/s]Loading train:  80%|███████▉  | 423/532 [06:24<01:39,  1.09it/s]Loading train:  80%|███████▉  | 424/532 [06:24<01:38,  1.10it/s]Loading train:  80%|███████▉  | 425/532 [06:25<01:40,  1.07it/s]Loading train:  80%|████████  | 426/532 [06:26<01:38,  1.07it/s]Loading train:  80%|████████  | 427/532 [06:27<01:36,  1.09it/s]Loading train:  80%|████████  | 428/532 [06:28<01:31,  1.13it/s]Loading train:  81%|████████  | 429/532 [06:29<01:29,  1.14it/s]Loading train:  81%|████████  | 430/532 [06:30<01:28,  1.15it/s]Loading train:  81%|████████  | 431/532 [06:31<01:34,  1.07it/s]Loading train:  81%|████████  | 432/532 [06:32<01:38,  1.01it/s]Loading train:  81%|████████▏ | 433/532 [06:33<01:38,  1.00it/s]Loading train:  82%|████████▏ | 434/532 [06:34<01:39,  1.02s/it]Loading train:  82%|████████▏ | 435/532 [06:35<01:39,  1.03s/it]Loading train:  82%|████████▏ | 436/532 [06:36<01:38,  1.02s/it]Loading train:  82%|████████▏ | 437/532 [06:37<01:31,  1.04it/s]Loading train:  82%|████████▏ | 438/532 [06:38<01:26,  1.09it/s]Loading train:  83%|████████▎ | 439/532 [06:38<01:20,  1.16it/s]Loading train:  83%|████████▎ | 440/532 [06:39<01:17,  1.18it/s]Loading train:  83%|████████▎ | 441/532 [06:40<01:16,  1.20it/s]Loading train:  83%|████████▎ | 442/532 [06:41<01:14,  1.21it/s]Loading train:  83%|████████▎ | 443/532 [06:42<01:12,  1.23it/s]Loading train:  83%|████████▎ | 444/532 [06:42<01:10,  1.25it/s]Loading train:  84%|████████▎ | 445/532 [06:43<01:08,  1.27it/s]Loading train:  84%|████████▍ | 446/532 [06:44<01:06,  1.29it/s]Loading train:  84%|████████▍ | 447/532 [06:45<01:04,  1.31it/s]Loading train:  84%|████████▍ | 448/532 [06:45<01:03,  1.33it/s]Loading train:  84%|████████▍ | 449/532 [06:46<01:03,  1.32it/s]Loading train:  85%|████████▍ | 450/532 [06:47<01:02,  1.30it/s]Loading train:  85%|████████▍ | 451/532 [06:48<01:02,  1.29it/s]Loading train:  85%|████████▍ | 452/532 [06:49<01:02,  1.28it/s]Loading train:  85%|████████▌ | 453/532 [06:49<01:02,  1.26it/s]Loading train:  85%|████████▌ | 454/532 [06:50<01:02,  1.24it/s]Loading train:  86%|████████▌ | 455/532 [06:51<01:05,  1.18it/s]Loading train:  86%|████████▌ | 456/532 [06:52<01:06,  1.15it/s]Loading train:  86%|████████▌ | 457/532 [06:53<01:05,  1.15it/s]Loading train:  86%|████████▌ | 458/532 [06:54<01:04,  1.14it/s]Loading train:  86%|████████▋ | 459/532 [06:55<01:04,  1.14it/s]Loading train:  86%|████████▋ | 460/532 [06:56<01:05,  1.10it/s]Loading train:  87%|████████▋ | 461/532 [06:57<01:07,  1.05it/s]Loading train:  87%|████████▋ | 462/532 [06:58<01:09,  1.01it/s]Loading train:  87%|████████▋ | 463/532 [06:59<01:08,  1.00it/s]Loading train:  87%|████████▋ | 464/532 [07:00<01:08,  1.00s/it]Loading train:  87%|████████▋ | 465/532 [07:01<01:07,  1.01s/it]Loading train:  88%|████████▊ | 466/532 [07:02<01:06,  1.01s/it]Loading train:  88%|████████▊ | 467/532 [07:03<01:03,  1.03it/s]Loading train:  88%|████████▊ | 468/532 [07:04<00:58,  1.09it/s]Loading train:  88%|████████▊ | 469/532 [07:04<00:55,  1.13it/s]Loading train:  88%|████████▊ | 470/532 [07:05<00:52,  1.18it/s]Loading train:  89%|████████▊ | 471/532 [07:06<00:50,  1.22it/s]Loading train:  89%|████████▊ | 472/532 [07:07<00:49,  1.22it/s]Loading train:  89%|████████▉ | 473/532 [07:08<00:49,  1.19it/s]Loading train:  89%|████████▉ | 474/532 [07:09<00:50,  1.15it/s]Loading train:  89%|████████▉ | 475/532 [07:09<00:49,  1.15it/s]Loading train:  89%|████████▉ | 476/532 [07:10<00:49,  1.13it/s]Loading train:  90%|████████▉ | 477/532 [07:11<00:49,  1.12it/s]Loading train:  90%|████████▉ | 478/532 [07:12<00:48,  1.11it/s]Loading train:  90%|█████████ | 479/532 [07:13<00:45,  1.17it/s]Loading train:  90%|█████████ | 480/532 [07:14<00:42,  1.22it/s]Loading train:  90%|█████████ | 481/532 [07:14<00:39,  1.28it/s]Loading train:  91%|█████████ | 482/532 [07:15<00:38,  1.30it/s]Loading train:  91%|█████████ | 483/532 [07:16<00:37,  1.31it/s]Loading train:  91%|█████████ | 484/532 [07:17<00:36,  1.32it/s]Loading train:  91%|█████████ | 485/532 [07:18<00:39,  1.18it/s]Loading train:  91%|█████████▏| 486/532 [07:19<00:41,  1.11it/s]Loading train:  92%|█████████▏| 487/532 [07:20<00:41,  1.08it/s]Loading train:  92%|█████████▏| 488/532 [07:21<00:41,  1.06it/s]Loading train:  92%|█████████▏| 489/532 [07:22<00:41,  1.05it/s]Loading train:  92%|█████████▏| 490/532 [07:23<00:40,  1.05it/s]Loading train:  92%|█████████▏| 491/532 [07:23<00:38,  1.07it/s]Loading train:  92%|█████████▏| 492/532 [07:24<00:36,  1.09it/s]Loading train:  93%|█████████▎| 493/532 [07:25<00:34,  1.13it/s]Loading train:  93%|█████████▎| 494/532 [07:26<00:31,  1.19it/s]Loading train:  93%|█████████▎| 495/532 [07:27<00:31,  1.18it/s]Loading train:  93%|█████████▎| 496/532 [07:28<00:30,  1.19it/s]Loading train:  93%|█████████▎| 497/532 [07:29<00:30,  1.15it/s]Loading train:  94%|█████████▎| 498/532 [07:29<00:28,  1.17it/s]Loading train:  94%|█████████▍| 499/532 [07:30<00:28,  1.17it/s]Loading train:  94%|█████████▍| 500/532 [07:31<00:27,  1.17it/s]Loading train:  94%|█████████▍| 501/532 [07:32<00:26,  1.19it/s]Loading train:  94%|█████████▍| 502/532 [07:33<00:24,  1.21it/s]Loading train:  95%|█████████▍| 503/532 [07:34<00:24,  1.16it/s]Loading train:  95%|█████████▍| 504/532 [07:34<00:23,  1.19it/s]Loading train:  95%|█████████▍| 505/532 [07:35<00:22,  1.22it/s]Loading train:  95%|█████████▌| 506/532 [07:36<00:20,  1.26it/s]Loading train:  95%|█████████▌| 507/532 [07:37<00:19,  1.27it/s]Loading train:  95%|█████████▌| 508/532 [07:37<00:18,  1.27it/s]Loading train:  96%|█████████▌| 509/532 [07:38<00:19,  1.21it/s]Loading train:  96%|█████████▌| 510/532 [07:39<00:19,  1.15it/s]Loading train:  96%|█████████▌| 511/532 [07:40<00:18,  1.14it/s]Loading train:  96%|█████████▌| 512/532 [07:41<00:17,  1.13it/s]Loading train:  96%|█████████▋| 513/532 [07:42<00:16,  1.12it/s]Loading train:  97%|█████████▋| 514/532 [07:43<00:16,  1.12it/s]Loading train:  97%|█████████▋| 515/532 [07:44<00:14,  1.19it/s]Loading train:  97%|█████████▋| 516/532 [07:44<00:13,  1.22it/s]Loading train:  97%|█████████▋| 517/532 [07:45<00:11,  1.26it/s]Loading train:  97%|█████████▋| 518/532 [07:46<00:11,  1.25it/s]Loading train:  98%|█████████▊| 519/532 [07:47<00:10,  1.25it/s]Loading train:  98%|█████████▊| 520/532 [07:48<00:09,  1.28it/s]Loading train:  98%|█████████▊| 521/532 [07:48<00:09,  1.21it/s]Loading train:  98%|█████████▊| 522/532 [07:49<00:08,  1.17it/s]Loading train:  98%|█████████▊| 523/532 [07:50<00:07,  1.15it/s]Loading train:  98%|█████████▊| 524/532 [07:51<00:06,  1.15it/s]Loading train:  99%|█████████▊| 525/532 [07:52<00:06,  1.14it/s]Loading train:  99%|█████████▉| 526/532 [07:53<00:05,  1.12it/s]Loading train:  99%|█████████▉| 527/532 [07:54<00:04,  1.14it/s]Loading train:  99%|█████████▉| 528/532 [07:55<00:03,  1.18it/s]Loading train:  99%|█████████▉| 529/532 [07:55<00:02,  1.20it/s]Loading train: 100%|█████████▉| 530/532 [07:56<00:01,  1.21it/s]Loading train: 100%|█████████▉| 531/532 [07:57<00:00,  1.20it/s]Loading train: 100%|██████████| 532/532 [07:58<00:00,  1.22it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   6%|▌         | 31/532 [00:00<00:01, 304.13it/s]concatenating: train:  11%|█▏        | 60/532 [00:00<00:01, 297.85it/s]concatenating: train:  18%|█▊        | 94/532 [00:00<00:01, 307.66it/s]concatenating: train:  24%|██▎       | 126/532 [00:00<00:01, 311.14it/s]concatenating: train:  30%|██▉       | 157/532 [00:00<00:01, 309.18it/s]concatenating: train:  36%|███▌      | 190/532 [00:00<00:01, 313.10it/s]concatenating: train:  41%|████      | 219/532 [00:00<00:01, 305.78it/s]concatenating: train:  47%|████▋     | 252/532 [00:00<00:00, 310.79it/s]concatenating: train:  53%|█████▎    | 282/532 [00:00<00:00, 305.62it/s]concatenating: train:  59%|█████▊    | 312/532 [00:01<00:00, 289.86it/s]concatenating: train:  64%|██████▍   | 341/532 [00:01<00:00, 287.90it/s]concatenating: train:  70%|██████▉   | 370/532 [00:01<00:00, 280.59it/s]concatenating: train:  75%|███████▍  | 398/532 [00:01<00:00, 270.68it/s]concatenating: train:  80%|███████▉  | 425/532 [00:01<00:00, 252.20it/s]concatenating: train:  86%|████████▌ | 456/532 [00:01<00:00, 263.98it/s]concatenating: train:  91%|█████████ | 484/532 [00:01<00:00, 267.37it/s]concatenating: train:  97%|█████████▋| 517/532 [00:01<00:00, 282.52it/s]concatenating: train: 100%|██████████| 532/532 [00:01<00:00, 291.67it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:09,  1.41it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:09,  1.34it/s]Loading test:  20%|██        | 3/15 [00:02<00:09,  1.22it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:09,  1.16it/s]Loading test:  33%|███▎      | 5/15 [00:04<00:09,  1.06it/s]Loading test:  40%|████      | 6/15 [00:05<00:08,  1.02it/s]Loading test:  47%|████▋     | 7/15 [00:06<00:07,  1.11it/s]Loading test:  53%|█████▎    | 8/15 [00:07<00:06,  1.01it/s]Loading test:  60%|██████    | 9/15 [00:08<00:05,  1.01it/s]Loading test:  67%|██████▋   | 10/15 [00:09<00:04,  1.07it/s]Loading test:  73%|███████▎  | 11/15 [00:10<00:03,  1.09it/s]Loading test:  80%|████████  | 12/15 [00:11<00:02,  1.08it/s]Loading test:  87%|████████▋ | 13/15 [00:12<00:01,  1.04it/s]Loading test:  93%|█████████▎| 14/15 [00:13<00:00,  1.09it/s]Loading test: 100%|██████████| 15/15 [00:14<00:00,  1.08it/s]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 525.34it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 56, 15)   150         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 56, 15)   60          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 56, 15)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 56, 15)   2040        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 56, 15)   60          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 56, 15)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 84, 56, 16)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 28, 16)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 28, 16)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 28, 30)   4350        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 28, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 28, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 28, 30)   8130        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 28, 30)   120         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 28, 30)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 42, 28, 46)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 14, 46)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 14, 46)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 14, 60)   24900       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 14, 60)   240         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 14, 60)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 14, 60)   32460       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 14, 60)   240         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 14, 60)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 21, 14, 106)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 14, 106)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 28, 30)   12750       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 28, 76)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 28, 30)   20550       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 28, 30)   120         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 28, 30)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 28, 30)   8130        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 28, 30)   120         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 28, 30)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 42, 28, 106)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-06 05:34:52.537941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 05:34:52.538052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 05:34:52.538066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 05:34:52.538075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 05:34:52.538497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 42, 28, 106)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 56, 15)   6375        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 56, 31)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 56, 15)   4200        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 56, 15)   60          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 56, 15)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 56, 15)   2040        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 56, 15)   60          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 56, 15)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 84, 56, 46)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 56, 46)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 56, 13)   611         dropout_5[0][0]                  
==================================================================================================
Total params: 127,886
Trainable params: 127,286
Non-trainable params: 600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53807809e-02 2.88974534e-02 1.16728790e-01 1.00198377e-02
 3.03363016e-02 5.79915656e-03 6.86746312e-02 1.28228722e-01
 7.55625878e-02 1.22514673e-02 2.73642650e-01 1.84278063e-01
 1.99559502e-04]
Train on 19871 samples, validate on 569 samples
Epoch 1/300
 - 26s - loss: 66.4156 - acc: 0.7656 - mDice: 0.0197 - val_loss: 9.1230 - val_acc: 0.9112 - val_mDice: 0.0060

Epoch 00001: val_mDice improved from -inf to 0.00598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 17s - loss: 8.0254 - acc: 0.8932 - mDice: 0.0562 - val_loss: 5.3627 - val_acc: 0.9107 - val_mDice: 0.0728

Epoch 00002: val_mDice improved from 0.00598 to 0.07281, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 18s - loss: 6.0243 - acc: 0.8962 - mDice: 0.1146 - val_loss: 4.1240 - val_acc: 0.9113 - val_mDice: 0.1632

Epoch 00003: val_mDice improved from 0.07281 to 0.16316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 17s - loss: 4.9703 - acc: 0.9041 - mDice: 0.1856 - val_loss: 3.7709 - val_acc: 0.9182 - val_mDice: 0.2317

Epoch 00004: val_mDice improved from 0.16316 to 0.23170, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 17s - loss: 4.2924 - acc: 0.9143 - mDice: 0.2544 - val_loss: 2.8646 - val_acc: 0.9359 - val_mDice: 0.3578

Epoch 00005: val_mDice improved from 0.23170 to 0.35781, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 17s - loss: 3.8207 - acc: 0.9210 - mDice: 0.3098 - val_loss: 2.3866 - val_acc: 0.9514 - val_mDice: 0.4347

Epoch 00006: val_mDice improved from 0.35781 to 0.43473, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 18s - loss: 3.4259 - acc: 0.9263 - mDice: 0.3597 - val_loss: 2.1187 - val_acc: 0.9568 - val_mDice: 0.4968

Epoch 00007: val_mDice improved from 0.43473 to 0.49677, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300
 - 17s - loss: 3.1342 - acc: 0.9307 - mDice: 0.4031 - val_loss: 1.9716 - val_acc: 0.9601 - val_mDice: 0.5394

Epoch 00008: val_mDice improved from 0.49677 to 0.53940, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300
 - 17s - loss: 2.9154 - acc: 0.9346 - mDice: 0.4370 - val_loss: 1.7865 - val_acc: 0.9638 - val_mDice: 0.5759

Epoch 00009: val_mDice improved from 0.53940 to 0.57595, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300
 - 17s - loss: 2.7369 - acc: 0.9380 - mDice: 0.4668 - val_loss: 1.7030 - val_acc: 0.9655 - val_mDice: 0.6042

Epoch 00010: val_mDice improved from 0.57595 to 0.60418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300
 - 17s - loss: 2.5938 - acc: 0.9409 - mDice: 0.4907 - val_loss: 1.6001 - val_acc: 0.9676 - val_mDice: 0.6224

Epoch 00011: val_mDice improved from 0.60418 to 0.62242, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300
 - 18s - loss: 2.4429 - acc: 0.9440 - mDice: 0.5155 - val_loss: 1.5624 - val_acc: 0.9676 - val_mDice: 0.6368

Epoch 00012: val_mDice improved from 0.62242 to 0.63677, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 13/300
 - 17s - loss: 2.3462 - acc: 0.9461 - mDice: 0.5325 - val_loss: 1.5232 - val_acc: 0.9688 - val_mDice: 0.6569

Epoch 00013: val_mDice improved from 0.63677 to 0.65689, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300
 - 17s - loss: 2.2350 - acc: 0.9482 - mDice: 0.5510 - val_loss: 1.4567 - val_acc: 0.9697 - val_mDice: 0.6669

Epoch 00014: val_mDice improved from 0.65689 to 0.66687, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 15/300
 - 17s - loss: 2.1674 - acc: 0.9494 - mDice: 0.5619 - val_loss: 1.4094 - val_acc: 0.9707 - val_mDice: 0.6732

Epoch 00015: val_mDice improved from 0.66687 to 0.67318, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 16/300
 - 18s - loss: 2.0785 - acc: 0.9508 - mDice: 0.5768 - val_loss: 1.3723 - val_acc: 0.9714 - val_mDice: 0.6825

Epoch 00016: val_mDice improved from 0.67318 to 0.68251, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 17/300
 - 17s - loss: 2.0133 - acc: 0.9519 - mDice: 0.5873 - val_loss: 1.3604 - val_acc: 0.9721 - val_mDice: 0.6875

Epoch 00017: val_mDice improved from 0.68251 to 0.68752, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 18/300
 - 17s - loss: 1.9632 - acc: 0.9527 - mDice: 0.5957 - val_loss: 1.4073 - val_acc: 0.9718 - val_mDice: 0.6865

Epoch 00018: val_mDice did not improve from 0.68752
Epoch 19/300
 - 18s - loss: 1.9093 - acc: 0.9536 - mDice: 0.6045 - val_loss: 1.3146 - val_acc: 0.9733 - val_mDice: 0.7009

Epoch 00019: val_mDice improved from 0.68752 to 0.70094, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 20/300
 - 17s - loss: 1.8685 - acc: 0.9543 - mDice: 0.6118 - val_loss: 1.3197 - val_acc: 0.9735 - val_mDice: 0.7012

Epoch 00020: val_mDice improved from 0.70094 to 0.70118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 21/300
 - 17s - loss: 1.8307 - acc: 0.9548 - mDice: 0.6178 - val_loss: 1.3540 - val_acc: 0.9727 - val_mDice: 0.6963

Epoch 00021: val_mDice did not improve from 0.70118
Epoch 22/300
 - 17s - loss: 1.7914 - acc: 0.9555 - mDice: 0.6254 - val_loss: 1.2849 - val_acc: 0.9728 - val_mDice: 0.7072

Epoch 00022: val_mDice improved from 0.70118 to 0.70718, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 23/300
 - 18s - loss: 1.7548 - acc: 0.9561 - mDice: 0.6319 - val_loss: 1.2536 - val_acc: 0.9737 - val_mDice: 0.7126

Epoch 00023: val_mDice improved from 0.70718 to 0.71262, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 24/300
 - 17s - loss: 1.7308 - acc: 0.9566 - mDice: 0.6361 - val_loss: 1.2721 - val_acc: 0.9730 - val_mDice: 0.7110

Epoch 00024: val_mDice did not improve from 0.71262
Epoch 25/300
 - 17s - loss: 1.7023 - acc: 0.9570 - mDice: 0.6414 - val_loss: 1.2568 - val_acc: 0.9736 - val_mDice: 0.7159

Epoch 00025: val_mDice improved from 0.71262 to 0.71591, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 26/300
 - 17s - loss: 1.6701 - acc: 0.9575 - mDice: 0.6473 - val_loss: 1.2511 - val_acc: 0.9745 - val_mDice: 0.7177

Epoch 00026: val_mDice improved from 0.71591 to 0.71765, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 27/300
 - 17s - loss: 1.6535 - acc: 0.9579 - mDice: 0.6504 - val_loss: 1.2303 - val_acc: 0.9746 - val_mDice: 0.7215

Epoch 00027: val_mDice improved from 0.71765 to 0.72147, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 28/300
 - 18s - loss: 1.6298 - acc: 0.9583 - mDice: 0.6553 - val_loss: 1.2381 - val_acc: 0.9750 - val_mDice: 0.7230

Epoch 00028: val_mDice improved from 0.72147 to 0.72298, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 29/300
 - 17s - loss: 1.6248 - acc: 0.9584 - mDice: 0.6564 - val_loss: 1.2281 - val_acc: 0.9754 - val_mDice: 0.7177

Epoch 00029: val_mDice did not improve from 0.72298
Epoch 30/300
 - 17s - loss: 1.5922 - acc: 0.9589 - mDice: 0.6622 - val_loss: 1.2091 - val_acc: 0.9747 - val_mDice: 0.7238

Epoch 00030: val_mDice improved from 0.72298 to 0.72384, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 31/300
 - 17s - loss: 1.5808 - acc: 0.9592 - mDice: 0.6647 - val_loss: 1.2006 - val_acc: 0.9756 - val_mDice: 0.7283

Epoch 00031: val_mDice improved from 0.72384 to 0.72826, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 32/300
 - 17s - loss: 1.5579 - acc: 0.9596 - mDice: 0.6689 - val_loss: 1.2096 - val_acc: 0.9754 - val_mDice: 0.7308

Epoch 00032: val_mDice improved from 0.72826 to 0.73081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 33/300
 - 18s - loss: 1.5505 - acc: 0.9597 - mDice: 0.6702 - val_loss: 1.1966 - val_acc: 0.9753 - val_mDice: 0.7295

Epoch 00033: val_mDice did not improve from 0.73081
Epoch 34/300
 - 17s - loss: 1.5222 - acc: 0.9602 - mDice: 0.6761 - val_loss: 1.1828 - val_acc: 0.9756 - val_mDice: 0.7309

Epoch 00034: val_mDice improved from 0.73081 to 0.73087, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 35/300
 - 17s - loss: 1.5200 - acc: 0.9601 - mDice: 0.6768 - val_loss: 1.1723 - val_acc: 0.9758 - val_mDice: 0.7346

Epoch 00035: val_mDice improved from 0.73087 to 0.73461, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 36/300
 - 17s - loss: 1.4992 - acc: 0.9603 - mDice: 0.6812 - val_loss: 1.1704 - val_acc: 0.9753 - val_mDice: 0.7334

Epoch 00036: val_mDice did not improve from 0.73461
Epoch 37/300
 - 18s - loss: 1.4881 - acc: 0.9605 - mDice: 0.6826 - val_loss: 1.1752 - val_acc: 0.9761 - val_mDice: 0.7369

Epoch 00037: val_mDice improved from 0.73461 to 0.73692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300
 - 17s - loss: 1.4792 - acc: 0.9606 - mDice: 0.6854 - val_loss: 1.1668 - val_acc: 0.9761 - val_mDice: 0.7349

Epoch 00038: val_mDice did not improve from 0.73692
Epoch 39/300
 - 17s - loss: 1.4634 - acc: 0.9608 - mDice: 0.6877 - val_loss: 1.1663 - val_acc: 0.9762 - val_mDice: 0.7387

Epoch 00039: val_mDice improved from 0.73692 to 0.73875, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 40/300
 - 17s - loss: 1.4516 - acc: 0.9610 - mDice: 0.6902 - val_loss: 1.1456 - val_acc: 0.9766 - val_mDice: 0.7396

Epoch 00040: val_mDice improved from 0.73875 to 0.73964, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 41/300
 - 18s - loss: 1.4358 - acc: 0.9613 - mDice: 0.6933 - val_loss: 1.1535 - val_acc: 0.9751 - val_mDice: 0.7414

Epoch 00041: val_mDice improved from 0.73964 to 0.74141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 42/300
 - 17s - loss: 1.4255 - acc: 0.9613 - mDice: 0.6952 - val_loss: 1.1564 - val_acc: 0.9768 - val_mDice: 0.7431

Epoch 00042: val_mDice improved from 0.74141 to 0.74315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 43/300
 - 17s - loss: 1.4204 - acc: 0.9614 - mDice: 0.6963 - val_loss: 1.1261 - val_acc: 0.9763 - val_mDice: 0.7451

Epoch 00043: val_mDice improved from 0.74315 to 0.74509, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 44/300
 - 17s - loss: 1.4056 - acc: 0.9616 - mDice: 0.6995 - val_loss: 1.1448 - val_acc: 0.9767 - val_mDice: 0.7420

Epoch 00044: val_mDice did not improve from 0.74509
Epoch 45/300
 - 18s - loss: 1.3989 - acc: 0.9618 - mDice: 0.7008 - val_loss: 1.1583 - val_acc: 0.9762 - val_mDice: 0.7405

Epoch 00045: val_mDice did not improve from 0.74509
Epoch 46/300
 - 17s - loss: 1.3941 - acc: 0.9618 - mDice: 0.7019 - val_loss: 1.1298 - val_acc: 0.9768 - val_mDice: 0.7473

Epoch 00046: val_mDice improved from 0.74509 to 0.74728, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 47/300
 - 17s - loss: 1.3822 - acc: 0.9619 - mDice: 0.7040 - val_loss: 1.1349 - val_acc: 0.9765 - val_mDice: 0.7439

Epoch 00047: val_mDice did not improve from 0.74728
Epoch 48/300
 - 17s - loss: 1.3726 - acc: 0.9621 - mDice: 0.7060 - val_loss: 1.1309 - val_acc: 0.9767 - val_mDice: 0.7456

Epoch 00048: val_mDice did not improve from 0.74728
Epoch 49/300
 - 18s - loss: 1.3696 - acc: 0.9621 - mDice: 0.7063 - val_loss: 1.1364 - val_acc: 0.9769 - val_mDice: 0.7463

Epoch 00049: val_mDice did not improve from 0.74728
Epoch 50/300
 - 18s - loss: 1.3607 - acc: 0.9622 - mDice: 0.7083 - val_loss: 1.1218 - val_acc: 0.9766 - val_mDice: 0.7491

Epoch 00050: val_mDice improved from 0.74728 to 0.74908, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 51/300
 - 17s - loss: 1.3557 - acc: 0.9623 - mDice: 0.7091 - val_loss: 1.1331 - val_acc: 0.9768 - val_mDice: 0.7480

Epoch 00051: val_mDice did not improve from 0.74908
Epoch 52/300
 - 17s - loss: 1.3492 - acc: 0.9624 - mDice: 0.7107 - val_loss: 1.1193 - val_acc: 0.9773 - val_mDice: 0.7483

Epoch 00052: val_mDice did not improve from 0.74908
Epoch 53/300
 - 17s - loss: 1.3424 - acc: 0.9624 - mDice: 0.7115 - val_loss: 1.1348 - val_acc: 0.9767 - val_mDice: 0.7477

Epoch 00053: val_mDice did not improve from 0.74908
Epoch 54/300
 - 18s - loss: 1.3346 - acc: 0.9625 - mDice: 0.7134 - val_loss: 1.1135 - val_acc: 0.9769 - val_mDice: 0.7515

Epoch 00054: val_mDice improved from 0.74908 to 0.75149, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 55/300
 - 17s - loss: 1.3303 - acc: 0.9625 - mDice: 0.7143 - val_loss: 1.1095 - val_acc: 0.9774 - val_mDice: 0.7499

Epoch 00055: val_mDice did not improve from 0.75149
Epoch 56/300
 - 17s - loss: 1.3272 - acc: 0.9625 - mDice: 0.7149 - val_loss: 1.1265 - val_acc: 0.9768 - val_mDice: 0.7516

Epoch 00056: val_mDice improved from 0.75149 to 0.75160, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 57/300
 - 17s - loss: 1.3189 - acc: 0.9626 - mDice: 0.7165 - val_loss: 1.1213 - val_acc: 0.9768 - val_mDice: 0.7477

Epoch 00057: val_mDice did not improve from 0.75160
Epoch 58/300
 - 19s - loss: 1.3156 - acc: 0.9626 - mDice: 0.7171 - val_loss: 1.1151 - val_acc: 0.9769 - val_mDice: 0.7526

Epoch 00058: val_mDice improved from 0.75160 to 0.75259, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 59/300
 - 17s - loss: 1.3091 - acc: 0.9627 - mDice: 0.7183 - val_loss: 1.1218 - val_acc: 0.9770 - val_mDice: 0.7532

Epoch 00059: val_mDice improved from 0.75259 to 0.75324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 60/300
 - 18s - loss: 1.3025 - acc: 0.9628 - mDice: 0.7198 - val_loss: 1.1521 - val_acc: 0.9770 - val_mDice: 0.7445

Epoch 00060: val_mDice did not improve from 0.75324
Epoch 61/300
 - 17s - loss: 1.3012 - acc: 0.9628 - mDice: 0.7200 - val_loss: 1.0945 - val_acc: 0.9771 - val_mDice: 0.7542

Epoch 00061: val_mDice improved from 0.75324 to 0.75419, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 62/300
 - 17s - loss: 1.3008 - acc: 0.9627 - mDice: 0.7201 - val_loss: 1.1194 - val_acc: 0.9771 - val_mDice: 0.7529

Epoch 00062: val_mDice did not improve from 0.75419
Epoch 63/300
 - 18s - loss: 1.2915 - acc: 0.9629 - mDice: 0.7218 - val_loss: 1.1401 - val_acc: 0.9768 - val_mDice: 0.7461

Epoch 00063: val_mDice did not improve from 0.75419
Epoch 64/300
 - 17s - loss: 1.2899 - acc: 0.9629 - mDice: 0.7220 - val_loss: 1.1113 - val_acc: 0.9770 - val_mDice: 0.7539

Epoch 00064: val_mDice did not improve from 0.75419
Epoch 65/300
 - 17s - loss: 1.2903 - acc: 0.9628 - mDice: 0.7218 - val_loss: 1.1270 - val_acc: 0.9774 - val_mDice: 0.7552

Epoch 00065: val_mDice improved from 0.75419 to 0.75518, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 66/300
 - 18s - loss: 1.2810 - acc: 0.9629 - mDice: 0.7240 - val_loss: 1.1097 - val_acc: 0.9773 - val_mDice: 0.7580

Epoch 00066: val_mDice improved from 0.75518 to 0.75803, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 67/300
 - 17s - loss: 1.2788 - acc: 0.9630 - mDice: 0.7245 - val_loss: 1.0944 - val_acc: 0.9779 - val_mDice: 0.7564

Epoch 00067: val_mDice did not improve from 0.75803
Epoch 68/300
 - 17s - loss: 1.2735 - acc: 0.9631 - mDice: 0.7254 - val_loss: 1.1200 - val_acc: 0.9777 - val_mDice: 0.7538

Epoch 00068: val_mDice did not improve from 0.75803
Epoch 69/300
 - 18s - loss: 1.2700 - acc: 0.9631 - mDice: 0.7261 - val_loss: 1.1078 - val_acc: 0.9778 - val_mDice: 0.7554

Epoch 00069: val_mDice did not improve from 0.75803
Epoch 70/300
 - 18s - loss: 1.2673 - acc: 0.9631 - mDice: 0.7266 - val_loss: 1.1281 - val_acc: 0.9775 - val_mDice: 0.7527

Epoch 00070: val_mDice did not improve from 0.75803
Epoch 71/300
 - 17s - loss: 1.2624 - acc: 0.9632 - mDice: 0.7277 - val_loss: 1.1022 - val_acc: 0.9777 - val_mDice: 0.7555

Epoch 00071: val_mDice did not improve from 0.75803
Epoch 72/300
 - 18s - loss: 1.2612 - acc: 0.9633 - mDice: 0.7278 - val_loss: 1.0991 - val_acc: 0.9777 - val_mDice: 0.7575

Epoch 00072: val_mDice did not improve from 0.75803
Epoch 73/300
 - 17s - loss: 1.2575 - acc: 0.9633 - mDice: 0.7288 - val_loss: 1.0948 - val_acc: 0.9771 - val_mDice: 0.7580

Epoch 00073: val_mDice did not improve from 0.75803
Epoch 74/300
 - 18s - loss: 1.2565 - acc: 0.9632 - mDice: 0.7285 - val_loss: 1.0878 - val_acc: 0.9775 - val_mDice: 0.7570

Epoch 00074: val_mDice did not improve from 0.75803
Epoch 75/300
 - 17s - loss: 1.2542 - acc: 0.9633 - mDice: 0.7291 - val_loss: 1.0965 - val_acc: 0.9775 - val_mDice: 0.7580

Epoch 00075: val_mDice did not improve from 0.75803
Epoch 76/300
 - 18s - loss: 1.2515 - acc: 0.9634 - mDice: 0.7295 - val_loss: 1.1208 - val_acc: 0.9776 - val_mDice: 0.7549

Epoch 00076: val_mDice did not improve from 0.75803
Epoch 77/300
 - 17s - loss: 1.2473 - acc: 0.9634 - mDice: 0.7307 - val_loss: 1.0807 - val_acc: 0.9781 - val_mDice: 0.7579

Epoch 00077: val_mDice did not improve from 0.75803
Epoch 78/300
 - 17s - loss: 1.2431 - acc: 0.9635 - mDice: 0.7313 - val_loss: 1.0877 - val_acc: 0.9781 - val_mDice: 0.7598

Epoch 00078: val_mDice improved from 0.75803 to 0.75976, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 79/300
 - 17s - loss: 1.2402 - acc: 0.9635 - mDice: 0.7319 - val_loss: 1.1039 - val_acc: 0.9775 - val_mDice: 0.7537

Epoch 00079: val_mDice did not improve from 0.75976
Epoch 80/300
 - 18s - loss: 1.2406 - acc: 0.9635 - mDice: 0.7317 - val_loss: 1.0819 - val_acc: 0.9774 - val_mDice: 0.7614

Epoch 00080: val_mDice improved from 0.75976 to 0.76141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 81/300
 - 17s - loss: 1.2397 - acc: 0.9636 - mDice: 0.7322 - val_loss: 1.0912 - val_acc: 0.9773 - val_mDice: 0.7593

Epoch 00081: val_mDice did not improve from 0.76141
Epoch 82/300
 - 17s - loss: 1.2319 - acc: 0.9637 - mDice: 0.7336 - val_loss: 1.0917 - val_acc: 0.9766 - val_mDice: 0.7622

Epoch 00082: val_mDice improved from 0.76141 to 0.76223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 83/300
 - 18s - loss: 1.2307 - acc: 0.9637 - mDice: 0.7340 - val_loss: 1.0920 - val_acc: 0.9772 - val_mDice: 0.7590

Epoch 00083: val_mDice did not improve from 0.76223
Epoch 84/300
 - 18s - loss: 1.2308 - acc: 0.9637 - mDice: 0.7337 - val_loss: 1.0874 - val_acc: 0.9780 - val_mDice: 0.7600

Epoch 00084: val_mDice did not improve from 0.76223
Epoch 85/300
 - 18s - loss: 1.2307 - acc: 0.9637 - mDice: 0.7338 - val_loss: 1.0875 - val_acc: 0.9772 - val_mDice: 0.7592

Epoch 00085: val_mDice did not improve from 0.76223
Epoch 86/300
 - 17s - loss: 1.2250 - acc: 0.9638 - mDice: 0.7347 - val_loss: 1.1069 - val_acc: 0.9779 - val_mDice: 0.7597

Epoch 00086: val_mDice did not improve from 0.76223
Epoch 87/300
 - 18s - loss: 1.2181 - acc: 0.9639 - mDice: 0.7362 - val_loss: 1.0891 - val_acc: 0.9780 - val_mDice: 0.7617

Epoch 00087: val_mDice did not improve from 0.76223
Epoch 88/300
 - 17s - loss: 1.2223 - acc: 0.9639 - mDice: 0.7356 - val_loss: 1.1039 - val_acc: 0.9775 - val_mDice: 0.7586

Epoch 00088: val_mDice did not improve from 0.76223
Epoch 89/300
 - 17s - loss: 1.2200 - acc: 0.9639 - mDice: 0.7359 - val_loss: 1.0861 - val_acc: 0.9777 - val_mDice: 0.7645

Epoch 00089: val_mDice improved from 0.76223 to 0.76448, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 90/300
 - 17s - loss: 1.2176 - acc: 0.9640 - mDice: 0.7365 - val_loss: 1.0771 - val_acc: 0.9781 - val_mDice: 0.7616

Epoch 00090: val_mDice did not improve from 0.76448
Epoch 91/300
 - 18s - loss: 1.2156 - acc: 0.9640 - mDice: 0.7366 - val_loss: 1.0992 - val_acc: 0.9771 - val_mDice: 0.7633

Epoch 00091: val_mDice did not improve from 0.76448
Epoch 92/300
 - 17s - loss: 1.2095 - acc: 0.9641 - mDice: 0.7382 - val_loss: 1.0692 - val_acc: 0.9780 - val_mDice: 0.7633

Epoch 00092: val_mDice did not improve from 0.76448
Epoch 93/300
 - 17s - loss: 1.2120 - acc: 0.9641 - mDice: 0.7375 - val_loss: 1.0856 - val_acc: 0.9777 - val_mDice: 0.7631

Epoch 00093: val_mDice did not improve from 0.76448
Epoch 94/300
 - 17s - loss: 1.2108 - acc: 0.9641 - mDice: 0.7379 - val_loss: 1.0738 - val_acc: 0.9780 - val_mDice: 0.7637

Epoch 00094: val_mDice did not improve from 0.76448
Epoch 95/300
 - 17s - loss: 1.2098 - acc: 0.9641 - mDice: 0.7381 - val_loss: 1.0941 - val_acc: 0.9773 - val_mDice: 0.7601

Epoch 00095: val_mDice did not improve from 0.76448
Epoch 96/300
 - 18s - loss: 1.2044 - acc: 0.9641 - mDice: 0.7389 - val_loss: 1.0716 - val_acc: 0.9777 - val_mDice: 0.7631

Epoch 00096: val_mDice did not improve from 0.76448
Epoch 97/300
 - 17s - loss: 1.2018 - acc: 0.9642 - mDice: 0.7395 - val_loss: 1.1208 - val_acc: 0.9768 - val_mDice: 0.7616

Epoch 00097: val_mDice did not improve from 0.76448
Epoch 98/300
 - 17s - loss: 1.2044 - acc: 0.9642 - mDice: 0.7392 - val_loss: 1.0854 - val_acc: 0.9778 - val_mDice: 0.7614

Epoch 00098: val_mDice did not improve from 0.76448
Epoch 99/300
 - 18s - loss: 1.2000 - acc: 0.9642 - mDice: 0.7400 - val_loss: 1.1276 - val_acc: 0.9774 - val_mDice: 0.7591

Epoch 00099: val_mDice did not improve from 0.76448
Epoch 100/300
 - 18s - loss: 1.2039 - acc: 0.9642 - mDice: 0.7392 - val_loss: 1.0883 - val_acc: 0.9774 - val_mDice: 0.7662

Epoch 00100: val_mDice improved from 0.76448 to 0.76619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 101/300
 - 17s - loss: 1.1957 - acc: 0.9643 - mDice: 0.7406 - val_loss: 1.0896 - val_acc: 0.9781 - val_mDice: 0.7596

Epoch 00101: val_mDice did not improve from 0.76619
Epoch 102/300
 - 17s - loss: 1.1930 - acc: 0.9644 - mDice: 0.7417 - val_loss: 1.0906 - val_acc: 0.9775 - val_mDice: 0.7615

Epoch 00102: val_mDice did not improve from 0.76619
Epoch 103/300
 - 18s - loss: 1.1962 - acc: 0.9643 - mDice: 0.7407 - val_loss: 1.0821 - val_acc: 0.9778 - val_mDice: 0.7618

Epoch 00103: val_mDice did not improve from 0.76619
Epoch 104/300
 - 17s - loss: 1.1933 - acc: 0.9644 - mDice: 0.7413 - val_loss: 1.0969 - val_acc: 0.9775 - val_mDice: 0.7624

Epoch 00104: val_mDice did not improve from 0.76619
Epoch 105/300
 - 17s - loss: 1.1913 - acc: 0.9644 - mDice: 0.7417 - val_loss: 1.0804 - val_acc: 0.9779 - val_mDice: 0.7648

Epoch 00105: val_mDice did not improve from 0.76619
Epoch 106/300
 - 18s - loss: 1.1871 - acc: 0.9645 - mDice: 0.7426 - val_loss: 1.1060 - val_acc: 0.9779 - val_mDice: 0.7645

Epoch 00106: val_mDice did not improve from 0.76619
Epoch 107/300
 - 17s - loss: 1.1903 - acc: 0.9644 - mDice: 0.7421 - val_loss: 1.0788 - val_acc: 0.9780 - val_mDice: 0.7647

Epoch 00107: val_mDice did not improve from 0.76619
Epoch 108/300
 - 17s - loss: 1.1872 - acc: 0.9645 - mDice: 0.7425 - val_loss: 1.0797 - val_acc: 0.9779 - val_mDice: 0.7658

Epoch 00108: val_mDice did not improve from 0.76619
Epoch 109/300
 - 18s - loss: 1.1878 - acc: 0.9645 - mDice: 0.7424 - val_loss: 1.0857 - val_acc: 0.9778 - val_mDice: 0.7635

Epoch 00109: val_mDice did not improve from 0.76619
Epoch 110/300
 - 17s - loss: 1.1860 - acc: 0.9645 - mDice: 0.7430 - val_loss: 1.0940 - val_acc: 0.9772 - val_mDice: 0.7644

Epoch 00110: val_mDice did not improve from 0.76619
Epoch 111/300
 - 17s - loss: 1.1816 - acc: 0.9646 - mDice: 0.7434 - val_loss: 1.0823 - val_acc: 0.9777 - val_mDice: 0.7628

Epoch 00111: val_mDice did not improve from 0.76619
Epoch 112/300
 - 18s - loss: 1.1848 - acc: 0.9645 - mDice: 0.7433 - val_loss: 1.0669 - val_acc: 0.9782 - val_mDice: 0.7644

Epoch 00112: val_mDice did not improve from 0.76619
Epoch 113/300
 - 18s - loss: 1.1774 - acc: 0.9646 - mDice: 0.7446 - val_loss: 1.0978 - val_acc: 0.9778 - val_mDice: 0.7643

Epoch 00113: val_mDice did not improve from 0.76619
Epoch 114/300
 - 18s - loss: 1.1811 - acc: 0.9646 - mDice: 0.7439 - val_loss: 1.0658 - val_acc: 0.9782 - val_mDice: 0.7660

Epoch 00114: val_mDice did not improve from 0.76619
Epoch 115/300
 - 18s - loss: 1.1786 - acc: 0.9646 - mDice: 0.7443 - val_loss: 1.0575 - val_acc: 0.9778 - val_mDice: 0.7680

Epoch 00115: val_mDice improved from 0.76619 to 0.76798, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 116/300
 - 18s - loss: 1.1786 - acc: 0.9647 - mDice: 0.7444 - val_loss: 1.0745 - val_acc: 0.9778 - val_mDice: 0.7648

Epoch 00116: val_mDice did not improve from 0.76798
Epoch 117/300
 - 18s - loss: 1.1748 - acc: 0.9647 - mDice: 0.7451 - val_loss: 1.0850 - val_acc: 0.9773 - val_mDice: 0.7641

Epoch 00117: val_mDice did not improve from 0.76798
Epoch 118/300
 - 18s - loss: 1.1757 - acc: 0.9647 - mDice: 0.7450 - val_loss: 1.0750 - val_acc: 0.9774 - val_mDice: 0.7606

Epoch 00118: val_mDice did not improve from 0.76798
Epoch 119/300
 - 17s - loss: 1.1736 - acc: 0.9647 - mDice: 0.7456 - val_loss: 1.0740 - val_acc: 0.9779 - val_mDice: 0.7665

Epoch 00119: val_mDice did not improve from 0.76798
Epoch 120/300
 - 17s - loss: 1.1689 - acc: 0.9648 - mDice: 0.7463 - val_loss: 1.0873 - val_acc: 0.9775 - val_mDice: 0.7662

Epoch 00120: val_mDice did not improve from 0.76798
Epoch 121/300
 - 18s - loss: 1.1682 - acc: 0.9648 - mDice: 0.7464 - val_loss: 1.0745 - val_acc: 0.9784 - val_mDice: 0.7666

Epoch 00121: val_mDice did not improve from 0.76798
Epoch 122/300
 - 17s - loss: 1.1689 - acc: 0.9648 - mDice: 0.7463 - val_loss: 1.0884 - val_acc: 0.9780 - val_mDice: 0.7659

Epoch 00122: val_mDice did not improve from 0.76798
Epoch 123/300
 - 17s - loss: 1.1679 - acc: 0.9648 - mDice: 0.7467 - val_loss: 1.0720 - val_acc: 0.9780 - val_mDice: 0.7667

Epoch 00123: val_mDice did not improve from 0.76798
Epoch 124/300
 - 17s - loss: 1.1658 - acc: 0.9648 - mDice: 0.7470 - val_loss: 1.0903 - val_acc: 0.9780 - val_mDice: 0.7657

Epoch 00124: val_mDice did not improve from 0.76798
Epoch 125/300
 - 18s - loss: 1.1652 - acc: 0.9649 - mDice: 0.7471 - val_loss: 1.0831 - val_acc: 0.9772 - val_mDice: 0.7652

Epoch 00125: val_mDice did not improve from 0.76798
Epoch 126/300
 - 17s - loss: 1.1661 - acc: 0.9648 - mDice: 0.7470 - val_loss: 1.0730 - val_acc: 0.9779 - val_mDice: 0.7683

Epoch 00126: val_mDice improved from 0.76798 to 0.76834, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 127/300
 - 17s - loss: 1.1643 - acc: 0.9649 - mDice: 0.7475 - val_loss: 1.0845 - val_acc: 0.9780 - val_mDice: 0.7661

Epoch 00127: val_mDice did not improve from 0.76834
Epoch 128/300
 - 18s - loss: 1.1639 - acc: 0.9649 - mDice: 0.7476 - val_loss: 1.0726 - val_acc: 0.9780 - val_mDice: 0.7690

Epoch 00128: val_mDice improved from 0.76834 to 0.76898, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 129/300
 - 17s - loss: 1.1588 - acc: 0.9649 - mDice: 0.7483 - val_loss: 1.0707 - val_acc: 0.9780 - val_mDice: 0.7675

Epoch 00129: val_mDice did not improve from 0.76898
Epoch 130/300
 - 18s - loss: 1.1591 - acc: 0.9650 - mDice: 0.7487 - val_loss: 1.0881 - val_acc: 0.9780 - val_mDice: 0.7673

Epoch 00130: val_mDice did not improve from 0.76898
Epoch 131/300
 - 17s - loss: 1.1583 - acc: 0.9650 - mDice: 0.7484 - val_loss: 1.0956 - val_acc: 0.9781 - val_mDice: 0.7688

Epoch 00131: val_mDice did not improve from 0.76898
Epoch 132/300
 - 18s - loss: 1.1560 - acc: 0.9650 - mDice: 0.7492 - val_loss: 1.0756 - val_acc: 0.9775 - val_mDice: 0.7662

Epoch 00132: val_mDice did not improve from 0.76898
Epoch 133/300
 - 17s - loss: 1.1568 - acc: 0.9650 - mDice: 0.7491 - val_loss: 1.0822 - val_acc: 0.9785 - val_mDice: 0.7666

Epoch 00133: val_mDice did not improve from 0.76898
Epoch 134/300
 - 17s - loss: 1.1562 - acc: 0.9650 - mDice: 0.7491 - val_loss: 1.0537 - val_acc: 0.9783 - val_mDice: 0.7680

Epoch 00134: val_mDice did not improve from 0.76898
Epoch 135/300
 - 18s - loss: 1.1542 - acc: 0.9651 - mDice: 0.7494 - val_loss: 1.0742 - val_acc: 0.9778 - val_mDice: 0.7666

Epoch 00135: val_mDice did not improve from 0.76898
Epoch 136/300
 - 17s - loss: 1.1579 - acc: 0.9650 - mDice: 0.7489 - val_loss: 1.0654 - val_acc: 0.9784 - val_mDice: 0.7684

Epoch 00136: val_mDice did not improve from 0.76898
Epoch 137/300
 - 17s - loss: 1.1551 - acc: 0.9650 - mDice: 0.7493 - val_loss: 1.0780 - val_acc: 0.9783 - val_mDice: 0.7697

Epoch 00137: val_mDice improved from 0.76898 to 0.76974, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 138/300
 - 17s - loss: 1.1529 - acc: 0.9651 - mDice: 0.7496 - val_loss: 1.0727 - val_acc: 0.9779 - val_mDice: 0.7700

Epoch 00138: val_mDice improved from 0.76974 to 0.76995, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 139/300
 - 18s - loss: 1.1523 - acc: 0.9652 - mDice: 0.7500 - val_loss: 1.0673 - val_acc: 0.9782 - val_mDice: 0.7675

Epoch 00139: val_mDice did not improve from 0.76995
Epoch 140/300
 - 17s - loss: 1.1520 - acc: 0.9652 - mDice: 0.7497 - val_loss: 1.0650 - val_acc: 0.9782 - val_mDice: 0.7719

Epoch 00140: val_mDice improved from 0.76995 to 0.77191, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 141/300
 - 18s - loss: 1.1506 - acc: 0.9651 - mDice: 0.7503 - val_loss: 1.0811 - val_acc: 0.9781 - val_mDice: 0.7681

Epoch 00141: val_mDice did not improve from 0.77191
Epoch 142/300
 - 17s - loss: 1.1495 - acc: 0.9652 - mDice: 0.7506 - val_loss: 1.0683 - val_acc: 0.9782 - val_mDice: 0.7676

Epoch 00142: val_mDice did not improve from 0.77191
Epoch 143/300
 - 17s - loss: 1.1486 - acc: 0.9652 - mDice: 0.7507 - val_loss: 1.0854 - val_acc: 0.9783 - val_mDice: 0.7661

Epoch 00143: val_mDice did not improve from 0.77191
Epoch 144/300
 - 18s - loss: 1.1446 - acc: 0.9652 - mDice: 0.7516 - val_loss: 1.0569 - val_acc: 0.9782 - val_mDice: 0.7694

Epoch 00144: val_mDice did not improve from 0.77191
Epoch 145/300
 - 17s - loss: 1.1452 - acc: 0.9652 - mDice: 0.7514 - val_loss: 1.0719 - val_acc: 0.9780 - val_mDice: 0.7700

Epoch 00145: val_mDice did not improve from 0.77191
Epoch 146/300
 - 18s - loss: 1.1482 - acc: 0.9652 - mDice: 0.7507 - val_loss: 1.1015 - val_acc: 0.9784 - val_mDice: 0.7688

Epoch 00146: val_mDice did not improve from 0.77191
Epoch 147/300
 - 17s - loss: 1.1459 - acc: 0.9652 - mDice: 0.7510 - val_loss: 1.0715 - val_acc: 0.9781 - val_mDice: 0.7711

Epoch 00147: val_mDice did not improve from 0.77191
Epoch 148/300
 - 17s - loss: 1.1462 - acc: 0.9653 - mDice: 0.7512 - val_loss: 1.0787 - val_acc: 0.9774 - val_mDice: 0.7689

Epoch 00148: val_mDice did not improve from 0.77191
Epoch 149/300
 - 18s - loss: 1.1435 - acc: 0.9653 - mDice: 0.7517 - val_loss: 1.0669 - val_acc: 0.9779 - val_mDice: 0.7678

Epoch 00149: val_mDice did not improve from 0.77191
Epoch 150/300
 - 17s - loss: 1.1409 - acc: 0.9653 - mDice: 0.7522 - val_loss: 1.0801 - val_acc: 0.9775 - val_mDice: 0.7705

Epoch 00150: val_mDice did not improve from 0.77191
Epoch 151/300
 - 17s - loss: 1.1404 - acc: 0.9653 - mDice: 0.7523 - val_loss: 1.0710 - val_acc: 0.9781 - val_mDice: 0.7699

Epoch 00151: val_mDice did not improve from 0.77191
Epoch 152/300
 - 16s - loss: 1.1424 - acc: 0.9653 - mDice: 0.7521 - val_loss: 1.0606 - val_acc: 0.9791 - val_mDice: 0.7707

Epoch 00152: val_mDice did not improve from 0.77191
Epoch 153/300
 - 18s - loss: 1.1392 - acc: 0.9653 - mDice: 0.7525 - val_loss: 1.0669 - val_acc: 0.9781 - val_mDice: 0.7717

Epoch 00153: val_mDice did not improve from 0.77191
Epoch 154/300
 - 16s - loss: 1.1397 - acc: 0.9653 - mDice: 0.7526 - val_loss: 1.0566 - val_acc: 0.9786 - val_mDice: 0.7709

Epoch 00154: val_mDice did not improve from 0.77191
Epoch 155/300
 - 16s - loss: 1.1378 - acc: 0.9654 - mDice: 0.7531 - val_loss: 1.0730 - val_acc: 0.9780 - val_mDice: 0.7699

Epoch 00155: val_mDice did not improve from 0.77191
Epoch 156/300
 - 16s - loss: 1.1376 - acc: 0.9654 - mDice: 0.7529 - val_loss: 1.0638 - val_acc: 0.9779 - val_mDice: 0.7697

Epoch 00156: val_mDice did not improve from 0.77191
Epoch 157/300
 - 16s - loss: 1.1393 - acc: 0.9654 - mDice: 0.7530 - val_loss: 1.0606 - val_acc: 0.9787 - val_mDice: 0.7658

Epoch 00157: val_mDice did not improve from 0.77191
Epoch 158/300
 - 17s - loss: 1.1336 - acc: 0.9654 - mDice: 0.7536 - val_loss: 1.0636 - val_acc: 0.9784 - val_mDice: 0.7724

Epoch 00158: val_mDice improved from 0.77191 to 0.77243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 159/300
 - 17s - loss: 1.1389 - acc: 0.9654 - mDice: 0.7526 - val_loss: 1.0589 - val_acc: 0.9786 - val_mDice: 0.7708

Epoch 00159: val_mDice did not improve from 0.77243
Epoch 160/300
 - 16s - loss: 1.1328 - acc: 0.9655 - mDice: 0.7539 - val_loss: 1.0557 - val_acc: 0.9781 - val_mDice: 0.7712

Epoch 00160: val_mDice did not improve from 0.77243
Epoch 161/300
 - 17s - loss: 1.1318 - acc: 0.9655 - mDice: 0.7541 - val_loss: 1.0692 - val_acc: 0.9783 - val_mDice: 0.7740

Epoch 00161: val_mDice improved from 0.77243 to 0.77405, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 162/300
 - 17s - loss: 1.1341 - acc: 0.9655 - mDice: 0.7537 - val_loss: 1.0572 - val_acc: 0.9783 - val_mDice: 0.7704

Epoch 00162: val_mDice did not improve from 0.77405
Epoch 163/300
 - 17s - loss: 1.1325 - acc: 0.9655 - mDice: 0.7539 - val_loss: 1.0587 - val_acc: 0.9782 - val_mDice: 0.7695

Epoch 00163: val_mDice did not improve from 0.77405
Epoch 164/300
 - 18s - loss: 1.1327 - acc: 0.9655 - mDice: 0.7539 - val_loss: 1.0811 - val_acc: 0.9787 - val_mDice: 0.7690

Epoch 00164: val_mDice did not improve from 0.77405
Epoch 165/300
 - 17s - loss: 1.1312 - acc: 0.9655 - mDice: 0.7542 - val_loss: 1.0507 - val_acc: 0.9788 - val_mDice: 0.7730

Epoch 00165: val_mDice did not improve from 0.77405
Epoch 166/300
 - 17s - loss: 1.1324 - acc: 0.9655 - mDice: 0.7541 - val_loss: 1.0750 - val_acc: 0.9787 - val_mDice: 0.7701

Epoch 00166: val_mDice did not improve from 0.77405
Epoch 167/300
 - 17s - loss: 1.1308 - acc: 0.9655 - mDice: 0.7541 - val_loss: 1.0466 - val_acc: 0.9786 - val_mDice: 0.7721

Epoch 00167: val_mDice did not improve from 0.77405
Epoch 168/300
 - 17s - loss: 1.1306 - acc: 0.9656 - mDice: 0.7546 - val_loss: 1.0547 - val_acc: 0.9786 - val_mDice: 0.7693

Epoch 00168: val_mDice did not improve from 0.77405
Epoch 169/300
 - 18s - loss: 1.1282 - acc: 0.9655 - mDice: 0.7549 - val_loss: 1.0459 - val_acc: 0.9789 - val_mDice: 0.7732

Epoch 00169: val_mDice did not improve from 0.77405
Epoch 170/300
 - 17s - loss: 1.1292 - acc: 0.9655 - mDice: 0.7547 - val_loss: 1.0649 - val_acc: 0.9782 - val_mDice: 0.7726

Epoch 00170: val_mDice did not improve from 0.77405
Epoch 171/300
 - 17s - loss: 1.1241 - acc: 0.9656 - mDice: 0.7559 - val_loss: 1.0492 - val_acc: 0.9786 - val_mDice: 0.7755

Epoch 00171: val_mDice improved from 0.77405 to 0.77553, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 172/300
 - 17s - loss: 1.1243 - acc: 0.9656 - mDice: 0.7557 - val_loss: 1.0687 - val_acc: 0.9784 - val_mDice: 0.7728

Epoch 00172: val_mDice did not improve from 0.77553
Epoch 173/300
 - 17s - loss: 1.1275 - acc: 0.9656 - mDice: 0.7550 - val_loss: 1.0780 - val_acc: 0.9778 - val_mDice: 0.7748

Epoch 00173: val_mDice did not improve from 0.77553
Epoch 174/300
 - 17s - loss: 1.1255 - acc: 0.9657 - mDice: 0.7555 - val_loss: 1.0940 - val_acc: 0.9787 - val_mDice: 0.7727

Epoch 00174: val_mDice did not improve from 0.77553
Epoch 175/300
 - 18s - loss: 1.1211 - acc: 0.9657 - mDice: 0.7565 - val_loss: 1.0495 - val_acc: 0.9783 - val_mDice: 0.7724

Epoch 00175: val_mDice did not improve from 0.77553
Epoch 176/300
 - 17s - loss: 1.1219 - acc: 0.9657 - mDice: 0.7564 - val_loss: 1.0577 - val_acc: 0.9789 - val_mDice: 0.7719

Epoch 00176: val_mDice did not improve from 0.77553
Epoch 177/300
 - 17s - loss: 1.1247 - acc: 0.9656 - mDice: 0.7559 - val_loss: 1.0576 - val_acc: 0.9785 - val_mDice: 0.7734

Epoch 00177: val_mDice did not improve from 0.77553
Epoch 178/300
 - 17s - loss: 1.1236 - acc: 0.9657 - mDice: 0.7559 - val_loss: 1.0721 - val_acc: 0.9790 - val_mDice: 0.7700

Epoch 00178: val_mDice did not improve from 0.77553
Epoch 179/300
 - 17s - loss: 1.1164 - acc: 0.9657 - mDice: 0.7573 - val_loss: 1.0675 - val_acc: 0.9782 - val_mDice: 0.7734

Epoch 00179: val_mDice did not improve from 0.77553
Epoch 180/300
 - 18s - loss: 1.1181 - acc: 0.9657 - mDice: 0.7572 - val_loss: 1.0487 - val_acc: 0.9787 - val_mDice: 0.7764

Epoch 00180: val_mDice improved from 0.77553 to 0.77638, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 181/300
 - 17s - loss: 1.1164 - acc: 0.9657 - mDice: 0.7577 - val_loss: 1.0504 - val_acc: 0.9786 - val_mDice: 0.7760

Epoch 00181: val_mDice did not improve from 0.77638
Epoch 182/300
 - 17s - loss: 1.1175 - acc: 0.9657 - mDice: 0.7576 - val_loss: 1.0474 - val_acc: 0.9787 - val_mDice: 0.7748

Epoch 00182: val_mDice did not improve from 0.77638
Epoch 183/300
 - 17s - loss: 1.1174 - acc: 0.9658 - mDice: 0.7574 - val_loss: 1.0434 - val_acc: 0.9787 - val_mDice: 0.7728

Epoch 00183: val_mDice did not improve from 0.77638
Epoch 184/300
 - 17s - loss: 1.1174 - acc: 0.9658 - mDice: 0.7570 - val_loss: 1.0582 - val_acc: 0.9783 - val_mDice: 0.7738

Epoch 00184: val_mDice did not improve from 0.77638
Epoch 185/300
 - 17s - loss: 1.1141 - acc: 0.9658 - mDice: 0.7580 - val_loss: 1.0544 - val_acc: 0.9784 - val_mDice: 0.7756

Epoch 00185: val_mDice did not improve from 0.77638
Epoch 186/300
 - 17s - loss: 1.1127 - acc: 0.9658 - mDice: 0.7587 - val_loss: 1.0385 - val_acc: 0.9787 - val_mDice: 0.7779

Epoch 00186: val_mDice improved from 0.77638 to 0.77791, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 187/300
 - 17s - loss: 1.1121 - acc: 0.9658 - mDice: 0.7587 - val_loss: 1.0571 - val_acc: 0.9791 - val_mDice: 0.7760

Epoch 00187: val_mDice did not improve from 0.77791
Epoch 188/300
 - 17s - loss: 1.1113 - acc: 0.9658 - mDice: 0.7589 - val_loss: 1.0441 - val_acc: 0.9787 - val_mDice: 0.7746

Epoch 00188: val_mDice did not improve from 0.77791
Epoch 189/300
 - 17s - loss: 1.1126 - acc: 0.9658 - mDice: 0.7585 - val_loss: 1.0536 - val_acc: 0.9785 - val_mDice: 0.7728

Epoch 00189: val_mDice did not improve from 0.77791
Epoch 190/300
 - 17s - loss: 1.1102 - acc: 0.9658 - mDice: 0.7591 - val_loss: 1.0682 - val_acc: 0.9787 - val_mDice: 0.7734

Epoch 00190: val_mDice did not improve from 0.77791
Epoch 191/300
 - 18s - loss: 1.1084 - acc: 0.9658 - mDice: 0.7594 - val_loss: 1.0683 - val_acc: 0.9784 - val_mDice: 0.7760

Epoch 00191: val_mDice did not improve from 0.77791
Epoch 192/300
 - 17s - loss: 1.1084 - acc: 0.9658 - mDice: 0.7596 - val_loss: 1.0543 - val_acc: 0.9785 - val_mDice: 0.7717

Epoch 00192: val_mDice did not improve from 0.77791
Epoch 193/300
 - 17s - loss: 1.1088 - acc: 0.9658 - mDice: 0.7593 - val_loss: 1.0560 - val_acc: 0.9789 - val_mDice: 0.7745

Epoch 00193: val_mDice did not improve from 0.77791
Epoch 194/300
 - 17s - loss: 1.1054 - acc: 0.9658 - mDice: 0.7601 - val_loss: 1.0466 - val_acc: 0.9787 - val_mDice: 0.7768

Epoch 00194: val_mDice did not improve from 0.77791
Epoch 195/300
 - 17s - loss: 1.1074 - acc: 0.9658 - mDice: 0.7597 - val_loss: 1.0397 - val_acc: 0.9786 - val_mDice: 0.7788

Epoch 00195: val_mDice improved from 0.77791 to 0.77879, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 196/300
 - 17s - loss: 1.1047 - acc: 0.9659 - mDice: 0.7604 - val_loss: 1.0411 - val_acc: 0.9789 - val_mDice: 0.7755

Epoch 00196: val_mDice did not improve from 0.77879
Epoch 197/300
 - 18s - loss: 1.1000 - acc: 0.9659 - mDice: 0.7610 - val_loss: 1.0604 - val_acc: 0.9783 - val_mDice: 0.7735

Epoch 00197: val_mDice did not improve from 0.77879
Epoch 198/300
 - 17s - loss: 1.1073 - acc: 0.9658 - mDice: 0.7596 - val_loss: 1.0378 - val_acc: 0.9792 - val_mDice: 0.7760

Epoch 00198: val_mDice did not improve from 0.77879
Epoch 199/300
 - 17s - loss: 1.1002 - acc: 0.9659 - mDice: 0.7616 - val_loss: 1.0539 - val_acc: 0.9786 - val_mDice: 0.7732

Epoch 00199: val_mDice did not improve from 0.77879
Epoch 200/300
 - 17s - loss: 1.1002 - acc: 0.9659 - mDice: 0.7616 - val_loss: 1.0507 - val_acc: 0.9789 - val_mDice: 0.7748

Epoch 00200: val_mDice did not improve from 0.77879
Epoch 201/300
 - 17s - loss: 1.1007 - acc: 0.9659 - mDice: 0.7612 - val_loss: 1.0388 - val_acc: 0.9791 - val_mDice: 0.7787

Epoch 00201: val_mDice did not improve from 0.77879
Epoch 202/300
 - 17s - loss: 1.1018 - acc: 0.9659 - mDice: 0.7610 - val_loss: 1.0434 - val_acc: 0.9784 - val_mDice: 0.7772

Epoch 00202: val_mDice did not improve from 0.77879
Epoch 203/300
 - 18s - loss: 1.0967 - acc: 0.9660 - mDice: 0.7620 - val_loss: 1.0310 - val_acc: 0.9790 - val_mDice: 0.7783

Epoch 00203: val_mDice did not improve from 0.77879
Epoch 204/300
 - 17s - loss: 1.0977 - acc: 0.9659 - mDice: 0.7619 - val_loss: 1.0530 - val_acc: 0.9785 - val_mDice: 0.7754

Epoch 00204: val_mDice did not improve from 0.77879
Epoch 205/300
 - 17s - loss: 1.0985 - acc: 0.9660 - mDice: 0.7618 - val_loss: 1.0499 - val_acc: 0.9787 - val_mDice: 0.7746

Epoch 00205: val_mDice did not improve from 0.77879
Epoch 206/300
 - 17s - loss: 1.0985 - acc: 0.9660 - mDice: 0.7620 - val_loss: 1.0482 - val_acc: 0.9788 - val_mDice: 0.7795

Epoch 00206: val_mDice improved from 0.77879 to 0.77953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 207/300
 - 17s - loss: 1.0924 - acc: 0.9660 - mDice: 0.7634 - val_loss: 1.0348 - val_acc: 0.9788 - val_mDice: 0.7782

Epoch 00207: val_mDice did not improve from 0.77953
Epoch 208/300
 - 17s - loss: 1.0949 - acc: 0.9660 - mDice: 0.7630 - val_loss: 1.0367 - val_acc: 0.9791 - val_mDice: 0.7771

Epoch 00208: val_mDice did not improve from 0.77953
Epoch 209/300
 - 18s - loss: 1.0913 - acc: 0.9660 - mDice: 0.7635 - val_loss: 1.0324 - val_acc: 0.9786 - val_mDice: 0.7775

Epoch 00209: val_mDice did not improve from 0.77953
Epoch 210/300
 - 17s - loss: 1.0956 - acc: 0.9660 - mDice: 0.7626 - val_loss: 1.0399 - val_acc: 0.9786 - val_mDice: 0.7811

Epoch 00210: val_mDice improved from 0.77953 to 0.78108, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 211/300
 - 17s - loss: 1.0925 - acc: 0.9661 - mDice: 0.7631 - val_loss: 1.0319 - val_acc: 0.9792 - val_mDice: 0.7741

Epoch 00211: val_mDice did not improve from 0.78108
Epoch 212/300
 - 17s - loss: 1.0936 - acc: 0.9660 - mDice: 0.7628 - val_loss: 1.0301 - val_acc: 0.9788 - val_mDice: 0.7767

Epoch 00212: val_mDice did not improve from 0.78108
Epoch 213/300
 - 17s - loss: 1.0914 - acc: 0.9660 - mDice: 0.7632 - val_loss: 1.0425 - val_acc: 0.9785 - val_mDice: 0.7776

Epoch 00213: val_mDice did not improve from 0.78108
Epoch 214/300
 - 18s - loss: 1.0884 - acc: 0.9661 - mDice: 0.7640 - val_loss: 1.0212 - val_acc: 0.9790 - val_mDice: 0.7807

Epoch 00214: val_mDice did not improve from 0.78108
Epoch 215/300
 - 17s - loss: 1.0903 - acc: 0.9661 - mDice: 0.7636 - val_loss: 1.0332 - val_acc: 0.9783 - val_mDice: 0.7779

Epoch 00215: val_mDice did not improve from 0.78108
Epoch 216/300
 - 17s - loss: 1.0922 - acc: 0.9661 - mDice: 0.7633 - val_loss: 1.0103 - val_acc: 0.9793 - val_mDice: 0.7810

Epoch 00216: val_mDice did not improve from 0.78108
Epoch 217/300
 - 17s - loss: 1.0879 - acc: 0.9661 - mDice: 0.7644 - val_loss: 1.0241 - val_acc: 0.9789 - val_mDice: 0.7783

Epoch 00217: val_mDice did not improve from 0.78108
Epoch 218/300
 - 17s - loss: 1.0901 - acc: 0.9661 - mDice: 0.7635 - val_loss: 1.0587 - val_acc: 0.9781 - val_mDice: 0.7782

Epoch 00218: val_mDice did not improve from 0.78108
Epoch 219/300
 - 18s - loss: 1.0891 - acc: 0.9661 - mDice: 0.7640 - val_loss: 1.0271 - val_acc: 0.9790 - val_mDice: 0.7801

Epoch 00219: val_mDice did not improve from 0.78108
Epoch 220/300
 - 17s - loss: 1.0854 - acc: 0.9662 - mDice: 0.7648 - val_loss: 1.0352 - val_acc: 0.9793 - val_mDice: 0.7814

Epoch 00220: val_mDice improved from 0.78108 to 0.78138, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 221/300
 - 17s - loss: 1.0850 - acc: 0.9662 - mDice: 0.7650 - val_loss: 1.0360 - val_acc: 0.9790 - val_mDice: 0.7808

Epoch 00221: val_mDice did not improve from 0.78138
Epoch 222/300
 - 17s - loss: 1.0875 - acc: 0.9662 - mDice: 0.7644 - val_loss: 1.0237 - val_acc: 0.9791 - val_mDice: 0.7781

Epoch 00222: val_mDice did not improve from 0.78138
Epoch 223/300
 - 17s - loss: 1.0828 - acc: 0.9662 - mDice: 0.7651 - val_loss: 1.0327 - val_acc: 0.9792 - val_mDice: 0.7803

Epoch 00223: val_mDice did not improve from 0.78138
Epoch 224/300
 - 17s - loss: 1.0813 - acc: 0.9662 - mDice: 0.7655 - val_loss: 1.0227 - val_acc: 0.9791 - val_mDice: 0.7810

Epoch 00224: val_mDice did not improve from 0.78138
Epoch 225/300
 - 18s - loss: 1.0852 - acc: 0.9662 - mDice: 0.7646 - val_loss: 1.0119 - val_acc: 0.9790 - val_mDice: 0.7798

Epoch 00225: val_mDice did not improve from 0.78138
Epoch 226/300
 - 18s - loss: 1.0815 - acc: 0.9662 - mDice: 0.7655 - val_loss: 1.0386 - val_acc: 0.9791 - val_mDice: 0.7789

Epoch 00226: val_mDice did not improve from 0.78138
Epoch 227/300
 - 17s - loss: 1.0820 - acc: 0.9662 - mDice: 0.7654 - val_loss: 1.0291 - val_acc: 0.9792 - val_mDice: 0.7803

Epoch 00227: val_mDice did not improve from 0.78138
Epoch 228/300
 - 17s - loss: 1.0859 - acc: 0.9661 - mDice: 0.7647 - val_loss: 1.0419 - val_acc: 0.9793 - val_mDice: 0.7824

Epoch 00228: val_mDice improved from 0.78138 to 0.78244, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 229/300
 - 17s - loss: 1.0814 - acc: 0.9663 - mDice: 0.7655 - val_loss: 1.0264 - val_acc: 0.9791 - val_mDice: 0.7820

Epoch 00229: val_mDice did not improve from 0.78244
Epoch 230/300
 - 17s - loss: 1.0789 - acc: 0.9663 - mDice: 0.7662 - val_loss: 1.0359 - val_acc: 0.9792 - val_mDice: 0.7806

Epoch 00230: val_mDice did not improve from 0.78244
Epoch 231/300
 - 18s - loss: 1.0810 - acc: 0.9663 - mDice: 0.7658 - val_loss: 1.0194 - val_acc: 0.9791 - val_mDice: 0.7794

Epoch 00231: val_mDice did not improve from 0.78244
Epoch 232/300
 - 17s - loss: 1.0786 - acc: 0.9663 - mDice: 0.7662 - val_loss: 1.0353 - val_acc: 0.9788 - val_mDice: 0.7819

Epoch 00232: val_mDice did not improve from 0.78244
Epoch 233/300
 - 17s - loss: 1.0804 - acc: 0.9663 - mDice: 0.7659 - val_loss: 1.0363 - val_acc: 0.9792 - val_mDice: 0.7799

Epoch 00233: val_mDice did not improve from 0.78244
Epoch 234/300
 - 17s - loss: 1.0796 - acc: 0.9663 - mDice: 0.7660 - val_loss: 1.0210 - val_acc: 0.9796 - val_mDice: 0.7808

Epoch 00234: val_mDice did not improve from 0.78244
Epoch 235/300
 - 17s - loss: 1.0791 - acc: 0.9663 - mDice: 0.7662 - val_loss: 1.0599 - val_acc: 0.9782 - val_mDice: 0.7764

Epoch 00235: val_mDice did not improve from 0.78244
Epoch 236/300
 - 17s - loss: 1.0770 - acc: 0.9663 - mDice: 0.7667 - val_loss: 1.0262 - val_acc: 0.9788 - val_mDice: 0.7809

Epoch 00236: val_mDice did not improve from 0.78244
Epoch 237/300
 - 18s - loss: 1.0797 - acc: 0.9663 - mDice: 0.7659 - val_loss: 1.0422 - val_acc: 0.9788 - val_mDice: 0.7799

Epoch 00237: val_mDice did not improve from 0.78244
Epoch 238/300
 - 17s - loss: 1.0783 - acc: 0.9663 - mDice: 0.7666 - val_loss: 1.0459 - val_acc: 0.9794 - val_mDice: 0.7786

Epoch 00238: val_mDice did not improve from 0.78244
Epoch 239/300
 - 17s - loss: 1.0798 - acc: 0.9663 - mDice: 0.7660 - val_loss: 1.0254 - val_acc: 0.9791 - val_mDice: 0.7828

Epoch 00239: val_mDice improved from 0.78244 to 0.78281, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 240/300
 - 17s - loss: 1.0755 - acc: 0.9663 - mDice: 0.7670 - val_loss: 1.0304 - val_acc: 0.9786 - val_mDice: 0.7821

Epoch 00240: val_mDice did not improve from 0.78281
Epoch 241/300
 - 17s - loss: 1.0762 - acc: 0.9664 - mDice: 0.7667 - val_loss: 1.0306 - val_acc: 0.9789 - val_mDice: 0.7836

Epoch 00241: val_mDice improved from 0.78281 to 0.78361, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 242/300
 - 17s - loss: 1.0774 - acc: 0.9663 - mDice: 0.7665 - val_loss: 1.0246 - val_acc: 0.9792 - val_mDice: 0.7821

Epoch 00242: val_mDice did not improve from 0.78361
Epoch 243/300
 - 18s - loss: 1.0760 - acc: 0.9663 - mDice: 0.7669 - val_loss: 1.0350 - val_acc: 0.9794 - val_mDice: 0.7804

Epoch 00243: val_mDice did not improve from 0.78361
Epoch 244/300
 - 17s - loss: 1.0771 - acc: 0.9663 - mDice: 0.7665 - val_loss: 1.0233 - val_acc: 0.9790 - val_mDice: 0.7818

Epoch 00244: val_mDice did not improve from 0.78361
Epoch 245/300
 - 17s - loss: 1.0762 - acc: 0.9663 - mDice: 0.7667 - val_loss: 1.0439 - val_acc: 0.9790 - val_mDice: 0.7811

Epoch 00245: val_mDice did not improve from 0.78361
Epoch 246/300
 - 17s - loss: 1.0731 - acc: 0.9664 - mDice: 0.7675 - val_loss: 1.0319 - val_acc: 0.9792 - val_mDice: 0.7826

Epoch 00246: val_mDice did not improve from 0.78361
Epoch 247/300
 - 17s - loss: 1.0733 - acc: 0.9664 - mDice: 0.7674 - val_loss: 1.0238 - val_acc: 0.9791 - val_mDice: 0.7830

Epoch 00247: val_mDice did not improve from 0.78361
Epoch 248/300
 - 17s - loss: 1.0705 - acc: 0.9664 - mDice: 0.7678 - val_loss: 1.0334 - val_acc: 0.9791 - val_mDice: 0.7813

Epoch 00248: val_mDice did not improve from 0.78361
Epoch 249/300
 - 18s - loss: 1.0764 - acc: 0.9663 - mDice: 0.7664 - val_loss: 1.0626 - val_acc: 0.9784 - val_mDice: 0.7761

Epoch 00249: val_mDice did not improve from 0.78361
Epoch 250/300
 - 17s - loss: 1.0717 - acc: 0.9664 - mDice: 0.7676 - val_loss: 1.0374 - val_acc: 0.9784 - val_mDice: 0.7773

Epoch 00250: val_mDice did not improve from 0.78361
Epoch 251/300
 - 17s - loss: 1.0723 - acc: 0.9664 - mDice: 0.7676 - val_loss: 1.0340 - val_acc: 0.9787 - val_mDice: 0.7863

Epoch 00251: val_mDice improved from 0.78361 to 0.78631, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 252/300
 - 17s - loss: 1.0706 - acc: 0.9664 - mDice: 0.7680 - val_loss: 1.0165 - val_acc: 0.9798 - val_mDice: 0.7836

Epoch 00252: val_mDice did not improve from 0.78631
Epoch 253/300
 - 17s - loss: 1.0716 - acc: 0.9664 - mDice: 0.7674 - val_loss: 1.0437 - val_acc: 0.9785 - val_mDice: 0.7788

Epoch 00253: val_mDice did not improve from 0.78631
Epoch 254/300
 - 17s - loss: 1.0704 - acc: 0.9664 - mDice: 0.7680 - val_loss: 1.0229 - val_acc: 0.9791 - val_mDice: 0.7828

Epoch 00254: val_mDice did not improve from 0.78631
Epoch 255/300
 - 18s - loss: 1.0699 - acc: 0.9664 - mDice: 0.7681 - val_loss: 1.0401 - val_acc: 0.9792 - val_mDice: 0.7822

Epoch 00255: val_mDice did not improve from 0.78631
Epoch 256/300
 - 17s - loss: 1.0699 - acc: 0.9665 - mDice: 0.7683 - val_loss: 1.0235 - val_acc: 0.9792 - val_mDice: 0.7797

Epoch 00256: val_mDice did not improve from 0.78631
Epoch 257/300
 - 17s - loss: 1.0675 - acc: 0.9664 - mDice: 0.7685 - val_loss: 1.0197 - val_acc: 0.9795 - val_mDice: 0.7848

Epoch 00257: val_mDice did not improve from 0.78631
Epoch 258/300
 - 17s - loss: 1.0695 - acc: 0.9664 - mDice: 0.7680 - val_loss: 1.0368 - val_acc: 0.9795 - val_mDice: 0.7826

Epoch 00258: val_mDice did not improve from 0.78631
Epoch 259/300
 - 17s - loss: 1.0702 - acc: 0.9664 - mDice: 0.7677 - val_loss: 1.0320 - val_acc: 0.9789 - val_mDice: 0.7849

Epoch 00259: val_mDice did not improve from 0.78631
Epoch 260/300
 - 17s - loss: 1.0678 - acc: 0.9665 - mDice: 0.7686 - val_loss: 1.0339 - val_acc: 0.9792 - val_mDice: 0.7861

Epoch 00260: val_mDice did not improve from 0.78631
Epoch 261/300
 - 18s - loss: 1.0670 - acc: 0.9665 - mDice: 0.7686 - val_loss: 1.0132 - val_acc: 0.9793 - val_mDice: 0.7856

Epoch 00261: val_mDice did not improve from 0.78631
Epoch 262/300
 - 17s - loss: 1.0700 - acc: 0.9665 - mDice: 0.7684 - val_loss: 1.0261 - val_acc: 0.9789 - val_mDice: 0.7831

Epoch 00262: val_mDice did not improve from 0.78631
Epoch 263/300
 - 17s - loss: 1.0667 - acc: 0.9665 - mDice: 0.7686 - val_loss: 1.0526 - val_acc: 0.9789 - val_mDice: 0.7800

Epoch 00263: val_mDice did not improve from 0.78631
Epoch 264/300
 - 17s - loss: 1.0675 - acc: 0.9665 - mDice: 0.7688 - val_loss: 1.0133 - val_acc: 0.9795 - val_mDice: 0.7828

Epoch 00264: val_mDice did not improve from 0.78631
Epoch 265/300
 - 18s - loss: 1.0670 - acc: 0.9665 - mDice: 0.7686 - val_loss: 1.0203 - val_acc: 0.9793 - val_mDice: 0.7854

Epoch 00265: val_mDice did not improve from 0.78631
Epoch 266/300
 - 17s - loss: 1.0679 - acc: 0.9665 - mDice: 0.7683 - val_loss: 1.0381 - val_acc: 0.9788 - val_mDice: 0.7846

Epoch 00266: val_mDice did not improve from 0.78631
Epoch 267/300
 - 17s - loss: 1.0655 - acc: 0.9665 - mDice: 0.7693 - val_loss: 1.0215 - val_acc: 0.9794 - val_mDice: 0.7851

Epoch 00267: val_mDice did not improve from 0.78631
Epoch 268/300
 - 17s - loss: 1.0668 - acc: 0.9665 - mDice: 0.7685 - val_loss: 1.0229 - val_acc: 0.9793 - val_mDice: 0.7831

Epoch 00268: val_mDice did not improve from 0.78631
Epoch 269/300
 - 17s - loss: 1.0630 - acc: 0.9666 - mDice: 0.7696 - val_loss: 1.0096 - val_acc: 0.9791 - val_mDice: 0.7810

Epoch 00269: val_mDice did not improve from 0.78631
Epoch 270/300
 - 17s - loss: 1.0668 - acc: 0.9665 - mDice: 0.7689 - val_loss: 1.0228 - val_acc: 0.9794 - val_mDice: 0.7815

Epoch 00270: val_mDice did not improve from 0.78631
Epoch 271/300
 - 18s - loss: 1.0665 - acc: 0.9665 - mDice: 0.7690 - val_loss: 1.0254 - val_acc: 0.9788 - val_mDice: 0.7833

Epoch 00271: val_mDice did not improve from 0.78631
Epoch 272/300
 - 17s - loss: 1.0654 - acc: 0.9666 - mDice: 0.7692 - val_loss: 1.0285 - val_acc: 0.9794 - val_mDice: 0.7834

Epoch 00272: val_mDice did not improve from 0.78631
Epoch 273/300
 - 17s - loss: 1.0692 - acc: 0.9665 - mDice: 0.7682 - val_loss: 1.0208 - val_acc: 0.9796 - val_mDice: 0.7843

Epoch 00273: val_mDice did not improve from 0.78631
Epoch 274/300
 - 17s - loss: 1.0660 - acc: 0.9665 - mDice: 0.7689 - val_loss: 1.0608 - val_acc: 0.9786 - val_mDice: 0.7805

Epoch 00274: val_mDice did not improve from 0.78631
Epoch 275/300
 - 17s - loss: 1.0657 - acc: 0.9665 - mDice: 0.7690 - val_loss: 1.0193 - val_acc: 0.9793 - val_mDice: 0.7831

Epoch 00275: val_mDice did not improve from 0.78631
Epoch 276/300
 - 18s - loss: 1.0615 - acc: 0.9666 - mDice: 0.7699 - val_loss: 1.0349 - val_acc: 0.9785 - val_mDice: 0.7832

Epoch 00276: val_mDice did not improve from 0.78631
Epoch 277/300
 - 17s - loss: 1.0634 - acc: 0.9665 - mDice: 0.7694 - val_loss: 1.0320 - val_acc: 0.9790 - val_mDice: 0.7793

Epoch 00277: val_mDice did not improve from 0.78631
Epoch 278/300
 - 17s - loss: 1.0621 - acc: 0.9666 - mDice: 0.7699 - val_loss: 1.0298 - val_acc: 0.9794 - val_mDice: 0.7859

Epoch 00278: val_mDice did not improve from 0.78631
Epoch 279/300
 - 17s - loss: 1.0644 - acc: 0.9665 - mDice: 0.7693 - val_loss: 1.0320 - val_acc: 0.9794 - val_mDice: 0.7834

Epoch 00279: val_mDice did not improve from 0.78631
Epoch 280/300
 - 17s - loss: 1.0618 - acc: 0.9666 - mDice: 0.7701 - val_loss: 1.0134 - val_acc: 0.9794 - val_mDice: 0.7852

Epoch 00280: val_mDice did not improve from 0.78631
Epoch 281/300
 - 17s - loss: 1.0630 - acc: 0.9666 - mDice: 0.7697 - val_loss: 1.0231 - val_acc: 0.9792 - val_mDice: 0.7829

Epoch 00281: val_mDice did not improve from 0.78631
Restoring model weights from the end of the best epoch
Epoch 00281: early stopping
{'val_loss': [9.123033290588165, 5.362680310314695, 4.123951361342767, 3.770928241247033, 2.864557152682742, 2.3865889511963188, 2.1187100552925955, 1.9715845100909, 1.7864679529712992, 1.702997205546535, 1.600118266677186, 1.562405531142424, 1.5232141845465126, 1.4566906782780464, 1.4093941636579945, 1.3722677107226031, 1.3603675608475723, 1.407275457373822, 1.3145817954008105, 1.3196550276660752, 1.3540216286278777, 1.284884995861087, 1.2535544188663703, 1.2720934698367998, 1.2567551125541512, 1.2511462435152494, 1.230275180616362, 1.2381126265743705, 1.228079689722279, 1.2091420819763559, 1.2006363079920803, 1.209569205299203, 1.1965824393899245, 1.1828071986225241, 1.1722934151785236, 1.1704141668359926, 1.1752179144555948, 1.1668442137840553, 1.166258252463148, 1.14556023211387, 1.153503458298782, 1.1564081737571945, 1.126096796172365, 1.1447659654441324, 1.1583033397872335, 1.1297825896467601, 1.1348609726332612, 1.1308943618789498, 1.136423272805809, 1.1217735743899757, 1.1330609227316242, 1.1192500165979555, 1.1348113223831766, 1.1134715790488807, 1.1094557363245316, 1.126453131070665, 1.1213300019659769, 1.115107951872378, 1.121822685893684, 1.1520671209677242, 1.0944591346859722, 1.1194485676309556, 1.1400777080566058, 1.11129671858987, 1.1270233792873174, 1.1097310954321877, 1.094417597267037, 1.1200493468341592, 1.1077873160005245, 1.1281340632371617, 1.1021869572269059, 1.0990889292400206, 1.094766296799866, 1.0878264430537166, 1.0965118210219331, 1.1208322363494778, 1.0807424027387622, 1.0876996140069408, 1.1038741899919426, 1.081928047438288, 1.0912329891863524, 1.0917134942824056, 1.0920193287525948, 1.0873899208430875, 1.0874869540621819, 1.1068943637326647, 1.0890706712625566, 1.1039312263993262, 1.086090411368908, 1.0770835008059738, 1.0992184081270322, 1.0692494672924437, 1.0855585840548698, 1.0738486063082315, 1.0941475006524204, 1.071624522887969, 1.1208286336729523, 1.0854241084340168, 1.1275729452159995, 1.0882515240008885, 1.089573538575734, 1.0905941713254355, 1.0821104032922084, 1.0969430201920978, 1.080385875094335, 1.105978876404687, 1.0787977736528394, 1.07965949539141, 1.0856605218248754, 1.0940006872891124, 1.082264138546057, 1.0668620197969916, 1.0978244837432418, 1.065789192234066, 1.0575026308296016, 1.0745094243587634, 1.085013391368209, 1.0749582296306095, 1.0740266769129279, 1.087298317827114, 1.0745287091325582, 1.0883824610542538, 1.0719853742260925, 1.0903288999634593, 1.0831354364150438, 1.072966503028501, 1.0845298973453903, 1.0726006617655024, 1.0707160974009804, 1.0881130154815835, 1.0955558640257126, 1.0756079361811672, 1.0822291839311329, 1.0536902522789364, 1.0742433888212448, 1.065372636322397, 1.0779535588145466, 1.0726992429245545, 1.067253631531459, 1.064969864691917, 1.081145729876151, 1.0682866699037736, 1.085437148023783, 1.0569292167578095, 1.071869247617537, 1.1015404011327479, 1.0714591388543167, 1.0786819430771946, 1.066904046728238, 1.080079133146048, 1.0709903005556398, 1.0605806210757558, 1.0669464043657473, 1.0565636126563502, 1.0730297977140792, 1.0637598481873847, 1.060554807445915, 1.063647775637454, 1.0589358784611909, 1.0556787641061003, 1.0692143723080574, 1.0572080452957555, 1.0587277826190205, 1.0810848016730512, 1.050714791659102, 1.0749745514564648, 1.0465603766206697, 1.0547324195897014, 1.0458661329348184, 1.0648915486930752, 1.0491947986329586, 1.068684100057203, 1.077992711733431, 1.0939621485390019, 1.0495348285706894, 1.0577254232497124, 1.057592011817939, 1.0721074981722973, 1.0674903773046336, 1.048684187639367, 1.0503660663569538, 1.0474372463192798, 1.043395219033548, 1.0582044372868664, 1.0543990041962408, 1.0385376845596126, 1.057092839783231, 1.044072344139716, 1.0536453244137218, 1.0681747387708385, 1.0682604388319126, 1.0543100752185224, 1.0560474865046872, 1.0465625683745936, 1.0396815708106766, 1.0411364872970983, 1.0603755309208835, 1.0377644888126998, 1.053893380508691, 1.0506855660037961, 1.0388435100210154, 1.043430323759994, 1.0309853337980113, 1.0530103505810031, 1.0498640437746298, 1.0482259974119323, 1.0347794135132238, 1.036692730886027, 1.0324313828102942, 1.0399268585893937, 1.0319425928781238, 1.0301423450974463, 1.0425181041073925, 1.0212257939189515, 1.0331619477230132, 1.0102983496310631, 1.0240555515607757, 1.0587436865838424, 1.0271085603375427, 1.0352488310978156, 1.0359697301065147, 1.023708902050615, 1.0326700955367465, 1.0226985203570347, 1.0118534885517114, 1.038618436387637, 1.0291195722372126, 1.0419289543046264, 1.0264049603985148, 1.0359437024656206, 1.0194101727490685, 1.0353189781177232, 1.0362614861481638, 1.0209526046717732, 1.0599306424808, 1.026240479338567, 1.042201584378529, 1.0459315462564962, 1.0254087078340863, 1.0304142991981105, 1.030637780057106, 1.0245842635107794, 1.0349840749127166, 1.0232698205066273, 1.0438789238200665, 1.0318859176602222, 1.023789402680154, 1.0333583878716601, 1.0625627439973224, 1.0373905413180118, 1.0340337038878187, 1.0165186580957972, 1.0437434077891399, 1.022850906178486, 1.0400636697904926, 1.0234967340274939, 1.0196680477926727, 1.0368298401103497, 1.0320052961264008, 1.033858834125874, 1.0132295374082556, 1.0260641681708016, 1.0525549560104397, 1.0132653945778711, 1.0202909438807013, 1.0380636565714603, 1.0215177827225, 1.0228528429628048, 1.0096200303369331, 1.0228084447304269, 1.025438798437638, 1.0285332439653903, 1.0207998731643328, 1.0607793683117428, 1.019313653449811, 1.034931300604071, 1.0319526279328788, 1.0297960589765873, 1.0320440925697032, 1.0133604308842357, 1.0231495382287172], 'val_acc': [0.911152898532016, 0.9106873940080666, 0.9113277562473276, 0.9181838944842401, 0.9358968237162893, 0.9514192723012767, 0.9567966683258701, 0.9601087343923237, 0.9638340275191255, 0.9654528821289854, 0.9675925466213997, 0.9676015145539818, 0.9687791458332685, 0.9697068309532527, 0.9707335031514428, 0.9714097399493722, 0.972142766146333, 0.9717874711343399, 0.9732609851079582, 0.9734803047456725, 0.9727495061282743, 0.9728003315012149, 0.9736865208940891, 0.9729852527641873, 0.9735879036998917, 0.9744875630092118, 0.9746336371282701, 0.975014338174897, 0.9753520822902136, 0.9747147113870444, 0.9755527224188739, 0.9753946776130288, 0.9752553235248438, 0.9756375361620228, 0.9757660614794713, 0.9752964194084932, 0.976088097204433, 0.9760791373378572, 0.9762188759755166, 0.9766290911681204, 0.975134643917553, 0.9768472814601837, 0.9762715396646456, 0.9766608456735661, 0.9762169956532429, 0.9767691960234215, 0.976488984636766, 0.9767486496005201, 0.976886510220479, 0.9765823978950353, 0.9767927439434876, 0.9773378378477583, 0.9766817803123086, 0.9768581159923533, 0.9774024781946139, 0.9767759345956343, 0.9768125422181061, 0.9768644717958983, 0.9769933671021085, 0.9769735575141303, 0.9770886364846322, 0.9770651072106588, 0.9768061762534881, 0.9769739351498851, 0.9774065845251503, 0.9773344883600312, 0.977890416063198, 0.9777170565300122, 0.9777760812603526, 0.9774514055419681, 0.9777320034684113, 0.9776912658411505, 0.9770931189843766, 0.9774984869261408, 0.9775414665889237, 0.977551541764204, 0.9781254120367483, 0.9780618960073837, 0.9774764513299, 0.9773950101202108, 0.9772941306730984, 0.9766007110816942, 0.9772190167740485, 0.9779916698987119, 0.9771902618802075, 0.9778552916431259, 0.9780032329902917, 0.9775321104405006, 0.9776864218376642, 0.9780817054906085, 0.9771271051547649, 0.9780006183471747, 0.977678203624665, 0.9780312527252626, 0.9773266409738202, 0.9777013624908635, 0.9768259925456793, 0.9777529148519144, 0.9774248878649962, 0.9773920181555991, 0.9781373684770822, 0.9775414435431912, 0.9777731012376205, 0.9775280078810841, 0.9778597803233169, 0.9778594025828088, 0.9779774561288697, 0.977854154650482, 0.9777663567992212, 0.9771794215866049, 0.9776811968463167, 0.9781687517367265, 0.9777637587071303, 0.9781919168881247, 0.9778097089648456, 0.9777861786433389, 0.9773389557752962, 0.9774028462978155, 0.9778638706265937, 0.9774962471951919, 0.9783820787506908, 0.9779841899871826, 0.9780312523062493, 0.9780066058380114, 0.977160371041256, 0.9778960244517335, 0.9780334952845515, 0.9780151894306257, 0.9780282739595705, 0.9780372433586992, 0.9781493199940097, 0.9774887773399822, 0.9784687556365672, 0.9782793410842574, 0.9778112115466113, 0.9784437331038744, 0.9782924299080887, 0.9778747295662892, 0.9781799631713773, 0.9781739793469072, 0.9780581698266819, 0.978206857227064, 0.9783144678089027, 0.97815865906764, 0.9780054822537937, 0.9784123471206437, 0.9781242902333371, 0.9774353384762112, 0.9779161875822003, 0.9774674791024314, 0.9780820795647499, 0.9791341544454671, 0.9780955285305298, 0.9786178375379901, 0.9779987609658802, 0.9779057319428255, 0.978677239170812, 0.978418704914502, 0.9786286603377867, 0.9781455838617416, 0.9782625350885106, 0.9783234124862457, 0.9782333879982021, 0.9787340185135236, 0.9788412539736877, 0.9787033835069157, 0.9786159621391229, 0.9785759822974515, 0.978935023079019, 0.9782173083620457, 0.978596524006453, 0.9784142164438178, 0.9777798254586272, 0.9786686493977511, 0.9783480865046094, 0.9788700179810683, 0.9784766080509381, 0.9789918313336498, 0.9781874232845272, 0.978690303168104, 0.9786395025169493, 0.9787321649033491, 0.9786906988214316, 0.9783208043378351, 0.9783831972019953, 0.9787351343459949, 0.9791472402314519, 0.978711241473213, 0.9784956575487536, 0.9787108623709118, 0.9783802097417768, 0.9785195665535482, 0.9789432440156048, 0.9787370141445019, 0.9786312838849368, 0.9789208251269296, 0.9783480826287362, 0.9791539558626855, 0.9786088725385013, 0.9788535798790166, 0.979059434733198, 0.9783566737635903, 0.9789802409014509, 0.9785035176101176, 0.978728796350516, 0.9788367582750237, 0.9788210801583811, 0.9791289385676594, 0.9786428716983024, 0.9785826981381918, 0.979164802127228, 0.9788412522976344, 0.9784851954146722, 0.9790217016409487, 0.9782726109970643, 0.9792866036426833, 0.9788524538854723, 0.9781108439911439, 0.978995166574924, 0.9792697826672103, 0.9789511071148154, 0.9790501059253941, 0.9792290488110695, 0.9791218417390579, 0.9789892074722397, 0.9790635485012209, 0.979249998743798, 0.9792552180784658, 0.9791476139913334, 0.9791689087720244, 0.9791147439676764, 0.9788046376566896, 0.9791558324138393, 0.9795600582300464, 0.978247207686017, 0.9787919385152458, 0.9787882014401977, 0.9793710302175244, 0.9790553357353948, 0.9785692518959984, 0.9788745041471794, 0.979206646682927, 0.9793661843284781, 0.9790445039268119, 0.9790213253669873, 0.9792227045303908, 0.9791251930075915, 0.9791334112205907, 0.978388060689601, 0.9784224381136769, 0.978656310712516, 0.9797659437769536, 0.9784818384894587, 0.9791274401760269, 0.9792058921446909, 0.9791565752197025, 0.9794726539370465, 0.979478626448157, 0.9789361411113102, 0.9791857267096508, 0.9792940727646195, 0.9788730260776929, 0.9788629186383869, 0.9795092633403248, 0.979269424201315, 0.9788289137171525, 0.979401298482724, 0.9793452597460554, 0.9791222011477331, 0.9793856006724554, 0.9788057539081741, 0.9794203592938991, 0.9796202414274635, 0.9785961599886313, 0.9792727800789952, 0.9785270480363776, 0.9789615467269307, 0.9793553493772953, 0.979435657889227, 0.9793811179632043, 0.9791887157411693], 'val_mDice': [0.005981639013682968, 0.07280652209304549, 0.1631629657137792, 0.23170128993703107, 0.35781315162856464, 0.434730064680371, 0.496771567106666, 0.5393981618286227, 0.5759454904834499, 0.6041755839684726, 0.622415480693321, 0.6367710009610087, 0.6568906822606843, 0.6668711538264538, 0.6731759806299461, 0.6825144881732853, 0.6875209221730961, 0.6865082195647362, 0.7009436694515611, 0.7011767537815918, 0.6962929576478231, 0.7071750628089234, 0.7126219644487847, 0.7109861858909285, 0.7159140822338513, 0.7176546673364086, 0.7214724055912038, 0.7229839492137486, 0.7176580738099472, 0.7238410775187983, 0.7282567501906142, 0.7308076743710858, 0.7294789420699402, 0.730868357556147, 0.7346101279627459, 0.7334173852823321, 0.7369198836844499, 0.7348825352263157, 0.7387481786454708, 0.7396365773908283, 0.7414095808206836, 0.7431468081600008, 0.7450920267976651, 0.741977072767717, 0.7404717462553081, 0.7472781301173259, 0.7439355831364127, 0.7455955597135011, 0.7463391505980533, 0.749078548541178, 0.7480005589436562, 0.7483328941207779, 0.7477009979408948, 0.7514934585886387, 0.7498786084890785, 0.7516048321195982, 0.7476750105252794, 0.7525898507274098, 0.7532412796322198, 0.7445164563785748, 0.7541919404048283, 0.7529235303087687, 0.7461046755628133, 0.7539032389702822, 0.7551825763052084, 0.7580317076564045, 0.7563818955044336, 0.7537554863886171, 0.7554018990855226, 0.7526752130637478, 0.7554505286191595, 0.7574512944908678, 0.7579659035000524, 0.7570225188937464, 0.757955014705658, 0.7548902206554983, 0.7579079346832784, 0.7597609063862497, 0.7537413475174057, 0.7614080344436458, 0.7592712804177524, 0.7622263841134592, 0.75900200245041, 0.7599542002686298, 0.7591549281705243, 0.7597175850181044, 0.7617251641511498, 0.7585518494222202, 0.7644776149877136, 0.7616412759036716, 0.7633370211338117, 0.7633059965495694, 0.7631195776282379, 0.7637112047425054, 0.7601292841463809, 0.7631475241406311, 0.761578644516179, 0.7614293542603826, 0.7590834727815249, 0.766185619919078, 0.7595561919815721, 0.7614758734334333, 0.7617744383367796, 0.7624209239319465, 0.7647932384470855, 0.7644570157691338, 0.7647393372021157, 0.7658303294114781, 0.7635143479898651, 0.7644407618234363, 0.762763142795261, 0.7643913292507715, 0.7642635536529059, 0.7659702446632519, 0.7679842892765789, 0.7647852326738395, 0.7640713432342181, 0.7606115168343529, 0.7665144125480853, 0.7661827559630355, 0.7665568235050396, 0.765858205005867, 0.7667400489163524, 0.76573026012662, 0.7652115464629523, 0.7683405524188479, 0.7660586200196211, 0.7689756804484684, 0.7675421665339143, 0.7673170428284443, 0.768784038630856, 0.7661547609498505, 0.7666282191846409, 0.7679511178775915, 0.7666248043308359, 0.768378912669284, 0.7697394885162059, 0.7699514145591347, 0.7675303407209829, 0.7719056626195857, 0.7681177180974051, 0.7676011826744817, 0.7660980400594969, 0.769412615806231, 0.7699502549397715, 0.7687825066134045, 0.771071687314548, 0.768941950714232, 0.7677563006722864, 0.7705301643465441, 0.7699036149861314, 0.7707169755690965, 0.7717366440643955, 0.7708991055748374, 0.7698944116728168, 0.7697023203586653, 0.7657895409788524, 0.7724283261123148, 0.770768585230219, 0.7712376677089081, 0.7740498254294974, 0.770375858501307, 0.7694757018231759, 0.7690245837025567, 0.7729951097173305, 0.7700881596608824, 0.7721437191711787, 0.7693495796518711, 0.7732110113586399, 0.7726472949311268, 0.7755284978760986, 0.7728048097898754, 0.7748334096689007, 0.7726708799338717, 0.7723936209150694, 0.7718667456052006, 0.7734045129668734, 0.7700267490686976, 0.7733906392263402, 0.7763838721913906, 0.7760106146021342, 0.7748056698348275, 0.7728367761065545, 0.773847232489259, 0.7756351638971607, 0.7779077464331642, 0.7760187216718503, 0.7746321847023989, 0.7728471075084801, 0.7734183596810473, 0.77596379017788, 0.7717494712144084, 0.7745178901248322, 0.7768148701304082, 0.7787902954383139, 0.7755316341907991, 0.7734529351936702, 0.7760466317300847, 0.7731545583225512, 0.7747908164411521, 0.7786799314151959, 0.777177442670497, 0.7782769470307027, 0.7753814815008367, 0.7745998540536381, 0.7795276049989388, 0.7781784361611351, 0.7771305922674169, 0.7774610418845983, 0.7810837202206228, 0.7740670158699653, 0.7766806856190592, 0.7776264291656039, 0.7806999833177389, 0.7779284125891218, 0.7810496106508118, 0.7783358862823259, 0.7781721221541688, 0.7800952231737348, 0.781381913668661, 0.7807613659407845, 0.7780866445891677, 0.7802676164622047, 0.7810346317626471, 0.7798395160002951, 0.7789099861741694, 0.7802723732061672, 0.7824392955625833, 0.7820413519083301, 0.7806276159462484, 0.7793853017902542, 0.7819319582153079, 0.779898076149617, 0.7808338991367963, 0.7763726353016804, 0.7808676231095997, 0.7799030427144994, 0.7786333795591063, 0.7828139611413483, 0.7820509548555987, 0.7836107882967314, 0.7820817573627814, 0.7804074802800934, 0.7818173116665523, 0.7810672286943727, 0.7826077613344511, 0.78302915978306, 0.7812841487475565, 0.7760945203434185, 0.7773135904687569, 0.786313332551812, 0.7835660109620521, 0.7787873177201551, 0.7828415066579942, 0.7821754469603353, 0.7797148435103244, 0.7847617341889648, 0.7826163763530644, 0.7848508079357851, 0.7860711738598158, 0.7855778605741022, 0.783057480160088, 0.7800301159412873, 0.7827700507452282, 0.7854172867086106, 0.7846328476820345, 0.7850676366440651, 0.7831389558336228, 0.781040156872285, 0.7815223733234908, 0.7833178658267526, 0.7834407757371926, 0.7843449488675028, 0.7805273410306035, 0.783140018975169, 0.7832047916254804, 0.7793339303382042, 0.7858881707350692, 0.7833572161009106, 0.785222708026638, 0.7829263091925368], 'loss': [66.41562838182675, 8.025377638212454, 6.024285543235497, 4.970348461699782, 4.292414563848379, 3.8206737734976177, 3.425860083473328, 3.134177331457464, 2.9154206248595775, 2.7369257529460636, 2.5937522462831217, 2.442850258016699, 2.3461573952235777, 2.2350267059915434, 2.167391924125956, 2.0785307308357033, 2.013313724970543, 1.9632146235267263, 1.9093134957431073, 1.8685488116128666, 1.830700207762051, 1.7913595849425867, 1.7548192318525704, 1.7308498885969794, 1.7022788366429862, 1.6700716908112079, 1.6535324928151134, 1.6297594042670462, 1.6247616423123628, 1.5922442153699865, 1.5807575168526111, 1.557915605231576, 1.550453630059914, 1.522230981964571, 1.519957363992445, 1.4992308376311871, 1.4880836659868149, 1.4791911435555598, 1.4634080102642748, 1.451623516908008, 1.435838760136298, 1.4254639574412484, 1.4204212408721864, 1.4056110253818057, 1.3989471878032849, 1.394066081968049, 1.3822224397231202, 1.3726305575473015, 1.3696242544960708, 1.360721935213113, 1.3557377232020338, 1.3492317281025632, 1.3424257483721607, 1.3345958267632767, 1.3303255993456877, 1.3272086185379781, 1.3189007359544247, 1.3155674626270397, 1.30914356876983, 1.3025486187038362, 1.3012339225905296, 1.300805994371398, 1.2914759668164775, 1.2898897620855452, 1.2902916778400364, 1.2809664502872637, 1.2788492309855777, 1.2734697673798423, 1.2700080546307628, 1.2673466444363455, 1.262438863928296, 1.2612439103427924, 1.2574948127985686, 1.2564789961361151, 1.2542210244807293, 1.2515467184370186, 1.2473397588347113, 1.2431127486660403, 1.2401941850497231, 1.2406375175075282, 1.2397245929649243, 1.2318967568676986, 1.2306777356969267, 1.2308417260241877, 1.2306586753157616, 1.2250079545831472, 1.2181248972642813, 1.222326985051507, 1.2199841638341127, 1.2175815166179718, 1.2155976483364852, 1.2094950895053644, 1.2119755787948052, 1.2107814440405198, 1.2097612762779926, 1.2044312004132118, 1.201818365667614, 1.204412745641154, 1.2000409173687092, 1.2039196675055688, 1.195674590346114, 1.1930470640409407, 1.1961951927224221, 1.1932729105723594, 1.1912827690482397, 1.187075262277633, 1.1903034437225255, 1.1871829707503385, 1.187783229177668, 1.1860143403649168, 1.1815996643947515, 1.1848329017719166, 1.1773649414492375, 1.1810928391951943, 1.1786081987246553, 1.1785864329117304, 1.1747649859772566, 1.1757356954427198, 1.1735756746158406, 1.1689166729046243, 1.1681873947209498, 1.1689286805994168, 1.167864676054816, 1.16578542563775, 1.1651607517609148, 1.1661426527205834, 1.164266779530625, 1.1638707217839732, 1.158823923303103, 1.1591212947363463, 1.1583478353648762, 1.1560405287041433, 1.156823590161618, 1.1561825402183374, 1.154165880881416, 1.1578729108015704, 1.1550741812092304, 1.152935653782462, 1.152310561521275, 1.1519989347753044, 1.150644737724137, 1.1494911414172773, 1.148608329104619, 1.1446302094011276, 1.1451529866825634, 1.148224518848146, 1.145875752647104, 1.1461743316120785, 1.1435439774833023, 1.14094277060774, 1.140356782428044, 1.1423848780008634, 1.1392008171072665, 1.1396862271387465, 1.137776389694809, 1.1375610305632937, 1.139340118123411, 1.1336465024556974, 1.1389044310658936, 1.1328297017097089, 1.1317604894961055, 1.134110706062839, 1.1324915945976874, 1.1327427001773582, 1.1311941028067236, 1.1323649379490883, 1.1307686070696132, 1.130646520892337, 1.1282256996527837, 1.1292057175874097, 1.124062349383433, 1.1242695123108708, 1.1275003462209476, 1.1255219975347386, 1.121122053255523, 1.1218870095629305, 1.1247276442381247, 1.1236321280157493, 1.116425744375929, 1.1180891460305964, 1.1163665321121505, 1.1174561500903266, 1.1173587196618715, 1.117441716920531, 1.1140802990363772, 1.1126756746379847, 1.1121000843987323, 1.1112514421052093, 1.1125819379877453, 1.110194577951687, 1.1084075297624703, 1.1084448552132853, 1.1088434425923397, 1.1053657996463138, 1.1074135961455083, 1.1046578874346127, 1.0999973530072487, 1.107250763817922, 1.1001996426528105, 1.1001600699800052, 1.1007099167389116, 1.1018462790365902, 1.0966766518505295, 1.0976958858919434, 1.0984665735889396, 1.0985258931566764, 1.0924044026953423, 1.094862883631857, 1.091300131858166, 1.095603257881114, 1.0924788026311523, 1.093630560525716, 1.0913507361355896, 1.088423923924539, 1.0902697420655365, 1.0922155746767646, 1.0879314352123302, 1.0900587677883637, 1.0891370527739854, 1.085442728652765, 1.0849523018185274, 1.087458600453081, 1.082796078537318, 1.0813211122532687, 1.0852225285028512, 1.0815229213571915, 1.0819662933194591, 1.0859014603669424, 1.0814219057952912, 1.0788762114811217, 1.080983939372289, 1.0785814589297704, 1.080384512218434, 1.0796238666067641, 1.0791386884997736, 1.0770455359884672, 1.0797002198057706, 1.0782837915160168, 1.0797612634928249, 1.075455683864817, 1.0761803299868018, 1.0773889610700293, 1.0759653911233746, 1.077132768014508, 1.0761741284131223, 1.073116536145481, 1.0733144776125594, 1.0704540898003088, 1.076416968297457, 1.0716711651918351, 1.072283030999072, 1.0705661373867685, 1.0716019664219392, 1.070441827351227, 1.0698896166776881, 1.0699146420197003, 1.0674611237585307, 1.0694948848711694, 1.0701656902384118, 1.0677952223284592, 1.0669705417830404, 1.0699973782729468, 1.0667126808653156, 1.0675079294884249, 1.066950496915878, 1.0679285547722628, 1.065532327256225, 1.0667622812494624, 1.0630477033613626, 1.0668076590325222, 1.0665132377367363, 1.0654049256950457, 1.0692177776973348, 1.0660485943402433, 1.0656613744107293, 1.0614754184551853, 1.0633687521825708, 1.0620956731744864, 1.06435770363292, 1.0618251583415546, 1.0629813120719214], 'acc': [0.7656145524881302, 0.8931837729608387, 0.8961504614381376, 0.9040695364507286, 0.9143054249134153, 0.9210429686190024, 0.9263139020052226, 0.9306688757392926, 0.9345553477927139, 0.9379890049495536, 0.940936826032635, 0.9440235190300114, 0.9460584662243882, 0.9481589296588684, 0.9493687081892497, 0.9508432397712345, 0.9519414129581637, 0.9527264317252676, 0.9535892685240417, 0.9542714943853887, 0.9548432315237586, 0.9555190388938332, 0.9561373455362108, 0.9565934309193439, 0.9569577632621804, 0.9575483605567825, 0.9578883814545344, 0.958267654927962, 0.9583960057690737, 0.9589304041174984, 0.959228604619525, 0.9595780533997007, 0.9597283875682466, 0.9601519405616858, 0.9600604298004272, 0.960301780576501, 0.96048118945043, 0.9606073785665669, 0.9608318039541978, 0.9609677377863455, 0.9612628052425543, 0.9613068713964751, 0.961436833023791, 0.9615942566205592, 0.9617918228312399, 0.9618323257936396, 0.9619196462892788, 0.9620839897690359, 0.96212937215945, 0.9622035230221743, 0.962268794775549, 0.9623626162808937, 0.9623667574283983, 0.9624529234458693, 0.962489820370754, 0.9624721786647227, 0.9625703551545968, 0.962627655855333, 0.962710942900371, 0.9628187811498847, 0.9627946686219163, 0.9627487805323898, 0.9628503179851329, 0.962893700666861, 0.9627901526019603, 0.9629398961233017, 0.9630187422038624, 0.9630765668651513, 0.9631107257678448, 0.9631399199164847, 0.9631712880733778, 0.9632670992568145, 0.9632996013057615, 0.9632486138860459, 0.9633256648442422, 0.96339955486953, 0.9634454616194408, 0.9635005173958648, 0.9635393397178523, 0.9635449982896483, 0.9635902532759217, 0.963691907907239, 0.9637226660756386, 0.9637251248759704, 0.963732999165173, 0.9637961085606614, 0.9638785287752094, 0.9638865333091546, 0.9639264239733847, 0.9639608813612374, 0.9639553641506257, 0.9640723615202377, 0.9640607316044891, 0.964092203666817, 0.9640835708049492, 0.9641443786080689, 0.9642205197038075, 0.9641794911730805, 0.9642045215393887, 0.9642206803012954, 0.9643045880313272, 0.9643731962440059, 0.9643423282130064, 0.9643868862919704, 0.9644274784468674, 0.9645093603328768, 0.9643935925928985, 0.9644656173326572, 0.964469828790306, 0.9644807220843122, 0.9646027673683207, 0.964492715645086, 0.9646159898148324, 0.9645978144026146, 0.9646493693937696, 0.9646658659923191, 0.9646639920710005, 0.9646809056051052, 0.9647179350334159, 0.9647838249281413, 0.964807585263479, 0.9647949093283588, 0.9648495660797171, 0.9648494051102813, 0.9648576006244554, 0.9648429858751131, 0.9649342306966505, 0.9649417416197887, 0.9649385532977195, 0.9649751755751035, 0.9649835402175696, 0.9650089485368973, 0.9650219242050018, 0.965035198403259, 0.9650739519688847, 0.9650414848140463, 0.9650232831795043, 0.9651192381537846, 0.9651540047742899, 0.965158456732226, 0.9651463031678812, 0.9651654943097167, 0.9651879945726881, 0.9652463417577309, 0.9652453714117515, 0.9652031860331886, 0.9652441701821367, 0.9652619937286782, 0.9652704443090977, 0.9653059846441457, 0.965319666533254, 0.9652974810941513, 0.9653027432715129, 0.9653180196471102, 0.9653883209214598, 0.9653644000846207, 0.965419077188126, 0.9654392525489962, 0.9653722193015432, 0.9654865609574261, 0.9654808483392062, 0.9654815121341571, 0.96548196882014, 0.9655213628589858, 0.9655198334633726, 0.9654999879332675, 0.9655138326665487, 0.965569611524613, 0.9655377630870443, 0.9655499910499602, 0.9656352254100286, 0.9655843011614577, 0.9655872835893887, 0.9656529940942604, 0.9656584394139496, 0.965690886165648, 0.965620291673401, 0.9656698455340693, 0.9657178379276199, 0.9657123369927267, 0.9657287164786929, 0.96570972556479, 0.9657768901848276, 0.9657532162193827, 0.9657715094550844, 0.9658141839840795, 0.9658111575523168, 0.9657553110987279, 0.965762812378218, 0.9658253757872415, 0.9657917375905567, 0.9658429930646972, 0.9657664708033895, 0.9658211279356943, 0.9658042898920076, 0.9658885921657334, 0.965906436727329, 0.9658421819963907, 0.9658956951910498, 0.9659100106423469, 0.9659420931281072, 0.9658825258430949, 0.9660031248438395, 0.9659174426375491, 0.9659662388404239, 0.9659723368926146, 0.9660459724335754, 0.9660039834764786, 0.9660220840701843, 0.9660232170233698, 0.9660605652919864, 0.9660487445399887, 0.9660334888464923, 0.9660954827163523, 0.9660600526458477, 0.9660746231723891, 0.9661095121817382, 0.9660737566628542, 0.9660869681878969, 0.9661650884661988, 0.9662012676696917, 0.9661530509855816, 0.9662137230097345, 0.9662439028562018, 0.9661822139955863, 0.9662142876085905, 0.9662167177419406, 0.966138320320488, 0.9662528259873804, 0.9662650649077603, 0.9662547280777504, 0.9662899032469863, 0.9662778907318695, 0.966269823515711, 0.966327532087273, 0.9663137304549501, 0.9662953819759922, 0.9663176772476988, 0.9662613077274321, 0.9663178819120095, 0.9663501548110782, 0.9663040463784388, 0.9663079942720333, 0.9663208217669083, 0.9663305364001362, 0.9663824335023349, 0.9663809997723115, 0.9663988788470691, 0.9663420590629191, 0.9664083753118793, 0.9663977002942756, 0.9664334198212049, 0.9663585445300291, 0.9664222516487304, 0.9664109976312893, 0.9664679861357885, 0.9664419580883328, 0.9664295794355402, 0.9664347149736537, 0.9664782553972701, 0.9665028748541784, 0.9665009795008641, 0.9665170174247095, 0.9665237350880449, 0.966491137466495, 0.9664759889749712, 0.9665158830677207, 0.9664691112210769, 0.9665523515656612, 0.9665145160873405, 0.9664741832010999, 0.9665539247671339, 0.966522398145454, 0.966526848270647, 0.966507699485874, 0.9665839568033209, 0.966531846173054, 0.9665716545468194, 0.9665136045511172, 0.9665596387171672, 0.9665648038911265], 'mDice': [0.019704117313658044, 0.056213020726833665, 0.11459635021591047, 0.18556453540990953, 0.2544216216375397, 0.30975613241424604, 0.35967674141417066, 0.40307407950030366, 0.4369979278051789, 0.466849384952291, 0.49074030100660376, 0.5155040763547631, 0.5324708733391398, 0.5509841033775393, 0.561868640780299, 0.5767622393292585, 0.587252375440169, 0.595653631631852, 0.6045478141144054, 0.6118204400395538, 0.6178335392758888, 0.6253870683877567, 0.6318765243306432, 0.6361439039024359, 0.6414488024425089, 0.6472667643356909, 0.6504435267903641, 0.6552860554052548, 0.6563915133470258, 0.6622274230083361, 0.6647083108549083, 0.6689426852822765, 0.6701866317410863, 0.6761350810728773, 0.6768171065593125, 0.6811802806098834, 0.682643895363161, 0.6853589193671334, 0.6876842172089092, 0.6901568326560847, 0.6933369268847186, 0.6952446530176142, 0.6963336281289056, 0.6995197758269799, 0.7007615220042193, 0.7018929314471762, 0.7040323237577074, 0.7059669555760337, 0.7062806110816179, 0.7082943148338454, 0.709132049174417, 0.7106626090115062, 0.7114776159909404, 0.7133635728463967, 0.7142551769266479, 0.7148826749735981, 0.7164642409477793, 0.7171349973978787, 0.7182804276224387, 0.7198360361502675, 0.7200142752871672, 0.7201299324836412, 0.7218322228529895, 0.7220020355860808, 0.7218498020957769, 0.7239695726598336, 0.7245468085584016, 0.7253597971177727, 0.7261233418110566, 0.7266332711824791, 0.727738132645863, 0.7277802943847397, 0.728842093884438, 0.7285282837295322, 0.7291375135812707, 0.7294884366577833, 0.7307034973247285, 0.7312770584444174, 0.7319238192991572, 0.7317379704299699, 0.7322451248153827, 0.7335690337382615, 0.7339953929463329, 0.7337361776644592, 0.733833733457341, 0.7347349842333433, 0.7361904704620563, 0.7355821082266976, 0.7359233367611661, 0.7364553939077744, 0.7366022095215422, 0.73820140810403, 0.7375467355223356, 0.7378966999692266, 0.7380999348782838, 0.7389199492118, 0.739533626432118, 0.7391935151810315, 0.7399840223717825, 0.7392339825504206, 0.7406465349484868, 0.7416562550795359, 0.7407177134948626, 0.7413095387543317, 0.7416835619047359, 0.7426020513332686, 0.7420788612466989, 0.7424588479931417, 0.7424219236371021, 0.7429864762695775, 0.7434322155779965, 0.7433162837490199, 0.7446436131104305, 0.7439128786848881, 0.7442886407185403, 0.7444146922319658, 0.7451014279281359, 0.7450434545476942, 0.74557087266159, 0.7463168189387743, 0.7464496644548343, 0.7463492683241036, 0.7466620923372443, 0.7469961208999862, 0.7471290567213873, 0.7470118423201982, 0.7475414185775712, 0.7475750865900764, 0.7482553026191566, 0.7486680836425826, 0.7484234780347627, 0.7491552870326743, 0.7490588763622722, 0.7491111338009598, 0.7493954485864501, 0.7489088213016016, 0.7492861556998437, 0.749596124476224, 0.7500270358821801, 0.7496633409739897, 0.7502777096453818, 0.7506136930850941, 0.7506607405750988, 0.7515614647929126, 0.7513933731285665, 0.7507392051781342, 0.7510488691402426, 0.7512133956393444, 0.751707732896429, 0.7521565291663311, 0.7522859024501196, 0.7521085385305342, 0.7525489161715379, 0.7526095572310313, 0.7530944181684819, 0.7529316275980421, 0.7529633802920317, 0.7535931854955707, 0.7526332262771658, 0.7538563334271758, 0.7540670731642257, 0.7536925945756636, 0.753861254825307, 0.7539433649913168, 0.7542495108005907, 0.7541180455110542, 0.7541040665735854, 0.7546276024816788, 0.7549379486501574, 0.7547438999803969, 0.7559314989208558, 0.755693544547683, 0.7549727189601455, 0.7555490720164679, 0.7565488488953365, 0.7563663116129395, 0.7558933926254975, 0.7559123019478377, 0.7573246160623058, 0.7571821132919748, 0.7576642752797571, 0.7576252554864505, 0.7573753438654138, 0.7569960791987518, 0.7579794189637323, 0.7586779340416993, 0.75869394610317, 0.7589140876922926, 0.758508243268512, 0.7591277119689952, 0.759440458251032, 0.7596371294351753, 0.7593002319935853, 0.7600759361813829, 0.7596909640557776, 0.7604455973072445, 0.7609935932505936, 0.759591165375298, 0.761592952952904, 0.7615542242627914, 0.7612202697266727, 0.761013960866514, 0.7619598851572986, 0.7619220861148704, 0.7618332776238146, 0.7619735234745147, 0.7633850502323525, 0.7630036100156533, 0.763475302924964, 0.762586943660863, 0.7630679715916144, 0.7628431895749749, 0.763169842204656, 0.7640142810584063, 0.7635890403305493, 0.7633077919672159, 0.7643933356105217, 0.7635233287548276, 0.7640020649108341, 0.7647586718736312, 0.7650009694099906, 0.7644207789585843, 0.7651169152050096, 0.7655453153449884, 0.7645602458244805, 0.7654695534081003, 0.7653749624596143, 0.7646827845813152, 0.7654633247092232, 0.7661856667461888, 0.765791759875033, 0.766176482882559, 0.7658634805864095, 0.7660358527347286, 0.7662450694832671, 0.7666632566518737, 0.765924437873575, 0.7666102944109834, 0.7659900603947801, 0.7669915619388895, 0.7666745598654185, 0.7665058271309358, 0.7669092787151386, 0.7664686795431884, 0.7666585140226831, 0.7675251356219137, 0.7673521769838072, 0.7677588066105202, 0.7664075066012529, 0.767611071649624, 0.7675873788228231, 0.768023513282625, 0.767418399660884, 0.7679796633713919, 0.7680621313632425, 0.768304209819312, 0.7685164410972816, 0.7679631174957499, 0.7677156897035364, 0.7685637555198684, 0.7686118146840594, 0.7683669528581555, 0.76855433359161, 0.7688097641150212, 0.7685822439772044, 0.7683164819510364, 0.7692607353020492, 0.7684942141439983, 0.7695863039256834, 0.7688535692455316, 0.7690433055457888, 0.7692079236460503, 0.7681627229915688, 0.768885078742612, 0.7689908802056371, 0.7699000530301472, 0.7694339780783258, 0.7698690874888164, 0.7692526815240069, 0.7701070880345169, 0.7696934610216795]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:01<00:27,  1.99s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:24,  1.87s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:22,  1.87s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:20,  1.86s/it]predicting test subjects:  33%|███▎      | 5/15 [00:09<00:19,  1.95s/it]predicting test subjects:  40%|████      | 6/15 [00:11<00:18,  2.04s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:14,  1.84s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:15<00:13,  1.97s/it]predicting test subjects:  60%|██████    | 9/15 [00:17<00:11,  1.95s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:18<00:08,  1.80s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:20<00:07,  1.75s/it]predicting test subjects:  80%|████████  | 12/15 [00:22<00:05,  1.82s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:24<00:03,  1.88s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:25<00:01,  1.81s/it]predicting test subjects: 100%|██████████| 15/15 [00:27<00:00,  1.83s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<19:32,  2.21s/it]predicting train subjects:   0%|          | 2/532 [00:03<17:48,  2.02s/it]predicting train subjects:   1%|          | 3/532 [00:05<16:46,  1.90s/it]predicting train subjects:   1%|          | 4/532 [00:07<15:56,  1.81s/it]predicting train subjects:   1%|          | 5/532 [00:08<15:40,  1.78s/it]predicting train subjects:   1%|          | 6/532 [00:10<15:02,  1.72s/it]predicting train subjects:   1%|▏         | 7/532 [00:11<14:55,  1.71s/it]predicting train subjects:   2%|▏         | 8/532 [00:13<14:16,  1.63s/it]predicting train subjects:   2%|▏         | 9/532 [00:15<14:55,  1.71s/it]predicting train subjects:   2%|▏         | 10/532 [00:16<14:35,  1.68s/it]predicting train subjects:   2%|▏         | 11/532 [00:18<13:48,  1.59s/it]predicting train subjects:   2%|▏         | 12/532 [00:20<15:06,  1.74s/it]predicting train subjects:   2%|▏         | 13/532 [00:21<14:06,  1.63s/it]predicting train subjects:   3%|▎         | 14/532 [00:23<13:19,  1.54s/it]predicting train subjects:   3%|▎         | 15/532 [00:24<13:28,  1.56s/it]predicting train subjects:   3%|▎         | 16/532 [00:26<13:45,  1.60s/it]predicting train subjects:   3%|▎         | 17/532 [00:27<13:20,  1.55s/it]predicting train subjects:   3%|▎         | 18/532 [00:29<14:10,  1.65s/it]predicting train subjects:   4%|▎         | 19/532 [00:31<13:34,  1.59s/it]predicting train subjects:   4%|▍         | 20/532 [00:32<13:48,  1.62s/it]predicting train subjects:   4%|▍         | 21/532 [00:34<14:43,  1.73s/it]predicting train subjects:   4%|▍         | 22/532 [00:36<14:16,  1.68s/it]predicting train subjects:   4%|▍         | 23/532 [00:38<14:26,  1.70s/it]predicting train subjects:   5%|▍         | 24/532 [00:39<13:41,  1.62s/it]predicting train subjects:   5%|▍         | 25/532 [00:41<15:08,  1.79s/it]predicting train subjects:   5%|▍         | 26/532 [00:43<14:32,  1.72s/it]predicting train subjects:   5%|▌         | 27/532 [00:45<15:50,  1.88s/it]predicting train subjects:   5%|▌         | 28/532 [00:47<15:17,  1.82s/it]predicting train subjects:   5%|▌         | 29/532 [00:49<15:53,  1.90s/it]predicting train subjects:   6%|▌         | 30/532 [00:50<14:54,  1.78s/it]predicting train subjects:   6%|▌         | 31/532 [00:52<14:37,  1.75s/it]predicting train subjects:   6%|▌         | 32/532 [00:54<14:14,  1.71s/it]predicting train subjects:   6%|▌         | 33/532 [00:55<13:30,  1.62s/it]predicting train subjects:   6%|▋         | 34/532 [00:57<14:20,  1.73s/it]predicting train subjects:   7%|▋         | 35/532 [00:59<14:04,  1.70s/it]predicting train subjects:   7%|▋         | 36/532 [01:01<14:17,  1.73s/it]predicting train subjects:   7%|▋         | 37/532 [01:02<14:18,  1.73s/it]predicting train subjects:   7%|▋         | 38/532 [01:04<14:53,  1.81s/it]predicting train subjects:   7%|▋         | 39/532 [01:06<14:32,  1.77s/it]predicting train subjects:   8%|▊         | 40/532 [01:07<14:01,  1.71s/it]predicting train subjects:   8%|▊         | 41/532 [01:09<14:17,  1.75s/it]predicting train subjects:   8%|▊         | 42/532 [01:11<14:15,  1.75s/it]predicting train subjects:   8%|▊         | 43/532 [01:13<13:34,  1.67s/it]predicting train subjects:   8%|▊         | 44/532 [01:14<12:53,  1.59s/it]predicting train subjects:   8%|▊         | 45/532 [01:16<12:56,  1.59s/it]predicting train subjects:   9%|▊         | 46/532 [01:17<13:28,  1.66s/it]predicting train subjects:   9%|▉         | 47/532 [01:19<14:20,  1.77s/it]predicting train subjects:   9%|▉         | 48/532 [01:21<14:15,  1.77s/it]predicting train subjects:   9%|▉         | 49/532 [01:23<13:33,  1.68s/it]predicting train subjects:   9%|▉         | 50/532 [01:25<14:08,  1.76s/it]predicting train subjects:  10%|▉         | 51/532 [01:26<13:49,  1.72s/it]predicting train subjects:  10%|▉         | 52/532 [01:28<13:47,  1.72s/it]predicting train subjects:  10%|▉         | 53/532 [01:29<13:15,  1.66s/it]predicting train subjects:  10%|█         | 54/532 [01:31<13:42,  1.72s/it]predicting train subjects:  10%|█         | 55/532 [01:33<13:48,  1.74s/it]predicting train subjects:  11%|█         | 56/532 [01:35<13:47,  1.74s/it]predicting train subjects:  11%|█         | 57/532 [01:37<13:41,  1.73s/it]predicting train subjects:  11%|█         | 58/532 [01:38<13:44,  1.74s/it]predicting train subjects:  11%|█         | 59/532 [01:40<14:45,  1.87s/it]predicting train subjects:  11%|█▏        | 60/532 [01:42<13:34,  1.73s/it]predicting train subjects:  11%|█▏        | 61/532 [01:43<12:51,  1.64s/it]predicting train subjects:  12%|█▏        | 62/532 [01:45<13:55,  1.78s/it]predicting train subjects:  12%|█▏        | 63/532 [01:47<14:19,  1.83s/it]predicting train subjects:  12%|█▏        | 64/532 [01:49<13:25,  1.72s/it]predicting train subjects:  12%|█▏        | 65/532 [01:50<13:14,  1.70s/it]predicting train subjects:  12%|█▏        | 66/532 [01:53<14:34,  1.88s/it]predicting train subjects:  13%|█▎        | 67/532 [01:55<14:53,  1.92s/it]predicting train subjects:  13%|█▎        | 68/532 [01:57<14:33,  1.88s/it]predicting train subjects:  13%|█▎        | 69/532 [01:58<13:53,  1.80s/it]predicting train subjects:  13%|█▎        | 70/532 [02:00<13:26,  1.75s/it]predicting train subjects:  13%|█▎        | 71/532 [02:01<12:53,  1.68s/it]predicting train subjects:  14%|█▎        | 72/532 [02:03<12:26,  1.62s/it]predicting train subjects:  14%|█▎        | 73/532 [02:05<13:00,  1.70s/it]predicting train subjects:  14%|█▍        | 74/532 [02:07<14:16,  1.87s/it]predicting train subjects:  14%|█▍        | 75/532 [02:10<16:25,  2.16s/it]predicting train subjects:  14%|█▍        | 76/532 [02:11<15:10,  2.00s/it]predicting train subjects:  14%|█▍        | 77/532 [02:13<14:37,  1.93s/it]predicting train subjects:  15%|█▍        | 78/532 [02:15<14:18,  1.89s/it]predicting train subjects:  15%|█▍        | 79/532 [02:17<13:59,  1.85s/it]predicting train subjects:  15%|█▌        | 80/532 [02:19<13:47,  1.83s/it]predicting train subjects:  15%|█▌        | 81/532 [02:20<13:37,  1.81s/it]predicting train subjects:  15%|█▌        | 82/532 [02:22<13:27,  1.79s/it]predicting train subjects:  16%|█▌        | 83/532 [02:24<12:48,  1.71s/it]predicting train subjects:  16%|█▌        | 84/532 [02:25<12:17,  1.65s/it]predicting train subjects:  16%|█▌        | 85/532 [02:27<11:52,  1.59s/it]predicting train subjects:  16%|█▌        | 86/532 [02:28<11:28,  1.54s/it]predicting train subjects:  16%|█▋        | 87/532 [02:29<11:17,  1.52s/it]predicting train subjects:  17%|█▋        | 88/532 [02:31<11:01,  1.49s/it]predicting train subjects:  17%|█▋        | 89/532 [02:32<11:14,  1.52s/it]predicting train subjects:  17%|█▋        | 90/532 [02:34<11:19,  1.54s/it]predicting train subjects:  17%|█▋        | 91/532 [02:36<11:32,  1.57s/it]predicting train subjects:  17%|█▋        | 92/532 [02:37<11:47,  1.61s/it]predicting train subjects:  17%|█▋        | 93/532 [02:39<11:56,  1.63s/it]predicting train subjects:  18%|█▊        | 94/532 [02:41<12:02,  1.65s/it]predicting train subjects:  18%|█▊        | 95/532 [02:43<12:51,  1.77s/it]predicting train subjects:  18%|█▊        | 96/532 [02:45<13:13,  1.82s/it]predicting train subjects:  18%|█▊        | 97/532 [02:47<13:37,  1.88s/it]predicting train subjects:  18%|█▊        | 98/532 [02:49<13:48,  1.91s/it]predicting train subjects:  19%|█▊        | 99/532 [02:51<13:55,  1.93s/it]predicting train subjects:  19%|█▉        | 100/532 [02:53<14:07,  1.96s/it]predicting train subjects:  19%|█▉        | 101/532 [02:54<13:00,  1.81s/it]predicting train subjects:  19%|█▉        | 102/532 [02:56<12:12,  1.70s/it]predicting train subjects:  19%|█▉        | 103/532 [02:57<11:47,  1.65s/it]predicting train subjects:  20%|█▉        | 104/532 [02:59<11:25,  1.60s/it]predicting train subjects:  20%|█▉        | 105/532 [03:00<11:10,  1.57s/it]predicting train subjects:  20%|█▉        | 106/532 [03:02<10:52,  1.53s/it]predicting train subjects:  20%|██        | 107/532 [03:03<10:39,  1.51s/it]predicting train subjects:  20%|██        | 108/532 [03:05<10:31,  1.49s/it]predicting train subjects:  20%|██        | 109/532 [03:06<10:29,  1.49s/it]predicting train subjects:  21%|██        | 110/532 [03:07<10:27,  1.49s/it]predicting train subjects:  21%|██        | 111/532 [03:09<10:27,  1.49s/it]predicting train subjects:  21%|██        | 112/532 [03:10<10:25,  1.49s/it]predicting train subjects:  21%|██        | 113/532 [03:12<11:01,  1.58s/it]predicting train subjects:  21%|██▏       | 114/532 [03:14<11:17,  1.62s/it]predicting train subjects:  22%|██▏       | 115/532 [03:16<11:36,  1.67s/it]predicting train subjects:  22%|██▏       | 116/532 [03:17<11:39,  1.68s/it]predicting train subjects:  22%|██▏       | 117/532 [03:19<11:42,  1.69s/it]predicting train subjects:  22%|██▏       | 118/532 [03:21<11:47,  1.71s/it]predicting train subjects:  22%|██▏       | 119/532 [03:23<11:41,  1.70s/it]predicting train subjects:  23%|██▎       | 120/532 [03:24<11:32,  1.68s/it]predicting train subjects:  23%|██▎       | 121/532 [03:26<11:33,  1.69s/it]predicting train subjects:  23%|██▎       | 122/532 [03:28<11:34,  1.69s/it]predicting train subjects:  23%|██▎       | 123/532 [03:29<11:32,  1.69s/it]predicting train subjects:  23%|██▎       | 124/532 [03:31<11:26,  1.68s/it]predicting train subjects:  23%|██▎       | 125/532 [03:33<11:43,  1.73s/it]predicting train subjects:  24%|██▎       | 126/532 [03:35<12:01,  1.78s/it]predicting train subjects:  24%|██▍       | 127/532 [03:37<12:16,  1.82s/it]predicting train subjects:  24%|██▍       | 128/532 [03:39<12:18,  1.83s/it]predicting train subjects:  24%|██▍       | 129/532 [03:40<12:19,  1.84s/it]predicting train subjects:  24%|██▍       | 130/532 [03:42<12:18,  1.84s/it]predicting train subjects:  25%|██▍       | 131/532 [03:44<13:04,  1.96s/it]predicting train subjects:  25%|██▍       | 132/532 [03:47<13:36,  2.04s/it]predicting train subjects:  25%|██▌       | 133/532 [03:49<13:45,  2.07s/it]predicting train subjects:  25%|██▌       | 134/532 [03:51<13:56,  2.10s/it]predicting train subjects:  25%|██▌       | 135/532 [03:53<14:16,  2.16s/it]predicting train subjects:  26%|██▌       | 136/532 [03:55<14:18,  2.17s/it]predicting train subjects:  26%|██▌       | 137/532 [03:58<14:19,  2.17s/it]predicting train subjects:  26%|██▌       | 138/532 [04:00<14:15,  2.17s/it]predicting train subjects:  26%|██▌       | 139/532 [04:02<14:13,  2.17s/it]predicting train subjects:  26%|██▋       | 140/532 [04:04<14:20,  2.20s/it]predicting train subjects:  27%|██▋       | 141/532 [04:06<14:14,  2.19s/it]predicting train subjects:  27%|██▋       | 142/532 [04:09<14:27,  2.22s/it]predicting train subjects:  27%|██▋       | 143/532 [04:10<13:21,  2.06s/it]predicting train subjects:  27%|██▋       | 144/532 [04:12<12:28,  1.93s/it]predicting train subjects:  27%|██▋       | 145/532 [04:14<11:46,  1.83s/it]predicting train subjects:  27%|██▋       | 146/532 [04:15<11:17,  1.76s/it]predicting train subjects:  28%|██▊       | 147/532 [04:17<10:56,  1.71s/it]predicting train subjects:  28%|██▊       | 148/532 [04:18<10:45,  1.68s/it]predicting train subjects:  28%|██▊       | 149/532 [04:20<10:51,  1.70s/it]predicting train subjects:  28%|██▊       | 150/532 [04:22<10:51,  1.70s/it]predicting train subjects:  28%|██▊       | 151/532 [04:24<10:52,  1.71s/it]predicting train subjects:  29%|██▊       | 152/532 [04:25<10:49,  1.71s/it]predicting train subjects:  29%|██▉       | 153/532 [04:27<10:54,  1.73s/it]predicting train subjects:  29%|██▉       | 154/532 [04:29<10:56,  1.74s/it]predicting train subjects:  29%|██▉       | 155/532 [04:31<12:11,  1.94s/it]predicting train subjects:  29%|██▉       | 156/532 [04:34<12:50,  2.05s/it]predicting train subjects:  30%|██▉       | 157/532 [04:36<13:35,  2.18s/it]predicting train subjects:  30%|██▉       | 158/532 [04:38<13:43,  2.20s/it]predicting train subjects:  30%|██▉       | 159/532 [04:41<13:51,  2.23s/it]predicting train subjects:  30%|███       | 160/532 [04:43<13:57,  2.25s/it]predicting train subjects:  30%|███       | 161/532 [04:45<12:51,  2.08s/it]predicting train subjects:  30%|███       | 162/532 [04:46<11:57,  1.94s/it]predicting train subjects:  31%|███       | 163/532 [04:48<11:19,  1.84s/it]predicting train subjects:  31%|███       | 164/532 [04:49<10:54,  1.78s/it]predicting train subjects:  31%|███       | 165/532 [04:51<10:41,  1.75s/it]predicting train subjects:  31%|███       | 166/532 [04:53<10:28,  1.72s/it]predicting train subjects:  31%|███▏      | 167/532 [04:54<10:28,  1.72s/it]predicting train subjects:  32%|███▏      | 168/532 [04:56<10:32,  1.74s/it]predicting train subjects:  32%|███▏      | 169/532 [04:58<10:31,  1.74s/it]predicting train subjects:  32%|███▏      | 170/532 [05:00<10:29,  1.74s/it]predicting train subjects:  32%|███▏      | 171/532 [05:02<10:37,  1.77s/it]predicting train subjects:  32%|███▏      | 172/532 [05:03<10:33,  1.76s/it]predicting train subjects:  33%|███▎      | 173/532 [05:05<10:12,  1.71s/it]predicting train subjects:  33%|███▎      | 174/532 [05:06<10:00,  1.68s/it]predicting train subjects:  33%|███▎      | 175/532 [05:08<09:44,  1.64s/it]predicting train subjects:  33%|███▎      | 176/532 [05:10<09:33,  1.61s/it]predicting train subjects:  33%|███▎      | 177/532 [05:11<09:27,  1.60s/it]predicting train subjects:  33%|███▎      | 178/532 [05:13<09:21,  1.59s/it]predicting train subjects:  34%|███▎      | 179/532 [05:14<09:24,  1.60s/it]predicting train subjects:  34%|███▍      | 180/532 [05:16<09:25,  1.61s/it]predicting train subjects:  34%|███▍      | 181/532 [05:18<09:21,  1.60s/it]predicting train subjects:  34%|███▍      | 182/532 [05:19<09:21,  1.60s/it]predicting train subjects:  34%|███▍      | 183/532 [05:21<09:15,  1.59s/it]predicting train subjects:  35%|███▍      | 184/532 [05:22<09:21,  1.61s/it]predicting train subjects:  35%|███▍      | 185/532 [05:24<09:04,  1.57s/it]predicting train subjects:  35%|███▍      | 186/532 [05:25<08:54,  1.54s/it]predicting train subjects:  35%|███▌      | 187/532 [05:27<08:40,  1.51s/it]predicting train subjects:  35%|███▌      | 188/532 [05:28<08:34,  1.50s/it]predicting train subjects:  36%|███▌      | 189/532 [05:30<08:31,  1.49s/it]predicting train subjects:  36%|███▌      | 190/532 [05:31<08:30,  1.49s/it]predicting train subjects:  36%|███▌      | 191/532 [05:33<09:45,  1.72s/it]predicting train subjects:  36%|███▌      | 192/532 [05:36<10:33,  1.86s/it]predicting train subjects:  36%|███▋      | 193/532 [05:38<11:09,  1.97s/it]predicting train subjects:  36%|███▋      | 194/532 [05:40<11:39,  2.07s/it]predicting train subjects:  37%|███▋      | 195/532 [05:42<11:58,  2.13s/it]predicting train subjects:  37%|███▋      | 196/532 [05:45<12:12,  2.18s/it]predicting train subjects:  37%|███▋      | 197/532 [05:47<11:50,  2.12s/it]predicting train subjects:  37%|███▋      | 198/532 [05:49<11:20,  2.04s/it]predicting train subjects:  37%|███▋      | 199/532 [05:51<11:11,  2.02s/it]predicting train subjects:  38%|███▊      | 200/532 [05:52<10:54,  1.97s/it]predicting train subjects:  38%|███▊      | 201/532 [05:54<10:46,  1.95s/it]predicting train subjects:  38%|███▊      | 202/532 [05:56<10:43,  1.95s/it]predicting train subjects:  38%|███▊      | 203/532 [05:58<10:16,  1.87s/it]predicting train subjects:  38%|███▊      | 204/532 [06:00<09:51,  1.80s/it]predicting train subjects:  39%|███▊      | 205/532 [06:01<09:55,  1.82s/it]predicting train subjects:  39%|███▊      | 206/532 [06:03<09:35,  1.77s/it]predicting train subjects:  39%|███▉      | 207/532 [06:05<09:20,  1.72s/it]predicting train subjects:  39%|███▉      | 208/532 [06:06<08:59,  1.67s/it]predicting train subjects:  39%|███▉      | 209/532 [06:08<08:30,  1.58s/it]predicting train subjects:  39%|███▉      | 210/532 [06:09<08:11,  1.52s/it]predicting train subjects:  40%|███▉      | 211/532 [06:10<07:58,  1.49s/it]predicting train subjects:  40%|███▉      | 212/532 [06:12<07:50,  1.47s/it]predicting train subjects:  40%|████      | 213/532 [06:13<07:46,  1.46s/it]predicting train subjects:  40%|████      | 214/532 [06:15<07:42,  1.45s/it]predicting train subjects:  40%|████      | 215/532 [06:17<08:39,  1.64s/it]predicting train subjects:  41%|████      | 216/532 [06:19<09:18,  1.77s/it]predicting train subjects:  41%|████      | 217/532 [06:21<09:44,  1.86s/it]predicting train subjects:  41%|████      | 218/532 [06:23<10:05,  1.93s/it]predicting train subjects:  41%|████      | 219/532 [06:25<10:15,  1.97s/it]predicting train subjects:  41%|████▏     | 220/532 [06:27<10:28,  2.02s/it]predicting train subjects:  42%|████▏     | 221/532 [06:29<09:25,  1.82s/it]predicting train subjects:  42%|████▏     | 222/532 [06:30<08:43,  1.69s/it]predicting train subjects:  42%|████▏     | 223/532 [06:31<08:22,  1.63s/it]predicting train subjects:  42%|████▏     | 224/532 [06:33<08:02,  1.57s/it]predicting train subjects:  42%|████▏     | 225/532 [06:34<07:46,  1.52s/it]predicting train subjects:  42%|████▏     | 226/532 [06:36<07:30,  1.47s/it]predicting train subjects:  43%|████▎     | 227/532 [06:37<07:12,  1.42s/it]predicting train subjects:  43%|████▎     | 228/532 [06:38<07:04,  1.40s/it]predicting train subjects:  43%|████▎     | 229/532 [06:40<06:57,  1.38s/it]predicting train subjects:  43%|████▎     | 230/532 [06:41<06:49,  1.36s/it]predicting train subjects:  43%|████▎     | 231/532 [06:42<06:50,  1.36s/it]predicting train subjects:  44%|████▎     | 232/532 [06:44<06:54,  1.38s/it]predicting train subjects:  44%|████▍     | 233/532 [06:45<07:12,  1.45s/it]predicting train subjects:  44%|████▍     | 234/532 [06:47<07:24,  1.49s/it]predicting train subjects:  44%|████▍     | 235/532 [06:48<07:27,  1.51s/it]predicting train subjects:  44%|████▍     | 236/532 [06:50<07:34,  1.53s/it]predicting train subjects:  45%|████▍     | 237/532 [06:52<07:41,  1.56s/it]predicting train subjects:  45%|████▍     | 238/532 [06:53<07:38,  1.56s/it]predicting train subjects:  45%|████▍     | 239/532 [06:55<07:50,  1.61s/it]predicting train subjects:  45%|████▌     | 240/532 [06:57<07:59,  1.64s/it]predicting train subjects:  45%|████▌     | 241/532 [06:58<08:03,  1.66s/it]predicting train subjects:  45%|████▌     | 242/532 [07:00<08:04,  1.67s/it]predicting train subjects:  46%|████▌     | 243/532 [07:02<08:07,  1.69s/it]predicting train subjects:  46%|████▌     | 244/532 [07:03<08:02,  1.68s/it]predicting train subjects:  46%|████▌     | 245/532 [07:05<07:35,  1.59s/it]predicting train subjects:  46%|████▌     | 246/532 [07:06<07:15,  1.52s/it]predicting train subjects:  46%|████▋     | 247/532 [07:08<06:56,  1.46s/it]predicting train subjects:  47%|████▋     | 248/532 [07:09<06:52,  1.45s/it]predicting train subjects:  47%|████▋     | 249/532 [07:10<06:41,  1.42s/it]predicting train subjects:  47%|████▋     | 250/532 [07:12<06:38,  1.41s/it]predicting train subjects:  47%|████▋     | 251/532 [07:13<06:43,  1.44s/it]predicting train subjects:  47%|████▋     | 252/532 [07:15<06:44,  1.44s/it]predicting train subjects:  48%|████▊     | 253/532 [07:16<06:49,  1.47s/it]predicting train subjects:  48%|████▊     | 254/532 [07:18<06:45,  1.46s/it]predicting train subjects:  48%|████▊     | 255/532 [07:19<06:43,  1.46s/it]predicting train subjects:  48%|████▊     | 256/532 [07:21<06:43,  1.46s/it]predicting train subjects:  48%|████▊     | 257/532 [07:22<07:20,  1.60s/it]predicting train subjects:  48%|████▊     | 258/532 [07:24<07:39,  1.68s/it]predicting train subjects:  49%|████▊     | 259/532 [07:26<07:58,  1.75s/it]predicting train subjects:  49%|████▉     | 260/532 [07:28<08:07,  1.79s/it]predicting train subjects:  49%|████▉     | 261/532 [07:30<08:11,  1.82s/it]predicting train subjects:  49%|████▉     | 262/532 [07:32<08:11,  1.82s/it]predicting train subjects:  49%|████▉     | 263/532 [07:33<07:32,  1.68s/it]predicting train subjects:  50%|████▉     | 264/532 [07:35<07:04,  1.58s/it]predicting train subjects:  50%|████▉     | 265/532 [07:36<06:41,  1.50s/it]predicting train subjects:  50%|█████     | 266/532 [07:37<06:23,  1.44s/it]predicting train subjects:  50%|█████     | 267/532 [07:39<06:17,  1.42s/it]predicting train subjects:  50%|█████     | 268/532 [07:40<06:07,  1.39s/it]predicting train subjects:  51%|█████     | 269/532 [07:42<06:29,  1.48s/it]predicting train subjects:  51%|█████     | 270/532 [07:43<06:45,  1.55s/it]predicting train subjects:  51%|█████     | 271/532 [07:45<06:51,  1.58s/it]predicting train subjects:  51%|█████     | 272/532 [07:47<07:05,  1.64s/it]predicting train subjects:  51%|█████▏    | 273/532 [07:48<07:16,  1.68s/it]predicting train subjects:  52%|█████▏    | 274/532 [07:50<07:18,  1.70s/it]predicting train subjects:  52%|█████▏    | 275/532 [07:52<07:50,  1.83s/it]predicting train subjects:  52%|█████▏    | 276/532 [07:54<08:11,  1.92s/it]predicting train subjects:  52%|█████▏    | 277/532 [07:57<08:25,  1.98s/it]predicting train subjects:  52%|█████▏    | 278/532 [07:59<08:36,  2.04s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:01<08:45,  2.08s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:03<08:52,  2.11s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:05<08:45,  2.10s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:07<08:41,  2.09s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:09<08:35,  2.07s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:11<08:30,  2.06s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:13<08:21,  2.03s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:15<08:12,  2.00s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:17<07:34,  1.86s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:18<07:04,  1.74s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:20<06:48,  1.68s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:21<06:37,  1.64s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:23<06:31,  1.63s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:25<06:30,  1.63s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:26<06:38,  1.67s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:28<06:42,  1.69s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:30<06:45,  1.71s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:32<06:49,  1.74s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:33<06:48,  1.74s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:35<06:47,  1.74s/it]predicting train subjects:  56%|█████▌    | 299/532 [08:36<06:23,  1.64s/it]predicting train subjects:  56%|█████▋    | 300/532 [08:38<06:05,  1.58s/it]predicting train subjects:  57%|█████▋    | 301/532 [08:39<05:56,  1.54s/it]predicting train subjects:  57%|█████▋    | 302/532 [08:41<05:49,  1.52s/it]predicting train subjects:  57%|█████▋    | 303/532 [08:42<05:43,  1.50s/it]predicting train subjects:  57%|█████▋    | 304/532 [08:44<05:38,  1.48s/it]predicting train subjects:  57%|█████▋    | 305/532 [08:46<06:18,  1.67s/it]predicting train subjects:  58%|█████▊    | 306/532 [08:48<06:46,  1.80s/it]predicting train subjects:  58%|█████▊    | 307/532 [08:50<07:14,  1.93s/it]predicting train subjects:  58%|█████▊    | 308/532 [08:52<07:17,  1.96s/it]predicting train subjects:  58%|█████▊    | 309/532 [08:54<07:24,  1.99s/it]predicting train subjects:  58%|█████▊    | 310/532 [08:56<07:30,  2.03s/it]predicting train subjects:  58%|█████▊    | 311/532 [08:59<08:24,  2.28s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:02<09:05,  2.48s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:05<09:31,  2.61s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:08<09:41,  2.67s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:11<09:50,  2.72s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:14<09:58,  2.77s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:15<08:38,  2.41s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:17<07:43,  2.17s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:18<07:05,  2.00s/it]predicting train subjects:  60%|██████    | 320/532 [09:20<06:32,  1.85s/it]predicting train subjects:  60%|██████    | 321/532 [09:21<06:08,  1.75s/it]predicting train subjects:  61%|██████    | 322/532 [09:23<05:55,  1.69s/it]predicting train subjects:  61%|██████    | 323/532 [09:25<06:33,  1.88s/it]predicting train subjects:  61%|██████    | 324/532 [09:28<06:56,  2.00s/it]predicting train subjects:  61%|██████    | 325/532 [09:30<07:08,  2.07s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:32<07:15,  2.12s/it]predicting train subjects:  61%|██████▏   | 327/532 [09:34<07:25,  2.17s/it]predicting train subjects:  62%|██████▏   | 328/532 [09:37<07:26,  2.19s/it]predicting train subjects:  62%|██████▏   | 329/532 [09:38<06:56,  2.05s/it]predicting train subjects:  62%|██████▏   | 330/532 [09:40<06:29,  1.93s/it]predicting train subjects:  62%|██████▏   | 331/532 [09:42<06:16,  1.87s/it]predicting train subjects:  62%|██████▏   | 332/532 [09:43<06:01,  1.81s/it]predicting train subjects:  63%|██████▎   | 333/532 [09:45<05:49,  1.76s/it]predicting train subjects:  63%|██████▎   | 334/532 [09:47<05:39,  1.71s/it]predicting train subjects:  63%|██████▎   | 335/532 [09:49<05:49,  1.78s/it]predicting train subjects:  63%|██████▎   | 336/532 [09:50<05:54,  1.81s/it]predicting train subjects:  63%|██████▎   | 337/532 [09:52<05:59,  1.84s/it]predicting train subjects:  64%|██████▎   | 338/532 [09:54<05:59,  1.85s/it]predicting train subjects:  64%|██████▎   | 339/532 [09:56<06:03,  1.88s/it]predicting train subjects:  64%|██████▍   | 340/532 [09:58<06:08,  1.92s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:00<05:38,  1.77s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:01<05:22,  1.69s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:03<05:06,  1.62s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:04<04:53,  1.56s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:05<04:46,  1.53s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:07<04:41,  1.51s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:09<04:48,  1.56s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:10<04:46,  1.56s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:12<04:52,  1.60s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:13<04:50,  1.60s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:15<04:50,  1.60s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:17<04:55,  1.64s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:18<04:52,  1.64s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:20<04:50,  1.63s/it]predicting train subjects:  67%|██████▋   | 355/532 [10:22<04:46,  1.62s/it]predicting train subjects:  67%|██████▋   | 356/532 [10:23<04:44,  1.62s/it]predicting train subjects:  67%|██████▋   | 357/532 [10:25<04:46,  1.64s/it]predicting train subjects:  67%|██████▋   | 358/532 [10:27<04:47,  1.65s/it]predicting train subjects:  67%|██████▋   | 359/532 [10:28<04:36,  1.60s/it]predicting train subjects:  68%|██████▊   | 360/532 [10:30<04:28,  1.56s/it]predicting train subjects:  68%|██████▊   | 361/532 [10:31<04:22,  1.54s/it]predicting train subjects:  68%|██████▊   | 362/532 [10:33<04:23,  1.55s/it]predicting train subjects:  68%|██████▊   | 363/532 [10:34<04:13,  1.50s/it]predicting train subjects:  68%|██████▊   | 364/532 [10:35<04:08,  1.48s/it]predicting train subjects:  69%|██████▊   | 365/532 [10:37<04:04,  1.47s/it]predicting train subjects:  69%|██████▉   | 366/532 [10:38<04:03,  1.47s/it]predicting train subjects:  69%|██████▉   | 367/532 [10:40<04:01,  1.47s/it]predicting train subjects:  69%|██████▉   | 368/532 [10:41<04:00,  1.47s/it]predicting train subjects:  69%|██████▉   | 369/532 [10:43<03:58,  1.46s/it]predicting train subjects:  70%|██████▉   | 370/532 [10:44<03:53,  1.44s/it]predicting train subjects:  70%|██████▉   | 371/532 [10:46<04:21,  1.63s/it]predicting train subjects:  70%|██████▉   | 372/532 [10:48<04:37,  1.73s/it]predicting train subjects:  70%|███████   | 373/532 [10:50<04:50,  1.83s/it]predicting train subjects:  70%|███████   | 374/532 [10:52<04:59,  1.89s/it]predicting train subjects:  70%|███████   | 375/532 [10:54<05:05,  1.95s/it]predicting train subjects:  71%|███████   | 376/532 [10:56<05:06,  1.96s/it]predicting train subjects:  71%|███████   | 377/532 [10:58<04:46,  1.85s/it]predicting train subjects:  71%|███████   | 378/532 [11:00<04:35,  1.79s/it]predicting train subjects:  71%|███████   | 379/532 [11:01<04:26,  1.74s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:03<04:19,  1.71s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:04<04:14,  1.69s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:06<04:11,  1.67s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:08<04:17,  1.73s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:10<04:18,  1.75s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:12<04:21,  1.78s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:13<04:20,  1.79s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:15<04:18,  1.78s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:17<04:12,  1.75s/it]predicting train subjects:  73%|███████▎  | 389/532 [11:19<04:10,  1.76s/it]predicting train subjects:  73%|███████▎  | 390/532 [11:20<04:08,  1.75s/it]predicting train subjects:  73%|███████▎  | 391/532 [11:22<04:07,  1.76s/it]predicting train subjects:  74%|███████▎  | 392/532 [11:24<04:10,  1.79s/it]predicting train subjects:  74%|███████▍  | 393/532 [11:26<04:07,  1.78s/it]predicting train subjects:  74%|███████▍  | 394/532 [11:28<04:10,  1.82s/it]predicting train subjects:  74%|███████▍  | 395/532 [11:29<04:08,  1.81s/it]predicting train subjects:  74%|███████▍  | 396/532 [11:31<04:07,  1.82s/it]predicting train subjects:  75%|███████▍  | 397/532 [11:33<04:06,  1.82s/it]predicting train subjects:  75%|███████▍  | 398/532 [11:35<04:01,  1.81s/it]predicting train subjects:  75%|███████▌  | 399/532 [11:37<03:56,  1.78s/it]predicting train subjects:  75%|███████▌  | 400/532 [11:38<03:54,  1.77s/it]predicting train subjects:  75%|███████▌  | 401/532 [11:40<03:58,  1.82s/it]predicting train subjects:  76%|███████▌  | 402/532 [11:42<04:05,  1.89s/it]predicting train subjects:  76%|███████▌  | 403/532 [11:44<04:09,  1.93s/it]predicting train subjects:  76%|███████▌  | 404/532 [11:46<04:14,  1.99s/it]predicting train subjects:  76%|███████▌  | 405/532 [11:48<04:13,  1.99s/it]predicting train subjects:  76%|███████▋  | 406/532 [11:50<04:10,  1.99s/it]predicting train subjects:  77%|███████▋  | 407/532 [11:52<03:58,  1.90s/it]predicting train subjects:  77%|███████▋  | 408/532 [11:54<03:46,  1.83s/it]predicting train subjects:  77%|███████▋  | 409/532 [11:55<03:37,  1.77s/it]predicting train subjects:  77%|███████▋  | 410/532 [11:57<03:31,  1.73s/it]predicting train subjects:  77%|███████▋  | 411/532 [11:59<03:27,  1.72s/it]predicting train subjects:  77%|███████▋  | 412/532 [12:00<03:23,  1.70s/it]predicting train subjects:  78%|███████▊  | 413/532 [12:02<03:19,  1.68s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:04<03:16,  1.66s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:05<03:09,  1.62s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:07<03:06,  1.61s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:08<03:03,  1.59s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:10<03:01,  1.59s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:12<03:07,  1.65s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:14<03:09,  1.69s/it]predicting train subjects:  79%|███████▉  | 421/532 [12:15<03:13,  1.74s/it]predicting train subjects:  79%|███████▉  | 422/532 [12:17<03:16,  1.79s/it]predicting train subjects:  80%|███████▉  | 423/532 [12:19<03:14,  1.78s/it]predicting train subjects:  80%|███████▉  | 424/532 [12:21<03:09,  1.75s/it]predicting train subjects:  80%|███████▉  | 425/532 [12:23<03:09,  1.77s/it]predicting train subjects:  80%|████████  | 426/532 [12:24<03:04,  1.74s/it]predicting train subjects:  80%|████████  | 427/532 [12:26<03:02,  1.73s/it]predicting train subjects:  80%|████████  | 428/532 [12:28<03:00,  1.73s/it]predicting train subjects:  81%|████████  | 429/532 [12:29<02:57,  1.72s/it]predicting train subjects:  81%|████████  | 430/532 [12:31<02:58,  1.75s/it]predicting train subjects:  81%|████████  | 431/532 [12:33<03:03,  1.82s/it]predicting train subjects:  81%|████████  | 432/532 [12:35<03:06,  1.86s/it]predicting train subjects:  81%|████████▏ | 433/532 [12:37<03:06,  1.89s/it]predicting train subjects:  82%|████████▏ | 434/532 [12:39<03:06,  1.90s/it]predicting train subjects:  82%|████████▏ | 435/532 [12:41<03:08,  1.94s/it]predicting train subjects:  82%|████████▏ | 436/532 [12:43<03:08,  1.96s/it]predicting train subjects:  82%|████████▏ | 437/532 [12:45<02:52,  1.82s/it]predicting train subjects:  82%|████████▏ | 438/532 [12:46<02:40,  1.71s/it]predicting train subjects:  83%|████████▎ | 439/532 [12:47<02:30,  1.62s/it]predicting train subjects:  83%|████████▎ | 440/532 [12:49<02:22,  1.55s/it]predicting train subjects:  83%|████████▎ | 441/532 [12:50<02:18,  1.52s/it]predicting train subjects:  83%|████████▎ | 442/532 [12:52<02:14,  1.50s/it]predicting train subjects:  83%|████████▎ | 443/532 [12:53<02:10,  1.47s/it]predicting train subjects:  83%|████████▎ | 444/532 [12:54<02:08,  1.46s/it]predicting train subjects:  84%|████████▎ | 445/532 [12:56<02:05,  1.44s/it]predicting train subjects:  84%|████████▍ | 446/532 [12:57<02:03,  1.43s/it]predicting train subjects:  84%|████████▍ | 447/532 [12:59<02:00,  1.42s/it]predicting train subjects:  84%|████████▍ | 448/532 [13:00<01:59,  1.42s/it]predicting train subjects:  84%|████████▍ | 449/532 [13:02<02:00,  1.45s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:03<02:00,  1.47s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:05<01:59,  1.47s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:06<01:59,  1.50s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:08<02:02,  1.55s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:09<02:03,  1.58s/it]predicting train subjects:  86%|████████▌ | 455/532 [13:11<02:06,  1.64s/it]predicting train subjects:  86%|████████▌ | 456/532 [13:13<02:06,  1.67s/it]predicting train subjects:  86%|████████▌ | 457/532 [13:15<02:07,  1.69s/it]predicting train subjects:  86%|████████▌ | 458/532 [13:16<02:05,  1.69s/it]predicting train subjects:  86%|████████▋ | 459/532 [13:18<02:05,  1.72s/it]predicting train subjects:  86%|████████▋ | 460/532 [13:20<02:05,  1.75s/it]predicting train subjects:  87%|████████▋ | 461/532 [13:22<02:11,  1.85s/it]predicting train subjects:  87%|████████▋ | 462/532 [13:24<02:14,  1.93s/it]predicting train subjects:  87%|████████▋ | 463/532 [13:26<02:16,  1.98s/it]predicting train subjects:  87%|████████▋ | 464/532 [13:28<02:17,  2.02s/it]predicting train subjects:  87%|████████▋ | 465/532 [13:31<02:15,  2.03s/it]predicting train subjects:  88%|████████▊ | 466/532 [13:33<02:14,  2.04s/it]predicting train subjects:  88%|████████▊ | 467/532 [13:34<02:06,  1.95s/it]predicting train subjects:  88%|████████▊ | 468/532 [13:36<01:59,  1.87s/it]predicting train subjects:  88%|████████▊ | 469/532 [13:38<01:52,  1.79s/it]predicting train subjects:  88%|████████▊ | 470/532 [13:39<01:47,  1.74s/it]predicting train subjects:  89%|████████▊ | 471/532 [13:41<01:45,  1.72s/it]predicting train subjects:  89%|████████▊ | 472/532 [13:43<01:41,  1.69s/it]predicting train subjects:  89%|████████▉ | 473/532 [13:44<01:43,  1.75s/it]predicting train subjects:  89%|████████▉ | 474/532 [13:46<01:44,  1.80s/it]predicting train subjects:  89%|████████▉ | 475/532 [13:48<01:43,  1.82s/it]predicting train subjects:  89%|████████▉ | 476/532 [13:50<01:42,  1.82s/it]predicting train subjects:  90%|████████▉ | 477/532 [13:52<01:41,  1.85s/it]predicting train subjects:  90%|████████▉ | 478/532 [13:54<01:41,  1.88s/it]predicting train subjects:  90%|█████████ | 479/532 [13:56<01:35,  1.80s/it]predicting train subjects:  90%|█████████ | 480/532 [13:57<01:29,  1.73s/it]predicting train subjects:  90%|█████████ | 481/532 [13:59<01:25,  1.68s/it]predicting train subjects:  91%|█████████ | 482/532 [14:00<01:21,  1.63s/it]predicting train subjects:  91%|█████████ | 483/532 [14:02<01:19,  1.63s/it]predicting train subjects:  91%|█████████ | 484/532 [14:03<01:16,  1.60s/it]predicting train subjects:  91%|█████████ | 485/532 [14:05<01:21,  1.73s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:07<01:23,  1.82s/it]predicting train subjects:  92%|█████████▏| 487/532 [14:09<01:24,  1.88s/it]predicting train subjects:  92%|█████████▏| 488/532 [14:11<01:25,  1.95s/it]predicting train subjects:  92%|█████████▏| 489/532 [14:14<01:25,  1.99s/it]predicting train subjects:  92%|█████████▏| 490/532 [14:16<01:25,  2.02s/it]predicting train subjects:  92%|█████████▏| 491/532 [14:17<01:18,  1.93s/it]predicting train subjects:  92%|█████████▏| 492/532 [14:19<01:14,  1.85s/it]predicting train subjects:  93%|█████████▎| 493/532 [14:21<01:10,  1.81s/it]predicting train subjects:  93%|█████████▎| 494/532 [14:22<01:07,  1.77s/it]predicting train subjects:  93%|█████████▎| 495/532 [14:24<01:04,  1.76s/it]predicting train subjects:  93%|█████████▎| 496/532 [14:26<01:02,  1.73s/it]predicting train subjects:  93%|█████████▎| 497/532 [14:28<01:00,  1.71s/it]predicting train subjects:  94%|█████████▎| 498/532 [14:29<00:58,  1.72s/it]predicting train subjects:  94%|█████████▍| 499/532 [14:31<00:57,  1.73s/it]predicting train subjects:  94%|█████████▍| 500/532 [14:33<00:55,  1.74s/it]predicting train subjects:  94%|█████████▍| 501/532 [14:35<00:53,  1.74s/it]predicting train subjects:  94%|█████████▍| 502/532 [14:36<00:52,  1.74s/it]predicting train subjects:  95%|█████████▍| 503/532 [14:38<00:50,  1.74s/it]predicting train subjects:  95%|█████████▍| 504/532 [14:40<00:47,  1.70s/it]predicting train subjects:  95%|█████████▍| 505/532 [14:41<00:45,  1.67s/it]predicting train subjects:  95%|█████████▌| 506/532 [14:43<00:42,  1.65s/it]predicting train subjects:  95%|█████████▌| 507/532 [14:44<00:40,  1.63s/it]predicting train subjects:  95%|█████████▌| 508/532 [14:46<00:38,  1.61s/it]predicting train subjects:  96%|█████████▌| 509/532 [14:48<00:39,  1.72s/it]predicting train subjects:  96%|█████████▌| 510/532 [14:50<00:39,  1.80s/it]predicting train subjects:  96%|█████████▌| 511/532 [14:52<00:38,  1.83s/it]predicting train subjects:  96%|█████████▌| 512/532 [14:54<00:36,  1.83s/it]predicting train subjects:  96%|█████████▋| 513/532 [14:55<00:35,  1.85s/it]predicting train subjects:  97%|█████████▋| 514/532 [14:57<00:33,  1.84s/it]predicting train subjects:  97%|█████████▋| 515/532 [14:59<00:30,  1.77s/it]predicting train subjects:  97%|█████████▋| 516/532 [15:01<00:27,  1.72s/it]predicting train subjects:  97%|█████████▋| 517/532 [15:02<00:25,  1.69s/it]predicting train subjects:  97%|█████████▋| 518/532 [15:04<00:23,  1.66s/it]predicting train subjects:  98%|█████████▊| 519/532 [15:05<00:21,  1.66s/it]predicting train subjects:  98%|█████████▊| 520/532 [15:07<00:19,  1.66s/it]predicting train subjects:  98%|█████████▊| 521/532 [15:09<00:18,  1.71s/it]predicting train subjects:  98%|█████████▊| 522/532 [15:11<00:17,  1.74s/it]predicting train subjects:  98%|█████████▊| 523/532 [15:12<00:15,  1.74s/it]predicting train subjects:  98%|█████████▊| 524/532 [15:14<00:14,  1.75s/it]predicting train subjects:  99%|█████████▊| 525/532 [15:16<00:12,  1.77s/it]predicting train subjects:  99%|█████████▉| 526/532 [15:18<00:10,  1.78s/it]predicting train subjects:  99%|█████████▉| 527/532 [15:19<00:08,  1.72s/it]predicting train subjects:  99%|█████████▉| 528/532 [15:21<00:06,  1.68s/it]predicting train subjects:  99%|█████████▉| 529/532 [15:23<00:04,  1.66s/it]predicting train subjects: 100%|█████████▉| 530/532 [15:24<00:03,  1.68s/it]predicting train subjects: 100%|█████████▉| 531/532 [15:26<00:01,  1.65s/it]predicting train subjects: 100%|██████████| 532/532 [15:28<00:00,  1.63s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists

Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<12:22,  1.40s/it]Loading train:   0%|          | 2/532 [00:02<11:40,  1.32s/it]Loading train:   1%|          | 3/532 [00:03<10:49,  1.23s/it]Loading train:   1%|          | 4/532 [00:04<10:12,  1.16s/it]Loading train:   1%|          | 5/532 [00:05<09:45,  1.11s/it]Loading train:   1%|          | 6/532 [00:06<09:16,  1.06s/it]Loading train:   1%|▏         | 7/532 [00:07<08:44,  1.00it/s]Loading train:   2%|▏         | 8/532 [00:08<08:27,  1.03it/s]Loading train:   2%|▏         | 9/532 [00:09<08:50,  1.01s/it]Loading train:   2%|▏         | 10/532 [00:10<08:15,  1.05it/s]Loading train:   2%|▏         | 11/532 [00:10<07:50,  1.11it/s]Loading train:   2%|▏         | 12/532 [00:12<08:46,  1.01s/it]Loading train:   2%|▏         | 13/532 [00:13<08:18,  1.04it/s]Loading train:   3%|▎         | 14/532 [00:13<07:47,  1.11it/s]Loading train:   3%|▎         | 15/532 [00:14<07:44,  1.11it/s]Loading train:   3%|▎         | 16/532 [00:15<07:49,  1.10it/s]Loading train:   3%|▎         | 17/532 [00:16<07:37,  1.12it/s]Loading train:   3%|▎         | 18/532 [00:17<08:05,  1.06it/s]Loading train:   4%|▎         | 19/532 [00:18<07:44,  1.10it/s]Loading train:   4%|▍         | 20/532 [00:19<07:44,  1.10it/s]Loading train:   4%|▍         | 21/532 [00:20<08:24,  1.01it/s]Loading train:   4%|▍         | 22/532 [00:21<08:01,  1.06it/s]Loading train:   4%|▍         | 23/532 [00:22<08:13,  1.03it/s]Loading train:   5%|▍         | 24/532 [00:23<07:47,  1.09it/s]Loading train:   5%|▍         | 25/532 [00:24<08:23,  1.01it/s]Loading train:   5%|▍         | 26/532 [00:25<07:45,  1.09it/s]Loading train:   5%|▌         | 27/532 [00:26<08:12,  1.02it/s]Loading train:   5%|▌         | 28/532 [00:26<07:46,  1.08it/s]Loading train:   5%|▌         | 29/532 [00:27<07:53,  1.06it/s]Loading train:   6%|▌         | 30/532 [00:28<07:30,  1.11it/s]Loading train:   6%|▌         | 31/532 [00:29<07:10,  1.16it/s]Loading train:   6%|▌         | 32/532 [00:30<07:00,  1.19it/s]Loading train:   6%|▌         | 33/532 [00:31<07:00,  1.19it/s]Loading train:   6%|▋         | 34/532 [00:32<07:35,  1.09it/s]Loading train:   7%|▋         | 35/532 [00:33<07:51,  1.05it/s]Loading train:   7%|▋         | 36/532 [00:34<07:59,  1.03it/s]Loading train:   7%|▋         | 37/532 [00:35<07:49,  1.05it/s]Loading train:   7%|▋         | 38/532 [00:36<08:02,  1.02it/s]Loading train:   7%|▋         | 39/532 [00:37<07:52,  1.04it/s]Loading train:   8%|▊         | 40/532 [00:38<07:38,  1.07it/s]Loading train:   8%|▊         | 41/532 [00:39<08:03,  1.01it/s]Loading train:   8%|▊         | 42/532 [00:40<08:08,  1.00it/s]Loading train:   8%|▊         | 43/532 [00:40<07:41,  1.06it/s]Loading train:   8%|▊         | 44/532 [00:41<07:20,  1.11it/s]Loading train:   8%|▊         | 45/532 [00:42<07:12,  1.13it/s]Loading train:   9%|▊         | 46/532 [00:43<07:37,  1.06it/s]Loading train:   9%|▉         | 47/532 [00:44<08:12,  1.02s/it]Loading train:   9%|▉         | 48/532 [00:45<08:21,  1.04s/it]Loading train:   9%|▉         | 49/532 [00:46<07:54,  1.02it/s]Loading train:   9%|▉         | 50/532 [00:47<08:19,  1.04s/it]Loading train:  10%|▉         | 51/532 [00:48<07:57,  1.01it/s]Loading train:  10%|▉         | 52/532 [00:49<07:45,  1.03it/s]Loading train:  10%|▉         | 53/532 [00:50<07:22,  1.08it/s]Loading train:  10%|█         | 54/532 [00:51<07:41,  1.04it/s]Loading train:  10%|█         | 55/532 [00:52<07:39,  1.04it/s]Loading train:  11%|█         | 56/532 [00:53<07:32,  1.05it/s]Loading train:  11%|█         | 57/532 [00:54<07:21,  1.08it/s]Loading train:  11%|█         | 58/532 [00:55<07:29,  1.05it/s]Loading train:  11%|█         | 59/532 [00:56<07:51,  1.00it/s]Loading train:  11%|█▏        | 60/532 [00:57<07:12,  1.09it/s]Loading train:  11%|█▏        | 61/532 [00:58<07:21,  1.07it/s]Loading train:  12%|█▏        | 62/532 [00:59<07:41,  1.02it/s]Loading train:  12%|█▏        | 63/532 [01:00<08:04,  1.03s/it]Loading train:  12%|█▏        | 64/532 [01:01<07:33,  1.03it/s]Loading train:  12%|█▏        | 65/532 [01:02<07:32,  1.03it/s]Loading train:  12%|█▏        | 66/532 [01:03<07:58,  1.03s/it]Loading train:  13%|█▎        | 67/532 [01:04<08:12,  1.06s/it]Loading train:  13%|█▎        | 68/532 [01:05<08:11,  1.06s/it]Loading train:  13%|█▎        | 69/532 [01:06<07:42,  1.00it/s]Loading train:  13%|█▎        | 70/532 [01:07<07:35,  1.01it/s]Loading train:  13%|█▎        | 71/532 [01:08<07:06,  1.08it/s]Loading train:  14%|█▎        | 72/532 [01:09<06:44,  1.14it/s]Loading train:  14%|█▎        | 73/532 [01:09<06:42,  1.14it/s]Loading train:  14%|█▍        | 74/532 [01:10<07:14,  1.05it/s]Loading train:  14%|█▍        | 75/532 [01:12<08:29,  1.12s/it]Loading train:  14%|█▍        | 76/532 [01:13<07:56,  1.05s/it]Loading train:  14%|█▍        | 77/532 [01:14<07:59,  1.05s/it]Loading train:  15%|█▍        | 78/532 [01:15<07:41,  1.02s/it]Loading train:  15%|█▍        | 79/532 [01:16<07:28,  1.01it/s]Loading train:  15%|█▌        | 80/532 [01:17<07:15,  1.04it/s]Loading train:  15%|█▌        | 81/532 [01:18<07:03,  1.06it/s]Loading train:  15%|█▌        | 82/532 [01:19<07:03,  1.06it/s]Loading train:  16%|█▌        | 83/532 [01:20<07:08,  1.05it/s]Loading train:  16%|█▌        | 84/532 [01:20<06:42,  1.11it/s]Loading train:  16%|█▌        | 85/532 [01:21<06:26,  1.16it/s]Loading train:  16%|█▌        | 86/532 [01:22<06:19,  1.17it/s]Loading train:  16%|█▋        | 87/532 [01:23<06:18,  1.17it/s]Loading train:  17%|█▋        | 88/532 [01:24<06:21,  1.16it/s]Loading train:  17%|█▋        | 89/532 [01:25<06:40,  1.11it/s]Loading train:  17%|█▋        | 90/532 [01:25<06:34,  1.12it/s]Loading train:  17%|█▋        | 91/532 [01:26<06:21,  1.16it/s]Loading train:  17%|█▋        | 92/532 [01:27<06:18,  1.16it/s]Loading train:  17%|█▋        | 93/532 [01:28<06:19,  1.16it/s]Loading train:  18%|█▊        | 94/532 [01:29<06:20,  1.15it/s]Loading train:  18%|█▊        | 95/532 [01:30<06:41,  1.09it/s]Loading train:  18%|█▊        | 96/532 [01:31<07:00,  1.04it/s]Loading train:  18%|█▊        | 97/532 [01:32<07:11,  1.01it/s]Loading train:  18%|█▊        | 98/532 [01:33<07:11,  1.01it/s]Loading train:  19%|█▊        | 99/532 [01:34<07:11,  1.00it/s]Loading train:  19%|█▉        | 100/532 [01:35<07:17,  1.01s/it]Loading train:  19%|█▉        | 101/532 [01:36<07:13,  1.01s/it]Loading train:  19%|█▉        | 102/532 [01:37<06:51,  1.05it/s]Loading train:  19%|█▉        | 103/532 [01:38<06:27,  1.11it/s]Loading train:  20%|█▉        | 104/532 [01:38<06:06,  1.17it/s]Loading train:  20%|█▉        | 105/532 [01:39<05:51,  1.21it/s]Loading train:  20%|█▉        | 106/532 [01:40<05:49,  1.22it/s]Loading train:  20%|██        | 107/532 [01:41<05:41,  1.24it/s]Loading train:  20%|██        | 108/532 [01:42<05:29,  1.29it/s]Loading train:  20%|██        | 109/532 [01:42<05:13,  1.35it/s]Loading train:  21%|██        | 110/532 [01:43<05:02,  1.40it/s]Loading train:  21%|██        | 111/532 [01:44<05:04,  1.38it/s]Loading train:  21%|██        | 112/532 [01:44<05:08,  1.36it/s]Loading train:  21%|██        | 113/532 [01:45<05:38,  1.24it/s]Loading train:  21%|██▏       | 114/532 [01:46<05:49,  1.20it/s]Loading train:  22%|██▏       | 115/532 [01:47<05:46,  1.20it/s]Loading train:  22%|██▏       | 116/532 [01:48<05:48,  1.19it/s]Loading train:  22%|██▏       | 117/532 [01:49<05:50,  1.19it/s]Loading train:  22%|██▏       | 118/532 [01:50<06:03,  1.14it/s]Loading train:  22%|██▏       | 119/532 [01:51<06:43,  1.02it/s]Loading train:  23%|██▎       | 120/532 [01:52<06:40,  1.03it/s]Loading train:  23%|██▎       | 121/532 [01:53<06:36,  1.04it/s]Loading train:  23%|██▎       | 122/532 [01:54<06:35,  1.04it/s]Loading train:  23%|██▎       | 123/532 [01:55<06:30,  1.05it/s]Loading train:  23%|██▎       | 124/532 [01:56<06:31,  1.04it/s]Loading train:  23%|██▎       | 125/532 [01:57<06:40,  1.02it/s]Loading train:  24%|██▎       | 126/532 [01:58<06:30,  1.04it/s]Loading train:  24%|██▍       | 127/532 [01:59<06:20,  1.06it/s]Loading train:  24%|██▍       | 128/532 [01:59<06:17,  1.07it/s]Loading train:  24%|██▍       | 129/532 [02:00<06:11,  1.08it/s]Loading train:  24%|██▍       | 130/532 [02:01<06:10,  1.08it/s]Loading train:  25%|██▍       | 131/532 [02:02<06:34,  1.02it/s]Loading train:  25%|██▍       | 132/532 [02:04<06:50,  1.03s/it]Loading train:  25%|██▌       | 133/532 [02:05<06:56,  1.04s/it]Loading train:  25%|██▌       | 134/532 [02:06<07:05,  1.07s/it]Loading train:  25%|██▌       | 135/532 [02:07<07:08,  1.08s/it]Loading train:  26%|██▌       | 136/532 [02:08<07:17,  1.10s/it]Loading train:  26%|██▌       | 137/532 [02:09<07:24,  1.12s/it]Loading train:  26%|██▌       | 138/532 [02:10<07:24,  1.13s/it]Loading train:  26%|██▌       | 139/532 [02:11<07:25,  1.13s/it]Loading train:  26%|██▋       | 140/532 [02:13<07:30,  1.15s/it]Loading train:  27%|██▋       | 141/532 [02:14<07:33,  1.16s/it]Loading train:  27%|██▋       | 142/532 [02:15<07:34,  1.17s/it]Loading train:  27%|██▋       | 143/532 [02:16<07:06,  1.10s/it]Loading train:  27%|██▋       | 144/532 [02:17<06:27,  1.00it/s]Loading train:  27%|██▋       | 145/532 [02:17<05:55,  1.09it/s]Loading train:  27%|██▋       | 146/532 [02:18<05:52,  1.10it/s]Loading train:  28%|██▊       | 147/532 [02:19<05:44,  1.12it/s]Loading train:  28%|██▊       | 148/532 [02:20<05:37,  1.14it/s]Loading train:  28%|██▊       | 149/532 [02:21<05:31,  1.16it/s]Loading train:  28%|██▊       | 150/532 [02:22<05:40,  1.12it/s]Loading train:  28%|██▊       | 151/532 [02:23<05:38,  1.13it/s]Loading train:  29%|██▊       | 152/532 [02:24<05:30,  1.15it/s]Loading train:  29%|██▉       | 153/532 [02:24<05:20,  1.18it/s]Loading train:  29%|██▉       | 154/532 [02:25<05:16,  1.20it/s]Loading train:  29%|██▉       | 155/532 [02:26<05:57,  1.05it/s]Loading train:  29%|██▉       | 156/532 [02:28<06:23,  1.02s/it]Loading train:  30%|██▉       | 157/532 [02:29<06:36,  1.06s/it]Loading train:  30%|██▉       | 158/532 [02:30<06:40,  1.07s/it]Loading train:  30%|██▉       | 159/532 [02:31<06:52,  1.11s/it]Loading train:  30%|███       | 160/532 [02:32<07:03,  1.14s/it]Loading train:  30%|███       | 161/532 [02:33<06:48,  1.10s/it]Loading train:  30%|███       | 162/532 [02:34<06:21,  1.03s/it]Loading train:  31%|███       | 163/532 [02:35<05:48,  1.06it/s]Loading train:  31%|███       | 164/532 [02:36<05:46,  1.06it/s]Loading train:  31%|███       | 165/532 [02:37<05:36,  1.09it/s]Loading train:  31%|███       | 166/532 [02:37<05:25,  1.13it/s]Loading train:  31%|███▏      | 167/532 [02:38<05:46,  1.05it/s]Loading train:  32%|███▏      | 168/532 [02:39<05:35,  1.09it/s]Loading train:  32%|███▏      | 169/532 [02:40<05:32,  1.09it/s]Loading train:  32%|███▏      | 170/532 [02:41<05:28,  1.10it/s]Loading train:  32%|███▏      | 171/532 [02:42<05:40,  1.06it/s]Loading train:  32%|███▏      | 172/532 [02:43<05:40,  1.06it/s]Loading train:  33%|███▎      | 173/532 [02:44<05:35,  1.07it/s]Loading train:  33%|███▎      | 174/532 [02:45<05:23,  1.11it/s]Loading train:  33%|███▎      | 175/532 [02:46<05:13,  1.14it/s]Loading train:  33%|███▎      | 176/532 [02:46<05:01,  1.18it/s]Loading train:  33%|███▎      | 177/532 [02:47<04:57,  1.19it/s]Loading train:  33%|███▎      | 178/532 [02:48<04:50,  1.22it/s]Loading train:  34%|███▎      | 179/532 [02:49<05:13,  1.13it/s]Loading train:  34%|███▍      | 180/532 [02:50<05:04,  1.16it/s]Loading train:  34%|███▍      | 181/532 [02:51<05:01,  1.16it/s]Loading train:  34%|███▍      | 182/532 [02:52<04:53,  1.19it/s]Loading train:  34%|███▍      | 183/532 [02:52<04:50,  1.20it/s]Loading train:  35%|███▍      | 184/532 [02:53<04:49,  1.20it/s]Loading train:  35%|███▍      | 185/532 [02:54<05:01,  1.15it/s]Loading train:  35%|███▍      | 186/532 [02:55<05:08,  1.12it/s]Loading train:  35%|███▌      | 187/532 [02:56<05:02,  1.14it/s]Loading train:  35%|███▌      | 188/532 [02:57<04:55,  1.16it/s]Loading train:  36%|███▌      | 189/532 [02:58<04:47,  1.19it/s]Loading train:  36%|███▌      | 190/532 [02:58<04:49,  1.18it/s]Loading train:  36%|███▌      | 191/532 [03:00<05:18,  1.07it/s]Loading train:  36%|███▌      | 192/532 [03:01<05:31,  1.02it/s]Loading train:  36%|███▋      | 193/532 [03:02<05:47,  1.03s/it]Loading train:  36%|███▋      | 194/532 [03:03<05:53,  1.04s/it]Loading train:  37%|███▋      | 195/532 [03:04<05:55,  1.06s/it]Loading train:  37%|███▋      | 196/532 [03:05<06:01,  1.08s/it]Loading train:  37%|███▋      | 197/532 [03:06<06:13,  1.12s/it]Loading train:  37%|███▋      | 198/532 [03:07<05:59,  1.08s/it]Loading train:  37%|███▋      | 199/532 [03:08<05:50,  1.05s/it]Loading train:  38%|███▊      | 200/532 [03:09<05:43,  1.03s/it]Loading train:  38%|███▊      | 201/532 [03:10<05:35,  1.01s/it]Loading train:  38%|███▊      | 202/532 [03:11<05:36,  1.02s/it]Loading train:  38%|███▊      | 203/532 [03:12<05:43,  1.04s/it]Loading train:  38%|███▊      | 204/532 [03:13<05:27,  1.00it/s]Loading train:  39%|███▊      | 205/532 [03:14<05:10,  1.05it/s]Loading train:  39%|███▊      | 206/532 [03:15<04:54,  1.11it/s]Loading train:  39%|███▉      | 207/532 [03:16<04:46,  1.13it/s]Loading train:  39%|███▉      | 208/532 [03:16<04:35,  1.17it/s]Loading train:  39%|███▉      | 209/532 [03:17<04:30,  1.19it/s]Loading train:  39%|███▉      | 210/532 [03:18<04:22,  1.23it/s]Loading train:  40%|███▉      | 211/532 [03:19<04:16,  1.25it/s]Loading train:  40%|███▉      | 212/532 [03:20<04:22,  1.22it/s]Loading train:  40%|████      | 213/532 [03:20<04:16,  1.24it/s]Loading train:  40%|████      | 214/532 [03:21<04:12,  1.26it/s]Loading train:  40%|████      | 215/532 [03:23<05:02,  1.05it/s]Loading train:  41%|████      | 216/532 [03:24<05:14,  1.01it/s]Loading train:  41%|████      | 217/532 [03:25<05:29,  1.05s/it]Loading train:  41%|████      | 218/532 [03:26<05:37,  1.08s/it]Loading train:  41%|████      | 219/532 [03:27<05:36,  1.07s/it]Loading train:  41%|████▏     | 220/532 [03:28<05:38,  1.08s/it]Loading train:  42%|████▏     | 221/532 [03:29<05:11,  1.00s/it]Loading train:  42%|████▏     | 222/532 [03:30<04:47,  1.08it/s]Loading train:  42%|████▏     | 223/532 [03:30<04:27,  1.15it/s]Loading train:  42%|████▏     | 224/532 [03:31<04:13,  1.21it/s]Loading train:  42%|████▏     | 225/532 [03:32<04:04,  1.25it/s]Loading train:  42%|████▏     | 226/532 [03:33<03:51,  1.32it/s]Loading train:  43%|████▎     | 227/532 [03:33<03:45,  1.35it/s]Loading train:  43%|████▎     | 228/532 [03:34<03:37,  1.40it/s]Loading train:  43%|████▎     | 229/532 [03:35<03:38,  1.39it/s]Loading train:  43%|████▎     | 230/532 [03:35<03:34,  1.41it/s]Loading train:  43%|████▎     | 231/532 [03:36<03:31,  1.42it/s]Loading train:  44%|████▎     | 232/532 [03:37<03:28,  1.44it/s]Loading train:  44%|████▍     | 233/532 [03:38<03:41,  1.35it/s]Loading train:  44%|████▍     | 234/532 [03:38<03:41,  1.35it/s]Loading train:  44%|████▍     | 235/532 [03:39<03:39,  1.35it/s]Loading train:  44%|████▍     | 236/532 [03:40<03:37,  1.36it/s]Loading train:  45%|████▍     | 237/532 [03:40<03:40,  1.34it/s]Loading train:  45%|████▍     | 238/532 [03:41<03:53,  1.26it/s]Loading train:  45%|████▍     | 239/532 [03:42<04:06,  1.19it/s]Loading train:  45%|████▌     | 240/532 [03:43<04:01,  1.21it/s]Loading train:  45%|████▌     | 241/532 [03:44<04:02,  1.20it/s]Loading train:  45%|████▌     | 242/532 [03:45<03:55,  1.23it/s]Loading train:  46%|████▌     | 243/532 [03:46<03:55,  1.23it/s]Loading train:  46%|████▌     | 244/532 [03:46<03:56,  1.22it/s]Loading train:  46%|████▌     | 245/532 [03:47<03:46,  1.27it/s]Loading train:  46%|████▌     | 246/532 [03:48<03:36,  1.32it/s]Loading train:  46%|████▋     | 247/532 [03:48<03:30,  1.35it/s]Loading train:  47%|████▋     | 248/532 [03:49<03:25,  1.38it/s]Loading train:  47%|████▋     | 249/532 [03:50<03:22,  1.40it/s]Loading train:  47%|████▋     | 250/532 [03:51<03:16,  1.43it/s]Loading train:  47%|████▋     | 251/532 [03:51<03:26,  1.36it/s]Loading train:  47%|████▋     | 252/532 [03:52<03:23,  1.37it/s]Loading train:  48%|████▊     | 253/532 [03:53<03:27,  1.35it/s]Loading train:  48%|████▊     | 254/532 [03:54<03:29,  1.32it/s]Loading train:  48%|████▊     | 255/532 [03:55<03:44,  1.24it/s]Loading train:  48%|████▊     | 256/532 [03:55<03:42,  1.24it/s]Loading train:  48%|████▊     | 257/532 [03:56<03:58,  1.15it/s]Loading train:  48%|████▊     | 258/532 [03:57<04:07,  1.11it/s]Loading train:  49%|████▊     | 259/532 [03:58<04:15,  1.07it/s]Loading train:  49%|████▉     | 260/532 [03:59<04:17,  1.06it/s]Loading train:  49%|████▉     | 261/532 [04:00<04:16,  1.06it/s]Loading train:  49%|████▉     | 262/532 [04:01<04:18,  1.04it/s]Loading train:  49%|████▉     | 263/532 [04:02<03:58,  1.13it/s]Loading train:  50%|████▉     | 264/532 [04:03<03:44,  1.19it/s]Loading train:  50%|████▉     | 265/532 [04:03<03:31,  1.26it/s]Loading train:  50%|█████     | 266/532 [04:04<03:24,  1.30it/s]Loading train:  50%|█████     | 267/532 [04:05<03:18,  1.33it/s]Loading train:  50%|█████     | 268/532 [04:06<03:15,  1.35it/s]Loading train:  51%|█████     | 269/532 [04:07<03:31,  1.24it/s]Loading train:  51%|█████     | 270/532 [04:07<03:33,  1.23it/s]Loading train:  51%|█████     | 271/532 [04:08<03:44,  1.16it/s]Loading train:  51%|█████     | 272/532 [04:09<03:46,  1.15it/s]Loading train:  51%|█████▏    | 273/532 [04:10<03:46,  1.14it/s]Loading train:  52%|█████▏    | 274/532 [04:11<03:47,  1.13it/s]Loading train:  52%|█████▏    | 275/532 [04:12<04:09,  1.03it/s]Loading train:  52%|█████▏    | 276/532 [04:13<04:16,  1.00s/it]Loading train:  52%|█████▏    | 277/532 [04:14<04:21,  1.02s/it]Loading train:  52%|█████▏    | 278/532 [04:16<04:39,  1.10s/it]Loading train:  52%|█████▏    | 279/532 [04:17<04:32,  1.08s/it]Loading train:  53%|█████▎    | 280/532 [04:18<04:30,  1.08s/it]Loading train:  53%|█████▎    | 281/532 [04:19<04:41,  1.12s/it]Loading train:  53%|█████▎    | 282/532 [04:20<04:31,  1.09s/it]Loading train:  53%|█████▎    | 283/532 [04:21<04:24,  1.06s/it]Loading train:  53%|█████▎    | 284/532 [04:22<04:17,  1.04s/it]Loading train:  54%|█████▎    | 285/532 [04:23<04:10,  1.01s/it]Loading train:  54%|█████▍    | 286/532 [04:24<04:14,  1.03s/it]Loading train:  54%|█████▍    | 287/532 [04:25<03:59,  1.02it/s]Loading train:  54%|█████▍    | 288/532 [04:26<03:49,  1.07it/s]Loading train:  54%|█████▍    | 289/532 [04:26<03:34,  1.13it/s]Loading train:  55%|█████▍    | 290/532 [04:27<03:27,  1.17it/s]Loading train:  55%|█████▍    | 291/532 [04:28<03:18,  1.21it/s]Loading train:  55%|█████▍    | 292/532 [04:29<03:11,  1.25it/s]Loading train:  55%|█████▌    | 293/532 [04:30<03:20,  1.19it/s]Loading train:  55%|█████▌    | 294/532 [04:30<03:20,  1.19it/s]Loading train:  55%|█████▌    | 295/532 [04:31<03:24,  1.16it/s]Loading train:  56%|█████▌    | 296/532 [04:32<03:25,  1.15it/s]Loading train:  56%|█████▌    | 297/532 [04:33<03:24,  1.15it/s]Loading train:  56%|█████▌    | 298/532 [04:34<03:28,  1.12it/s]Loading train:  56%|█████▌    | 299/532 [04:35<03:23,  1.14it/s]Loading train:  56%|█████▋    | 300/532 [04:36<03:15,  1.19it/s]Loading train:  57%|█████▋    | 301/532 [04:36<03:08,  1.23it/s]Loading train:  57%|█████▋    | 302/532 [04:37<03:04,  1.24it/s]Loading train:  57%|█████▋    | 303/532 [04:38<03:00,  1.27it/s]Loading train:  57%|█████▋    | 304/532 [04:39<02:57,  1.29it/s]Loading train:  57%|█████▋    | 305/532 [04:40<03:21,  1.13it/s]Loading train:  58%|█████▊    | 306/532 [04:41<03:33,  1.06it/s]Loading train:  58%|█████▊    | 307/532 [04:42<03:49,  1.02s/it]Loading train:  58%|█████▊    | 308/532 [04:43<03:51,  1.03s/it]Loading train:  58%|█████▊    | 309/532 [04:44<03:53,  1.05s/it]Loading train:  58%|█████▊    | 310/532 [04:45<03:54,  1.06s/it]Loading train:  58%|█████▊    | 311/532 [04:47<04:21,  1.18s/it]Loading train:  59%|█████▊    | 312/532 [04:48<04:33,  1.24s/it]Loading train:  59%|█████▉    | 313/532 [04:50<04:44,  1.30s/it]Loading train:  59%|█████▉    | 314/532 [04:51<04:48,  1.32s/it]Loading train:  59%|█████▉    | 315/532 [04:52<04:52,  1.35s/it]Loading train:  59%|█████▉    | 316/532 [04:54<04:54,  1.36s/it]Loading train:  60%|█████▉    | 317/532 [04:55<04:25,  1.23s/it]Loading train:  60%|█████▉    | 318/532 [04:56<03:56,  1.10s/it]Loading train:  60%|█████▉    | 319/532 [04:56<03:39,  1.03s/it]Loading train:  60%|██████    | 320/532 [04:57<03:23,  1.04it/s]Loading train:  60%|██████    | 321/532 [04:58<03:13,  1.09it/s]Loading train:  61%|██████    | 322/532 [04:59<03:02,  1.15it/s]Loading train:  61%|██████    | 323/532 [05:00<03:24,  1.02it/s]Loading train:  61%|██████    | 324/532 [05:01<03:34,  1.03s/it]Loading train:  61%|██████    | 325/532 [05:02<03:42,  1.07s/it]Loading train:  61%|██████▏   | 326/532 [05:03<03:42,  1.08s/it]Loading train:  61%|██████▏   | 327/532 [05:05<03:42,  1.09s/it]Loading train:  62%|██████▏   | 328/532 [05:06<03:44,  1.10s/it]Loading train:  62%|██████▏   | 329/532 [05:07<03:31,  1.04s/it]Loading train:  62%|██████▏   | 330/532 [05:07<03:14,  1.04it/s]Loading train:  62%|██████▏   | 331/532 [05:08<03:00,  1.11it/s]Loading train:  62%|██████▏   | 332/532 [05:09<02:50,  1.18it/s]Loading train:  63%|██████▎   | 333/532 [05:10<02:44,  1.21it/s]Loading train:  63%|██████▎   | 334/532 [05:10<02:47,  1.19it/s]Loading train:  63%|██████▎   | 335/532 [05:12<02:57,  1.11it/s]Loading train:  63%|██████▎   | 336/532 [05:12<02:56,  1.11it/s]Loading train:  63%|██████▎   | 337/532 [05:13<02:53,  1.13it/s]Loading train:  64%|██████▎   | 338/532 [05:14<02:58,  1.09it/s]Loading train:  64%|██████▎   | 339/532 [05:15<03:02,  1.06it/s]Loading train:  64%|██████▍   | 340/532 [05:16<03:09,  1.01it/s]Loading train:  64%|██████▍   | 341/532 [05:17<02:57,  1.07it/s]Loading train:  64%|██████▍   | 342/532 [05:18<02:49,  1.12it/s]Loading train:  64%|██████▍   | 343/532 [05:19<02:44,  1.15it/s]Loading train:  65%|██████▍   | 344/532 [05:20<02:38,  1.18it/s]Loading train:  65%|██████▍   | 345/532 [05:20<02:31,  1.24it/s]Loading train:  65%|██████▌   | 346/532 [05:21<02:28,  1.26it/s]Loading train:  65%|██████▌   | 347/532 [05:22<02:39,  1.16it/s]Loading train:  65%|██████▌   | 348/532 [05:23<02:39,  1.15it/s]Loading train:  66%|██████▌   | 349/532 [05:24<02:37,  1.17it/s]Loading train:  66%|██████▌   | 350/532 [05:25<02:41,  1.13it/s]Loading train:  66%|██████▌   | 351/532 [05:26<02:40,  1.13it/s]Loading train:  66%|██████▌   | 352/532 [05:26<02:34,  1.17it/s]Loading train:  66%|██████▋   | 353/532 [05:27<02:40,  1.12it/s]Loading train:  67%|██████▋   | 354/532 [05:28<02:36,  1.14it/s]Loading train:  67%|██████▋   | 355/532 [05:29<02:30,  1.18it/s]Loading train:  67%|██████▋   | 356/532 [05:30<02:25,  1.21it/s]Loading train:  67%|██████▋   | 357/532 [05:31<02:22,  1.23it/s]Loading train:  67%|██████▋   | 358/532 [05:31<02:19,  1.25it/s]Loading train:  67%|██████▋   | 359/532 [05:32<02:19,  1.24it/s]Loading train:  68%|██████▊   | 360/532 [05:33<02:14,  1.28it/s]Loading train:  68%|██████▊   | 361/532 [05:34<02:10,  1.31it/s]Loading train:  68%|██████▊   | 362/532 [05:34<02:13,  1.27it/s]Loading train:  68%|██████▊   | 363/532 [05:35<02:11,  1.28it/s]Loading train:  68%|██████▊   | 364/532 [05:36<02:16,  1.23it/s]Loading train:  69%|██████▊   | 365/532 [05:37<02:14,  1.24it/s]Loading train:  69%|██████▉   | 366/532 [05:38<02:14,  1.24it/s]Loading train:  69%|██████▉   | 367/532 [05:38<02:06,  1.30it/s]Loading train:  69%|██████▉   | 368/532 [05:39<02:09,  1.27it/s]Loading train:  69%|██████▉   | 369/532 [05:40<02:09,  1.26it/s]Loading train:  70%|██████▉   | 370/532 [05:41<02:05,  1.29it/s]Loading train:  70%|██████▉   | 371/532 [05:42<02:20,  1.15it/s]Loading train:  70%|██████▉   | 372/532 [05:43<02:26,  1.09it/s]Loading train:  70%|███████   | 373/532 [05:44<02:33,  1.04it/s]Loading train:  70%|███████   | 374/532 [05:45<02:38,  1.00s/it]Loading train:  70%|███████   | 375/532 [05:46<02:41,  1.03s/it]Loading train:  71%|███████   | 376/532 [05:47<02:39,  1.03s/it]Loading train:  71%|███████   | 377/532 [05:48<02:29,  1.04it/s]Loading train:  71%|███████   | 378/532 [05:49<02:26,  1.05it/s]Loading train:  71%|███████   | 379/532 [05:50<02:24,  1.06it/s]Loading train:  71%|███████▏  | 380/532 [05:51<02:22,  1.07it/s]Loading train:  72%|███████▏  | 381/532 [05:52<02:17,  1.10it/s]Loading train:  72%|███████▏  | 382/532 [05:52<02:15,  1.11it/s]Loading train:  72%|███████▏  | 383/532 [05:53<02:16,  1.10it/s]Loading train:  72%|███████▏  | 384/532 [05:54<02:14,  1.10it/s]Loading train:  72%|███████▏  | 385/532 [05:55<02:11,  1.12it/s]Loading train:  73%|███████▎  | 386/532 [05:56<02:09,  1.13it/s]Loading train:  73%|███████▎  | 387/532 [05:57<02:09,  1.12it/s]Loading train:  73%|███████▎  | 388/532 [05:58<02:06,  1.14it/s]Loading train:  73%|███████▎  | 389/532 [05:59<02:07,  1.12it/s]Loading train:  73%|███████▎  | 390/532 [06:00<02:09,  1.10it/s]Loading train:  73%|███████▎  | 391/532 [06:01<02:15,  1.04it/s]Loading train:  74%|███████▎  | 392/532 [06:02<02:10,  1.07it/s]Loading train:  74%|███████▍  | 393/532 [06:03<02:10,  1.07it/s]Loading train:  74%|███████▍  | 394/532 [06:04<02:10,  1.06it/s]Loading train:  74%|███████▍  | 395/532 [06:05<02:11,  1.04it/s]Loading train:  74%|███████▍  | 396/532 [06:05<02:08,  1.05it/s]Loading train:  75%|███████▍  | 397/532 [06:06<02:07,  1.06it/s]Loading train:  75%|███████▍  | 398/532 [06:07<02:02,  1.09it/s]Loading train:  75%|███████▌  | 399/532 [06:08<02:03,  1.08it/s]Loading train:  75%|███████▌  | 400/532 [06:09<02:02,  1.08it/s]Loading train:  75%|███████▌  | 401/532 [06:10<02:05,  1.05it/s]Loading train:  76%|███████▌  | 402/532 [06:11<02:06,  1.03it/s]Loading train:  76%|███████▌  | 403/532 [06:12<02:07,  1.01it/s]Loading train:  76%|███████▌  | 404/532 [06:13<02:04,  1.03it/s]Loading train:  76%|███████▌  | 405/532 [06:14<02:05,  1.01it/s]Loading train:  76%|███████▋  | 406/532 [06:15<02:03,  1.02it/s]Loading train:  77%|███████▋  | 407/532 [06:16<01:56,  1.07it/s]Loading train:  77%|███████▋  | 408/532 [06:17<01:52,  1.10it/s]Loading train:  77%|███████▋  | 409/532 [06:18<01:49,  1.12it/s]Loading train:  77%|███████▋  | 410/532 [06:18<01:45,  1.15it/s]Loading train:  77%|███████▋  | 411/532 [06:19<01:43,  1.17it/s]Loading train:  77%|███████▋  | 412/532 [06:20<01:41,  1.19it/s]Loading train:  78%|███████▊  | 413/532 [06:21<01:41,  1.17it/s]Loading train:  78%|███████▊  | 414/532 [06:22<01:39,  1.18it/s]Loading train:  78%|███████▊  | 415/532 [06:23<01:37,  1.20it/s]Loading train:  78%|███████▊  | 416/532 [06:23<01:36,  1.20it/s]Loading train:  78%|███████▊  | 417/532 [06:24<01:34,  1.21it/s]Loading train:  79%|███████▊  | 418/532 [06:25<01:32,  1.23it/s]Loading train:  79%|███████▉  | 419/532 [06:26<01:36,  1.17it/s]Loading train:  79%|███████▉  | 420/532 [06:27<01:35,  1.17it/s]Loading train:  79%|███████▉  | 421/532 [06:28<01:38,  1.12it/s]Loading train:  79%|███████▉  | 422/532 [06:29<01:39,  1.11it/s]Loading train:  80%|███████▉  | 423/532 [06:30<01:39,  1.10it/s]Loading train:  80%|███████▉  | 424/532 [06:31<01:37,  1.10it/s]Loading train:  80%|███████▉  | 425/532 [06:31<01:38,  1.09it/s]Loading train:  80%|████████  | 426/532 [06:32<01:38,  1.07it/s]Loading train:  80%|████████  | 427/532 [06:33<01:39,  1.06it/s]Loading train:  80%|████████  | 428/532 [06:34<01:36,  1.07it/s]Loading train:  81%|████████  | 429/532 [06:35<01:37,  1.06it/s]Loading train:  81%|████████  | 430/532 [06:36<01:34,  1.08it/s]Loading train:  81%|████████  | 431/532 [06:37<01:38,  1.02it/s]Loading train:  81%|████████  | 432/532 [06:38<01:38,  1.02it/s]Loading train:  81%|████████▏ | 433/532 [06:39<01:37,  1.02it/s]Loading train:  82%|████████▏ | 434/532 [06:40<01:36,  1.01it/s]Loading train:  82%|████████▏ | 435/532 [06:41<01:37,  1.00s/it]Loading train:  82%|████████▏ | 436/532 [06:42<01:38,  1.03s/it]Loading train:  82%|████████▏ | 437/532 [06:43<01:31,  1.04it/s]Loading train:  82%|████████▏ | 438/532 [06:44<01:27,  1.08it/s]Loading train:  83%|████████▎ | 439/532 [06:45<01:19,  1.17it/s]Loading train:  83%|████████▎ | 440/532 [06:45<01:12,  1.28it/s]Loading train:  83%|████████▎ | 441/532 [06:46<01:08,  1.33it/s]Loading train:  83%|████████▎ | 442/532 [06:47<01:05,  1.37it/s]Loading train:  83%|████████▎ | 443/532 [06:47<01:05,  1.35it/s]Loading train:  83%|████████▎ | 444/532 [06:48<01:05,  1.35it/s]Loading train:  84%|████████▎ | 445/532 [06:49<01:04,  1.36it/s]Loading train:  84%|████████▍ | 446/532 [06:50<00:59,  1.45it/s]Loading train:  84%|████████▍ | 447/532 [06:50<00:59,  1.44it/s]Loading train:  84%|████████▍ | 448/532 [06:51<00:59,  1.42it/s]Loading train:  84%|████████▍ | 449/532 [06:52<01:01,  1.35it/s]Loading train:  85%|████████▍ | 450/532 [06:53<01:01,  1.32it/s]Loading train:  85%|████████▍ | 451/532 [06:53<01:03,  1.28it/s]Loading train:  85%|████████▍ | 452/532 [06:54<01:01,  1.30it/s]Loading train:  85%|████████▌ | 453/532 [06:55<01:03,  1.24it/s]Loading train:  85%|████████▌ | 454/532 [06:56<01:03,  1.23it/s]Loading train:  86%|████████▌ | 455/532 [06:57<01:07,  1.14it/s]Loading train:  86%|████████▌ | 456/532 [06:58<01:05,  1.16it/s]Loading train:  86%|████████▌ | 457/532 [06:59<01:06,  1.13it/s]Loading train:  86%|████████▌ | 458/532 [07:00<01:07,  1.10it/s]Loading train:  86%|████████▋ | 459/532 [07:01<01:07,  1.08it/s]Loading train:  86%|████████▋ | 460/532 [07:01<01:06,  1.09it/s]Loading train:  87%|████████▋ | 461/532 [07:03<01:10,  1.01it/s]Loading train:  87%|████████▋ | 462/532 [07:04<01:11,  1.02s/it]Loading train:  87%|████████▋ | 463/532 [07:05<01:11,  1.03s/it]Loading train:  87%|████████▋ | 464/532 [07:06<01:12,  1.06s/it]Loading train:  87%|████████▋ | 465/532 [07:07<01:15,  1.12s/it]Loading train:  88%|████████▊ | 466/532 [07:08<01:14,  1.14s/it]Loading train:  88%|████████▊ | 467/532 [07:09<01:09,  1.07s/it]Loading train:  88%|████████▊ | 468/532 [07:10<01:04,  1.00s/it]Loading train:  88%|████████▊ | 469/532 [07:11<01:01,  1.02it/s]Loading train:  88%|████████▊ | 470/532 [07:12<00:58,  1.06it/s]Loading train:  89%|████████▊ | 471/532 [07:13<00:56,  1.08it/s]Loading train:  89%|████████▊ | 472/532 [07:14<00:54,  1.10it/s]Loading train:  89%|████████▉ | 473/532 [07:15<00:54,  1.08it/s]Loading train:  89%|████████▉ | 474/532 [07:16<00:53,  1.09it/s]Loading train:  89%|████████▉ | 475/532 [07:16<00:52,  1.08it/s]Loading train:  89%|████████▉ | 476/532 [07:17<00:52,  1.06it/s]Loading train:  90%|████████▉ | 477/532 [07:18<00:51,  1.07it/s]Loading train:  90%|████████▉ | 478/532 [07:19<00:50,  1.08it/s]Loading train:  90%|█████████ | 479/532 [07:20<00:47,  1.12it/s]Loading train:  90%|█████████ | 480/532 [07:21<00:45,  1.15it/s]Loading train:  90%|█████████ | 481/532 [07:22<00:43,  1.17it/s]Loading train:  91%|█████████ | 482/532 [07:22<00:41,  1.21it/s]Loading train:  91%|█████████ | 483/532 [07:23<00:39,  1.24it/s]Loading train:  91%|█████████ | 484/532 [07:24<00:39,  1.23it/s]Loading train:  91%|█████████ | 485/532 [07:25<00:41,  1.13it/s]Loading train:  91%|█████████▏| 486/532 [07:26<00:42,  1.08it/s]Loading train:  92%|█████████▏| 487/532 [07:27<00:43,  1.03it/s]Loading train:  92%|█████████▏| 488/532 [07:28<00:42,  1.03it/s]Loading train:  92%|█████████▏| 489/532 [07:29<00:42,  1.02it/s]Loading train:  92%|█████████▏| 490/532 [07:30<00:41,  1.01it/s]Loading train:  92%|█████████▏| 491/532 [07:31<00:39,  1.03it/s]Loading train:  92%|█████████▏| 492/532 [07:32<00:36,  1.09it/s]Loading train:  93%|█████████▎| 493/532 [07:33<00:34,  1.12it/s]Loading train:  93%|█████████▎| 494/532 [07:34<00:32,  1.16it/s]Loading train:  93%|█████████▎| 495/532 [07:34<00:30,  1.20it/s]Loading train:  93%|█████████▎| 496/532 [07:35<00:29,  1.22it/s]Loading train:  93%|█████████▎| 497/532 [07:36<00:30,  1.15it/s]Loading train:  94%|█████████▎| 498/532 [07:37<00:30,  1.11it/s]Loading train:  94%|█████████▍| 499/532 [07:38<00:29,  1.13it/s]Loading train:  94%|█████████▍| 500/532 [07:39<00:29,  1.10it/s]Loading train:  94%|█████████▍| 501/532 [07:40<00:28,  1.09it/s]Loading train:  94%|█████████▍| 502/532 [07:41<00:27,  1.09it/s]Loading train:  95%|█████████▍| 503/532 [07:42<00:25,  1.13it/s]Loading train:  95%|█████████▍| 504/532 [07:42<00:24,  1.17it/s]Loading train:  95%|█████████▍| 505/532 [07:43<00:22,  1.20it/s]Loading train:  95%|█████████▌| 506/532 [07:44<00:21,  1.23it/s]Loading train:  95%|█████████▌| 507/532 [07:45<00:20,  1.25it/s]Loading train:  95%|█████████▌| 508/532 [07:45<00:19,  1.26it/s]Loading train:  96%|█████████▌| 509/532 [07:46<00:19,  1.16it/s]Loading train:  96%|█████████▌| 510/532 [07:48<00:20,  1.08it/s]Loading train:  96%|█████████▌| 511/532 [07:49<00:20,  1.03it/s]Loading train:  96%|█████████▌| 512/532 [07:50<00:19,  1.04it/s]Loading train:  96%|█████████▋| 513/532 [07:51<00:19,  1.00s/it]Loading train:  97%|█████████▋| 514/532 [07:52<00:17,  1.00it/s]Loading train:  97%|█████████▋| 515/532 [07:52<00:15,  1.09it/s]Loading train:  97%|█████████▋| 516/532 [07:53<00:14,  1.14it/s]Loading train:  97%|█████████▋| 517/532 [07:54<00:12,  1.19it/s]Loading train:  97%|█████████▋| 518/532 [07:55<00:11,  1.23it/s]Loading train:  98%|█████████▊| 519/532 [07:55<00:10,  1.24it/s]Loading train:  98%|█████████▊| 520/532 [07:56<00:09,  1.21it/s]Loading train:  98%|█████████▊| 521/532 [07:57<00:09,  1.14it/s]Loading train:  98%|█████████▊| 522/532 [07:58<00:08,  1.14it/s]Loading train:  98%|█████████▊| 523/532 [07:59<00:07,  1.15it/s]Loading train:  98%|█████████▊| 524/532 [08:00<00:06,  1.16it/s]Loading train:  99%|█████████▊| 525/532 [08:01<00:05,  1.18it/s]Loading train:  99%|█████████▉| 526/532 [08:01<00:04,  1.22it/s]Loading train:  99%|█████████▉| 527/532 [08:02<00:04,  1.18it/s]Loading train:  99%|█████████▉| 528/532 [08:03<00:03,  1.20it/s]Loading train:  99%|█████████▉| 529/532 [08:04<00:02,  1.18it/s]Loading train: 100%|█████████▉| 530/532 [08:05<00:01,  1.18it/s]Loading train: 100%|█████████▉| 531/532 [08:06<00:00,  1.20it/s]Loading train: 100%|██████████| 532/532 [08:07<00:00,  1.21it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   4%|▍         | 23/532 [00:00<00:02, 228.06it/s]concatenating: train:  10%|▉         | 51/532 [00:00<00:01, 240.71it/s]concatenating: train:  15%|█▌        | 82/532 [00:00<00:01, 258.00it/s]concatenating: train:  21%|██▏       | 114/532 [00:00<00:01, 272.95it/s]concatenating: train:  26%|██▌       | 139/532 [00:00<00:01, 264.03it/s]concatenating: train:  31%|███       | 163/532 [00:00<00:01, 253.99it/s]concatenating: train:  35%|███▌      | 187/532 [00:00<00:01, 237.64it/s]concatenating: train:  40%|████      | 214/532 [00:00<00:01, 245.69it/s]concatenating: train:  45%|████▍     | 238/532 [00:00<00:01, 238.73it/s]concatenating: train:  49%|████▉     | 263/532 [00:01<00:01, 240.45it/s]concatenating: train:  55%|█████▍    | 292/532 [00:01<00:00, 251.87it/s]concatenating: train:  60%|█████▉    | 318/532 [00:01<00:00, 250.22it/s]concatenating: train:  65%|██████▍   | 345/532 [00:01<00:00, 253.77it/s]concatenating: train:  70%|██████▉   | 371/532 [00:01<00:00, 254.41it/s]concatenating: train:  75%|███████▍  | 398/532 [00:01<00:00, 257.56it/s]concatenating: train:  80%|███████▉  | 424/532 [00:01<00:00, 253.93it/s]concatenating: train:  85%|████████▍ | 452/532 [00:01<00:00, 259.32it/s]concatenating: train:  90%|████████▉ | 478/532 [00:01<00:00, 253.30it/s]concatenating: train:  95%|█████████▍| 504/532 [00:01<00:00, 252.27it/s]concatenating: train: 100%|██████████| 532/532 [00:02<00:00, 257.81it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:10,  1.33it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:09,  1.32it/s]Loading test:  20%|██        | 3/15 [00:02<00:10,  1.20it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:09,  1.12it/s]Loading test:  33%|███▎      | 5/15 [00:04<00:09,  1.06it/s]Loading test:  40%|████      | 6/15 [00:05<00:08,  1.02it/s]Loading test:  47%|████▋     | 7/15 [00:06<00:07,  1.11it/s]Loading test:  53%|█████▎    | 8/15 [00:07<00:06,  1.01it/s]Loading test:  60%|██████    | 9/15 [00:08<00:05,  1.02it/s]Loading test:  67%|██████▋   | 10/15 [00:09<00:04,  1.06it/s]Loading test:  73%|███████▎  | 11/15 [00:10<00:03,  1.10it/s]Loading test:  80%|████████  | 12/15 [00:11<00:02,  1.05it/s]Loading test:  87%|████████▋ | 13/15 [00:12<00:01,  1.01it/s]Loading test:  93%|█████████▎| 14/15 [00:13<00:00,  1.05it/s]Loading test: 100%|██████████| 15/15 [00:14<00:00,  1.05it/s]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 609.66it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 84, 21)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 42, 21)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 42, 21)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 42, 40)   7600        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 42, 61)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 21, 61)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 21, 61)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 21, 80)   44000       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 21, 141)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 21, 141)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 42, 40)   22600       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 42, 101)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 42, 40)   36400       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________2019-07-06 07:20:49.499955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 07:20:49.500067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 07:20:49.500093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 07:20:49.500103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 07:20:49.500503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

activation_7 (Activation)       (None, 28, 42, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 42, 141)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 28, 42, 141)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 84, 20)   11300       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 84, 41)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 84, 20)   7400        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 84, 61)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 84, 61)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 84, 13)   806         dropout_5[0][0]                  
==================================================================================================
Total params: 225,706
Trainable params: 224,906
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53383120e-02 2.88786827e-02 1.16652967e-01 1.00133292e-02
 3.03165963e-02 5.79538965e-03 6.85058777e-02 1.28145429e-01
 7.55135052e-02 1.22435092e-02 2.73464902e-01 1.84941443e-01
 1.90056842e-04]
Train on 20749 samples, validate on 584 samples
Epoch 1/300
 - 29s - loss: 48.2963 - acc: 0.6775 - mDice: 0.0199 - val_loss: 5.3622 - val_acc: 0.9134 - val_mDice: 0.0129

Epoch 00001: val_mDice improved from -inf to 0.01289, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 22s - loss: 5.2147 - acc: 0.8938 - mDice: 0.0503 - val_loss: 4.2749 - val_acc: 0.9134 - val_mDice: 0.0332

Epoch 00002: val_mDice improved from 0.01289 to 0.03318, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 21s - loss: 3.9635 - acc: 0.9037 - mDice: 0.1075 - val_loss: 2.9469 - val_acc: 0.9157 - val_mDice: 0.1331

Epoch 00003: val_mDice improved from 0.03318 to 0.13309, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 22s - loss: 3.1453 - acc: 0.9158 - mDice: 0.2030 - val_loss: 2.0327 - val_acc: 0.9410 - val_mDice: 0.3128

Epoch 00004: val_mDice improved from 0.13309 to 0.31276, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 22s - loss: 2.5303 - acc: 0.9283 - mDice: 0.3038 - val_loss: 1.4337 - val_acc: 0.9589 - val_mDice: 0.4884

Epoch 00005: val_mDice improved from 0.31276 to 0.48840, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 21s - loss: 2.0814 - acc: 0.9381 - mDice: 0.3969 - val_loss: 1.1757 - val_acc: 0.9651 - val_mDice: 0.5820

Epoch 00006: val_mDice improved from 0.48840 to 0.58199, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 21s - loss: 1.8155 - acc: 0.9447 - mDice: 0.4574 - val_loss: 1.1165 - val_acc: 0.9670 - val_mDice: 0.6133

Epoch 00007: val_mDice improved from 0.58199 to 0.61326, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300
 - 22s - loss: 1.6418 - acc: 0.9486 - mDice: 0.5002 - val_loss: 1.0181 - val_acc: 0.9696 - val_mDice: 0.6523

Epoch 00008: val_mDice improved from 0.61326 to 0.65230, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300
 - 21s - loss: 1.4941 - acc: 0.9518 - mDice: 0.5381 - val_loss: 0.9618 - val_acc: 0.9716 - val_mDice: 0.6695

Epoch 00009: val_mDice improved from 0.65230 to 0.66953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300
 - 21s - loss: 1.3824 - acc: 0.9542 - mDice: 0.5677 - val_loss: 0.9032 - val_acc: 0.9730 - val_mDice: 0.6894

Epoch 00010: val_mDice improved from 0.66953 to 0.68944, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 21s - loss: 1.3008 - acc: 0.9559 - mDice: 0.5917 - val_loss: 0.9256 - val_acc: 0.9741 - val_mDice: 0.6940

Epoch 00011: val_mDice improved from 0.68944 to 0.69403, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 22s - loss: 1.2268 - acc: 0.9576 - mDice: 0.6125 - val_loss: 0.8394 - val_acc: 0.9741 - val_mDice: 0.7142

Epoch 00012: val_mDice improved from 0.69403 to 0.71417, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300
 - 21s - loss: 1.1820 - acc: 0.9588 - mDice: 0.6257 - val_loss: 0.8477 - val_acc: 0.9746 - val_mDice: 0.7160

Epoch 00013: val_mDice improved from 0.71417 to 0.71596, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 21s - loss: 1.1365 - acc: 0.9598 - mDice: 0.6373 - val_loss: 0.8062 - val_acc: 0.9750 - val_mDice: 0.7237

Epoch 00014: val_mDice improved from 0.71596 to 0.72370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 21s - loss: 1.0960 - acc: 0.9607 - mDice: 0.6486 - val_loss: 0.8111 - val_acc: 0.9755 - val_mDice: 0.7285

Epoch 00015: val_mDice improved from 0.72370 to 0.72851, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300
 - 22s - loss: 1.0733 - acc: 0.9612 - mDice: 0.6552 - val_loss: 0.7736 - val_acc: 0.9759 - val_mDice: 0.7359

Epoch 00016: val_mDice improved from 0.72851 to 0.73594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 17/300
 - 21s - loss: 1.0479 - acc: 0.9618 - mDice: 0.6616 - val_loss: 0.7874 - val_acc: 0.9764 - val_mDice: 0.7331

Epoch 00017: val_mDice did not improve from 0.73594
Epoch 18/300
 - 21s - loss: 1.0264 - acc: 0.9623 - mDice: 0.6675 - val_loss: 0.7988 - val_acc: 0.9760 - val_mDice: 0.7336

Epoch 00018: val_mDice did not improve from 0.73594
Epoch 19/300
 - 21s - loss: 1.0015 - acc: 0.9628 - mDice: 0.6746 - val_loss: 0.7650 - val_acc: 0.9776 - val_mDice: 0.7410

Epoch 00019: val_mDice improved from 0.73594 to 0.74103, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300
 - 21s - loss: 0.9816 - acc: 0.9633 - mDice: 0.6796 - val_loss: 0.7640 - val_acc: 0.9767 - val_mDice: 0.7417

Epoch 00020: val_mDice improved from 0.74103 to 0.74174, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300
 - 22s - loss: 0.9653 - acc: 0.9638 - mDice: 0.6844 - val_loss: 0.7537 - val_acc: 0.9769 - val_mDice: 0.7433

Epoch 00021: val_mDice improved from 0.74174 to 0.74331, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300
 - 21s - loss: 0.9475 - acc: 0.9642 - mDice: 0.6891 - val_loss: 0.7616 - val_acc: 0.9765 - val_mDice: 0.7442

Epoch 00022: val_mDice improved from 0.74331 to 0.74420, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300
 - 21s - loss: 0.9349 - acc: 0.9645 - mDice: 0.6933 - val_loss: 0.7446 - val_acc: 0.9774 - val_mDice: 0.7493

Epoch 00023: val_mDice improved from 0.74420 to 0.74930, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 24/300
 - 21s - loss: 0.9203 - acc: 0.9649 - mDice: 0.6970 - val_loss: 0.7418 - val_acc: 0.9773 - val_mDice: 0.7485

Epoch 00024: val_mDice did not improve from 0.74930
Epoch 25/300
 - 21s - loss: 0.9015 - acc: 0.9653 - mDice: 0.7021 - val_loss: 0.7375 - val_acc: 0.9769 - val_mDice: 0.7513

Epoch 00025: val_mDice improved from 0.74930 to 0.75126, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300
 - 22s - loss: 0.8941 - acc: 0.9655 - mDice: 0.7049 - val_loss: 0.7306 - val_acc: 0.9771 - val_mDice: 0.7556

Epoch 00026: val_mDice improved from 0.75126 to 0.75562, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 27/300
 - 21s - loss: 0.8811 - acc: 0.9658 - mDice: 0.7085 - val_loss: 0.7242 - val_acc: 0.9766 - val_mDice: 0.7560

Epoch 00027: val_mDice improved from 0.75562 to 0.75596, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300
 - 21s - loss: 0.8689 - acc: 0.9660 - mDice: 0.7118 - val_loss: 0.7393 - val_acc: 0.9764 - val_mDice: 0.7531

Epoch 00028: val_mDice did not improve from 0.75596
Epoch 29/300
 - 21s - loss: 0.8605 - acc: 0.9663 - mDice: 0.7145 - val_loss: 0.7246 - val_acc: 0.9773 - val_mDice: 0.7569

Epoch 00029: val_mDice improved from 0.75596 to 0.75694, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 22s - loss: 0.8483 - acc: 0.9666 - mDice: 0.7177 - val_loss: 0.7258 - val_acc: 0.9772 - val_mDice: 0.7602

Epoch 00030: val_mDice improved from 0.75694 to 0.76024, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 31/300
 - 21s - loss: 0.8433 - acc: 0.9667 - mDice: 0.7193 - val_loss: 0.7235 - val_acc: 0.9769 - val_mDice: 0.7579

Epoch 00031: val_mDice did not improve from 0.76024
Epoch 32/300
 - 21s - loss: 0.8340 - acc: 0.9669 - mDice: 0.7217 - val_loss: 0.7314 - val_acc: 0.9773 - val_mDice: 0.7582

Epoch 00032: val_mDice did not improve from 0.76024
Epoch 33/300
 - 21s - loss: 0.8261 - acc: 0.9670 - mDice: 0.7242 - val_loss: 0.7223 - val_acc: 0.9783 - val_mDice: 0.7564

Epoch 00033: val_mDice did not improve from 0.76024
Epoch 34/300
 - 22s - loss: 0.8159 - acc: 0.9670 - mDice: 0.7274 - val_loss: 0.7139 - val_acc: 0.9779 - val_mDice: 0.7628

Epoch 00034: val_mDice improved from 0.76024 to 0.76278, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 35/300
 - 21s - loss: 0.8106 - acc: 0.9671 - mDice: 0.7291 - val_loss: 0.7131 - val_acc: 0.9782 - val_mDice: 0.7617

Epoch 00035: val_mDice did not improve from 0.76278
Epoch 36/300
 - 21s - loss: 0.8029 - acc: 0.9672 - mDice: 0.7314 - val_loss: 0.7112 - val_acc: 0.9788 - val_mDice: 0.7618

Epoch 00036: val_mDice did not improve from 0.76278
Epoch 37/300
 - 21s - loss: 0.7975 - acc: 0.9671 - mDice: 0.7333 - val_loss: 0.7014 - val_acc: 0.9785 - val_mDice: 0.7645

Epoch 00037: val_mDice improved from 0.76278 to 0.76447, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 38/300
 - 21s - loss: 0.7874 - acc: 0.9673 - mDice: 0.7358 - val_loss: 0.6973 - val_acc: 0.9783 - val_mDice: 0.7662

Epoch 00038: val_mDice improved from 0.76447 to 0.76620, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 39/300
 - 22s - loss: 0.7846 - acc: 0.9674 - mDice: 0.7369 - val_loss: 0.7087 - val_acc: 0.9786 - val_mDice: 0.7636

Epoch 00039: val_mDice did not improve from 0.76620
Epoch 40/300
 - 21s - loss: 0.7783 - acc: 0.9676 - mDice: 0.7390 - val_loss: 0.7060 - val_acc: 0.9790 - val_mDice: 0.7645

Epoch 00040: val_mDice did not improve from 0.76620
Epoch 41/300
 - 21s - loss: 0.7737 - acc: 0.9677 - mDice: 0.7402 - val_loss: 0.7157 - val_acc: 0.9782 - val_mDice: 0.7639

Epoch 00041: val_mDice did not improve from 0.76620
Epoch 42/300
 - 21s - loss: 0.7682 - acc: 0.9678 - mDice: 0.7418 - val_loss: 0.6987 - val_acc: 0.9789 - val_mDice: 0.7664

Epoch 00042: val_mDice improved from 0.76620 to 0.76640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 43/300
 - 22s - loss: 0.7596 - acc: 0.9679 - mDice: 0.7444 - val_loss: 0.7038 - val_acc: 0.9785 - val_mDice: 0.7652

Epoch 00043: val_mDice did not improve from 0.76640
Epoch 44/300
 - 22s - loss: 0.7581 - acc: 0.9679 - mDice: 0.7448 - val_loss: 0.7018 - val_acc: 0.9792 - val_mDice: 0.7668

Epoch 00044: val_mDice improved from 0.76640 to 0.76681, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 45/300
 - 21s - loss: 0.7533 - acc: 0.9680 - mDice: 0.7462 - val_loss: 0.6942 - val_acc: 0.9791 - val_mDice: 0.7678

Epoch 00045: val_mDice improved from 0.76681 to 0.76783, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 46/300
 - 21s - loss: 0.7499 - acc: 0.9680 - mDice: 0.7473 - val_loss: 0.7121 - val_acc: 0.9782 - val_mDice: 0.7659

Epoch 00046: val_mDice did not improve from 0.76783
Epoch 47/300
 - 21s - loss: 0.7459 - acc: 0.9681 - mDice: 0.7485 - val_loss: 0.6871 - val_acc: 0.9793 - val_mDice: 0.7709

Epoch 00047: val_mDice improved from 0.76783 to 0.77085, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 48/300
 - 22s - loss: 0.7416 - acc: 0.9682 - mDice: 0.7500 - val_loss: 0.7044 - val_acc: 0.9782 - val_mDice: 0.7675

Epoch 00048: val_mDice did not improve from 0.77085
Epoch 49/300
 - 21s - loss: 0.7387 - acc: 0.9682 - mDice: 0.7505 - val_loss: 0.7063 - val_acc: 0.9793 - val_mDice: 0.7656

Epoch 00049: val_mDice did not improve from 0.77085
Epoch 50/300
 - 21s - loss: 0.7356 - acc: 0.9683 - mDice: 0.7517 - val_loss: 0.6959 - val_acc: 0.9790 - val_mDice: 0.7728

Epoch 00050: val_mDice improved from 0.77085 to 0.77280, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 51/300
 - 21s - loss: 0.7323 - acc: 0.9684 - mDice: 0.7527 - val_loss: 0.6854 - val_acc: 0.9797 - val_mDice: 0.7699

Epoch 00051: val_mDice did not improve from 0.77280
Epoch 52/300
 - 22s - loss: 0.7271 - acc: 0.9684 - mDice: 0.7543 - val_loss: 0.7035 - val_acc: 0.9787 - val_mDice: 0.7677

Epoch 00052: val_mDice did not improve from 0.77280
Epoch 53/300
 - 22s - loss: 0.7234 - acc: 0.9684 - mDice: 0.7550 - val_loss: 0.6929 - val_acc: 0.9790 - val_mDice: 0.7708

Epoch 00053: val_mDice did not improve from 0.77280
Epoch 54/300
 - 21s - loss: 0.7209 - acc: 0.9685 - mDice: 0.7562 - val_loss: 0.6990 - val_acc: 0.9790 - val_mDice: 0.7713

Epoch 00054: val_mDice did not improve from 0.77280
Epoch 55/300
 - 21s - loss: 0.7179 - acc: 0.9686 - mDice: 0.7569 - val_loss: 0.7061 - val_acc: 0.9797 - val_mDice: 0.7665

Epoch 00055: val_mDice did not improve from 0.77280
Epoch 56/300
 - 21s - loss: 0.7147 - acc: 0.9685 - mDice: 0.7579 - val_loss: 0.6984 - val_acc: 0.9795 - val_mDice: 0.7705

Epoch 00056: val_mDice did not improve from 0.77280
Epoch 57/300
 - 22s - loss: 0.7129 - acc: 0.9686 - mDice: 0.7584 - val_loss: 0.6872 - val_acc: 0.9790 - val_mDice: 0.7741

Epoch 00057: val_mDice improved from 0.77280 to 0.77407, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 58/300
 - 21s - loss: 0.7092 - acc: 0.9687 - mDice: 0.7595 - val_loss: 0.7073 - val_acc: 0.9797 - val_mDice: 0.7662

Epoch 00058: val_mDice did not improve from 0.77407
Epoch 59/300
 - 21s - loss: 0.7081 - acc: 0.9687 - mDice: 0.7597 - val_loss: 0.6874 - val_acc: 0.9801 - val_mDice: 0.7698

Epoch 00059: val_mDice did not improve from 0.77407
Epoch 60/300
 - 21s - loss: 0.7029 - acc: 0.9688 - mDice: 0.7616 - val_loss: 0.6910 - val_acc: 0.9787 - val_mDice: 0.7735

Epoch 00060: val_mDice did not improve from 0.77407
Epoch 61/300
 - 22s - loss: 0.7027 - acc: 0.9688 - mDice: 0.7615 - val_loss: 0.6864 - val_acc: 0.9797 - val_mDice: 0.7724

Epoch 00061: val_mDice did not improve from 0.77407
Epoch 62/300
 - 21s - loss: 0.6995 - acc: 0.9688 - mDice: 0.7623 - val_loss: 0.6777 - val_acc: 0.9795 - val_mDice: 0.7753

Epoch 00062: val_mDice improved from 0.77407 to 0.77533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 63/300
 - 21s - loss: 0.6969 - acc: 0.9688 - mDice: 0.7633 - val_loss: 0.6862 - val_acc: 0.9796 - val_mDice: 0.7738

Epoch 00063: val_mDice did not improve from 0.77533
Epoch 64/300
 - 21s - loss: 0.6944 - acc: 0.9689 - mDice: 0.7639 - val_loss: 0.6803 - val_acc: 0.9797 - val_mDice: 0.7727

Epoch 00064: val_mDice did not improve from 0.77533
Epoch 65/300
 - 22s - loss: 0.6915 - acc: 0.9689 - mDice: 0.7648 - val_loss: 0.6823 - val_acc: 0.9796 - val_mDice: 0.7753

Epoch 00065: val_mDice improved from 0.77533 to 0.77535, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 66/300
 - 21s - loss: 0.6926 - acc: 0.9689 - mDice: 0.7643 - val_loss: 0.6908 - val_acc: 0.9794 - val_mDice: 0.7717

Epoch 00066: val_mDice did not improve from 0.77535
Epoch 67/300
 - 21s - loss: 0.6881 - acc: 0.9690 - mDice: 0.7658 - val_loss: 0.6870 - val_acc: 0.9792 - val_mDice: 0.7703

Epoch 00067: val_mDice did not improve from 0.77535
Epoch 68/300
 - 21s - loss: 0.6870 - acc: 0.9690 - mDice: 0.7663 - val_loss: 0.6873 - val_acc: 0.9798 - val_mDice: 0.7732

Epoch 00068: val_mDice did not improve from 0.77535
Epoch 69/300
 - 22s - loss: 0.6842 - acc: 0.9691 - mDice: 0.7672 - val_loss: 0.6814 - val_acc: 0.9794 - val_mDice: 0.7753

Epoch 00069: val_mDice did not improve from 0.77535
Epoch 70/300
 - 21s - loss: 0.6824 - acc: 0.9691 - mDice: 0.7676 - val_loss: 0.6926 - val_acc: 0.9797 - val_mDice: 0.7732

Epoch 00070: val_mDice did not improve from 0.77535
Epoch 71/300
 - 21s - loss: 0.6800 - acc: 0.9691 - mDice: 0.7685 - val_loss: 0.6844 - val_acc: 0.9792 - val_mDice: 0.7747

Epoch 00071: val_mDice did not improve from 0.77535
Epoch 72/300
 - 22s - loss: 0.6804 - acc: 0.9691 - mDice: 0.7685 - val_loss: 0.7161 - val_acc: 0.9791 - val_mDice: 0.7680

Epoch 00072: val_mDice did not improve from 0.77535
Epoch 73/300
 - 22s - loss: 0.6760 - acc: 0.9692 - mDice: 0.7694 - val_loss: 0.6864 - val_acc: 0.9794 - val_mDice: 0.7742

Epoch 00073: val_mDice did not improve from 0.77535
Epoch 74/300
 - 21s - loss: 0.6767 - acc: 0.9692 - mDice: 0.7695 - val_loss: 0.6807 - val_acc: 0.9795 - val_mDice: 0.7776

Epoch 00074: val_mDice improved from 0.77535 to 0.77757, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 75/300
 - 22s - loss: 0.6753 - acc: 0.9692 - mDice: 0.7697 - val_loss: 0.7059 - val_acc: 0.9782 - val_mDice: 0.7749

Epoch 00075: val_mDice did not improve from 0.77757
Epoch 76/300
 - 21s - loss: 0.6726 - acc: 0.9692 - mDice: 0.7706 - val_loss: 0.6797 - val_acc: 0.9797 - val_mDice: 0.7789

Epoch 00076: val_mDice improved from 0.77757 to 0.77887, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 77/300
 - 21s - loss: 0.6699 - acc: 0.9693 - mDice: 0.7713 - val_loss: 0.6858 - val_acc: 0.9796 - val_mDice: 0.7765

Epoch 00077: val_mDice did not improve from 0.77887
Epoch 78/300
 - 21s - loss: 0.6717 - acc: 0.9693 - mDice: 0.7710 - val_loss: 0.6900 - val_acc: 0.9799 - val_mDice: 0.7755

Epoch 00078: val_mDice did not improve from 0.77887
Epoch 79/300
 - 22s - loss: 0.6675 - acc: 0.9693 - mDice: 0.7720 - val_loss: 0.6871 - val_acc: 0.9790 - val_mDice: 0.7775

Epoch 00079: val_mDice did not improve from 0.77887
Epoch 80/300
 - 22s - loss: 0.6665 - acc: 0.9694 - mDice: 0.7725 - val_loss: 0.6826 - val_acc: 0.9797 - val_mDice: 0.7784

Epoch 00080: val_mDice did not improve from 0.77887
Epoch 81/300
 - 21s - loss: 0.6640 - acc: 0.9694 - mDice: 0.7734 - val_loss: 0.6899 - val_acc: 0.9800 - val_mDice: 0.7755

Epoch 00081: val_mDice did not improve from 0.77887
Epoch 82/300
 - 21s - loss: 0.6642 - acc: 0.9694 - mDice: 0.7730 - val_loss: 0.6851 - val_acc: 0.9791 - val_mDice: 0.7772

Epoch 00082: val_mDice did not improve from 0.77887
Epoch 83/300
 - 23s - loss: 0.6616 - acc: 0.9694 - mDice: 0.7741 - val_loss: 0.6894 - val_acc: 0.9798 - val_mDice: 0.7742

Epoch 00083: val_mDice did not improve from 0.77887
Epoch 84/300
 - 22s - loss: 0.6600 - acc: 0.9695 - mDice: 0.7745 - val_loss: 0.6924 - val_acc: 0.9789 - val_mDice: 0.7764

Epoch 00084: val_mDice did not improve from 0.77887
Epoch 85/300
 - 22s - loss: 0.6615 - acc: 0.9695 - mDice: 0.7742 - val_loss: 0.6804 - val_acc: 0.9799 - val_mDice: 0.7779

Epoch 00085: val_mDice did not improve from 0.77887
Epoch 86/300
 - 23s - loss: 0.6579 - acc: 0.9695 - mDice: 0.7752 - val_loss: 0.6864 - val_acc: 0.9796 - val_mDice: 0.7806

Epoch 00086: val_mDice improved from 0.77887 to 0.78061, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 87/300
 - 22s - loss: 0.6574 - acc: 0.9696 - mDice: 0.7753 - val_loss: 0.6804 - val_acc: 0.9804 - val_mDice: 0.7766

Epoch 00087: val_mDice did not improve from 0.78061
Epoch 88/300
 - 23s - loss: 0.6546 - acc: 0.9696 - mDice: 0.7761 - val_loss: 0.6909 - val_acc: 0.9799 - val_mDice: 0.7781

Epoch 00088: val_mDice did not improve from 0.78061
Epoch 89/300
 - 22s - loss: 0.6557 - acc: 0.9696 - mDice: 0.7759 - val_loss: 0.6822 - val_acc: 0.9797 - val_mDice: 0.7775

Epoch 00089: val_mDice did not improve from 0.78061
Epoch 90/300
 - 23s - loss: 0.6545 - acc: 0.9696 - mDice: 0.7763 - val_loss: 0.6904 - val_acc: 0.9793 - val_mDice: 0.7804

Epoch 00090: val_mDice did not improve from 0.78061
Epoch 91/300
 - 22s - loss: 0.6506 - acc: 0.9697 - mDice: 0.7774 - val_loss: 0.6827 - val_acc: 0.9795 - val_mDice: 0.7803

Epoch 00091: val_mDice did not improve from 0.78061
Epoch 92/300
 - 22s - loss: 0.6526 - acc: 0.9697 - mDice: 0.7768 - val_loss: 0.6789 - val_acc: 0.9801 - val_mDice: 0.7795

Epoch 00092: val_mDice did not improve from 0.78061
Epoch 93/300
 - 21s - loss: 0.6501 - acc: 0.9697 - mDice: 0.7777 - val_loss: 0.6862 - val_acc: 0.9794 - val_mDice: 0.7780

Epoch 00093: val_mDice did not improve from 0.78061
Epoch 94/300
 - 22s - loss: 0.6479 - acc: 0.9697 - mDice: 0.7785 - val_loss: 0.6777 - val_acc: 0.9796 - val_mDice: 0.7798

Epoch 00094: val_mDice did not improve from 0.78061
Epoch 95/300
 - 21s - loss: 0.6494 - acc: 0.9698 - mDice: 0.7780 - val_loss: 0.6702 - val_acc: 0.9800 - val_mDice: 0.7806

Epoch 00095: val_mDice did not improve from 0.78061
Epoch 96/300
 - 21s - loss: 0.6471 - acc: 0.9698 - mDice: 0.7783 - val_loss: 0.6859 - val_acc: 0.9798 - val_mDice: 0.7813

Epoch 00096: val_mDice improved from 0.78061 to 0.78127, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 97/300
 - 22s - loss: 0.6441 - acc: 0.9698 - mDice: 0.7797 - val_loss: 0.6728 - val_acc: 0.9797 - val_mDice: 0.7818

Epoch 00097: val_mDice improved from 0.78127 to 0.78182, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 98/300
 - 22s - loss: 0.6432 - acc: 0.9699 - mDice: 0.7798 - val_loss: 0.6839 - val_acc: 0.9792 - val_mDice: 0.7829

Epoch 00098: val_mDice improved from 0.78182 to 0.78285, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 99/300
 - 21s - loss: 0.6450 - acc: 0.9698 - mDice: 0.7793 - val_loss: 0.6870 - val_acc: 0.9791 - val_mDice: 0.7804

Epoch 00099: val_mDice did not improve from 0.78285
Epoch 100/300
 - 22s - loss: 0.6423 - acc: 0.9699 - mDice: 0.7802 - val_loss: 0.6877 - val_acc: 0.9798 - val_mDice: 0.7815

Epoch 00100: val_mDice did not improve from 0.78285
Epoch 101/300
 - 22s - loss: 0.6418 - acc: 0.9699 - mDice: 0.7802 - val_loss: 0.6827 - val_acc: 0.9799 - val_mDice: 0.7818

Epoch 00101: val_mDice did not improve from 0.78285
Epoch 102/300
 - 21s - loss: 0.6402 - acc: 0.9699 - mDice: 0.7810 - val_loss: 0.6755 - val_acc: 0.9799 - val_mDice: 0.7828

Epoch 00102: val_mDice did not improve from 0.78285
Epoch 103/300
 - 22s - loss: 0.6398 - acc: 0.9699 - mDice: 0.7808 - val_loss: 0.6777 - val_acc: 0.9793 - val_mDice: 0.7823

Epoch 00103: val_mDice did not improve from 0.78285
Epoch 104/300
 - 22s - loss: 0.6385 - acc: 0.9700 - mDice: 0.7814 - val_loss: 0.6885 - val_acc: 0.9795 - val_mDice: 0.7800

Epoch 00104: val_mDice did not improve from 0.78285
Epoch 105/300
 - 22s - loss: 0.6395 - acc: 0.9700 - mDice: 0.7810 - val_loss: 0.6798 - val_acc: 0.9791 - val_mDice: 0.7838

Epoch 00105: val_mDice improved from 0.78285 to 0.78377, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 106/300
 - 23s - loss: 0.6366 - acc: 0.9700 - mDice: 0.7820 - val_loss: 0.6844 - val_acc: 0.9794 - val_mDice: 0.7815

Epoch 00106: val_mDice did not improve from 0.78377
Epoch 107/300
 - 22s - loss: 0.6368 - acc: 0.9700 - mDice: 0.7821 - val_loss: 0.6746 - val_acc: 0.9798 - val_mDice: 0.7824

Epoch 00107: val_mDice did not improve from 0.78377
Epoch 108/300
 - 22s - loss: 0.6356 - acc: 0.9700 - mDice: 0.7822 - val_loss: 0.6804 - val_acc: 0.9799 - val_mDice: 0.7819

Epoch 00108: val_mDice did not improve from 0.78377
Epoch 109/300
 - 21s - loss: 0.6349 - acc: 0.9700 - mDice: 0.7827 - val_loss: 0.6740 - val_acc: 0.9799 - val_mDice: 0.7820

Epoch 00109: val_mDice did not improve from 0.78377
Epoch 110/300
 - 23s - loss: 0.6358 - acc: 0.9700 - mDice: 0.7823 - val_loss: 0.6853 - val_acc: 0.9795 - val_mDice: 0.7832

Epoch 00110: val_mDice did not improve from 0.78377
Epoch 111/300
 - 21s - loss: 0.6336 - acc: 0.9701 - mDice: 0.7828 - val_loss: 0.6866 - val_acc: 0.9800 - val_mDice: 0.7824

Epoch 00111: val_mDice did not improve from 0.78377
Epoch 112/300
 - 21s - loss: 0.6327 - acc: 0.9701 - mDice: 0.7833 - val_loss: 0.6786 - val_acc: 0.9802 - val_mDice: 0.7833

Epoch 00112: val_mDice did not improve from 0.78377
Epoch 113/300
 - 22s - loss: 0.6310 - acc: 0.9701 - mDice: 0.7838 - val_loss: 0.6756 - val_acc: 0.9797 - val_mDice: 0.7847

Epoch 00113: val_mDice improved from 0.78377 to 0.78472, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 114/300
 - 22s - loss: 0.6301 - acc: 0.9701 - mDice: 0.7843 - val_loss: 0.6777 - val_acc: 0.9798 - val_mDice: 0.7841

Epoch 00114: val_mDice did not improve from 0.78472
Epoch 115/300
 - 21s - loss: 0.6309 - acc: 0.9701 - mDice: 0.7841 - val_loss: 0.6820 - val_acc: 0.9793 - val_mDice: 0.7840

Epoch 00115: val_mDice did not improve from 0.78472
Epoch 116/300
 - 21s - loss: 0.6273 - acc: 0.9702 - mDice: 0.7852 - val_loss: 0.6854 - val_acc: 0.9800 - val_mDice: 0.7826

Epoch 00116: val_mDice did not improve from 0.78472
Epoch 117/300
 - 23s - loss: 0.6283 - acc: 0.9702 - mDice: 0.7848 - val_loss: 0.6845 - val_acc: 0.9792 - val_mDice: 0.7848

Epoch 00117: val_mDice improved from 0.78472 to 0.78479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 118/300
 - 21s - loss: 0.6276 - acc: 0.9702 - mDice: 0.7851 - val_loss: 0.6835 - val_acc: 0.9802 - val_mDice: 0.7817

Epoch 00118: val_mDice did not improve from 0.78479
Epoch 119/300
 - 22s - loss: 0.6260 - acc: 0.9702 - mDice: 0.7856 - val_loss: 0.6857 - val_acc: 0.9800 - val_mDice: 0.7833

Epoch 00119: val_mDice did not improve from 0.78479
Epoch 120/300
 - 22s - loss: 0.6255 - acc: 0.9702 - mDice: 0.7857 - val_loss: 0.6783 - val_acc: 0.9795 - val_mDice: 0.7842

Epoch 00120: val_mDice did not improve from 0.78479
Epoch 121/300
 - 22s - loss: 0.6247 - acc: 0.9703 - mDice: 0.7858 - val_loss: 0.6776 - val_acc: 0.9802 - val_mDice: 0.7824

Epoch 00121: val_mDice did not improve from 0.78479
Epoch 122/300
 - 22s - loss: 0.6241 - acc: 0.9703 - mDice: 0.7861 - val_loss: 0.6859 - val_acc: 0.9797 - val_mDice: 0.7813

Epoch 00122: val_mDice did not improve from 0.78479
Epoch 123/300
 - 22s - loss: 0.6230 - acc: 0.9703 - mDice: 0.7865 - val_loss: 0.6792 - val_acc: 0.9796 - val_mDice: 0.7861

Epoch 00123: val_mDice improved from 0.78479 to 0.78610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 124/300
 - 22s - loss: 0.6218 - acc: 0.9703 - mDice: 0.7868 - val_loss: 0.6767 - val_acc: 0.9796 - val_mDice: 0.7858

Epoch 00124: val_mDice did not improve from 0.78610
Epoch 125/300
 - 21s - loss: 0.6215 - acc: 0.9703 - mDice: 0.7870 - val_loss: 0.6765 - val_acc: 0.9795 - val_mDice: 0.7853

Epoch 00125: val_mDice did not improve from 0.78610
Epoch 126/300
 - 21s - loss: 0.6239 - acc: 0.9703 - mDice: 0.7863 - val_loss: 0.6791 - val_acc: 0.9797 - val_mDice: 0.7834

Epoch 00126: val_mDice did not improve from 0.78610
Epoch 127/300
 - 23s - loss: 0.6212 - acc: 0.9703 - mDice: 0.7872 - val_loss: 0.6828 - val_acc: 0.9792 - val_mDice: 0.7857

Epoch 00127: val_mDice did not improve from 0.78610
Epoch 128/300
 - 22s - loss: 0.6219 - acc: 0.9703 - mDice: 0.7870 - val_loss: 0.6784 - val_acc: 0.9796 - val_mDice: 0.7862

Epoch 00128: val_mDice improved from 0.78610 to 0.78620, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 129/300
 - 22s - loss: 0.6190 - acc: 0.9704 - mDice: 0.7880 - val_loss: 0.6861 - val_acc: 0.9793 - val_mDice: 0.7832

Epoch 00129: val_mDice did not improve from 0.78620
Epoch 130/300
 - 21s - loss: 0.6196 - acc: 0.9704 - mDice: 0.7878 - val_loss: 0.6792 - val_acc: 0.9798 - val_mDice: 0.7846

Epoch 00130: val_mDice did not improve from 0.78620
Epoch 131/300
 - 23s - loss: 0.6195 - acc: 0.9704 - mDice: 0.7880 - val_loss: 0.6844 - val_acc: 0.9799 - val_mDice: 0.7867

Epoch 00131: val_mDice improved from 0.78620 to 0.78669, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 132/300
 - 21s - loss: 0.6160 - acc: 0.9704 - mDice: 0.7888 - val_loss: 0.6818 - val_acc: 0.9795 - val_mDice: 0.7856

Epoch 00132: val_mDice did not improve from 0.78669
Epoch 133/300
 - 21s - loss: 0.6172 - acc: 0.9704 - mDice: 0.7886 - val_loss: 0.6870 - val_acc: 0.9798 - val_mDice: 0.7836

Epoch 00133: val_mDice did not improve from 0.78669
Epoch 134/300
 - 23s - loss: 0.6179 - acc: 0.9704 - mDice: 0.7881 - val_loss: 0.6777 - val_acc: 0.9796 - val_mDice: 0.7858

Epoch 00134: val_mDice did not improve from 0.78669
Epoch 135/300
 - 22s - loss: 0.6166 - acc: 0.9704 - mDice: 0.7888 - val_loss: 0.6776 - val_acc: 0.9793 - val_mDice: 0.7853

Epoch 00135: val_mDice did not improve from 0.78669
Epoch 136/300
 - 22s - loss: 0.6158 - acc: 0.9704 - mDice: 0.7890 - val_loss: 0.6828 - val_acc: 0.9796 - val_mDice: 0.7875

Epoch 00136: val_mDice improved from 0.78669 to 0.78753, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 137/300
 - 22s - loss: 0.6150 - acc: 0.9705 - mDice: 0.7895 - val_loss: 0.6905 - val_acc: 0.9792 - val_mDice: 0.7846

Epoch 00137: val_mDice did not improve from 0.78753
Epoch 138/300
 - 21s - loss: 0.6133 - acc: 0.9704 - mDice: 0.7900 - val_loss: 0.6774 - val_acc: 0.9792 - val_mDice: 0.7865

Epoch 00138: val_mDice did not improve from 0.78753
Epoch 139/300
 - 23s - loss: 0.6135 - acc: 0.9705 - mDice: 0.7897 - val_loss: 0.6780 - val_acc: 0.9798 - val_mDice: 0.7884

Epoch 00139: val_mDice improved from 0.78753 to 0.78845, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 140/300
 - 21s - loss: 0.6125 - acc: 0.9704 - mDice: 0.7902 - val_loss: 0.6842 - val_acc: 0.9798 - val_mDice: 0.7876

Epoch 00140: val_mDice did not improve from 0.78845
Epoch 141/300
 - 22s - loss: 0.6124 - acc: 0.9705 - mDice: 0.7903 - val_loss: 0.6809 - val_acc: 0.9801 - val_mDice: 0.7864

Epoch 00141: val_mDice did not improve from 0.78845
Epoch 142/300
 - 23s - loss: 0.6129 - acc: 0.9705 - mDice: 0.7899 - val_loss: 0.6769 - val_acc: 0.9791 - val_mDice: 0.7886

Epoch 00142: val_mDice improved from 0.78845 to 0.78865, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 143/300
 - 22s - loss: 0.6108 - acc: 0.9705 - mDice: 0.7908 - val_loss: 0.6728 - val_acc: 0.9800 - val_mDice: 0.7868

Epoch 00143: val_mDice did not improve from 0.78865
Epoch 144/300
 - 22s - loss: 0.6100 - acc: 0.9705 - mDice: 0.7911 - val_loss: 0.6894 - val_acc: 0.9791 - val_mDice: 0.7865

Epoch 00144: val_mDice did not improve from 0.78865
Epoch 145/300
 - 23s - loss: 0.6107 - acc: 0.9705 - mDice: 0.7908 - val_loss: 0.6824 - val_acc: 0.9800 - val_mDice: 0.7865

Epoch 00145: val_mDice did not improve from 0.78865
Epoch 146/300
 - 21s - loss: 0.6107 - acc: 0.9705 - mDice: 0.7908 - val_loss: 0.6770 - val_acc: 0.9797 - val_mDice: 0.7863

Epoch 00146: val_mDice did not improve from 0.78865
Epoch 147/300
 - 23s - loss: 0.6100 - acc: 0.9705 - mDice: 0.7912 - val_loss: 0.6798 - val_acc: 0.9795 - val_mDice: 0.7874

Epoch 00147: val_mDice did not improve from 0.78865
Epoch 148/300
 - 22s - loss: 0.6091 - acc: 0.9705 - mDice: 0.7914 - val_loss: 0.6881 - val_acc: 0.9788 - val_mDice: 0.7860

Epoch 00148: val_mDice did not improve from 0.78865
Epoch 149/300
 - 22s - loss: 0.6099 - acc: 0.9706 - mDice: 0.7912 - val_loss: 0.6774 - val_acc: 0.9799 - val_mDice: 0.7877

Epoch 00149: val_mDice did not improve from 0.78865
Epoch 150/300
 - 21s - loss: 0.6071 - acc: 0.9706 - mDice: 0.7919 - val_loss: 0.6784 - val_acc: 0.9796 - val_mDice: 0.7876

Epoch 00150: val_mDice did not improve from 0.78865
Epoch 151/300
 - 22s - loss: 0.6068 - acc: 0.9706 - mDice: 0.7923 - val_loss: 0.6722 - val_acc: 0.9798 - val_mDice: 0.7863

Epoch 00151: val_mDice did not improve from 0.78865
Epoch 152/300
 - 22s - loss: 0.6076 - acc: 0.9706 - mDice: 0.7920 - val_loss: 0.6707 - val_acc: 0.9798 - val_mDice: 0.7870

Epoch 00152: val_mDice did not improve from 0.78865
Epoch 153/300
 - 21s - loss: 0.6058 - acc: 0.9707 - mDice: 0.7924 - val_loss: 0.6841 - val_acc: 0.9801 - val_mDice: 0.7868

Epoch 00153: val_mDice did not improve from 0.78865
Epoch 154/300
 - 23s - loss: 0.6051 - acc: 0.9707 - mDice: 0.7927 - val_loss: 0.6736 - val_acc: 0.9799 - val_mDice: 0.7910

Epoch 00154: val_mDice improved from 0.78865 to 0.79102, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 155/300
 - 21s - loss: 0.6047 - acc: 0.9706 - mDice: 0.7931 - val_loss: 0.6905 - val_acc: 0.9791 - val_mDice: 0.7886

Epoch 00155: val_mDice did not improve from 0.79102
Epoch 156/300
 - 21s - loss: 0.6037 - acc: 0.9707 - mDice: 0.7932 - val_loss: 0.6733 - val_acc: 0.9802 - val_mDice: 0.7883

Epoch 00156: val_mDice did not improve from 0.79102
Epoch 157/300
 - 23s - loss: 0.6021 - acc: 0.9707 - mDice: 0.7938 - val_loss: 0.6744 - val_acc: 0.9792 - val_mDice: 0.7910

Epoch 00157: val_mDice did not improve from 0.79102
Epoch 158/300
 - 22s - loss: 0.6034 - acc: 0.9707 - mDice: 0.7936 - val_loss: 0.6720 - val_acc: 0.9797 - val_mDice: 0.7886

Epoch 00158: val_mDice did not improve from 0.79102
Epoch 159/300
 - 23s - loss: 0.6028 - acc: 0.9707 - mDice: 0.7936 - val_loss: 0.6806 - val_acc: 0.9801 - val_mDice: 0.7884

Epoch 00159: val_mDice did not improve from 0.79102
Epoch 160/300
 - 22s - loss: 0.6012 - acc: 0.9707 - mDice: 0.7940 - val_loss: 0.6871 - val_acc: 0.9794 - val_mDice: 0.7880

Epoch 00160: val_mDice did not improve from 0.79102
Epoch 161/300
 - 23s - loss: 0.6022 - acc: 0.9707 - mDice: 0.7938 - val_loss: 0.6772 - val_acc: 0.9802 - val_mDice: 0.7876

Epoch 00161: val_mDice did not improve from 0.79102
Epoch 162/300
 - 22s - loss: 0.6011 - acc: 0.9707 - mDice: 0.7940 - val_loss: 0.6786 - val_acc: 0.9798 - val_mDice: 0.7892

Epoch 00162: val_mDice did not improve from 0.79102
Epoch 163/300
 - 22s - loss: 0.6012 - acc: 0.9707 - mDice: 0.7941 - val_loss: 0.6695 - val_acc: 0.9801 - val_mDice: 0.7876

Epoch 00163: val_mDice did not improve from 0.79102
Epoch 164/300
 - 21s - loss: 0.6012 - acc: 0.9707 - mDice: 0.7942 - val_loss: 0.6763 - val_acc: 0.9798 - val_mDice: 0.7885

Epoch 00164: val_mDice did not improve from 0.79102
Epoch 165/300
 - 23s - loss: 0.5997 - acc: 0.9708 - mDice: 0.7948 - val_loss: 0.6715 - val_acc: 0.9799 - val_mDice: 0.7895

Epoch 00165: val_mDice did not improve from 0.79102
Epoch 166/300
 - 21s - loss: 0.5988 - acc: 0.9707 - mDice: 0.7950 - val_loss: 0.6774 - val_acc: 0.9803 - val_mDice: 0.7857

Epoch 00166: val_mDice did not improve from 0.79102
Epoch 167/300
 - 21s - loss: 0.5970 - acc: 0.9708 - mDice: 0.7955 - val_loss: 0.6781 - val_acc: 0.9791 - val_mDice: 0.7918

Epoch 00167: val_mDice improved from 0.79102 to 0.79175, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 168/300
 - 22s - loss: 0.5977 - acc: 0.9708 - mDice: 0.7955 - val_loss: 0.6754 - val_acc: 0.9799 - val_mDice: 0.7906

Epoch 00168: val_mDice did not improve from 0.79175
Epoch 169/300
 - 21s - loss: 0.5975 - acc: 0.9708 - mDice: 0.7955 - val_loss: 0.6677 - val_acc: 0.9803 - val_mDice: 0.7895

Epoch 00169: val_mDice did not improve from 0.79175
Epoch 170/300
 - 23s - loss: 0.5975 - acc: 0.9708 - mDice: 0.7955 - val_loss: 0.6831 - val_acc: 0.9792 - val_mDice: 0.7877

Epoch 00170: val_mDice did not improve from 0.79175
Epoch 171/300
 - 21s - loss: 0.5963 - acc: 0.9708 - mDice: 0.7959 - val_loss: 0.6811 - val_acc: 0.9802 - val_mDice: 0.7891

Epoch 00171: val_mDice did not improve from 0.79175
Epoch 172/300
 - 22s - loss: 0.5955 - acc: 0.9708 - mDice: 0.7960 - val_loss: 0.6649 - val_acc: 0.9802 - val_mDice: 0.7926

Epoch 00172: val_mDice improved from 0.79175 to 0.79258, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 173/300
 - 22s - loss: 0.5943 - acc: 0.9709 - mDice: 0.7964 - val_loss: 0.6670 - val_acc: 0.9801 - val_mDice: 0.7892

Epoch 00173: val_mDice did not improve from 0.79258
Epoch 174/300
 - 23s - loss: 0.5957 - acc: 0.9708 - mDice: 0.7963 - val_loss: 0.6730 - val_acc: 0.9798 - val_mDice: 0.7898

Epoch 00174: val_mDice did not improve from 0.79258
Epoch 175/300
 - 22s - loss: 0.5931 - acc: 0.9709 - mDice: 0.7969 - val_loss: 0.6781 - val_acc: 0.9800 - val_mDice: 0.7897

Epoch 00175: val_mDice did not improve from 0.79258
Epoch 176/300
 - 22s - loss: 0.5928 - acc: 0.9709 - mDice: 0.7972 - val_loss: 0.6762 - val_acc: 0.9797 - val_mDice: 0.7903

Epoch 00176: val_mDice did not improve from 0.79258
Epoch 177/300
 - 23s - loss: 0.5926 - acc: 0.9709 - mDice: 0.7972 - val_loss: 0.6798 - val_acc: 0.9800 - val_mDice: 0.7914

Epoch 00177: val_mDice did not improve from 0.79258
Epoch 178/300
 - 22s - loss: 0.5929 - acc: 0.9709 - mDice: 0.7971 - val_loss: 0.6741 - val_acc: 0.9794 - val_mDice: 0.7937

Epoch 00178: val_mDice improved from 0.79258 to 0.79369, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 179/300
 - 22s - loss: 0.5911 - acc: 0.9709 - mDice: 0.7978 - val_loss: 0.6729 - val_acc: 0.9798 - val_mDice: 0.7918

Epoch 00179: val_mDice did not improve from 0.79369
Epoch 180/300
 - 22s - loss: 0.5896 - acc: 0.9709 - mDice: 0.7983 - val_loss: 0.6725 - val_acc: 0.9796 - val_mDice: 0.7924

Epoch 00180: val_mDice did not improve from 0.79369
Epoch 181/300
 - 22s - loss: 0.5921 - acc: 0.9709 - mDice: 0.7975 - val_loss: 0.6752 - val_acc: 0.9799 - val_mDice: 0.7949

Epoch 00181: val_mDice improved from 0.79369 to 0.79485, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 182/300
 - 23s - loss: 0.5908 - acc: 0.9709 - mDice: 0.7982 - val_loss: 0.6660 - val_acc: 0.9807 - val_mDice: 0.7905

Epoch 00182: val_mDice did not improve from 0.79485
Epoch 183/300
 - 21s - loss: 0.5895 - acc: 0.9709 - mDice: 0.7984 - val_loss: 0.6644 - val_acc: 0.9801 - val_mDice: 0.7927

Epoch 00183: val_mDice did not improve from 0.79485
Epoch 184/300
 - 23s - loss: 0.5877 - acc: 0.9709 - mDice: 0.7991 - val_loss: 0.6683 - val_acc: 0.9799 - val_mDice: 0.7909

Epoch 00184: val_mDice did not improve from 0.79485
Epoch 185/300
 - 22s - loss: 0.5875 - acc: 0.9710 - mDice: 0.7991 - val_loss: 0.6590 - val_acc: 0.9803 - val_mDice: 0.7961

Epoch 00185: val_mDice improved from 0.79485 to 0.79606, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 186/300
 - 22s - loss: 0.5860 - acc: 0.9710 - mDice: 0.7999 - val_loss: 0.6795 - val_acc: 0.9792 - val_mDice: 0.7939

Epoch 00186: val_mDice did not improve from 0.79606
Epoch 187/300
 - 22s - loss: 0.5865 - acc: 0.9710 - mDice: 0.7996 - val_loss: 0.6627 - val_acc: 0.9801 - val_mDice: 0.7948

Epoch 00187: val_mDice did not improve from 0.79606
Epoch 188/300
 - 21s - loss: 0.5869 - acc: 0.9710 - mDice: 0.7993 - val_loss: 0.6691 - val_acc: 0.9800 - val_mDice: 0.7949

Epoch 00188: val_mDice did not improve from 0.79606
Epoch 189/300
 - 21s - loss: 0.5861 - acc: 0.9710 - mDice: 0.7998 - val_loss: 0.6638 - val_acc: 0.9800 - val_mDice: 0.7936

Epoch 00189: val_mDice did not improve from 0.79606
Epoch 190/300
 - 22s - loss: 0.5853 - acc: 0.9710 - mDice: 0.8001 - val_loss: 0.6634 - val_acc: 0.9799 - val_mDice: 0.7926

Epoch 00190: val_mDice did not improve from 0.79606
Epoch 191/300
 - 22s - loss: 0.5849 - acc: 0.9710 - mDice: 0.8000 - val_loss: 0.6658 - val_acc: 0.9799 - val_mDice: 0.7937

Epoch 00191: val_mDice did not improve from 0.79606
Epoch 192/300
 - 21s - loss: 0.5848 - acc: 0.9710 - mDice: 0.8004 - val_loss: 0.6604 - val_acc: 0.9803 - val_mDice: 0.7950

Epoch 00192: val_mDice did not improve from 0.79606
Epoch 193/300
 - 21s - loss: 0.5830 - acc: 0.9711 - mDice: 0.8009 - val_loss: 0.6618 - val_acc: 0.9803 - val_mDice: 0.7953

Epoch 00193: val_mDice did not improve from 0.79606
Epoch 194/300
 - 23s - loss: 0.5825 - acc: 0.9711 - mDice: 0.8011 - val_loss: 0.6620 - val_acc: 0.9804 - val_mDice: 0.7964

Epoch 00194: val_mDice improved from 0.79606 to 0.79642, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 195/300
 - 21s - loss: 0.5814 - acc: 0.9711 - mDice: 0.8015 - val_loss: 0.6632 - val_acc: 0.9805 - val_mDice: 0.7956

Epoch 00195: val_mDice did not improve from 0.79642
Epoch 196/300
 - 22s - loss: 0.5823 - acc: 0.9711 - mDice: 0.8012 - val_loss: 0.6682 - val_acc: 0.9806 - val_mDice: 0.7936

Epoch 00196: val_mDice did not improve from 0.79642
Epoch 197/300
 - 22s - loss: 0.5822 - acc: 0.9711 - mDice: 0.8012 - val_loss: 0.6580 - val_acc: 0.9802 - val_mDice: 0.7955

Epoch 00197: val_mDice did not improve from 0.79642
Epoch 198/300
 - 22s - loss: 0.5805 - acc: 0.9711 - mDice: 0.8019 - val_loss: 0.6580 - val_acc: 0.9804 - val_mDice: 0.7974

Epoch 00198: val_mDice improved from 0.79642 to 0.79745, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 199/300
 - 23s - loss: 0.5821 - acc: 0.9711 - mDice: 0.8012 - val_loss: 0.6665 - val_acc: 0.9802 - val_mDice: 0.7931

Epoch 00199: val_mDice did not improve from 0.79745
Epoch 200/300
 - 23s - loss: 0.5804 - acc: 0.9711 - mDice: 0.8019 - val_loss: 0.6544 - val_acc: 0.9805 - val_mDice: 0.7959

Epoch 00200: val_mDice did not improve from 0.79745
Epoch 201/300
 - 22s - loss: 0.5795 - acc: 0.9711 - mDice: 0.8022 - val_loss: 0.6664 - val_acc: 0.9805 - val_mDice: 0.7940

Epoch 00201: val_mDice did not improve from 0.79745
Epoch 202/300
 - 22s - loss: 0.5788 - acc: 0.9711 - mDice: 0.8024 - val_loss: 0.6588 - val_acc: 0.9806 - val_mDice: 0.7949

Epoch 00202: val_mDice did not improve from 0.79745
Epoch 203/300
 - 21s - loss: 0.5798 - acc: 0.9711 - mDice: 0.8022 - val_loss: 0.6547 - val_acc: 0.9808 - val_mDice: 0.7962

Epoch 00203: val_mDice did not improve from 0.79745
Epoch 204/300
 - 23s - loss: 0.5770 - acc: 0.9712 - mDice: 0.8033 - val_loss: 0.6577 - val_acc: 0.9804 - val_mDice: 0.7966

Epoch 00204: val_mDice did not improve from 0.79745
Epoch 205/300
 - 21s - loss: 0.5778 - acc: 0.9712 - mDice: 0.8030 - val_loss: 0.6544 - val_acc: 0.9802 - val_mDice: 0.7965

Epoch 00205: val_mDice did not improve from 0.79745
Epoch 206/300
 - 21s - loss: 0.5787 - acc: 0.9712 - mDice: 0.8028 - val_loss: 0.6581 - val_acc: 0.9798 - val_mDice: 0.7973

Epoch 00206: val_mDice did not improve from 0.79745
Epoch 207/300
 - 22s - loss: 0.5770 - acc: 0.9712 - mDice: 0.8031 - val_loss: 0.6558 - val_acc: 0.9808 - val_mDice: 0.7976

Epoch 00207: val_mDice improved from 0.79745 to 0.79755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 208/300
 - 22s - loss: 0.5762 - acc: 0.9712 - mDice: 0.8033 - val_loss: 0.6634 - val_acc: 0.9807 - val_mDice: 0.7955

Epoch 00208: val_mDice did not improve from 0.79755
Epoch 209/300
 - 21s - loss: 0.5766 - acc: 0.9712 - mDice: 0.8033 - val_loss: 0.6619 - val_acc: 0.9800 - val_mDice: 0.7966

Epoch 00209: val_mDice did not improve from 0.79755
Epoch 210/300
 - 23s - loss: 0.5754 - acc: 0.9712 - mDice: 0.8038 - val_loss: 0.6666 - val_acc: 0.9808 - val_mDice: 0.7971

Epoch 00210: val_mDice did not improve from 0.79755
Epoch 211/300
 - 22s - loss: 0.5746 - acc: 0.9712 - mDice: 0.8038 - val_loss: 0.6547 - val_acc: 0.9808 - val_mDice: 0.7975

Epoch 00211: val_mDice did not improve from 0.79755
Epoch 212/300
 - 23s - loss: 0.5753 - acc: 0.9712 - mDice: 0.8038 - val_loss: 0.6568 - val_acc: 0.9806 - val_mDice: 0.7951

Epoch 00212: val_mDice did not improve from 0.79755
Epoch 213/300
 - 22s - loss: 0.5745 - acc: 0.9712 - mDice: 0.8041 - val_loss: 0.6569 - val_acc: 0.9805 - val_mDice: 0.7988

Epoch 00213: val_mDice improved from 0.79755 to 0.79878, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 214/300
 - 22s - loss: 0.5741 - acc: 0.9712 - mDice: 0.8042 - val_loss: 0.6705 - val_acc: 0.9801 - val_mDice: 0.7966

Epoch 00214: val_mDice did not improve from 0.79878
Epoch 215/300
 - 21s - loss: 0.5749 - acc: 0.9712 - mDice: 0.8042 - val_loss: 0.6637 - val_acc: 0.9801 - val_mDice: 0.7975

Epoch 00215: val_mDice did not improve from 0.79878
Epoch 216/300
 - 23s - loss: 0.5726 - acc: 0.9713 - mDice: 0.8047 - val_loss: 0.6529 - val_acc: 0.9806 - val_mDice: 0.7992

Epoch 00216: val_mDice improved from 0.79878 to 0.79922, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 217/300
 - 23s - loss: 0.5720 - acc: 0.9713 - mDice: 0.8050 - val_loss: 0.6575 - val_acc: 0.9803 - val_mDice: 0.7979

Epoch 00217: val_mDice did not improve from 0.79922
Epoch 218/300
 - 22s - loss: 0.5725 - acc: 0.9713 - mDice: 0.8047 - val_loss: 0.6763 - val_acc: 0.9798 - val_mDice: 0.7976

Epoch 00218: val_mDice did not improve from 0.79922
Epoch 219/300
 - 23s - loss: 0.5720 - acc: 0.9713 - mDice: 0.8050 - val_loss: 0.6612 - val_acc: 0.9807 - val_mDice: 0.7962

Epoch 00219: val_mDice did not improve from 0.79922
Epoch 220/300
 - 22s - loss: 0.5727 - acc: 0.9713 - mDice: 0.8047 - val_loss: 0.6575 - val_acc: 0.9801 - val_mDice: 0.7974

Epoch 00220: val_mDice did not improve from 0.79922
Epoch 221/300
 - 22s - loss: 0.5713 - acc: 0.9713 - mDice: 0.8053 - val_loss: 0.6592 - val_acc: 0.9808 - val_mDice: 0.7974

Epoch 00221: val_mDice did not improve from 0.79922
Epoch 222/300
 - 23s - loss: 0.5704 - acc: 0.9713 - mDice: 0.8055 - val_loss: 0.6663 - val_acc: 0.9805 - val_mDice: 0.7982

Epoch 00222: val_mDice did not improve from 0.79922
Epoch 223/300
 - 22s - loss: 0.5700 - acc: 0.9713 - mDice: 0.8059 - val_loss: 0.6563 - val_acc: 0.9805 - val_mDice: 0.7991

Epoch 00223: val_mDice did not improve from 0.79922
Epoch 224/300
 - 22s - loss: 0.5694 - acc: 0.9713 - mDice: 0.8059 - val_loss: 0.6574 - val_acc: 0.9806 - val_mDice: 0.7978

Epoch 00224: val_mDice did not improve from 0.79922
Epoch 225/300
 - 22s - loss: 0.5687 - acc: 0.9713 - mDice: 0.8065 - val_loss: 0.6472 - val_acc: 0.9809 - val_mDice: 0.7992

Epoch 00225: val_mDice did not improve from 0.79922
Epoch 226/300
 - 22s - loss: 0.5679 - acc: 0.9713 - mDice: 0.8064 - val_loss: 0.6559 - val_acc: 0.9809 - val_mDice: 0.8005

Epoch 00226: val_mDice improved from 0.79922 to 0.80052, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 227/300
 - 21s - loss: 0.5704 - acc: 0.9713 - mDice: 0.8058 - val_loss: 0.6664 - val_acc: 0.9807 - val_mDice: 0.7955

Epoch 00227: val_mDice did not improve from 0.80052
Epoch 228/300
 - 22s - loss: 0.5698 - acc: 0.9713 - mDice: 0.8059 - val_loss: 0.6566 - val_acc: 0.9808 - val_mDice: 0.7996

Epoch 00228: val_mDice did not improve from 0.80052
Epoch 229/300
 - 21s - loss: 0.5677 - acc: 0.9714 - mDice: 0.8065 - val_loss: 0.6667 - val_acc: 0.9809 - val_mDice: 0.7959

Epoch 00229: val_mDice did not improve from 0.80052
Epoch 230/300
 - 21s - loss: 0.5674 - acc: 0.9714 - mDice: 0.8067 - val_loss: 0.6678 - val_acc: 0.9802 - val_mDice: 0.7983

Epoch 00230: val_mDice did not improve from 0.80052
Epoch 231/300
 - 21s - loss: 0.5672 - acc: 0.9714 - mDice: 0.8068 - val_loss: 0.6548 - val_acc: 0.9807 - val_mDice: 0.7973

Epoch 00231: val_mDice did not improve from 0.80052
Epoch 232/300
 - 22s - loss: 0.5671 - acc: 0.9714 - mDice: 0.8067 - val_loss: 0.6650 - val_acc: 0.9809 - val_mDice: 0.7985

Epoch 00232: val_mDice did not improve from 0.80052
Epoch 233/300
 - 21s - loss: 0.5658 - acc: 0.9714 - mDice: 0.8072 - val_loss: 0.6587 - val_acc: 0.9799 - val_mDice: 0.7990

Epoch 00233: val_mDice did not improve from 0.80052
Epoch 234/300
 - 22s - loss: 0.5654 - acc: 0.9714 - mDice: 0.8075 - val_loss: 0.6553 - val_acc: 0.9807 - val_mDice: 0.7995

Epoch 00234: val_mDice did not improve from 0.80052
Epoch 235/300
 - 22s - loss: 0.5656 - acc: 0.9714 - mDice: 0.8074 - val_loss: 0.6636 - val_acc: 0.9804 - val_mDice: 0.7998

Epoch 00235: val_mDice did not improve from 0.80052
Epoch 236/300
 - 21s - loss: 0.5649 - acc: 0.9714 - mDice: 0.8076 - val_loss: 0.6638 - val_acc: 0.9803 - val_mDice: 0.7976

Epoch 00236: val_mDice did not improve from 0.80052
Epoch 237/300
 - 23s - loss: 0.5646 - acc: 0.9714 - mDice: 0.8078 - val_loss: 0.6572 - val_acc: 0.9806 - val_mDice: 0.7990

Epoch 00237: val_mDice did not improve from 0.80052
Epoch 238/300
 - 21s - loss: 0.5649 - acc: 0.9714 - mDice: 0.8077 - val_loss: 0.6570 - val_acc: 0.9804 - val_mDice: 0.7975

Epoch 00238: val_mDice did not improve from 0.80052
Epoch 239/300
 - 21s - loss: 0.5641 - acc: 0.9714 - mDice: 0.8079 - val_loss: 0.6640 - val_acc: 0.9801 - val_mDice: 0.7984

Epoch 00239: val_mDice did not improve from 0.80052
Epoch 240/300
 - 22s - loss: 0.5628 - acc: 0.9714 - mDice: 0.8085 - val_loss: 0.6507 - val_acc: 0.9802 - val_mDice: 0.7964

Epoch 00240: val_mDice did not improve from 0.80052
Epoch 241/300
 - 22s - loss: 0.5645 - acc: 0.9714 - mDice: 0.8078 - val_loss: 0.6602 - val_acc: 0.9809 - val_mDice: 0.7993

Epoch 00241: val_mDice did not improve from 0.80052
Epoch 242/300
 - 23s - loss: 0.5628 - acc: 0.9714 - mDice: 0.8084 - val_loss: 0.6639 - val_acc: 0.9803 - val_mDice: 0.7992

Epoch 00242: val_mDice did not improve from 0.80052
Epoch 243/300
 - 22s - loss: 0.5628 - acc: 0.9714 - mDice: 0.8082 - val_loss: 0.6564 - val_acc: 0.9808 - val_mDice: 0.8020

Epoch 00243: val_mDice improved from 0.80052 to 0.80196, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 244/300
 - 22s - loss: 0.5626 - acc: 0.9714 - mDice: 0.8084 - val_loss: 0.6603 - val_acc: 0.9805 - val_mDice: 0.7991

Epoch 00244: val_mDice did not improve from 0.80196
Epoch 245/300
 - 21s - loss: 0.5622 - acc: 0.9714 - mDice: 0.8087 - val_loss: 0.6627 - val_acc: 0.9804 - val_mDice: 0.8012

Epoch 00245: val_mDice did not improve from 0.80196
Epoch 246/300
 - 21s - loss: 0.5606 - acc: 0.9715 - mDice: 0.8092 - val_loss: 0.6512 - val_acc: 0.9807 - val_mDice: 0.8012

Epoch 00246: val_mDice did not improve from 0.80196
Epoch 247/300
 - 22s - loss: 0.5615 - acc: 0.9714 - mDice: 0.8089 - val_loss: 0.6392 - val_acc: 0.9807 - val_mDice: 0.8029

Epoch 00247: val_mDice improved from 0.80196 to 0.80292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 248/300
 - 21s - loss: 0.5597 - acc: 0.9715 - mDice: 0.8094 - val_loss: 0.6514 - val_acc: 0.9806 - val_mDice: 0.8023

Epoch 00248: val_mDice did not improve from 0.80292
Epoch 249/300
 - 21s - loss: 0.5609 - acc: 0.9715 - mDice: 0.8090 - val_loss: 0.6555 - val_acc: 0.9800 - val_mDice: 0.8020

Epoch 00249: val_mDice did not improve from 0.80292
Epoch 250/300
 - 21s - loss: 0.5608 - acc: 0.9715 - mDice: 0.8093 - val_loss: 0.6548 - val_acc: 0.9805 - val_mDice: 0.8006

Epoch 00250: val_mDice did not improve from 0.80292
Epoch 251/300
 - 22s - loss: 0.5598 - acc: 0.9715 - mDice: 0.8095 - val_loss: 0.6472 - val_acc: 0.9804 - val_mDice: 0.8020

Epoch 00251: val_mDice did not improve from 0.80292
Epoch 252/300
 - 21s - loss: 0.5596 - acc: 0.9715 - mDice: 0.8095 - val_loss: 0.6536 - val_acc: 0.9807 - val_mDice: 0.8019

Epoch 00252: val_mDice did not improve from 0.80292
Epoch 253/300
 - 21s - loss: 0.5597 - acc: 0.9715 - mDice: 0.8093 - val_loss: 0.6616 - val_acc: 0.9800 - val_mDice: 0.8002

Epoch 00253: val_mDice did not improve from 0.80292
Epoch 254/300
 - 21s - loss: 0.5598 - acc: 0.9715 - mDice: 0.8096 - val_loss: 0.6575 - val_acc: 0.9804 - val_mDice: 0.8011

Epoch 00254: val_mDice did not improve from 0.80292
Epoch 255/300
 - 21s - loss: 0.5586 - acc: 0.9715 - mDice: 0.8099 - val_loss: 0.6656 - val_acc: 0.9807 - val_mDice: 0.7977

Epoch 00255: val_mDice did not improve from 0.80292
Epoch 256/300
 - 22s - loss: 0.5582 - acc: 0.9715 - mDice: 0.8100 - val_loss: 0.6499 - val_acc: 0.9810 - val_mDice: 0.8005

Epoch 00256: val_mDice did not improve from 0.80292
Epoch 257/300
 - 21s - loss: 0.5586 - acc: 0.9715 - mDice: 0.8101 - val_loss: 0.6470 - val_acc: 0.9808 - val_mDice: 0.8024

Epoch 00257: val_mDice did not improve from 0.80292
Epoch 258/300
 - 21s - loss: 0.5562 - acc: 0.9715 - mDice: 0.8106 - val_loss: 0.6615 - val_acc: 0.9805 - val_mDice: 0.8008

Epoch 00258: val_mDice did not improve from 0.80292
Epoch 259/300
 - 22s - loss: 0.5569 - acc: 0.9715 - mDice: 0.8104 - val_loss: 0.6425 - val_acc: 0.9803 - val_mDice: 0.8013

Epoch 00259: val_mDice did not improve from 0.80292
Epoch 260/300
 - 22s - loss: 0.5572 - acc: 0.9715 - mDice: 0.8104 - val_loss: 0.6430 - val_acc: 0.9808 - val_mDice: 0.8033

Epoch 00260: val_mDice improved from 0.80292 to 0.80328, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 261/300
 - 21s - loss: 0.5572 - acc: 0.9715 - mDice: 0.8103 - val_loss: 0.6542 - val_acc: 0.9808 - val_mDice: 0.8035

Epoch 00261: val_mDice improved from 0.80328 to 0.80351, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 262/300
 - 21s - loss: 0.5574 - acc: 0.9715 - mDice: 0.8102 - val_loss: 0.6551 - val_acc: 0.9808 - val_mDice: 0.8015

Epoch 00262: val_mDice did not improve from 0.80351
Epoch 263/300
 - 22s - loss: 0.5551 - acc: 0.9715 - mDice: 0.8109 - val_loss: 0.6634 - val_acc: 0.9801 - val_mDice: 0.8017

Epoch 00263: val_mDice did not improve from 0.80351
Epoch 264/300
 - 21s - loss: 0.5541 - acc: 0.9716 - mDice: 0.8114 - val_loss: 0.6571 - val_acc: 0.9804 - val_mDice: 0.8017

Epoch 00264: val_mDice did not improve from 0.80351
Epoch 265/300
 - 21s - loss: 0.5547 - acc: 0.9716 - mDice: 0.8113 - val_loss: 0.6405 - val_acc: 0.9809 - val_mDice: 0.8025

Epoch 00265: val_mDice did not improve from 0.80351
Epoch 266/300
 - 21s - loss: 0.5537 - acc: 0.9716 - mDice: 0.8115 - val_loss: 0.6494 - val_acc: 0.9808 - val_mDice: 0.8011

Epoch 00266: val_mDice did not improve from 0.80351
Epoch 267/300
 - 23s - loss: 0.5540 - acc: 0.9716 - mDice: 0.8116 - val_loss: 0.6455 - val_acc: 0.9810 - val_mDice: 0.8020

Epoch 00267: val_mDice did not improve from 0.80351
Epoch 268/300
 - 22s - loss: 0.5551 - acc: 0.9715 - mDice: 0.8112 - val_loss: 0.6493 - val_acc: 0.9808 - val_mDice: 0.8033

Epoch 00268: val_mDice did not improve from 0.80351
Epoch 269/300
 - 22s - loss: 0.5548 - acc: 0.9715 - mDice: 0.8112 - val_loss: 0.6599 - val_acc: 0.9803 - val_mDice: 0.8023

Epoch 00269: val_mDice did not improve from 0.80351
Epoch 270/300
 - 23s - loss: 0.5518 - acc: 0.9716 - mDice: 0.8123 - val_loss: 0.6423 - val_acc: 0.9809 - val_mDice: 0.8037

Epoch 00270: val_mDice improved from 0.80351 to 0.80369, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 271/300
 - 23s - loss: 0.5524 - acc: 0.9716 - mDice: 0.8122 - val_loss: 0.6539 - val_acc: 0.9805 - val_mDice: 0.8018

Epoch 00271: val_mDice did not improve from 0.80369
Epoch 272/300
 - 22s - loss: 0.5522 - acc: 0.9716 - mDice: 0.8121 - val_loss: 0.6480 - val_acc: 0.9804 - val_mDice: 0.8028

Epoch 00272: val_mDice did not improve from 0.80369
Epoch 273/300
 - 23s - loss: 0.5524 - acc: 0.9716 - mDice: 0.8122 - val_loss: 0.6536 - val_acc: 0.9803 - val_mDice: 0.7998

Epoch 00273: val_mDice did not improve from 0.80369
Epoch 274/300
 - 23s - loss: 0.5534 - acc: 0.9716 - mDice: 0.8118 - val_loss: 0.6481 - val_acc: 0.9809 - val_mDice: 0.8032

Epoch 00274: val_mDice did not improve from 0.80369
Epoch 275/300
 - 22s - loss: 0.5507 - acc: 0.9716 - mDice: 0.8127 - val_loss: 0.6421 - val_acc: 0.9809 - val_mDice: 0.8036

Epoch 00275: val_mDice did not improve from 0.80369
Epoch 276/300
 - 22s - loss: 0.5506 - acc: 0.9716 - mDice: 0.8126 - val_loss: 0.6524 - val_acc: 0.9805 - val_mDice: 0.8031

Epoch 00276: val_mDice did not improve from 0.80369
Epoch 277/300
 - 21s - loss: 0.5512 - acc: 0.9716 - mDice: 0.8124 - val_loss: 0.6471 - val_acc: 0.9809 - val_mDice: 0.8035

Epoch 00277: val_mDice did not improve from 0.80369
Epoch 278/300
 - 22s - loss: 0.5495 - acc: 0.9717 - mDice: 0.8129 - val_loss: 0.6555 - val_acc: 0.9805 - val_mDice: 0.8020

Epoch 00278: val_mDice did not improve from 0.80369
Epoch 279/300
 - 22s - loss: 0.5498 - acc: 0.9717 - mDice: 0.8129 - val_loss: 0.6554 - val_acc: 0.9804 - val_mDice: 0.8021

Epoch 00279: val_mDice did not improve from 0.80369
Epoch 280/300
 - 21s - loss: 0.5482 - acc: 0.9717 - mDice: 0.8135 - val_loss: 0.6602 - val_acc: 0.9811 - val_mDice: 0.8020

Epoch 00280: val_mDice did not improve from 0.80369
Epoch 281/300
 - 23s - loss: 0.5493 - acc: 0.9717 - mDice: 0.8130 - val_loss: 0.6590 - val_acc: 0.9804 - val_mDice: 0.8035

Epoch 00281: val_mDice did not improve from 0.80369
Epoch 282/300
 - 22s - loss: 0.5488 - acc: 0.9717 - mDice: 0.8132 - val_loss: 0.6451 - val_acc: 0.9804 - val_mDice: 0.8022

Epoch 00282: val_mDice did not improve from 0.80369
Epoch 283/300
 - 22s - loss: 0.5494 - acc: 0.9716 - mDice: 0.8132 - val_loss: 0.6576 - val_acc: 0.9802 - val_mDice: 0.8022

Epoch 00283: val_mDice did not improve from 0.80369
Epoch 284/300
 - 22s - loss: 0.5481 - acc: 0.9716 - mDice: 0.8135 - val_loss: 0.6552 - val_acc: 0.9810 - val_mDice: 0.8036

Epoch 00284: val_mDice did not improve from 0.80369
Epoch 285/300
 - 22s - loss: 0.5486 - acc: 0.9717 - mDice: 0.8135 - val_loss: 0.6478 - val_acc: 0.9810 - val_mDice: 0.8028

Epoch 00285: val_mDice did not improve from 0.80369
Epoch 286/300
 - 21s - loss: 0.5468 - acc: 0.9717 - mDice: 0.8139 - val_loss: 0.6534 - val_acc: 0.9808 - val_mDice: 0.8027

Epoch 00286: val_mDice did not improve from 0.80369
Epoch 287/300
 - 22s - loss: 0.5472 - acc: 0.9717 - mDice: 0.8137 - val_loss: 0.6512 - val_acc: 0.9810 - val_mDice: 0.8010

Epoch 00287: val_mDice did not improve from 0.80369
Epoch 288/300
 - 22s - loss: 0.5463 - acc: 0.9717 - mDice: 0.8141 - val_loss: 0.6351 - val_acc: 0.9812 - val_mDice: 0.8028

Epoch 00288: val_mDice did not improve from 0.80369
Epoch 289/300
 - 23s - loss: 0.5459 - acc: 0.9717 - mDice: 0.8141 - val_loss: 0.6568 - val_acc: 0.9809 - val_mDice: 0.8031

Epoch 00289: val_mDice did not improve from 0.80369
Epoch 290/300
 - 23s - loss: 0.5465 - acc: 0.9717 - mDice: 0.8140 - val_loss: 0.6489 - val_acc: 0.9807 - val_mDice: 0.8032

Epoch 00290: val_mDice did not improve from 0.80369
Epoch 291/300
 - 22s - loss: 0.5465 - acc: 0.9717 - mDice: 0.8140 - val_loss: 0.6388 - val_acc: 0.9812 - val_mDice: 0.8045

Epoch 00291: val_mDice improved from 0.80369 to 0.80449, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 292/300
 - 23s - loss: 0.5457 - acc: 0.9717 - mDice: 0.8143 - val_loss: 0.6406 - val_acc: 0.9808 - val_mDice: 0.8043

Epoch 00292: val_mDice did not improve from 0.80449
Epoch 293/300
 - 21s - loss: 0.5452 - acc: 0.9717 - mDice: 0.8144 - val_loss: 0.6523 - val_acc: 0.9810 - val_mDice: 0.8046

Epoch 00293: val_mDice improved from 0.80449 to 0.80460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 294/300
 - 22s - loss: 0.5445 - acc: 0.9718 - mDice: 0.8149 - val_loss: 0.6605 - val_acc: 0.9808 - val_mDice: 0.8037

Epoch 00294: val_mDice did not improve from 0.80460
Epoch 295/300
 - 22s - loss: 0.5460 - acc: 0.9717 - mDice: 0.8142 - val_loss: 0.6520 - val_acc: 0.9811 - val_mDice: 0.8012

Epoch 00295: val_mDice did not improve from 0.80460
Epoch 296/300
 - 21s - loss: 0.5444 - acc: 0.9717 - mDice: 0.8145 - val_loss: 0.6487 - val_acc: 0.9807 - val_mDice: 0.8051

Epoch 00296: val_mDice improved from 0.80460 to 0.80509, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 297/300
 - 21s - loss: 0.5434 - acc: 0.9718 - mDice: 0.8153 - val_loss: 0.6593 - val_acc: 0.9803 - val_mDice: 0.8047

Epoch 00297: val_mDice did not improve from 0.80509
Epoch 298/300
 - 23s - loss: 0.5450 - acc: 0.9717 - mDice: 0.8145 - val_loss: 0.6411 - val_acc: 0.9810 - val_mDice: 0.8034

Epoch 00298: val_mDice did not improve from 0.80509
Epoch 299/300
 - 22s - loss: 0.5453 - acc: 0.9717 - mDice: 0.8144 - val_loss: 0.6577 - val_acc: 0.9807 - val_mDice: 0.8038

Epoch 00299: val_mDice did not improve from 0.80509
Epoch 300/300
 - 23s - loss: 0.5435 - acc: 0.9718 - mDice: 0.8150 - val_loss: 0.6561 - val_acc: 0.9807 - val_mDice: 0.8037

Epoch 00300: val_mDice did not improve from 0.80509
{'val_loss': [5.362249501763958, 4.2748654852174734, 2.9468969384284867, 2.032704618695664, 1.433733540855042, 1.175737696151211, 1.1164557313265866, 1.0181050378165832, 0.9617527739642417, 0.903244237377219, 0.925596053877922, 0.8394445744279313, 0.847721177012953, 0.8061984014021207, 0.8110539790702193, 0.773580595238568, 0.7873841658846973, 0.7987533027995123, 0.7650007225879251, 0.7640147368385367, 0.7537492538151676, 0.7616199340722333, 0.7446189347195299, 0.7417825811529812, 0.7375164844401895, 0.730634479081794, 0.7241901947210913, 0.7392609984907386, 0.7246400317917131, 0.725757897308428, 0.7234750361475226, 0.7313900863471097, 0.7222803647387518, 0.7138895208704962, 0.7130581365056234, 0.7112259746414341, 0.7013985281937742, 0.6972519861508722, 0.7087029835132703, 0.7059861490987751, 0.7156724962469649, 0.6987202837042612, 0.7038012110207179, 0.7017681512930621, 0.694222582529669, 0.7121431317231427, 0.6871197966680135, 0.7044460034533723, 0.7063275600132877, 0.6959488008120288, 0.6853633486244777, 0.7035085469076078, 0.6929374529074316, 0.6989519473624556, 0.7060680536374654, 0.6984113136382952, 0.6872083634546359, 0.7073260725361027, 0.6873995053441557, 0.6910477003006086, 0.6863932446257709, 0.6776937466778167, 0.6861785715573454, 0.6802669098115948, 0.6823480827351139, 0.6908400050581318, 0.687021014216828, 0.6873300663412434, 0.6813705171624275, 0.6925653223305532, 0.6843740397120175, 0.7160966449404416, 0.6863755608258182, 0.6807081938198168, 0.7059422833462284, 0.6796564698627551, 0.6857755669992264, 0.6899591970117125, 0.6871108367835006, 0.6826046190849723, 0.6898828597917949, 0.6850643019153647, 0.6893951313136375, 0.6924415316483746, 0.6804179206286392, 0.6864470545559713, 0.6803588291553602, 0.6908827971105707, 0.6821901545132676, 0.6904255053768419, 0.682665570141518, 0.6789055317640305, 0.6861719752419485, 0.6777014154685687, 0.6702295147801098, 0.6859277582740131, 0.6728494357164592, 0.6839039603324786, 0.6870442592117885, 0.6876848254301776, 0.6827405756467009, 0.675475391214841, 0.6777013181007072, 0.6885156059918338, 0.6798184032309545, 0.6844107451504224, 0.6746140273466502, 0.6803520924016221, 0.6740483408921385, 0.6852776657228601, 0.6866188853570859, 0.6786211892350079, 0.6756244154825602, 0.6777317258593154, 0.6820059118613805, 0.6853977801048592, 0.684458754242283, 0.6835007879832019, 0.6856508136612095, 0.6783399583950435, 0.6776267508529636, 0.6859356026126914, 0.6791857142154485, 0.6767298419998117, 0.6764944618287152, 0.6791311339156269, 0.6828077560010022, 0.6783887980735466, 0.686064696475251, 0.6791645090465677, 0.6844120378771873, 0.6817959302092251, 0.6870384212226084, 0.6776764711288557, 0.6775982694266593, 0.682843900298419, 0.6905202363451867, 0.6774457372622947, 0.6779763329110734, 0.6842418435501726, 0.6808907405970848, 0.6769214378644343, 0.6728186176656044, 0.6894065737724304, 0.6824361873816137, 0.6770308074885851, 0.6798152552075583, 0.6880827491005806, 0.6774459207711154, 0.6784484586487077, 0.6722373607223981, 0.6707245123712984, 0.6840613911004916, 0.6736443001930028, 0.6905419430504106, 0.673279489964655, 0.6744291606831224, 0.6720261347212203, 0.6806261715007155, 0.6870701706981006, 0.6772028028148495, 0.6786442037722836, 0.6694721942895079, 0.6763231731849174, 0.6715046330674054, 0.6773716325629248, 0.6780851013856392, 0.6753898810033929, 0.6677453899628496, 0.6830740650630978, 0.6811266131188771, 0.6648733470946142, 0.6670211667883886, 0.6729699891723998, 0.6780522759646586, 0.6762116437088953, 0.6798072210321687, 0.6740736604145129, 0.672893789328941, 0.6724738480701838, 0.6751944720745087, 0.6660028667482611, 0.6644402877108692, 0.6683147092388101, 0.6589829133797999, 0.6795254584452878, 0.6626897229723734, 0.6690524218425359, 0.6638418660588461, 0.6634313415174615, 0.6658384297808556, 0.6603975787962952, 0.661787092073323, 0.661953918737908, 0.6632334583834426, 0.6681843318351327, 0.6580088461098605, 0.6579999145987916, 0.6665123341834709, 0.6543730174841946, 0.6663773696716517, 0.6587505861096186, 0.6546983402477552, 0.6577023343272406, 0.6543810443110663, 0.658111865798088, 0.655829667636793, 0.6633894355737999, 0.6619120885248053, 0.6665580701338102, 0.6547277690204856, 0.6568117668367413, 0.6569036031014299, 0.6705276712979356, 0.6637341502594621, 0.652922170619442, 0.6574688161889167, 0.6762603213117547, 0.6612374798892295, 0.6575196736479458, 0.6592291491488887, 0.6663151705918247, 0.6562925356708161, 0.657373870276425, 0.6472258090156399, 0.6559424239070448, 0.6664103955438693, 0.6565537191417119, 0.6666535549376109, 0.66784434122582, 0.654780387878418, 0.6650149085342068, 0.6586898075391169, 0.6553368811329751, 0.6635848255190131, 0.6637938688062641, 0.6572139406040923, 0.6570277385515709, 0.6640467360003354, 0.6507230421451673, 0.6601649806515811, 0.6638760568752681, 0.656378860547118, 0.6602790225042056, 0.6627009682459374, 0.651223797912467, 0.6392459857137236, 0.6514174738158919, 0.6555111726669416, 0.6547618308295943, 0.6471525967529376, 0.653553281132489, 0.6615584006864731, 0.6575131065224948, 0.6655887622539312, 0.649872121745593, 0.6470133637728757, 0.6615132446566673, 0.6425135613304295, 0.6429500910517287, 0.6541961306986743, 0.6551378553044306, 0.6633583188873448, 0.6571378281263456, 0.6405050078891728, 0.6493548652897142, 0.6454726600483672, 0.6493410014942901, 0.6598982627261175, 0.6422554321893273, 0.6538826112877832, 0.6479963243824162, 0.6535687585399576, 0.648108351108146, 0.6420706561575197, 0.6523976499495441, 0.6470796772061962, 0.6555333333472683, 0.6554067832966374, 0.6602043821795346, 0.6590276252325267, 0.6451210093824831, 0.6576047423359466, 0.6551895917278446, 0.6477659957049644, 0.6533806019858138, 0.6511744401634556, 0.6351266115495603, 0.6568005478953662, 0.6489420265775837, 0.6387662111896358, 0.6405643753401221, 0.6522598342127996, 0.6604525075383383, 0.6519911530083173, 0.6486637400437708, 0.6593223271304613, 0.6411201655864716, 0.6577183042895304, 0.6561075085646486], 'val_acc': [0.9134473159705123, 0.9134451318277071, 0.9157162304610422, 0.9409781633174583, 0.9588549651511727, 0.9650960144114821, 0.9670027269892496, 0.969594511267257, 0.9715612755246359, 0.9730155431244472, 0.9740995764732361, 0.974104302794966, 0.9745724184872353, 0.9750223494555852, 0.9754788275450876, 0.9758617886125225, 0.9764496549351575, 0.9760215882569143, 0.9775868303155246, 0.9766829830326446, 0.9769279732279581, 0.9765457625258459, 0.9774350589268828, 0.9773152870674656, 0.9768537148220898, 0.9770823019008114, 0.9765665024927218, 0.9763797623653935, 0.977311275184971, 0.9771846067415525, 0.9769086764283377, 0.9772745076924154, 0.9783159751598149, 0.9779308250505631, 0.978234785876862, 0.9788102940337299, 0.9785365597025989, 0.9783454557804212, 0.9786188243186638, 0.9789588202352393, 0.9782471681294376, 0.9789169496052885, 0.9784987025881466, 0.9792405624095708, 0.9790545346802229, 0.9781507041356335, 0.9792722279894842, 0.9781750865178566, 0.9792973435904881, 0.9789810172499043, 0.9796518948796678, 0.9786526801651472, 0.9789861142635345, 0.9790137920477618, 0.9797392565093629, 0.979517207162021, 0.9789868393172957, 0.9796882916803229, 0.9800595955489433, 0.9787087334345465, 0.9796864786376692, 0.9794917294423874, 0.9795579840875652, 0.9797425429298453, 0.9795871078968048, 0.9793865284691118, 0.9791794015120153, 0.9797745747108982, 0.9794385963923311, 0.9796890261238569, 0.9791746821305524, 0.9791146051393796, 0.9794356700492232, 0.9794793529053257, 0.9781598167060173, 0.979685027305394, 0.9796075004420869, 0.9798663185067373, 0.9790167045103361, 0.9796755583318946, 0.9800326538412538, 0.9791087871544981, 0.9797840445008996, 0.9788641578530612, 0.9799456522889334, 0.9796020266127913, 0.9803573464694089, 0.9799489350351569, 0.9797389029640041, 0.9793188261659178, 0.9794797150239553, 0.9801018201325038, 0.9794025449720147, 0.9796475363920812, 0.9800475888872799, 0.979779304298636, 0.9796653683871439, 0.9791590175400041, 0.9790727549219784, 0.979754191147138, 0.9798637738783066, 0.9798768579143368, 0.9793188375969456, 0.9794950056565951, 0.9790629156648296, 0.9794382138611519, 0.9797672747749172, 0.9799303714543173, 0.9798772147257034, 0.9795095745014818, 0.9799627747437726, 0.9801680784519404, 0.9797206953780292, 0.9798357278516848, 0.9793359339237213, 0.9799598614646964, 0.9792179832719776, 0.9802102948704811, 0.9799969759705949, 0.9795467032961649, 0.9801782643141812, 0.9796500761215001, 0.9796267837694247, 0.9795659939720206, 0.9795244972999781, 0.9796631773040719, 0.9791641190443954, 0.97958310826184, 0.9792984454599145, 0.9798138941804023, 0.9798768619968466, 0.9795230402521891, 0.979764021422765, 0.9796238778388664, 0.9793439462576827, 0.979584908240462, 0.9792252627954091, 0.9792165298984475, 0.9797694691239971, 0.9797560029650387, 0.9801225719386584, 0.9790629283206104, 0.9800421268972632, 0.9790938549662289, 0.9799718632273478, 0.9796755611896515, 0.9795419692176662, 0.9787520578462784, 0.9798954243529333, 0.9795849172219838, 0.9798448367478096, 0.9797571040179631, 0.9800949068102118, 0.9798794054005244, 0.9790883946092162, 0.9802317721386479, 0.9791703224182129, 0.9797480000208502, 0.9800803379653251, 0.9794429646779413, 0.9802434199476895, 0.9798026015497234, 0.9801247442421848, 0.9798375490593584, 0.9798775935826236, 0.9803275204684636, 0.9791419212132284, 0.9798899558309007, 0.9802652654582507, 0.9791510199030785, 0.9801768105324, 0.9801924473618808, 0.9801014563808702, 0.9798237162910096, 0.9800031736288985, 0.9796679179145865, 0.9799638582419042, 0.9793584926487648, 0.979763290245239, 0.979622775561189, 0.9798648524774264, 0.9807170192672782, 0.9800614236968838, 0.9798874132437249, 0.9803413413975337, 0.9792300042224257, 0.980130572433341, 0.9800166553013945, 0.9799656888393506, 0.9798714114378576, 0.9799052444222855, 0.9802598124497557, 0.9803227912889768, 0.9804177843544581, 0.9804552977215754, 0.9805626718148793, 0.9802274026282846, 0.9804225188412078, 0.9802339619969669, 0.980539375788545, 0.980484762828644, 0.9806183548006293, 0.9808447859058641, 0.9804450873642752, 0.980233952198943, 0.9798462652180293, 0.9807595896394286, 0.9806773221656068, 0.9800392083109242, 0.9808076428223963, 0.9808058285549895, 0.9805509995107782, 0.9805105994008991, 0.980128043318448, 0.9800501306579538, 0.9806449409217051, 0.9802980255590726, 0.9797563699826802, 0.9806613207679905, 0.9801156549421075, 0.9807916304020032, 0.9804541917696391, 0.9805360754875287, 0.9805830508878787, 0.9809335980513324, 0.980876820136423, 0.9807450305925657, 0.9807930960230631, 0.9808673421814017, 0.9801789995742171, 0.9806736936307934, 0.9808757247990125, 0.9799314598514609, 0.9807068170749977, 0.9804250695934035, 0.9802634422093222, 0.9806445763535696, 0.9804337931005922, 0.9800912627618606, 0.9802437894148369, 0.980875358597873, 0.9803278723808184, 0.980761417787369, 0.9805422711045775, 0.9804261673803198, 0.9807410313658518, 0.9806882514529032, 0.980616179639346, 0.9800031699546395, 0.980540837327095, 0.9804483627619809, 0.980720288132968, 0.9800133652066532, 0.980407228208568, 0.9806977020551081, 0.980994750783868, 0.9807938190355693, 0.98047166899459, 0.980266711075012, 0.980761414521361, 0.9807519573871404, 0.9807610544439864, 0.9800745142649298, 0.980446551352331, 0.9809408824737758, 0.9808378623772974, 0.9809761810792635, 0.9807741560348092, 0.980306771111815, 0.9808538703069295, 0.9804596443698831, 0.9804316281455837, 0.9802816473457912, 0.9809474304114303, 0.9808669963928118, 0.9805382702448596, 0.9809321442695513, 0.9804986127435344, 0.9803515325670373, 0.9811316250938259, 0.980418519206243, 0.9804381744502342, 0.9801640771839717, 0.9809820129446787, 0.9810173144079235, 0.9808338598845756, 0.9810398992610304, 0.9812164306640625, 0.9808615172562534, 0.9806773282893716, 0.9812040565765068, 0.9808349466487153, 0.9809991117209604, 0.9808280321016704, 0.9810970315378006, 0.9807071840926392, 0.9803074973903291, 0.9810391684917554, 0.9806795112074238, 0.9806853385820781], 'val_mDice': [0.012891571745531608, 0.03318229636611187, 0.1330936218369497, 0.3127617248117107, 0.48840069362561994, 0.5819929585064927, 0.6132628958519191, 0.6522966394685719, 0.6695330873744129, 0.6894364499882476, 0.6940328936054282, 0.714166913130512, 0.7159649917524155, 0.72369633023053, 0.7285069745697387, 0.7359362575289321, 0.7330925648343073, 0.7336150015870185, 0.7410308408410582, 0.7417350333847411, 0.7433077852203421, 0.7442032041615003, 0.7493043019346994, 0.7485398809387259, 0.7512582609914753, 0.7556196120503831, 0.755960560416522, 0.7531182173996755, 0.7569360831012465, 0.7602427209893318, 0.7578615342101006, 0.7582135890444665, 0.7563630391473639, 0.7627767067249507, 0.7616558744482798, 0.7618466313571146, 0.7644714406908375, 0.7662001399144734, 0.7635519614774887, 0.7644571629289079, 0.7638909253355575, 0.7663974868108149, 0.7652342727739517, 0.7668099983097756, 0.7678329724155061, 0.765924434139304, 0.7708510862637873, 0.7675477792955425, 0.7656380032023339, 0.7728012330728035, 0.76992695952115, 0.7676912733953293, 0.7707676928337306, 0.7713238796959184, 0.7664530971278883, 0.7704689947709645, 0.7740698942582901, 0.7661680717990823, 0.7697885918290648, 0.7734671030959038, 0.7723680946924915, 0.775327887437115, 0.7737552148022063, 0.7727064984301998, 0.7753469964412794, 0.7717205710607032, 0.7702793315665363, 0.7732387462707415, 0.7753223184853384, 0.773171944160984, 0.7747360455663237, 0.7680211499945758, 0.7741708400314802, 0.7775694527854659, 0.7748775796530998, 0.7788665964178842, 0.7764639646223147, 0.775483442904198, 0.7775149075952295, 0.7784071834120032, 0.7754890294107672, 0.7771540998596035, 0.7742371763268562, 0.776419851469667, 0.7779487244886895, 0.780613947404574, 0.7766330642242955, 0.7780766180933338, 0.7774981684880714, 0.7803963214567263, 0.7802547677739026, 0.7795017173845474, 0.7779778846322674, 0.77980691072059, 0.7806000484995645, 0.7812655466060116, 0.7818214023766452, 0.7828532404279056, 0.7804252125629006, 0.7815459819689189, 0.7817943977983031, 0.7827696857387072, 0.7823323521712054, 0.7799512939910366, 0.7837719778492026, 0.7814957323956163, 0.7823951901638344, 0.7819184220000489, 0.7820105969089352, 0.7832360892263177, 0.78235423197485, 0.7833350504914375, 0.7847222889939399, 0.7840610134274992, 0.7839670601772936, 0.7825733713091236, 0.7847890894706935, 0.781681916893345, 0.7833283180243349, 0.7841635154534693, 0.7823882429567102, 0.7813141166347347, 0.7860969766362073, 0.7858390440679577, 0.7852940044990958, 0.7833753052639635, 0.7856505692821659, 0.7862011361612032, 0.7831696288226402, 0.7846044322399244, 0.7866886968482031, 0.7855894079763596, 0.7835942772153306, 0.7857862584394951, 0.7852853136519863, 0.787527812670355, 0.7845613499210305, 0.7865410667576201, 0.7884480459232853, 0.787561395805176, 0.786425865676305, 0.7886465693989845, 0.7867742887098496, 0.7864616378529431, 0.7864950968794626, 0.786330277789129, 0.7874014316356346, 0.7859944092900786, 0.7876842785371493, 0.7875667443014172, 0.7863032050328712, 0.7870290981580134, 0.7867980109502192, 0.791017573173732, 0.7885965627350219, 0.7882651160024616, 0.7909555859761696, 0.7886353812805594, 0.7883967181591138, 0.7879614397271039, 0.7876120948628204, 0.7892289912863953, 0.7875673068712835, 0.7885362877421183, 0.7895272414978236, 0.7857361737995932, 0.791752180008039, 0.790612179939061, 0.789494756149919, 0.7876825177506225, 0.7890978831134431, 0.7925846413390277, 0.7891579033577278, 0.7898427448044084, 0.7897057790462285, 0.7902715785862648, 0.7913872256670913, 0.7936889378175344, 0.79178961254146, 0.7923906988477054, 0.794854364574772, 0.7904997745605364, 0.7926946825360599, 0.7908578016986586, 0.7960624976517403, 0.7939289502085072, 0.7948329750805685, 0.7948542833328247, 0.7936473766418353, 0.7925771119659895, 0.7937345500678232, 0.7949982971361239, 0.7952523758150127, 0.7964244015412788, 0.7955658064313131, 0.7936052459560029, 0.7955077980479149, 0.7974491164292374, 0.7931208345171523, 0.7959431593548761, 0.7939817003191334, 0.7948614391562057, 0.7961518968621345, 0.7965689681164206, 0.796489469809075, 0.7972584775049393, 0.797552079778828, 0.7955218524965522, 0.7966167534867378, 0.7970953302024162, 0.7975459233538745, 0.7950559891250035, 0.7987813002442661, 0.7965868756379166, 0.7975428496321587, 0.799223683468283, 0.7979493092184198, 0.7975620333462545, 0.7962376083413215, 0.7974450682124047, 0.7974192810385194, 0.7982181367808825, 0.7991168156062087, 0.79781856438885, 0.7991814662332404, 0.8005192275733164, 0.7955235455134143, 0.7996373074511959, 0.7958521700068696, 0.7983187502377653, 0.7972754897320107, 0.7985250958024639, 0.7989637945612816, 0.7994738653914569, 0.7998378603425744, 0.7976487699436815, 0.7989579598381095, 0.7975405801648963, 0.7984022040889688, 0.7963615913097173, 0.7993181486652322, 0.799162407443948, 0.8019588638658393, 0.7991155267578282, 0.8012098808811136, 0.8012179115863696, 0.8029239067476089, 0.8023439874387768, 0.8020400302867366, 0.8005713171338382, 0.8020142218837999, 0.8018698120770389, 0.8001601781747113, 0.8010624030681506, 0.7977122827751996, 0.800517981183039, 0.8023960623022628, 0.8008093980893697, 0.8013470450492755, 0.8032781694033374, 0.8035129610806295, 0.8015011691883819, 0.8017039009153026, 0.8017128473275328, 0.8025119292409453, 0.8011489653424041, 0.8020302681890252, 0.8032739889131834, 0.802338471559629, 0.8036878823417507, 0.8018427176834786, 0.8028313088906954, 0.7998357043690878, 0.8032175000399759, 0.8035784702594966, 0.8030857394819391, 0.8035083788714997, 0.8020140006117624, 0.80214203138874, 0.8020172074233016, 0.8035108059236448, 0.8021666566803031, 0.8021943605925939, 0.8036450278269102, 0.8028175630798079, 0.8027159939073536, 0.8010032254539124, 0.8027722651827826, 0.8031105701237509, 0.8032019971168205, 0.8044902829274739, 0.8043424850457335, 0.8046015999088548, 0.8036523722622493, 0.8012327894772568, 0.8050851185027867, 0.8046525684121537, 0.8033765901441443, 0.8038391572971867, 0.8036657886145866], 'loss': [48.296333821872736, 5.21465581226041, 3.9635201036007963, 3.145280842982382, 2.5302896041905507, 2.081393533617314, 1.815519249598959, 1.6417696972893912, 1.4940984225473068, 1.3824298323777862, 1.3007562012234402, 1.226805131632688, 1.1819767248108275, 1.1365193153290618, 1.0959773140704179, 1.0732882307982685, 1.0478513116414039, 1.0263900086545814, 1.0015369656416437, 0.9816228745500636, 0.9652551308865317, 0.9475225686532881, 0.9349271508548752, 0.9202844336068029, 0.9015350516712621, 0.8940732886006536, 0.8810622470138377, 0.8689127398563342, 0.8604646882994662, 0.8482662215581129, 0.8433300694923377, 0.8340249549360688, 0.8261285717602012, 0.8158724800271467, 0.8105760331130658, 0.802890317805664, 0.7975272354482025, 0.7874328712249837, 0.7845596432631272, 0.7782647191280284, 0.773682851818155, 0.7681785618516588, 0.7596207262090938, 0.7581034023805758, 0.7532853632282019, 0.7499195487089941, 0.7458961498014002, 0.7415816851642151, 0.7386651035202366, 0.7355733685243204, 0.7322647719835624, 0.727067666300188, 0.7233934353041852, 0.7208962538721039, 0.7179453275514985, 0.7146911728789452, 0.7129406926200915, 0.7092080144036149, 0.7080647721394693, 0.7029018171598058, 0.7026871537003404, 0.6995014189435715, 0.6968580535925626, 0.6944202443474292, 0.6914580408978378, 0.6925740716796225, 0.6881113631494695, 0.6869963858924927, 0.6842303664042315, 0.6824265431108759, 0.6800021158338392, 0.6803829708259199, 0.6760371748731995, 0.6767318451601061, 0.6752560858588844, 0.6725544713753908, 0.6698516459773894, 0.6717327893745515, 0.6675011365621876, 0.6665279904749487, 0.6639764452346567, 0.6642404601679542, 0.6616248812920915, 0.6600169905571025, 0.6614819302048256, 0.6579453377251085, 0.6574441116972712, 0.6545797246879276, 0.6556677689488649, 0.6544520724675208, 0.6505509509905958, 0.6526064893510969, 0.6501293511334727, 0.6479356395786909, 0.6493828144137949, 0.6471019055841745, 0.6441379550755917, 0.6431718133643102, 0.6450354432943637, 0.6422565048979278, 0.6417701164982889, 0.6401875976284037, 0.6398284565388964, 0.6385187953100118, 0.6394810546113242, 0.6365827039394777, 0.6368238359978414, 0.63559277156668, 0.6348773814295049, 0.6358140451565462, 0.6336421660514813, 0.632714639333847, 0.6309642254519138, 0.6300945441457667, 0.6309127599981941, 0.6273245228788147, 0.6282825881722197, 0.627566941741392, 0.6260266170294189, 0.6255194713445541, 0.6246917382700655, 0.6240915265624636, 0.6230003907497351, 0.6218181527665199, 0.6214894775100671, 0.6238600374664236, 0.6211653665750192, 0.6218771477973147, 0.6189585012210996, 0.619622796824182, 0.619539982491536, 0.6160342920067235, 0.6172242161078455, 0.6178815126263945, 0.6165932471381365, 0.6158353304499873, 0.6150026732372648, 0.6132990435247703, 0.6135210475221738, 0.6124814760915354, 0.6124158239567433, 0.6129123822233064, 0.6108277631217333, 0.6099664113384723, 0.610716425261961, 0.6107103632061652, 0.609953696759122, 0.6091040586329602, 0.6098796496495171, 0.6070856144829965, 0.6067600427572915, 0.6076037201403285, 0.6057560755718852, 0.60510389584163, 0.6047136556962018, 0.6036887175663734, 0.6021309706514281, 0.6034067911486194, 0.6027764059691907, 0.6011960400897995, 0.602184157751228, 0.6011170064077543, 0.6011832832192747, 0.6012150038336609, 0.5997017940657243, 0.5987998064273615, 0.59701674702633, 0.5976537569780638, 0.5974518705120465, 0.5974832057329516, 0.5963378392306803, 0.5955012981343185, 0.594307892474446, 0.5956577175114205, 0.5931328968060344, 0.592776749649877, 0.5926091567100551, 0.5928569743748636, 0.5910510788656635, 0.5895964742982306, 0.5920686022210588, 0.5907590298949911, 0.5894989313157278, 0.587744563677218, 0.5875402945651509, 0.58603036524451, 0.5864864052278494, 0.5869435918975194, 0.5860592140376502, 0.5853485507608075, 0.5849484500094916, 0.5847515344743275, 0.5829887680461237, 0.5825058414040488, 0.581418400786286, 0.582325074437314, 0.5821944772795002, 0.5804922467321079, 0.5820574838774216, 0.5803959615113127, 0.579487038717838, 0.578825830614672, 0.5798175389352801, 0.5769517890984388, 0.5777628603536104, 0.5786643431321001, 0.5770325809939809, 0.5761847413731251, 0.576558594667996, 0.575447243886417, 0.5746332181250368, 0.5753305316591867, 0.574506671914216, 0.5740769135704373, 0.5748625815839697, 0.572606609096768, 0.5720385712203339, 0.5725160572042097, 0.5720135786191912, 0.572674387861168, 0.5713407218677462, 0.570363750836494, 0.5700391509000454, 0.5694030460647643, 0.5686932355661014, 0.567866216816611, 0.5703759342266844, 0.5698054092502851, 0.5677461668226345, 0.5673933186150842, 0.567153117327387, 0.5671445962288126, 0.5657758942034866, 0.5653718193098232, 0.565646809989317, 0.5649462504940519, 0.5645861530399902, 0.5649495288902745, 0.5641260493565217, 0.5627776563757901, 0.5644732877742101, 0.5627656143423138, 0.562801750342618, 0.5626317955978325, 0.5621666115656537, 0.5605592801093285, 0.5615444363996296, 0.5596743888168761, 0.5609247327221463, 0.5608126007020833, 0.55975737438612, 0.5596035917008708, 0.5596936196857225, 0.5598254088265527, 0.5585916120449637, 0.5582083062747225, 0.5585800091386318, 0.5562400087097293, 0.5568586125724994, 0.5571784423218502, 0.557152091444208, 0.557414146126842, 0.5550837032987075, 0.554116458704721, 0.5547044860569217, 0.5537259829487305, 0.5539641911793666, 0.5550738227709822, 0.5547617959156916, 0.5518483936821124, 0.5523761718742783, 0.552190863936486, 0.5523967664769439, 0.5533879064786543, 0.5506597579120182, 0.5506177560824291, 0.5512021030582367, 0.5495148824449849, 0.5498224181596661, 0.548173052736682, 0.5492715834465318, 0.548770840266876, 0.5494098236827518, 0.5481283605268211, 0.5485934338340863, 0.5468292915782007, 0.5471892322027296, 0.546305388644538, 0.5458808851012827, 0.5464623143968367, 0.5465463311825955, 0.5457081970142866, 0.5452077010837404, 0.5444964984019548, 0.5460127322711085, 0.5444347948860022, 0.5434180878778212, 0.5449602078317682, 0.5452882020335535, 0.5434862442350173], 'acc': [0.6774545692712107, 0.8937547234628475, 0.9037456475911045, 0.9157888458788472, 0.9282746145027656, 0.9381302861720604, 0.9446664474700938, 0.9486166226001491, 0.9517517847189576, 0.954163319309382, 0.9559359572911482, 0.957644245390019, 0.9588258680110783, 0.9598177694787151, 0.9606829259710327, 0.9611759002946751, 0.9617677156687001, 0.9622666127503973, 0.9628169503985983, 0.9633315163144974, 0.9637706693818617, 0.9641620293457115, 0.9644914044547446, 0.964851279253902, 0.9652507122473358, 0.965509125638154, 0.9658097510418494, 0.9660399888123574, 0.9662894790813408, 0.9665841607709892, 0.9667129468858096, 0.9668731471102889, 0.9670174552783132, 0.9670205189122413, 0.9670923014015536, 0.9671769586751142, 0.9671088778528514, 0.967301985253322, 0.9673747703066056, 0.9675568234972785, 0.967653140156371, 0.9677503797696455, 0.9678851315340793, 0.9679045680344975, 0.9680068613023688, 0.9680380077211304, 0.9681347555307327, 0.9681728484487067, 0.9682119847941189, 0.9682573626986974, 0.9683886082340502, 0.9684245707017898, 0.9684405140666722, 0.9685241364847867, 0.9685751116903749, 0.9685470653853421, 0.9686114083738739, 0.9686753721235803, 0.9687010472272067, 0.9687886575347424, 0.9687645799326916, 0.9687943500210575, 0.9688279481616455, 0.9689037461580498, 0.9689441316730689, 0.9689230566122681, 0.9689958229100103, 0.9690333516248778, 0.9690525515452139, 0.9690650309865529, 0.9690727431910073, 0.9691147628396533, 0.9692076696183941, 0.9691665857646821, 0.9692210807697567, 0.9692400946342041, 0.969339602191429, 0.9692699918057972, 0.9693271330715265, 0.9693597432584485, 0.9694175277251992, 0.9694238189783629, 0.9694378149958138, 0.9694573443747985, 0.9694717799222833, 0.9695342652849062, 0.9695669482935423, 0.9695825958902321, 0.9695792847084387, 0.9695974296360419, 0.9696865165496172, 0.9696518033175556, 0.9697223647834996, 0.9697360334910452, 0.9697654163316334, 0.9697703462956043, 0.9698322981666373, 0.969868801336322, 0.9698490005118846, 0.9698786799939513, 0.9698739679378282, 0.9699131977018652, 0.9699233006762277, 0.9699618023552661, 0.9699586166939774, 0.9699790155412468, 0.9699900388097159, 0.9699840581591166, 0.9700289622227125, 0.9700130841043623, 0.9700819221230735, 0.9701082102648809, 0.9701120976057562, 0.9701317456137548, 0.9701168687810461, 0.9701860571799263, 0.970186324365237, 0.970174030439728, 0.9702189771219807, 0.9702054011436237, 0.970253034099693, 0.9702732675396923, 0.9702803066732958, 0.9702778574679121, 0.9702973065017085, 0.9702802666773698, 0.9702930519785259, 0.970321012256266, 0.9703561135031032, 0.9703741065450069, 0.9703574970065096, 0.9704076694036898, 0.970431263389125, 0.9704008352194035, 0.9704370340661397, 0.9704268405888935, 0.9704651069001087, 0.9704472793602726, 0.9704585887879152, 0.9704401379114427, 0.9705065290286367, 0.9705063935113049, 0.9705403612730813, 0.9705238525531592, 0.9705413332346968, 0.9705358505323712, 0.9705481650691543, 0.9705347848332665, 0.9705516819241731, 0.9706020483402317, 0.970612385013407, 0.9705881655540827, 0.9706526704021601, 0.9706645865740043, 0.9706477844874263, 0.9706883260764709, 0.9707109284685908, 0.9706516166963751, 0.9706712772320067, 0.9707239096012683, 0.9706793503592609, 0.9707134172964558, 0.9707173927674047, 0.9707268996464844, 0.9707547168575646, 0.9707340726025553, 0.9707867355713, 0.9707991012154249, 0.9708036804142914, 0.9708033231885941, 0.9708158615595048, 0.9708288831908224, 0.9708565056131537, 0.9708342533139379, 0.9708639144225261, 0.9708711474285028, 0.9708784929619817, 0.9708662396528915, 0.9708950303567105, 0.970932805263552, 0.9708677335522928, 0.9709321920070495, 0.9709472511612, 0.970942171448782, 0.9709686331389537, 0.9710104995086639, 0.970989257903023, 0.9709968488557327, 0.9710037146707841, 0.9710480063597813, 0.9709749974752427, 0.9710046576272379, 0.9710622897314286, 0.9710874424433903, 0.9710797472766316, 0.9710712541275194, 0.9711056888818775, 0.971129817427529, 0.9710954023479605, 0.9711374721243742, 0.9711212838889375, 0.9711386591384219, 0.9711385659610997, 0.9711801031335726, 0.9711741302276418, 0.9711625611553986, 0.9711765578306867, 0.9711976637552546, 0.9711823574324288, 0.971174158230248, 0.9712089753977114, 0.971196197097207, 0.9712228570376725, 0.9712415961358821, 0.971222600343285, 0.97128431030834, 0.9712834908442989, 0.9712650772031345, 0.9712702100078964, 0.9712701687336405, 0.9713101273784006, 0.9713001193020785, 0.9713294522533291, 0.9713344121330921, 0.9713304972664707, 0.9713376693406377, 0.9713088992509654, 0.9712805818767971, 0.9713569626910893, 0.9713619713743277, 0.9714078625125583, 0.9713521454154891, 0.9713823513281685, 0.9713981984936993, 0.9713910577429238, 0.9714156374557201, 0.9714142417895075, 0.9714199415924658, 0.9714306280655346, 0.9714366822560108, 0.9714007114001288, 0.9714417442614053, 0.9714392529314607, 0.9714125846373247, 0.9714218482574076, 0.9714777549264908, 0.9714377373406684, 0.9714836390330204, 0.971457591774527, 0.9714710965685812, 0.9714741929105186, 0.9714823153324716, 0.9714626324275033, 0.9714770476739604, 0.9714967004390698, 0.9714856059719347, 0.9714840290643972, 0.9715194147410717, 0.9715204594238585, 0.971527152379982, 0.971515921901419, 0.9714908820128428, 0.9715287999547448, 0.97155529495331, 0.9715615453544976, 0.9715757858661854, 0.9715585119121362, 0.9715431328853749, 0.9715251759038649, 0.9716092761961648, 0.971591533270277, 0.9715941350191977, 0.9716088156038505, 0.971564616724476, 0.9716061448480969, 0.9715990422834769, 0.9716301676887933, 0.9716602183174263, 0.9716659991664998, 0.9716882106826831, 0.9716620762133645, 0.9716681923439511, 0.9716461419662779, 0.971640343829588, 0.9716713671236361, 0.9717294806210103, 0.9716720125538348, 0.9716943074487262, 0.971687545319156, 0.9716989565449478, 0.971693896783094, 0.9717249000834881, 0.9717268676256592, 0.9717594181648467, 0.9717125356889666, 0.9717428807011742, 0.9717872422587999, 0.9717403911091838, 0.971683966811292, 0.9717679303050576], 'mDice': [0.019939267380289918, 0.050340433661346176, 0.10745833652988389, 0.2029732045388933, 0.30379676402169564, 0.39689777954307615, 0.457408793932375, 0.5002480238579837, 0.5381373537954718, 0.5676782670190659, 0.5916686655871926, 0.612483713605489, 0.6256947990635618, 0.637326642394761, 0.6486276881689853, 0.655236290264626, 0.661647639450863, 0.6674959870515211, 0.6746182287679855, 0.6796010917218612, 0.6843758819792563, 0.6890552527037315, 0.6933131635640063, 0.6969531620230766, 0.7021499277453886, 0.7049393380852835, 0.7085282151351705, 0.7118202021991933, 0.714522018174768, 0.7176975750728966, 0.7192890302055606, 0.7217411044488626, 0.7242438400031297, 0.727378946665402, 0.7291002684521085, 0.7314309076238146, 0.7332590888842565, 0.735848602656715, 0.736863186571522, 0.7389554001376212, 0.7401629707843024, 0.7418387894734078, 0.7443994449025965, 0.7448375430145725, 0.7462084883724076, 0.7472594230024147, 0.748511132030431, 0.7499624204104754, 0.7505308862530851, 0.7517182438383563, 0.7527144017050172, 0.7542558986972089, 0.754962146612551, 0.7562124474984294, 0.756852977396155, 0.7578645305861289, 0.7584033974326199, 0.7594621866184509, 0.759669936135899, 0.7616327956467515, 0.7614546142444191, 0.762296738234007, 0.7632601616169907, 0.7639432438419919, 0.7648466065439111, 0.7643408144782599, 0.7657655018129385, 0.7662626404987254, 0.7671577384129589, 0.7676310376998252, 0.7684763240391355, 0.7684696108193287, 0.7693550078786938, 0.7694549328896263, 0.769719577587118, 0.770564999113405, 0.7713205184389111, 0.7709699018678697, 0.7720345329207635, 0.7724593739704003, 0.773376589241048, 0.7730113017575713, 0.7740523597953853, 0.774514456919414, 0.7742169281621571, 0.7751758694821101, 0.7752713350981044, 0.7760927512723973, 0.7758860234392885, 0.7763188879394182, 0.7774092423392466, 0.7768128284334108, 0.7776670433353576, 0.7784984959878773, 0.77796835876142, 0.7783449977080479, 0.7796595604989986, 0.7797706192433389, 0.7792674584000121, 0.7802051013105145, 0.7802243882922978, 0.7809944086416329, 0.7808232792854722, 0.7813541902175059, 0.7810318339180674, 0.7820259020974178, 0.7821280664292017, 0.7821619397238356, 0.782673037639899, 0.7823484646015222, 0.7828101303193752, 0.7832558280249964, 0.7837570310012422, 0.7843139076544305, 0.7841133592967453, 0.7851585424677757, 0.7847677421567526, 0.7850726359355559, 0.7855564030206452, 0.7857410575264684, 0.7857984331833195, 0.786094648070241, 0.7865256978388929, 0.7868048393115623, 0.7870292243937296, 0.7863293276751689, 0.787157061726359, 0.7869783839442206, 0.788025686089243, 0.7878302066313156, 0.7879670905552115, 0.788840937616739, 0.7886352343607296, 0.7881323723564596, 0.7888149872901233, 0.7890066411490727, 0.7894704317499617, 0.7900127265543999, 0.7897352552154253, 0.7902493402609636, 0.7903356051363424, 0.7898889061651883, 0.7908167854763697, 0.7911468057197182, 0.7908134792958623, 0.7907551164983054, 0.7912153199274136, 0.7914281991126882, 0.7911598823623109, 0.7919486917394105, 0.7923041129550036, 0.791979744271305, 0.7923608212949247, 0.7927246713036198, 0.7930924912131926, 0.793213807009428, 0.7937842204666257, 0.7935678131856472, 0.7935954997571579, 0.7939784899695108, 0.7938327043422694, 0.7940044217675247, 0.7941371730749472, 0.7942275472063979, 0.7948450460750641, 0.7949959923028429, 0.7955489504727417, 0.795467524639043, 0.7954529855870541, 0.7954787574530716, 0.7959027145993504, 0.7960276402009873, 0.7963962247385578, 0.7963346277491387, 0.7969283656480141, 0.7972151944881933, 0.7972053049403424, 0.7971053121463265, 0.7978188019527562, 0.7982904959761429, 0.797542227676756, 0.7982175793784044, 0.7984391462056573, 0.7991333657950974, 0.7990729623980979, 0.7999054367004851, 0.7995629728463789, 0.7993292157762257, 0.7997947064122382, 0.8000785155269667, 0.8000183007051711, 0.8003880450441623, 0.8009099587772983, 0.8010586208421366, 0.8014900126545232, 0.801186291242118, 0.8012486015798339, 0.80189177789621, 0.8012261743332726, 0.8019061798853807, 0.802157705238316, 0.8024204339433667, 0.8022408364933596, 0.803341389745165, 0.8029714054392357, 0.8027545617536681, 0.8030743185563446, 0.8032650490129785, 0.8033228238908185, 0.8037598968195659, 0.8037710854033803, 0.8037595769699364, 0.8041325184550766, 0.8041511642208409, 0.8041608804673757, 0.8046820074416073, 0.8049819482206477, 0.8047488211516823, 0.8050065082155645, 0.8046892942580909, 0.8053198594622764, 0.8054773282146412, 0.8058526842954608, 0.8058917728198443, 0.8064647296910298, 0.8063692954702432, 0.8058172251679615, 0.8058633133011374, 0.8064529879408358, 0.8067049069475614, 0.806844196487228, 0.8067195174739908, 0.8072374656267169, 0.8074776893024233, 0.8074359819167344, 0.80755494537052, 0.8077531430892378, 0.8076720219232059, 0.8079425375503749, 0.8084984937026257, 0.8077664848315962, 0.8084091518555981, 0.8082488502293956, 0.8084173177752393, 0.8087462139385879, 0.8091736663576551, 0.808853156555474, 0.8094090859374861, 0.8089725174233507, 0.8093486109942297, 0.8094808629687605, 0.8095210839813948, 0.8093183471803878, 0.8095924179291655, 0.809880812148932, 0.8099960125901968, 0.8100569986241072, 0.810604236598095, 0.8104331763234562, 0.8103994795934707, 0.8103124146990285, 0.8102484501564702, 0.81089757434362, 0.8114140824907584, 0.8113223828748425, 0.8115353800600205, 0.8115751083928239, 0.811193658605335, 0.8112425545886819, 0.8123376734935035, 0.812182620109561, 0.8121228478771731, 0.8121879023410331, 0.8117732348784038, 0.8127443595555117, 0.8126137134678261, 0.8124037860439418, 0.812864277579881, 0.812912541272537, 0.8134701855297899, 0.8130153644871463, 0.8132113402846624, 0.8132498169859965, 0.813455806783242, 0.8135381732867639, 0.8139282828457666, 0.8137093707793442, 0.8141157185808998, 0.814149238763473, 0.813987550066031, 0.8139577021891367, 0.8142770519744483, 0.8144358599454249, 0.8149093894695592, 0.8141944376214325, 0.8145435326380571, 0.8152632154845704, 0.8144869513210374, 0.8144069822778288, 0.8150047227396288]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:01<00:26,  1.88s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:23,  1.82s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:22,  1.86s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:20,  1.88s/it]predicting test subjects:  33%|███▎      | 5/15 [00:09<00:20,  2.04s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:19,  2.15s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:15,  1.94s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:16<00:14,  2.09s/it]predicting test subjects:  60%|██████    | 9/15 [00:18<00:12,  2.04s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:19<00:09,  1.90s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:21<00:07,  1.85s/it]predicting test subjects:  80%|████████  | 12/15 [00:23<00:05,  1.91s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:25<00:03,  1.97s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:27<00:01,  1.92s/it]predicting test subjects: 100%|██████████| 15/15 [00:29<00:00,  1.96s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:44,  2.34s/it]predicting train subjects:   0%|          | 2/532 [00:03<18:42,  2.12s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:39,  2.00s/it]predicting train subjects:   1%|          | 4/532 [00:07<17:08,  1.95s/it]predicting train subjects:   1%|          | 5/532 [00:09<16:49,  1.92s/it]predicting train subjects:   1%|          | 6/532 [00:10<16:01,  1.83s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<15:37,  1.79s/it]predicting train subjects:   2%|▏         | 8/532 [00:14<15:10,  1.74s/it]predicting train subjects:   2%|▏         | 9/532 [00:16<15:54,  1.83s/it]predicting train subjects:   2%|▏         | 10/532 [00:18<15:38,  1.80s/it]predicting train subjects:   2%|▏         | 11/532 [00:19<14:51,  1.71s/it]predicting train subjects:   2%|▏         | 12/532 [00:21<16:19,  1.88s/it]predicting train subjects:   2%|▏         | 13/532 [00:23<15:13,  1.76s/it]predicting train subjects:   3%|▎         | 14/532 [00:24<14:28,  1.68s/it]predicting train subjects:   3%|▎         | 15/532 [00:26<14:27,  1.68s/it]predicting train subjects:   3%|▎         | 16/532 [00:28<15:01,  1.75s/it]predicting train subjects:   3%|▎         | 17/532 [00:29<14:33,  1.70s/it]predicting train subjects:   3%|▎         | 18/532 [00:31<15:18,  1.79s/it]predicting train subjects:   4%|▎         | 19/532 [00:33<14:24,  1.68s/it]predicting train subjects:   4%|▍         | 20/532 [00:35<14:39,  1.72s/it]predicting train subjects:   4%|▍         | 21/532 [00:37<15:39,  1.84s/it]predicting train subjects:   4%|▍         | 22/532 [00:38<14:58,  1.76s/it]predicting train subjects:   4%|▍         | 23/532 [00:40<15:03,  1.78s/it]predicting train subjects:   5%|▍         | 24/532 [00:42<14:15,  1.68s/it]predicting train subjects:   5%|▍         | 25/532 [00:44<15:34,  1.84s/it]predicting train subjects:   5%|▍         | 26/532 [00:46<15:05,  1.79s/it]predicting train subjects:   5%|▌         | 27/532 [00:48<16:28,  1.96s/it]predicting train subjects:   5%|▌         | 28/532 [00:50<15:44,  1.87s/it]predicting train subjects:   5%|▌         | 29/532 [00:52<16:17,  1.94s/it]predicting train subjects:   6%|▌         | 30/532 [00:53<15:19,  1.83s/it]predicting train subjects:   6%|▌         | 31/532 [00:55<15:02,  1.80s/it]predicting train subjects:   6%|▌         | 32/532 [00:57<14:47,  1.78s/it]predicting train subjects:   6%|▌         | 33/532 [00:58<14:08,  1.70s/it]predicting train subjects:   6%|▋         | 34/532 [01:00<15:21,  1.85s/it]predicting train subjects:   7%|▋         | 35/532 [01:02<15:26,  1.87s/it]predicting train subjects:   7%|▋         | 36/532 [01:04<15:44,  1.90s/it]predicting train subjects:   7%|▋         | 37/532 [01:06<15:29,  1.88s/it]predicting train subjects:   7%|▋         | 38/532 [01:08<15:48,  1.92s/it]predicting train subjects:   7%|▋         | 39/532 [01:10<15:25,  1.88s/it]predicting train subjects:   8%|▊         | 40/532 [01:12<14:50,  1.81s/it]predicting train subjects:   8%|▊         | 41/532 [01:14<15:07,  1.85s/it]predicting train subjects:   8%|▊         | 42/532 [01:15<15:14,  1.87s/it]predicting train subjects:   8%|▊         | 43/532 [01:17<14:12,  1.74s/it]predicting train subjects:   8%|▊         | 44/532 [01:18<13:18,  1.64s/it]predicting train subjects:   8%|▊         | 45/532 [01:20<13:03,  1.61s/it]predicting train subjects:   9%|▊         | 46/532 [01:22<13:25,  1.66s/it]predicting train subjects:   9%|▉         | 47/532 [01:24<14:32,  1.80s/it]predicting train subjects:   9%|▉         | 48/532 [01:26<14:30,  1.80s/it]predicting train subjects:   9%|▉         | 49/532 [01:27<13:57,  1.73s/it]predicting train subjects:   9%|▉         | 50/532 [01:29<14:39,  1.83s/it]predicting train subjects:  10%|▉         | 51/532 [01:31<14:04,  1.76s/it]predicting train subjects:  10%|▉         | 52/532 [01:32<13:57,  1.75s/it]predicting train subjects:  10%|▉         | 53/532 [01:34<13:27,  1.69s/it]predicting train subjects:  10%|█         | 54/532 [01:36<14:12,  1.78s/it]predicting train subjects:  10%|█         | 55/532 [01:38<14:03,  1.77s/it]predicting train subjects:  11%|█         | 56/532 [01:40<14:06,  1.78s/it]predicting train subjects:  11%|█         | 57/532 [01:41<13:52,  1.75s/it]predicting train subjects:  11%|█         | 58/532 [01:43<14:01,  1.77s/it]predicting train subjects:  11%|█         | 59/532 [01:45<15:22,  1.95s/it]predicting train subjects:  11%|█▏        | 60/532 [01:47<14:05,  1.79s/it]predicting train subjects:  11%|█▏        | 61/532 [01:48<13:23,  1.71s/it]predicting train subjects:  12%|█▏        | 62/532 [01:50<14:00,  1.79s/it]predicting train subjects:  12%|█▏        | 63/532 [01:52<14:40,  1.88s/it]predicting train subjects:  12%|█▏        | 64/532 [01:54<13:44,  1.76s/it]predicting train subjects:  12%|█▏        | 65/532 [01:56<13:55,  1.79s/it]predicting train subjects:  12%|█▏        | 66/532 [01:58<15:22,  1.98s/it]predicting train subjects:  13%|█▎        | 67/532 [02:00<15:50,  2.04s/it]predicting train subjects:  13%|█▎        | 68/532 [02:02<15:15,  1.97s/it]predicting train subjects:  13%|█▎        | 69/532 [02:04<14:28,  1.88s/it]predicting train subjects:  13%|█▎        | 70/532 [02:06<14:05,  1.83s/it]predicting train subjects:  13%|█▎        | 71/532 [02:07<13:26,  1.75s/it]predicting train subjects:  14%|█▎        | 72/532 [02:09<12:47,  1.67s/it]predicting train subjects:  14%|█▎        | 73/532 [02:10<13:15,  1.73s/it]predicting train subjects:  14%|█▍        | 74/532 [02:13<14:30,  1.90s/it]predicting train subjects:  14%|█▍        | 75/532 [02:16<16:40,  2.19s/it]predicting train subjects:  14%|█▍        | 76/532 [02:17<15:34,  2.05s/it]predicting train subjects:  14%|█▍        | 77/532 [02:19<14:58,  1.97s/it]predicting train subjects:  15%|█▍        | 78/532 [02:21<14:43,  1.95s/it]predicting train subjects:  15%|█▍        | 79/532 [02:23<14:27,  1.92s/it]predicting train subjects:  15%|█▌        | 80/532 [02:25<14:20,  1.90s/it]predicting train subjects:  15%|█▌        | 81/532 [02:27<14:12,  1.89s/it]predicting train subjects:  15%|█▌        | 82/532 [02:28<14:04,  1.88s/it]predicting train subjects:  16%|█▌        | 83/532 [02:30<13:10,  1.76s/it]predicting train subjects:  16%|█▌        | 84/532 [02:31<12:36,  1.69s/it]predicting train subjects:  16%|█▌        | 85/532 [02:33<12:08,  1.63s/it]predicting train subjects:  16%|█▌        | 86/532 [02:34<11:50,  1.59s/it]predicting train subjects:  16%|█▋        | 87/532 [02:36<11:39,  1.57s/it]predicting train subjects:  17%|█▋        | 88/532 [02:38<11:30,  1.56s/it]predicting train subjects:  17%|█▋        | 89/532 [02:39<11:50,  1.60s/it]predicting train subjects:  17%|█▋        | 90/532 [02:41<11:57,  1.62s/it]predicting train subjects:  17%|█▋        | 91/532 [02:43<12:13,  1.66s/it]predicting train subjects:  17%|█▋        | 92/532 [02:44<12:20,  1.68s/it]predicting train subjects:  17%|█▋        | 93/532 [02:46<12:28,  1.71s/it]predicting train subjects:  18%|█▊        | 94/532 [02:48<12:28,  1.71s/it]predicting train subjects:  18%|█▊        | 95/532 [02:50<13:14,  1.82s/it]predicting train subjects:  18%|█▊        | 96/532 [02:52<13:37,  1.88s/it]predicting train subjects:  18%|█▊        | 97/532 [02:54<13:48,  1.90s/it]predicting train subjects:  18%|█▊        | 98/532 [02:56<14:02,  1.94s/it]predicting train subjects:  19%|█▊        | 99/532 [02:58<14:28,  2.00s/it]predicting train subjects:  19%|█▉        | 100/532 [03:00<14:36,  2.03s/it]predicting train subjects:  19%|█▉        | 101/532 [03:02<13:37,  1.90s/it]predicting train subjects:  19%|█▉        | 102/532 [03:03<12:57,  1.81s/it]predicting train subjects:  19%|█▉        | 103/532 [03:05<12:24,  1.74s/it]predicting train subjects:  20%|█▉        | 104/532 [03:06<12:00,  1.68s/it]predicting train subjects:  20%|█▉        | 105/532 [03:08<11:40,  1.64s/it]predicting train subjects:  20%|█▉        | 106/532 [03:10<11:18,  1.59s/it]predicting train subjects:  20%|██        | 107/532 [03:11<11:06,  1.57s/it]predicting train subjects:  20%|██        | 108/532 [03:13<11:01,  1.56s/it]predicting train subjects:  20%|██        | 109/532 [03:14<10:50,  1.54s/it]predicting train subjects:  21%|██        | 110/532 [03:16<10:50,  1.54s/it]predicting train subjects:  21%|██        | 111/532 [03:17<10:59,  1.57s/it]predicting train subjects:  21%|██        | 112/532 [03:19<10:47,  1.54s/it]predicting train subjects:  21%|██        | 113/532 [03:20<11:15,  1.61s/it]predicting train subjects:  21%|██▏       | 114/532 [03:22<11:34,  1.66s/it]predicting train subjects:  22%|██▏       | 115/532 [03:24<11:44,  1.69s/it]predicting train subjects:  22%|██▏       | 116/532 [03:26<11:59,  1.73s/it]predicting train subjects:  22%|██▏       | 117/532 [03:28<12:04,  1.75s/it]predicting train subjects:  22%|██▏       | 118/532 [03:29<12:17,  1.78s/it]predicting train subjects:  22%|██▏       | 119/532 [03:31<12:17,  1.79s/it]predicting train subjects:  23%|██▎       | 120/532 [03:33<12:33,  1.83s/it]predicting train subjects:  23%|██▎       | 121/532 [03:35<12:34,  1.84s/it]predicting train subjects:  23%|██▎       | 122/532 [03:37<12:38,  1.85s/it]predicting train subjects:  23%|██▎       | 123/532 [03:39<12:20,  1.81s/it]predicting train subjects:  23%|██▎       | 124/532 [03:40<12:00,  1.77s/it]predicting train subjects:  23%|██▎       | 125/532 [03:42<12:20,  1.82s/it]predicting train subjects:  24%|██▎       | 126/532 [03:44<12:28,  1.84s/it]predicting train subjects:  24%|██▍       | 127/532 [03:46<12:31,  1.86s/it]predicting train subjects:  24%|██▍       | 128/532 [03:48<12:35,  1.87s/it]predicting train subjects:  24%|██▍       | 129/532 [03:50<12:35,  1.87s/it]predicting train subjects:  24%|██▍       | 130/532 [03:52<12:45,  1.90s/it]predicting train subjects:  25%|██▍       | 131/532 [03:54<13:32,  2.03s/it]predicting train subjects:  25%|██▍       | 132/532 [03:56<13:46,  2.07s/it]predicting train subjects:  25%|██▌       | 133/532 [03:59<14:03,  2.11s/it]predicting train subjects:  25%|██▌       | 134/532 [04:01<14:11,  2.14s/it]predicting train subjects:  25%|██▌       | 135/532 [04:03<14:37,  2.21s/it]predicting train subjects:  26%|██▌       | 136/532 [04:05<14:38,  2.22s/it]predicting train subjects:  26%|██▌       | 137/532 [04:08<14:54,  2.27s/it]predicting train subjects:  26%|██▌       | 138/532 [04:10<14:57,  2.28s/it]predicting train subjects:  26%|██▌       | 139/532 [04:12<14:56,  2.28s/it]predicting train subjects:  26%|██▋       | 140/532 [04:15<14:53,  2.28s/it]predicting train subjects:  27%|██▋       | 141/532 [04:17<14:44,  2.26s/it]predicting train subjects:  27%|██▋       | 142/532 [04:19<14:48,  2.28s/it]predicting train subjects:  27%|██▋       | 143/532 [04:21<13:31,  2.09s/it]predicting train subjects:  27%|██▋       | 144/532 [04:22<12:35,  1.95s/it]predicting train subjects:  27%|██▋       | 145/532 [04:24<11:55,  1.85s/it]predicting train subjects:  27%|██▋       | 146/532 [04:26<11:32,  1.79s/it]predicting train subjects:  28%|██▊       | 147/532 [04:27<11:09,  1.74s/it]predicting train subjects:  28%|██▊       | 148/532 [04:29<10:47,  1.69s/it]predicting train subjects:  28%|██▊       | 149/532 [04:30<10:43,  1.68s/it]predicting train subjects:  28%|██▊       | 150/532 [04:32<10:44,  1.69s/it]predicting train subjects:  28%|██▊       | 151/532 [04:34<10:47,  1.70s/it]predicting train subjects:  29%|██▊       | 152/532 [04:36<10:46,  1.70s/it]predicting train subjects:  29%|██▉       | 153/532 [04:37<10:47,  1.71s/it]predicting train subjects:  29%|██▉       | 154/532 [04:39<10:44,  1.71s/it]predicting train subjects:  29%|██▉       | 155/532 [04:41<11:47,  1.88s/it]predicting train subjects:  29%|██▉       | 156/532 [04:44<12:35,  2.01s/it]predicting train subjects:  30%|██▉       | 157/532 [04:46<13:06,  2.10s/it]predicting train subjects:  30%|██▉       | 158/532 [04:48<13:26,  2.16s/it]predicting train subjects:  30%|██▉       | 159/532 [04:51<13:49,  2.22s/it]predicting train subjects:  30%|███       | 160/532 [04:53<14:35,  2.35s/it]predicting train subjects:  30%|███       | 161/532 [04:55<13:25,  2.17s/it]predicting train subjects:  30%|███       | 162/532 [04:57<12:35,  2.04s/it]predicting train subjects:  31%|███       | 163/532 [04:58<11:53,  1.93s/it]predicting train subjects:  31%|███       | 164/532 [05:00<11:23,  1.86s/it]predicting train subjects:  31%|███       | 165/532 [05:02<11:08,  1.82s/it]predicting train subjects:  31%|███       | 166/532 [05:04<10:51,  1.78s/it]predicting train subjects:  31%|███▏      | 167/532 [05:05<11:06,  1.83s/it]predicting train subjects:  32%|███▏      | 168/532 [05:07<11:02,  1.82s/it]predicting train subjects:  32%|███▏      | 169/532 [05:09<11:00,  1.82s/it]predicting train subjects:  32%|███▏      | 170/532 [05:11<10:56,  1.81s/it]predicting train subjects:  32%|███▏      | 171/532 [05:13<11:00,  1.83s/it]predicting train subjects:  32%|███▏      | 172/532 [05:15<10:56,  1.82s/it]predicting train subjects:  33%|███▎      | 173/532 [05:16<10:37,  1.78s/it]predicting train subjects:  33%|███▎      | 174/532 [05:18<10:18,  1.73s/it]predicting train subjects:  33%|███▎      | 175/532 [05:19<10:05,  1.70s/it]predicting train subjects:  33%|███▎      | 176/532 [05:21<09:53,  1.67s/it]predicting train subjects:  33%|███▎      | 177/532 [05:23<09:47,  1.65s/it]predicting train subjects:  33%|███▎      | 178/532 [05:24<09:46,  1.66s/it]predicting train subjects:  34%|███▎      | 179/532 [05:26<09:56,  1.69s/it]predicting train subjects:  34%|███▍      | 180/532 [05:28<09:58,  1.70s/it]predicting train subjects:  34%|███▍      | 181/532 [05:30<09:55,  1.70s/it]predicting train subjects:  34%|███▍      | 182/532 [05:31<09:53,  1.70s/it]predicting train subjects:  34%|███▍      | 183/532 [05:33<09:39,  1.66s/it]predicting train subjects:  35%|███▍      | 184/532 [05:34<09:36,  1.66s/it]predicting train subjects:  35%|███▍      | 185/532 [05:36<09:27,  1.63s/it]predicting train subjects:  35%|███▍      | 186/532 [05:38<09:25,  1.63s/it]predicting train subjects:  35%|███▌      | 187/532 [05:39<09:22,  1.63s/it]predicting train subjects:  35%|███▌      | 188/532 [05:41<09:21,  1.63s/it]predicting train subjects:  36%|███▌      | 189/532 [05:43<09:14,  1.62s/it]predicting train subjects:  36%|███▌      | 190/532 [05:44<09:11,  1.61s/it]predicting train subjects:  36%|███▌      | 191/532 [05:46<10:26,  1.84s/it]predicting train subjects:  36%|███▌      | 192/532 [05:49<11:15,  1.99s/it]predicting train subjects:  36%|███▋      | 193/532 [05:51<11:49,  2.09s/it]predicting train subjects:  36%|███▋      | 194/532 [05:54<12:13,  2.17s/it]predicting train subjects:  37%|███▋      | 195/532 [05:56<12:36,  2.24s/it]predicting train subjects:  37%|███▋      | 196/532 [05:58<12:46,  2.28s/it]predicting train subjects:  37%|███▋      | 197/532 [06:00<12:24,  2.22s/it]predicting train subjects:  37%|███▋      | 198/532 [06:02<12:05,  2.17s/it]predicting train subjects:  37%|███▋      | 199/532 [06:05<11:51,  2.14s/it]predicting train subjects:  38%|███▊      | 200/532 [06:07<11:43,  2.12s/it]predicting train subjects:  38%|███▊      | 201/532 [06:09<11:32,  2.09s/it]predicting train subjects:  38%|███▊      | 202/532 [06:11<11:26,  2.08s/it]predicting train subjects:  38%|███▊      | 203/532 [06:12<10:56,  2.00s/it]predicting train subjects:  38%|███▊      | 204/532 [06:14<10:34,  1.94s/it]predicting train subjects:  39%|███▊      | 205/532 [06:16<10:11,  1.87s/it]predicting train subjects:  39%|███▊      | 206/532 [06:18<09:56,  1.83s/it]predicting train subjects:  39%|███▉      | 207/532 [06:19<09:46,  1.81s/it]predicting train subjects:  39%|███▉      | 208/532 [06:21<09:34,  1.77s/it]predicting train subjects:  39%|███▉      | 209/532 [06:23<09:12,  1.71s/it]predicting train subjects:  39%|███▉      | 210/532 [06:24<08:50,  1.65s/it]predicting train subjects:  40%|███▉      | 211/532 [06:26<08:33,  1.60s/it]predicting train subjects:  40%|███▉      | 212/532 [06:27<08:21,  1.57s/it]predicting train subjects:  40%|████      | 213/532 [06:29<08:13,  1.55s/it]predicting train subjects:  40%|████      | 214/532 [06:30<08:08,  1.54s/it]predicting train subjects:  40%|████      | 215/532 [06:32<09:16,  1.76s/it]predicting train subjects:  41%|████      | 216/532 [06:35<10:01,  1.90s/it]predicting train subjects:  41%|████      | 217/532 [06:37<10:29,  2.00s/it]predicting train subjects:  41%|████      | 218/532 [06:39<10:44,  2.05s/it]predicting train subjects:  41%|████      | 219/532 [06:41<10:51,  2.08s/it]predicting train subjects:  41%|████▏     | 220/532 [06:43<10:57,  2.11s/it]predicting train subjects:  42%|████▏     | 221/532 [06:45<09:57,  1.92s/it]predicting train subjects:  42%|████▏     | 222/532 [06:46<09:09,  1.77s/it]predicting train subjects:  42%|████▏     | 223/532 [06:48<08:39,  1.68s/it]predicting train subjects:  42%|████▏     | 224/532 [06:49<08:22,  1.63s/it]predicting train subjects:  42%|████▏     | 225/532 [06:51<08:11,  1.60s/it]predicting train subjects:  42%|████▏     | 226/532 [06:52<07:57,  1.56s/it]predicting train subjects:  43%|████▎     | 227/532 [06:54<07:45,  1.53s/it]predicting train subjects:  43%|████▎     | 228/532 [06:55<07:35,  1.50s/it]predicting train subjects:  43%|████▎     | 229/532 [06:57<07:25,  1.47s/it]predicting train subjects:  43%|████▎     | 230/532 [06:58<07:20,  1.46s/it]predicting train subjects:  43%|████▎     | 231/532 [06:59<07:17,  1.45s/it]predicting train subjects:  44%|████▎     | 232/532 [07:01<07:15,  1.45s/it]predicting train subjects:  44%|████▍     | 233/532 [07:03<07:32,  1.51s/it]predicting train subjects:  44%|████▍     | 234/532 [07:04<07:41,  1.55s/it]predicting train subjects:  44%|████▍     | 235/532 [07:06<07:48,  1.58s/it]predicting train subjects:  44%|████▍     | 236/532 [07:08<07:52,  1.60s/it]predicting train subjects:  45%|████▍     | 237/532 [07:09<08:01,  1.63s/it]predicting train subjects:  45%|████▍     | 238/532 [07:11<08:05,  1.65s/it]predicting train subjects:  45%|████▍     | 239/532 [07:13<08:21,  1.71s/it]predicting train subjects:  45%|████▌     | 240/532 [07:15<08:28,  1.74s/it]predicting train subjects:  45%|████▌     | 241/532 [07:16<08:34,  1.77s/it]predicting train subjects:  45%|████▌     | 242/532 [07:18<08:37,  1.78s/it]predicting train subjects:  46%|████▌     | 243/532 [07:20<08:40,  1.80s/it]predicting train subjects:  46%|████▌     | 244/532 [07:22<08:41,  1.81s/it]predicting train subjects:  46%|████▌     | 245/532 [07:23<08:07,  1.70s/it]predicting train subjects:  46%|████▌     | 246/532 [07:25<07:45,  1.63s/it]predicting train subjects:  46%|████▋     | 247/532 [07:26<07:26,  1.57s/it]predicting train subjects:  47%|████▋     | 248/532 [07:28<07:24,  1.56s/it]predicting train subjects:  47%|████▋     | 249/532 [07:29<07:18,  1.55s/it]predicting train subjects:  47%|████▋     | 250/532 [07:31<07:08,  1.52s/it]predicting train subjects:  47%|████▋     | 251/532 [07:32<07:13,  1.54s/it]predicting train subjects:  47%|████▋     | 252/532 [07:34<07:14,  1.55s/it]predicting train subjects:  48%|████▊     | 253/532 [07:36<07:14,  1.56s/it]predicting train subjects:  48%|████▊     | 254/532 [07:37<07:16,  1.57s/it]predicting train subjects:  48%|████▊     | 255/532 [07:39<07:12,  1.56s/it]predicting train subjects:  48%|████▊     | 256/532 [07:40<07:15,  1.58s/it]predicting train subjects:  48%|████▊     | 257/532 [07:42<07:53,  1.72s/it]predicting train subjects:  48%|████▊     | 258/532 [07:44<08:18,  1.82s/it]predicting train subjects:  49%|████▊     | 259/532 [07:46<08:37,  1.89s/it]predicting train subjects:  49%|████▉     | 260/532 [07:48<08:45,  1.93s/it]predicting train subjects:  49%|████▉     | 261/532 [07:51<08:55,  1.97s/it]predicting train subjects:  49%|████▉     | 262/532 [07:53<09:01,  2.01s/it]predicting train subjects:  49%|████▉     | 263/532 [07:54<08:16,  1.85s/it]predicting train subjects:  50%|████▉     | 264/532 [07:56<07:42,  1.72s/it]predicting train subjects:  50%|████▉     | 265/532 [07:57<07:18,  1.64s/it]predicting train subjects:  50%|█████     | 266/532 [07:58<06:55,  1.56s/it]predicting train subjects:  50%|█████     | 267/532 [08:00<06:43,  1.52s/it]predicting train subjects:  50%|█████     | 268/532 [08:01<06:30,  1.48s/it]predicting train subjects:  51%|█████     | 269/532 [08:03<06:57,  1.59s/it]predicting train subjects:  51%|█████     | 270/532 [08:05<07:11,  1.65s/it]predicting train subjects:  51%|█████     | 271/532 [08:07<07:19,  1.68s/it]predicting train subjects:  51%|█████     | 272/532 [08:08<07:26,  1.72s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:10<07:30,  1.74s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:12<07:29,  1.74s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:14<08:01,  1.87s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:16<08:26,  1.98s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:19<08:45,  2.06s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:21<08:58,  2.12s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:23<09:11,  2.18s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:25<09:15,  2.21s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:27<09:06,  2.18s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:30<08:57,  2.15s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:32<08:53,  2.14s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:34<09:12,  2.23s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:36<09:07,  2.22s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:38<08:58,  2.19s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:40<08:13,  2.01s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:42<07:40,  1.89s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:43<07:15,  1.79s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:45<06:59,  1.73s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:46<06:45,  1.68s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:48<06:41,  1.67s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:50<06:53,  1.73s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:52<06:58,  1.76s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:54<07:04,  1.79s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:55<07:10,  1.82s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:57<07:09,  1.83s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:59<07:09,  1.84s/it]predicting train subjects:  56%|█████▌    | 299/532 [09:01<06:43,  1.73s/it]predicting train subjects:  56%|█████▋    | 300/532 [09:02<06:18,  1.63s/it]predicting train subjects:  57%|█████▋    | 301/532 [09:04<06:05,  1.58s/it]predicting train subjects:  57%|█████▋    | 302/532 [09:05<05:55,  1.54s/it]predicting train subjects:  57%|█████▋    | 303/532 [09:06<05:44,  1.50s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:08<05:38,  1.49s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:10<06:25,  1.70s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:12<06:53,  1.83s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:14<07:16,  1.94s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:17<07:31,  2.02s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:19<07:38,  2.06s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:21<07:45,  2.10s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:24<08:43,  2.37s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:27<09:21,  2.55s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:30<09:38,  2.64s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:33<09:56,  2.74s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:36<10:07,  2.80s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:39<10:11,  2.83s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:40<08:55,  2.49s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:42<07:58,  2.24s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:43<07:16,  2.05s/it]predicting train subjects:  60%|██████    | 320/532 [09:45<06:46,  1.92s/it]predicting train subjects:  60%|██████    | 321/532 [09:47<06:21,  1.81s/it]predicting train subjects:  61%|██████    | 322/532 [09:48<06:14,  1.78s/it]predicting train subjects:  61%|██████    | 323/532 [09:51<06:49,  1.96s/it]predicting train subjects:  61%|██████    | 324/532 [09:53<07:06,  2.05s/it]predicting train subjects:  61%|██████    | 325/532 [09:55<07:23,  2.14s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:58<07:31,  2.19s/it]predicting train subjects:  61%|██████▏   | 327/532 [10:00<07:42,  2.25s/it]predicting train subjects:  62%|██████▏   | 328/532 [10:02<07:41,  2.26s/it]predicting train subjects:  62%|██████▏   | 329/532 [10:04<07:15,  2.15s/it]predicting train subjects:  62%|██████▏   | 330/532 [10:06<06:52,  2.04s/it]predicting train subjects:  62%|██████▏   | 331/532 [10:08<06:33,  1.96s/it]predicting train subjects:  62%|██████▏   | 332/532 [10:10<06:21,  1.91s/it]predicting train subjects:  63%|██████▎   | 333/532 [10:11<06:09,  1.86s/it]predicting train subjects:  63%|██████▎   | 334/532 [10:13<06:00,  1.82s/it]predicting train subjects:  63%|██████▎   | 335/532 [10:15<06:12,  1.89s/it]predicting train subjects:  63%|██████▎   | 336/532 [10:17<06:21,  1.95s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:19<06:27,  1.98s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:21<06:30,  2.01s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:23<06:35,  2.05s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:25<06:31,  2.04s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:27<05:59,  1.88s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:29<05:38,  1.78s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:30<05:28,  1.74s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:32<05:13,  1.67s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:33<05:02,  1.62s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:35<04:57,  1.60s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:36<05:01,  1.63s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:38<05:07,  1.67s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:40<05:08,  1.68s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:42<05:14,  1.73s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:44<05:16,  1.75s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:45<05:14,  1.74s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:47<05:13,  1.75s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:49<05:10,  1.74s/it]predicting train subjects:  67%|██████▋   | 355/532 [10:51<05:06,  1.73s/it]predicting train subjects:  67%|██████▋   | 356/532 [10:52<05:02,  1.72s/it]predicting train subjects:  67%|██████▋   | 357/532 [10:54<05:02,  1.73s/it]predicting train subjects:  67%|██████▋   | 358/532 [10:56<05:03,  1.74s/it]predicting train subjects:  67%|██████▋   | 359/532 [10:57<04:54,  1.70s/it]predicting train subjects:  68%|██████▊   | 360/532 [10:59<04:47,  1.67s/it]predicting train subjects:  68%|██████▊   | 361/532 [11:00<04:39,  1.63s/it]predicting train subjects:  68%|██████▊   | 362/532 [11:02<04:34,  1.62s/it]predicting train subjects:  68%|██████▊   | 363/532 [11:04<04:30,  1.60s/it]predicting train subjects:  68%|██████▊   | 364/532 [11:05<04:26,  1.59s/it]predicting train subjects:  69%|██████▊   | 365/532 [11:07<04:22,  1.57s/it]predicting train subjects:  69%|██████▉   | 366/532 [11:08<04:20,  1.57s/it]predicting train subjects:  69%|██████▉   | 367/532 [11:10<04:17,  1.56s/it]predicting train subjects:  69%|██████▉   | 368/532 [11:11<04:15,  1.56s/it]predicting train subjects:  69%|██████▉   | 369/532 [11:13<04:12,  1.55s/it]predicting train subjects:  70%|██████▉   | 370/532 [11:14<04:10,  1.55s/it]predicting train subjects:  70%|██████▉   | 371/532 [11:17<04:42,  1.75s/it]predicting train subjects:  70%|██████▉   | 372/532 [11:19<05:02,  1.89s/it]predicting train subjects:  70%|███████   | 373/532 [11:21<05:11,  1.96s/it]predicting train subjects:  70%|███████   | 374/532 [11:23<05:17,  2.01s/it]predicting train subjects:  70%|███████   | 375/532 [11:25<05:24,  2.07s/it]predicting train subjects:  71%|███████   | 376/532 [11:27<05:27,  2.10s/it]predicting train subjects:  71%|███████   | 377/532 [11:29<05:12,  2.02s/it]predicting train subjects:  71%|███████   | 378/532 [11:31<05:02,  1.96s/it]predicting train subjects:  71%|███████   | 379/532 [11:33<04:50,  1.90s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:35<04:43,  1.86s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:36<04:33,  1.81s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:38<04:29,  1.80s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:40<04:31,  1.82s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:42<04:30,  1.83s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:44<04:28,  1.83s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:46<04:29,  1.84s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:47<04:27,  1.84s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:49<04:21,  1.81s/it]predicting train subjects:  73%|███████▎  | 389/532 [11:51<04:22,  1.84s/it]predicting train subjects:  73%|███████▎  | 390/532 [11:53<04:25,  1.87s/it]predicting train subjects:  73%|███████▎  | 391/532 [11:55<04:26,  1.89s/it]predicting train subjects:  74%|███████▎  | 392/532 [11:57<04:25,  1.89s/it]predicting train subjects:  74%|███████▍  | 393/532 [11:59<04:26,  1.92s/it]predicting train subjects:  74%|███████▍  | 394/532 [12:01<04:24,  1.92s/it]predicting train subjects:  74%|███████▍  | 395/532 [12:03<04:20,  1.90s/it]predicting train subjects:  74%|███████▍  | 396/532 [12:04<04:15,  1.88s/it]predicting train subjects:  75%|███████▍  | 397/532 [12:06<04:09,  1.85s/it]predicting train subjects:  75%|███████▍  | 398/532 [12:08<04:05,  1.83s/it]predicting train subjects:  75%|███████▌  | 399/532 [12:10<04:01,  1.81s/it]predicting train subjects:  75%|███████▌  | 400/532 [12:12<04:04,  1.85s/it]predicting train subjects:  75%|███████▌  | 401/532 [12:14<04:13,  1.94s/it]predicting train subjects:  76%|███████▌  | 402/532 [12:16<04:15,  1.97s/it]predicting train subjects:  76%|███████▌  | 403/532 [12:18<04:21,  2.03s/it]predicting train subjects:  76%|███████▌  | 404/532 [12:20<04:20,  2.04s/it]predicting train subjects:  76%|███████▌  | 405/532 [12:22<04:19,  2.04s/it]predicting train subjects:  76%|███████▋  | 406/532 [12:24<04:14,  2.02s/it]predicting train subjects:  77%|███████▋  | 407/532 [12:26<04:02,  1.94s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:28<03:52,  1.88s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:29<03:48,  1.85s/it]predicting train subjects:  77%|███████▋  | 410/532 [12:31<03:42,  1.82s/it]predicting train subjects:  77%|███████▋  | 411/532 [12:33<03:38,  1.80s/it]predicting train subjects:  77%|███████▋  | 412/532 [12:35<03:34,  1.79s/it]predicting train subjects:  78%|███████▊  | 413/532 [12:36<03:30,  1.77s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:38<03:25,  1.74s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:40<03:19,  1.70s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:41<03:17,  1.70s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:43<03:14,  1.69s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:45<03:11,  1.68s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:47<03:21,  1.78s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:49<03:23,  1.82s/it]predicting train subjects:  79%|███████▉  | 421/532 [12:51<03:26,  1.86s/it]predicting train subjects:  79%|███████▉  | 422/532 [12:52<03:27,  1.89s/it]predicting train subjects:  80%|███████▉  | 423/532 [12:54<03:25,  1.89s/it]predicting train subjects:  80%|███████▉  | 424/532 [12:56<03:23,  1.89s/it]predicting train subjects:  80%|███████▉  | 425/532 [12:58<03:28,  1.95s/it]predicting train subjects:  80%|████████  | 426/532 [13:00<03:25,  1.94s/it]predicting train subjects:  80%|████████  | 427/532 [13:02<03:24,  1.95s/it]predicting train subjects:  80%|████████  | 428/532 [13:04<03:21,  1.93s/it]predicting train subjects:  81%|████████  | 429/532 [13:06<03:22,  1.96s/it]predicting train subjects:  81%|████████  | 430/532 [13:08<03:23,  1.99s/it]predicting train subjects:  81%|████████  | 431/532 [13:10<03:23,  2.01s/it]predicting train subjects:  81%|████████  | 432/532 [13:12<03:22,  2.02s/it]predicting train subjects:  81%|████████▏ | 433/532 [13:14<03:20,  2.02s/it]predicting train subjects:  82%|████████▏ | 434/532 [13:16<03:20,  2.04s/it]predicting train subjects:  82%|████████▏ | 435/532 [13:19<03:19,  2.05s/it]predicting train subjects:  82%|████████▏ | 436/532 [13:21<03:20,  2.08s/it]predicting train subjects:  82%|████████▏ | 437/532 [13:22<03:02,  1.92s/it]predicting train subjects:  82%|████████▏ | 438/532 [13:24<02:48,  1.80s/it]predicting train subjects:  83%|████████▎ | 439/532 [13:25<02:38,  1.71s/it]predicting train subjects:  83%|████████▎ | 440/532 [13:27<02:30,  1.63s/it]predicting train subjects:  83%|████████▎ | 441/532 [13:28<02:26,  1.61s/it]predicting train subjects:  83%|████████▎ | 442/532 [13:30<02:22,  1.58s/it]predicting train subjects:  83%|████████▎ | 443/532 [13:31<02:16,  1.53s/it]predicting train subjects:  83%|████████▎ | 444/532 [13:33<02:12,  1.50s/it]predicting train subjects:  84%|████████▎ | 445/532 [13:34<02:07,  1.46s/it]predicting train subjects:  84%|████████▍ | 446/532 [13:35<02:04,  1.45s/it]predicting train subjects:  84%|████████▍ | 447/532 [13:37<02:02,  1.44s/it]predicting train subjects:  84%|████████▍ | 448/532 [13:38<01:59,  1.43s/it]predicting train subjects:  84%|████████▍ | 449/532 [13:40<02:03,  1.49s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:41<02:05,  1.53s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:43<02:07,  1.58s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:45<02:06,  1.59s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:46<02:05,  1.58s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:48<02:03,  1.58s/it]predicting train subjects:  86%|████████▌ | 455/532 [13:50<02:07,  1.65s/it]predicting train subjects:  86%|████████▌ | 456/532 [13:52<02:09,  1.70s/it]predicting train subjects:  86%|████████▌ | 457/532 [13:53<02:10,  1.74s/it]predicting train subjects:  86%|████████▌ | 458/532 [13:55<02:10,  1.77s/it]predicting train subjects:  86%|████████▋ | 459/532 [13:57<02:11,  1.80s/it]predicting train subjects:  86%|████████▋ | 460/532 [13:59<02:11,  1.83s/it]predicting train subjects:  87%|████████▋ | 461/532 [14:01<02:20,  1.98s/it]predicting train subjects:  87%|████████▋ | 462/532 [14:04<02:23,  2.05s/it]predicting train subjects:  87%|████████▋ | 463/532 [14:06<02:25,  2.10s/it]predicting train subjects:  87%|████████▋ | 464/532 [14:08<02:25,  2.14s/it]predicting train subjects:  87%|████████▋ | 465/532 [14:10<02:26,  2.19s/it]predicting train subjects:  88%|████████▊ | 466/532 [14:13<02:24,  2.20s/it]predicting train subjects:  88%|████████▊ | 467/532 [14:14<02:15,  2.08s/it]predicting train subjects:  88%|████████▊ | 468/532 [14:16<02:07,  1.99s/it]predicting train subjects:  88%|████████▊ | 469/532 [14:18<02:00,  1.91s/it]predicting train subjects:  88%|████████▊ | 470/532 [14:20<01:55,  1.86s/it]predicting train subjects:  89%|████████▊ | 471/532 [14:21<01:49,  1.79s/it]predicting train subjects:  89%|████████▊ | 472/532 [14:23<01:47,  1.78s/it]predicting train subjects:  89%|████████▉ | 473/532 [14:25<01:46,  1.80s/it]predicting train subjects:  89%|████████▉ | 474/532 [14:27<01:46,  1.83s/it]predicting train subjects:  89%|████████▉ | 475/532 [14:29<01:45,  1.85s/it]predicting train subjects:  89%|████████▉ | 476/532 [14:30<01:43,  1.86s/it]predicting train subjects:  90%|████████▉ | 477/532 [14:32<01:42,  1.86s/it]predicting train subjects:  90%|████████▉ | 478/532 [14:34<01:41,  1.88s/it]predicting train subjects:  90%|█████████ | 479/532 [14:36<01:36,  1.82s/it]predicting train subjects:  90%|█████████ | 480/532 [14:38<01:31,  1.76s/it]predicting train subjects:  90%|█████████ | 481/532 [14:39<01:27,  1.71s/it]predicting train subjects:  91%|█████████ | 482/532 [14:41<01:24,  1.68s/it]predicting train subjects:  91%|█████████ | 483/532 [14:42<01:21,  1.65s/it]predicting train subjects:  91%|█████████ | 484/532 [14:44<01:18,  1.64s/it]predicting train subjects:  91%|█████████ | 485/532 [14:46<01:23,  1.78s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:48<01:26,  1.88s/it]predicting train subjects:  92%|█████████▏| 487/532 [14:50<01:27,  1.95s/it]predicting train subjects:  92%|█████████▏| 488/532 [14:52<01:28,  2.01s/it]predicting train subjects:  92%|█████████▏| 489/532 [14:55<01:27,  2.04s/it]predicting train subjects:  92%|█████████▏| 490/532 [14:57<01:26,  2.06s/it]predicting train subjects:  92%|█████████▏| 491/532 [14:58<01:20,  1.98s/it]predicting train subjects:  92%|█████████▏| 492/532 [15:00<01:15,  1.88s/it]predicting train subjects:  93%|█████████▎| 493/532 [15:02<01:11,  1.84s/it]predicting train subjects:  93%|█████████▎| 494/532 [15:04<01:09,  1.82s/it]predicting train subjects:  93%|█████████▎| 495/532 [15:05<01:07,  1.81s/it]predicting train subjects:  93%|█████████▎| 496/532 [15:07<01:04,  1.78s/it]predicting train subjects:  93%|█████████▎| 497/532 [15:09<01:05,  1.86s/it]predicting train subjects:  94%|█████████▎| 498/532 [15:11<01:02,  1.84s/it]predicting train subjects:  94%|█████████▍| 499/532 [15:13<01:01,  1.86s/it]predicting train subjects:  94%|█████████▍| 500/532 [15:15<00:59,  1.85s/it]predicting train subjects:  94%|█████████▍| 501/532 [15:17<00:57,  1.86s/it]predicting train subjects:  94%|█████████▍| 502/532 [15:18<00:56,  1.87s/it]predicting train subjects:  95%|█████████▍| 503/532 [15:20<00:52,  1.81s/it]predicting train subjects:  95%|█████████▍| 504/532 [15:22<00:49,  1.77s/it]predicting train subjects:  95%|█████████▍| 505/532 [15:23<00:46,  1.72s/it]predicting train subjects:  95%|█████████▌| 506/532 [15:25<00:43,  1.68s/it]predicting train subjects:  95%|█████████▌| 507/532 [15:27<00:41,  1.66s/it]predicting train subjects:  95%|█████████▌| 508/532 [15:28<00:39,  1.65s/it]predicting train subjects:  96%|█████████▌| 509/532 [15:30<00:41,  1.79s/it]predicting train subjects:  96%|█████████▌| 510/532 [15:33<00:42,  1.93s/it]predicting train subjects:  96%|█████████▌| 511/532 [15:35<00:41,  2.00s/it]predicting train subjects:  96%|█████████▌| 512/532 [15:37<00:40,  2.04s/it]predicting train subjects:  96%|█████████▋| 513/532 [15:39<00:39,  2.06s/it]predicting train subjects:  97%|█████████▋| 514/532 [15:41<00:36,  2.04s/it]predicting train subjects:  97%|█████████▋| 515/532 [15:43<00:32,  1.93s/it]predicting train subjects:  97%|█████████▋| 516/532 [15:44<00:30,  1.88s/it]predicting train subjects:  97%|█████████▋| 517/532 [15:46<00:27,  1.85s/it]predicting train subjects:  97%|█████████▋| 518/532 [15:48<00:25,  1.83s/it]predicting train subjects:  98%|█████████▊| 519/532 [15:50<00:23,  1.82s/it]predicting train subjects:  98%|█████████▊| 520/532 [15:52<00:21,  1.79s/it]predicting train subjects:  98%|█████████▊| 521/532 [15:54<00:20,  1.87s/it]predicting train subjects:  98%|█████████▊| 522/532 [15:55<00:18,  1.86s/it]predicting train subjects:  98%|█████████▊| 523/532 [15:57<00:16,  1.86s/it]predicting train subjects:  98%|█████████▊| 524/532 [15:59<00:14,  1.86s/it]predicting train subjects:  99%|█████████▊| 525/532 [16:01<00:13,  1.86s/it]predicting train subjects:  99%|█████████▉| 526/532 [16:03<00:11,  1.87s/it]predicting train subjects:  99%|█████████▉| 527/532 [16:05<00:09,  1.83s/it]predicting train subjects:  99%|█████████▉| 528/532 [16:06<00:07,  1.80s/it]predicting train subjects:  99%|█████████▉| 529/532 [16:08<00:05,  1.76s/it]predicting train subjects: 100%|█████████▉| 530/532 [16:10<00:03,  1.73s/it]predicting train subjects: 100%|█████████▉| 531/532 [16:11<00:01,  1.73s/it]predicting train subjects: 100%|██████████| 532/532 [16:13<00:00,  1.71s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<13:22,  1.51s/it]Loading train:   0%|          | 2/532 [00:02<12:08,  1.38s/it]Loading train:   1%|          | 3/532 [00:03<11:23,  1.29s/it]Loading train:   1%|          | 4/532 [00:04<10:42,  1.22s/it]Loading train:   1%|          | 5/532 [00:05<10:23,  1.18s/it]Loading train:   1%|          | 6/532 [00:06<09:52,  1.13s/it]Loading train:   1%|▏         | 7/532 [00:07<09:42,  1.11s/it]Loading train:   2%|▏         | 8/532 [00:08<09:24,  1.08s/it]Loading train:   2%|▏         | 9/532 [00:09<09:28,  1.09s/it]Loading train:   2%|▏         | 10/532 [00:11<09:27,  1.09s/it]Loading train:   2%|▏         | 11/532 [00:12<09:17,  1.07s/it]Loading train:   2%|▏         | 12/532 [00:13<09:41,  1.12s/it]Loading train:   2%|▏         | 13/532 [00:14<09:22,  1.08s/it]Loading train:   3%|▎         | 14/532 [00:15<09:14,  1.07s/it]Loading train:   3%|▎         | 15/532 [00:16<09:06,  1.06s/it]Loading train:   3%|▎         | 16/532 [00:17<09:00,  1.05s/it]Loading train:   3%|▎         | 17/532 [00:18<08:43,  1.02s/it]Loading train:   3%|▎         | 18/532 [00:19<08:58,  1.05s/it]Loading train:   4%|▎         | 19/532 [00:20<08:43,  1.02s/it]Loading train:   4%|▍         | 20/532 [00:21<09:12,  1.08s/it]Loading train:   4%|▍         | 21/532 [00:23<09:55,  1.16s/it]Loading train:   4%|▍         | 22/532 [00:24<09:49,  1.16s/it]Loading train:   4%|▍         | 23/532 [00:25<09:47,  1.15s/it]Loading train:   5%|▍         | 24/532 [00:26<09:26,  1.12s/it]Loading train:   5%|▍         | 25/532 [00:27<09:59,  1.18s/it]Loading train:   5%|▍         | 26/532 [00:28<09:34,  1.14s/it]Loading train:   5%|▌         | 27/532 [00:29<09:48,  1.17s/it]Loading train:   5%|▌         | 28/532 [00:30<09:12,  1.10s/it]Loading train:   5%|▌         | 29/532 [00:31<09:14,  1.10s/it]Loading train:   6%|▌         | 30/532 [00:32<08:45,  1.05s/it]Loading train:   6%|▌         | 31/532 [00:33<08:33,  1.03s/it]Loading train:   6%|▌         | 32/532 [00:34<08:38,  1.04s/it]Loading train:   6%|▌         | 33/532 [00:35<08:17,  1.00it/s]Loading train:   6%|▋         | 34/532 [00:37<08:41,  1.05s/it]Loading train:   7%|▋         | 35/532 [00:38<08:38,  1.04s/it]Loading train:   7%|▋         | 36/532 [00:39<08:44,  1.06s/it]Loading train:   7%|▋         | 37/532 [00:40<08:49,  1.07s/it]Loading train:   7%|▋         | 38/532 [00:41<09:11,  1.12s/it]Loading train:   7%|▋         | 39/532 [00:42<08:58,  1.09s/it]Loading train:   8%|▊         | 40/532 [00:43<08:42,  1.06s/it]Loading train:   8%|▊         | 41/532 [00:44<08:55,  1.09s/it]Loading train:   8%|▊         | 42/532 [00:45<08:58,  1.10s/it]Loading train:   8%|▊         | 43/532 [00:46<08:33,  1.05s/it]Loading train:   8%|▊         | 44/532 [00:47<08:25,  1.04s/it]Loading train:   8%|▊         | 45/532 [00:48<08:35,  1.06s/it]Loading train:   9%|▊         | 46/532 [00:49<08:40,  1.07s/it]Loading train:   9%|▉         | 47/532 [00:51<08:54,  1.10s/it]Loading train:   9%|▉         | 48/532 [00:52<08:52,  1.10s/it]Loading train:   9%|▉         | 49/532 [00:53<08:19,  1.03s/it]Loading train:   9%|▉         | 50/532 [00:54<08:25,  1.05s/it]Loading train:  10%|▉         | 51/532 [00:55<08:35,  1.07s/it]Loading train:  10%|▉         | 52/532 [00:56<08:25,  1.05s/it]Loading train:  10%|▉         | 53/532 [00:57<08:24,  1.05s/it]Loading train:  10%|█         | 54/532 [00:58<08:39,  1.09s/it]Loading train:  10%|█         | 55/532 [00:59<08:44,  1.10s/it]Loading train:  11%|█         | 56/532 [01:00<08:30,  1.07s/it]Loading train:  11%|█         | 57/532 [01:01<08:15,  1.04s/it]Loading train:  11%|█         | 58/532 [01:02<08:20,  1.06s/it]Loading train:  11%|█         | 59/532 [01:03<08:45,  1.11s/it]Loading train:  11%|█▏        | 60/532 [01:04<08:08,  1.03s/it]Loading train:  11%|█▏        | 61/532 [01:05<07:45,  1.01it/s]Loading train:  12%|█▏        | 62/532 [01:06<08:02,  1.03s/it]Loading train:  12%|█▏        | 63/532 [01:07<08:19,  1.06s/it]Loading train:  12%|█▏        | 64/532 [01:08<08:03,  1.03s/it]Loading train:  12%|█▏        | 65/532 [01:09<08:06,  1.04s/it]Loading train:  12%|█▏        | 66/532 [01:11<08:38,  1.11s/it]Loading train:  13%|█▎        | 67/532 [01:12<08:50,  1.14s/it]Loading train:  13%|█▎        | 68/532 [01:13<08:31,  1.10s/it]Loading train:  13%|█▎        | 69/532 [01:14<08:14,  1.07s/it]Loading train:  13%|█▎        | 70/532 [01:15<07:47,  1.01s/it]Loading train:  13%|█▎        | 71/532 [01:16<07:45,  1.01s/it]Loading train:  14%|█▎        | 72/532 [01:17<07:33,  1.01it/s]Loading train:  14%|█▎        | 73/532 [01:18<07:48,  1.02s/it]Loading train:  14%|█▍        | 74/532 [01:19<08:13,  1.08s/it]Loading train:  14%|█▍        | 75/532 [01:21<09:31,  1.25s/it]Loading train:  14%|█▍        | 76/532 [01:22<09:02,  1.19s/it]Loading train:  14%|█▍        | 77/532 [01:23<08:43,  1.15s/it]Loading train:  15%|█▍        | 78/532 [01:24<08:36,  1.14s/it]Loading train:  15%|█▍        | 79/532 [01:25<08:21,  1.11s/it]Loading train:  15%|█▌        | 80/532 [01:26<08:18,  1.10s/it]Loading train:  15%|█▌        | 81/532 [01:27<08:06,  1.08s/it]Loading train:  15%|█▌        | 82/532 [01:28<08:03,  1.07s/it]Loading train:  16%|█▌        | 83/532 [01:29<07:49,  1.05s/it]Loading train:  16%|█▌        | 84/532 [01:30<07:50,  1.05s/it]Loading train:  16%|█▌        | 85/532 [01:31<07:29,  1.00s/it]Loading train:  16%|█▌        | 86/532 [01:32<07:19,  1.01it/s]Loading train:  16%|█▋        | 87/532 [01:33<07:13,  1.03it/s]Loading train:  17%|█▋        | 88/532 [01:34<07:04,  1.05it/s]Loading train:  17%|█▋        | 89/532 [01:35<07:02,  1.05it/s]Loading train:  17%|█▋        | 90/532 [01:36<07:12,  1.02it/s]Loading train:  17%|█▋        | 91/532 [01:37<07:16,  1.01it/s]Loading train:  17%|█▋        | 92/532 [01:38<07:19,  1.00it/s]Loading train:  17%|█▋        | 93/532 [01:39<07:11,  1.02it/s]Loading train:  18%|█▊        | 94/532 [01:40<07:02,  1.04it/s]Loading train:  18%|█▊        | 95/532 [01:41<07:46,  1.07s/it]Loading train:  18%|█▊        | 96/532 [01:42<07:42,  1.06s/it]Loading train:  18%|█▊        | 97/532 [01:43<07:42,  1.06s/it]Loading train:  18%|█▊        | 98/532 [01:44<07:45,  1.07s/it]Loading train:  19%|█▊        | 99/532 [01:45<07:54,  1.10s/it]Loading train:  19%|█▉        | 100/532 [01:47<07:47,  1.08s/it]Loading train:  19%|█▉        | 101/532 [01:47<07:20,  1.02s/it]Loading train:  19%|█▉        | 102/532 [01:48<06:59,  1.02it/s]Loading train:  19%|█▉        | 103/532 [01:49<06:46,  1.06it/s]Loading train:  20%|█▉        | 104/532 [01:50<06:55,  1.03it/s]Loading train:  20%|█▉        | 105/532 [01:51<06:59,  1.02it/s]Loading train:  20%|█▉        | 106/532 [01:52<06:58,  1.02it/s]Loading train:  20%|██        | 107/532 [01:53<06:46,  1.04it/s]Loading train:  20%|██        | 108/532 [01:54<06:51,  1.03it/s]Loading train:  20%|██        | 109/532 [01:55<06:46,  1.04it/s]Loading train:  21%|██        | 110/532 [01:56<06:43,  1.05it/s]Loading train:  21%|██        | 111/532 [01:57<06:31,  1.08it/s]Loading train:  21%|██        | 112/532 [01:58<06:28,  1.08it/s]Loading train:  21%|██        | 113/532 [01:59<06:42,  1.04it/s]Loading train:  21%|██▏       | 114/532 [02:00<06:38,  1.05it/s]Loading train:  22%|██▏       | 115/532 [02:01<06:49,  1.02it/s]Loading train:  22%|██▏       | 116/532 [02:02<06:45,  1.03it/s]Loading train:  22%|██▏       | 117/532 [02:03<06:40,  1.04it/s]Loading train:  22%|██▏       | 118/532 [02:04<06:43,  1.03it/s]Loading train:  22%|██▏       | 119/532 [02:05<06:45,  1.02it/s]Loading train:  23%|██▎       | 120/532 [02:06<06:45,  1.02it/s]Loading train:  23%|██▎       | 121/532 [02:07<06:48,  1.01it/s]Loading train:  23%|██▎       | 122/532 [02:08<06:51,  1.00s/it]Loading train:  23%|██▎       | 123/532 [02:09<06:43,  1.01it/s]Loading train:  23%|██▎       | 124/532 [02:10<06:40,  1.02it/s]Loading train:  23%|██▎       | 125/532 [02:11<06:49,  1.01s/it]Loading train:  24%|██▎       | 126/532 [02:12<06:45,  1.00it/s]Loading train:  24%|██▍       | 127/532 [02:13<06:48,  1.01s/it]Loading train:  24%|██▍       | 128/532 [02:14<06:48,  1.01s/it]Loading train:  24%|██▍       | 129/532 [02:15<06:49,  1.02s/it]Loading train:  24%|██▍       | 130/532 [02:16<06:45,  1.01s/it]Loading train:  25%|██▍       | 131/532 [02:17<07:04,  1.06s/it]Loading train:  25%|██▍       | 132/532 [02:18<07:15,  1.09s/it]Loading train:  25%|██▌       | 133/532 [02:19<07:42,  1.16s/it]Loading train:  25%|██▌       | 134/532 [02:21<07:47,  1.17s/it]Loading train:  25%|██▌       | 135/532 [02:22<07:46,  1.18s/it]Loading train:  26%|██▌       | 136/532 [02:23<08:01,  1.21s/it]Loading train:  26%|██▌       | 137/532 [02:24<08:13,  1.25s/it]Loading train:  26%|██▌       | 138/532 [02:26<08:09,  1.24s/it]Loading train:  26%|██▌       | 139/532 [02:27<08:09,  1.25s/it]Loading train:  26%|██▋       | 140/532 [02:28<08:17,  1.27s/it]Loading train:  27%|██▋       | 141/532 [02:29<08:12,  1.26s/it]Loading train:  27%|██▋       | 142/532 [02:31<08:01,  1.23s/it]Loading train:  27%|██▋       | 143/532 [02:32<07:33,  1.17s/it]Loading train:  27%|██▋       | 144/532 [02:33<07:10,  1.11s/it]Loading train:  27%|██▋       | 145/532 [02:34<06:50,  1.06s/it]Loading train:  27%|██▋       | 146/532 [02:34<06:35,  1.02s/it]Loading train:  28%|██▊       | 147/532 [02:35<06:27,  1.01s/it]Loading train:  28%|██▊       | 148/532 [02:36<06:17,  1.02it/s]Loading train:  28%|██▊       | 149/532 [02:37<06:27,  1.01s/it]Loading train:  28%|██▊       | 150/532 [02:38<06:23,  1.01s/it]Loading train:  28%|██▊       | 151/532 [02:39<06:21,  1.00s/it]Loading train:  29%|██▊       | 152/532 [02:40<06:15,  1.01it/s]Loading train:  29%|██▉       | 153/532 [02:41<06:10,  1.02it/s]Loading train:  29%|██▉       | 154/532 [02:42<06:12,  1.01it/s]Loading train:  29%|██▉       | 155/532 [02:44<06:45,  1.08s/it]Loading train:  29%|██▉       | 156/532 [02:45<07:05,  1.13s/it]Loading train:  30%|██▉       | 157/532 [02:46<07:18,  1.17s/it]Loading train:  30%|██▉       | 158/532 [02:47<07:32,  1.21s/it]Loading train:  30%|██▉       | 159/532 [02:49<07:37,  1.23s/it]Loading train:  30%|███       | 160/532 [02:50<07:44,  1.25s/it]Loading train:  30%|███       | 161/532 [02:51<07:14,  1.17s/it]Loading train:  30%|███       | 162/532 [02:52<06:51,  1.11s/it]Loading train:  31%|███       | 163/532 [02:53<06:46,  1.10s/it]Loading train:  31%|███       | 164/532 [02:54<06:28,  1.06s/it]Loading train:  31%|███       | 165/532 [02:55<06:24,  1.05s/it]Loading train:  31%|███       | 166/532 [02:56<06:23,  1.05s/it]Loading train:  31%|███▏      | 167/532 [02:57<06:17,  1.03s/it]Loading train:  32%|███▏      | 168/532 [02:58<06:09,  1.01s/it]Loading train:  32%|███▏      | 169/532 [02:59<05:59,  1.01it/s]Loading train:  32%|███▏      | 170/532 [03:00<05:48,  1.04it/s]Loading train:  32%|███▏      | 171/532 [03:01<05:41,  1.06it/s]Loading train:  32%|███▏      | 172/532 [03:02<05:38,  1.06it/s]Loading train:  33%|███▎      | 173/532 [03:03<05:33,  1.08it/s]Loading train:  33%|███▎      | 174/532 [03:03<05:22,  1.11it/s]Loading train:  33%|███▎      | 175/532 [03:04<05:14,  1.13it/s]Loading train:  33%|███▎      | 176/532 [03:05<05:14,  1.13it/s]Loading train:  33%|███▎      | 177/532 [03:06<05:10,  1.14it/s]Loading train:  33%|███▎      | 178/532 [03:07<05:09,  1.14it/s]Loading train:  34%|███▎      | 179/532 [03:08<05:22,  1.09it/s]Loading train:  34%|███▍      | 180/532 [03:09<05:30,  1.07it/s]Loading train:  34%|███▍      | 181/532 [03:10<05:31,  1.06it/s]Loading train:  34%|███▍      | 182/532 [03:11<05:26,  1.07it/s]Loading train:  34%|███▍      | 183/532 [03:12<05:26,  1.07it/s]Loading train:  35%|███▍      | 184/532 [03:13<05:30,  1.05it/s]Loading train:  35%|███▍      | 185/532 [03:14<05:29,  1.05it/s]Loading train:  35%|███▍      | 186/532 [03:15<05:22,  1.07it/s]Loading train:  35%|███▌      | 187/532 [03:15<05:18,  1.08it/s]Loading train:  35%|███▌      | 188/532 [03:16<05:14,  1.09it/s]Loading train:  36%|███▌      | 189/532 [03:17<05:12,  1.10it/s]Loading train:  36%|███▌      | 190/532 [03:18<05:07,  1.11it/s]Loading train:  36%|███▌      | 191/532 [03:19<05:42,  1.00s/it]Loading train:  36%|███▌      | 192/532 [03:21<06:02,  1.07s/it]Loading train:  36%|███▋      | 193/532 [03:22<06:20,  1.12s/it]Loading train:  36%|███▋      | 194/532 [03:23<06:28,  1.15s/it]Loading train:  37%|███▋      | 195/532 [03:24<06:36,  1.18s/it]Loading train:  37%|███▋      | 196/532 [03:26<06:38,  1.19s/it]Loading train:  37%|███▋      | 197/532 [03:27<06:35,  1.18s/it]Loading train:  37%|███▋      | 198/532 [03:28<06:28,  1.16s/it]Loading train:  37%|███▋      | 199/532 [03:29<06:16,  1.13s/it]Loading train:  38%|███▊      | 200/532 [03:30<06:09,  1.11s/it]Loading train:  38%|███▊      | 201/532 [03:31<06:07,  1.11s/it]Loading train:  38%|███▊      | 202/532 [03:32<06:05,  1.11s/it]Loading train:  38%|███▊      | 203/532 [03:33<05:47,  1.06s/it]Loading train:  38%|███▊      | 204/532 [03:34<05:25,  1.01it/s]Loading train:  39%|███▊      | 205/532 [03:35<05:16,  1.03it/s]Loading train:  39%|███▊      | 206/532 [03:36<05:07,  1.06it/s]Loading train:  39%|███▉      | 207/532 [03:37<05:07,  1.06it/s]Loading train:  39%|███▉      | 208/532 [03:38<05:08,  1.05it/s]Loading train:  39%|███▉      | 209/532 [03:39<05:08,  1.05it/s]Loading train:  39%|███▉      | 210/532 [03:39<04:57,  1.08it/s]Loading train:  40%|███▉      | 211/532 [03:40<04:58,  1.07it/s]Loading train:  40%|███▉      | 212/532 [03:41<04:56,  1.08it/s]Loading train:  40%|████      | 213/532 [03:42<04:44,  1.12it/s]Loading train:  40%|████      | 214/532 [03:43<04:32,  1.17it/s]Loading train:  40%|████      | 215/532 [03:44<05:02,  1.05it/s]Loading train:  41%|████      | 216/532 [03:45<05:11,  1.02it/s]Loading train:  41%|████      | 217/532 [03:46<05:18,  1.01s/it]Loading train:  41%|████      | 218/532 [03:47<05:20,  1.02s/it]Loading train:  41%|████      | 219/532 [03:48<05:28,  1.05s/it]Loading train:  41%|████▏     | 220/532 [03:50<05:40,  1.09s/it]Loading train:  42%|████▏     | 221/532 [03:50<05:22,  1.04s/it]Loading train:  42%|████▏     | 222/532 [03:51<05:05,  1.02it/s]Loading train:  42%|████▏     | 223/532 [03:52<04:53,  1.05it/s]Loading train:  42%|████▏     | 224/532 [03:53<04:40,  1.10it/s]Loading train:  42%|████▏     | 225/532 [03:54<04:30,  1.14it/s]Loading train:  42%|████▏     | 226/532 [03:55<04:26,  1.15it/s]Loading train:  43%|████▎     | 227/532 [03:56<04:21,  1.17it/s]Loading train:  43%|████▎     | 228/532 [03:56<04:13,  1.20it/s]Loading train:  43%|████▎     | 229/532 [03:57<04:09,  1.21it/s]Loading train:  43%|████▎     | 230/532 [03:58<04:04,  1.23it/s]Loading train:  43%|████▎     | 231/532 [03:59<04:01,  1.24it/s]Loading train:  44%|████▎     | 232/532 [03:59<04:00,  1.25it/s]Loading train:  44%|████▍     | 233/532 [04:00<04:06,  1.21it/s]Loading train:  44%|████▍     | 234/532 [04:01<04:16,  1.16it/s]Loading train:  44%|████▍     | 235/532 [04:02<04:16,  1.16it/s]Loading train:  44%|████▍     | 236/532 [04:03<04:17,  1.15it/s]Loading train:  45%|████▍     | 237/532 [04:04<04:15,  1.15it/s]Loading train:  45%|████▍     | 238/532 [04:05<04:16,  1.15it/s]Loading train:  45%|████▍     | 239/532 [04:06<04:25,  1.10it/s]Loading train:  45%|████▌     | 240/532 [04:07<04:25,  1.10it/s]Loading train:  45%|████▌     | 241/532 [04:08<04:24,  1.10it/s]Loading train:  45%|████▌     | 242/532 [04:08<04:24,  1.10it/s]Loading train:  46%|████▌     | 243/532 [04:09<04:27,  1.08it/s]Loading train:  46%|████▌     | 244/532 [04:10<04:28,  1.07it/s]Loading train:  46%|████▌     | 245/532 [04:11<04:23,  1.09it/s]Loading train:  46%|████▌     | 246/532 [04:12<04:12,  1.13it/s]Loading train:  46%|████▋     | 247/532 [04:13<04:02,  1.17it/s]Loading train:  47%|████▋     | 248/532 [04:14<03:57,  1.20it/s]Loading train:  47%|████▋     | 249/532 [04:14<03:53,  1.21it/s]Loading train:  47%|████▋     | 250/532 [04:15<03:50,  1.22it/s]Loading train:  47%|████▋     | 251/532 [04:16<03:50,  1.22it/s]Loading train:  47%|████▋     | 252/532 [04:17<03:47,  1.23it/s]Loading train:  48%|████▊     | 253/532 [04:18<03:54,  1.19it/s]Loading train:  48%|████▊     | 254/532 [04:19<03:58,  1.17it/s]Loading train:  48%|████▊     | 255/532 [04:20<03:54,  1.18it/s]Loading train:  48%|████▊     | 256/532 [04:20<03:54,  1.18it/s]Loading train:  48%|████▊     | 257/532 [04:21<04:15,  1.08it/s]Loading train:  48%|████▊     | 258/532 [04:23<04:24,  1.04it/s]Loading train:  49%|████▊     | 259/532 [04:24<04:31,  1.01it/s]Loading train:  49%|████▉     | 260/532 [04:25<04:35,  1.01s/it]Loading train:  49%|████▉     | 261/532 [04:26<04:38,  1.03s/it]Loading train:  49%|████▉     | 262/532 [04:27<04:42,  1.05s/it]Loading train:  49%|████▉     | 263/532 [04:28<04:22,  1.02it/s]Loading train:  50%|████▉     | 264/532 [04:28<04:08,  1.08it/s]Loading train:  50%|████▉     | 265/532 [04:29<03:56,  1.13it/s]Loading train:  50%|█████     | 266/532 [04:30<04:15,  1.04it/s]Loading train:  50%|█████     | 267/532 [04:32<04:38,  1.05s/it]Loading train:  50%|█████     | 268/532 [04:33<04:47,  1.09s/it]Loading train:  51%|█████     | 269/532 [04:34<05:19,  1.21s/it]Loading train:  51%|█████     | 270/532 [04:36<05:26,  1.25s/it]Loading train:  51%|█████     | 271/532 [04:37<05:29,  1.26s/it]Loading train:  51%|█████     | 272/532 [04:38<05:40,  1.31s/it]Loading train:  51%|█████▏    | 273/532 [04:40<05:49,  1.35s/it]Loading train:  52%|█████▏    | 274/532 [04:41<05:44,  1.34s/it]Loading train:  52%|█████▏    | 275/532 [04:43<06:01,  1.41s/it]Loading train:  52%|█████▏    | 276/532 [04:44<06:06,  1.43s/it]Loading train:  52%|█████▏    | 277/532 [04:45<05:58,  1.40s/it]Loading train:  52%|█████▏    | 278/532 [04:47<05:50,  1.38s/it]Loading train:  52%|█████▏    | 279/532 [04:48<05:53,  1.40s/it]Loading train:  53%|█████▎    | 280/532 [04:50<05:59,  1.43s/it]Loading train:  53%|█████▎    | 281/532 [04:51<06:20,  1.51s/it]Loading train:  53%|█████▎    | 282/532 [04:53<06:19,  1.52s/it]Loading train:  53%|█████▎    | 283/532 [04:54<06:06,  1.47s/it]Loading train:  53%|█████▎    | 284/532 [04:56<06:03,  1.47s/it]Loading train:  54%|█████▎    | 285/532 [04:57<05:57,  1.45s/it]Loading train:  54%|█████▍    | 286/532 [04:59<05:53,  1.44s/it]Loading train:  54%|█████▍    | 287/532 [05:00<05:44,  1.41s/it]Loading train:  54%|█████▍    | 288/532 [05:01<05:36,  1.38s/it]Loading train:  54%|█████▍    | 289/532 [05:02<05:23,  1.33s/it]Loading train:  55%|█████▍    | 290/532 [05:04<05:13,  1.29s/it]Loading train:  55%|█████▍    | 291/532 [05:05<05:12,  1.30s/it]Loading train:  55%|█████▍    | 292/532 [05:06<05:10,  1.30s/it]Loading train:  55%|█████▌    | 293/532 [05:08<05:18,  1.33s/it]Loading train:  55%|█████▌    | 294/532 [05:09<05:11,  1.31s/it]Loading train:  55%|█████▌    | 295/532 [05:10<05:19,  1.35s/it]Loading train:  56%|█████▌    | 296/532 [05:12<05:19,  1.35s/it]Loading train:  56%|█████▌    | 297/532 [05:13<05:17,  1.35s/it]Loading train:  56%|█████▌    | 298/532 [05:14<05:08,  1.32s/it]Loading train:  56%|█████▌    | 299/532 [05:16<04:55,  1.27s/it]Loading train:  56%|█████▋    | 300/532 [05:17<04:58,  1.29s/it]Loading train:  57%|█████▋    | 301/532 [05:18<04:53,  1.27s/it]Loading train:  57%|█████▋    | 302/532 [05:19<04:45,  1.24s/it]Loading train:  57%|█████▋    | 303/532 [05:21<04:48,  1.26s/it]Loading train:  57%|█████▋    | 304/532 [05:22<04:46,  1.25s/it]Loading train:  57%|█████▋    | 305/532 [05:23<04:58,  1.32s/it]Loading train:  58%|█████▊    | 306/532 [05:25<05:30,  1.46s/it]Loading train:  58%|█████▊    | 307/532 [05:27<05:39,  1.51s/it]Loading train:  58%|█████▊    | 308/532 [05:28<05:43,  1.53s/it]Loading train:  58%|█████▊    | 309/532 [05:30<05:50,  1.57s/it]Loading train:  58%|█████▊    | 310/532 [05:32<06:02,  1.63s/it]Loading train:  58%|█████▊    | 311/532 [05:34<06:36,  1.80s/it]Loading train:  59%|█████▊    | 312/532 [05:36<06:57,  1.90s/it]Loading train:  59%|█████▉    | 313/532 [05:38<07:04,  1.94s/it]Loading train:  59%|█████▉    | 314/532 [05:40<06:58,  1.92s/it]Loading train:  59%|█████▉    | 315/532 [05:42<07:11,  1.99s/it]Loading train:  59%|█████▉    | 316/532 [05:44<07:12,  2.00s/it]Loading train:  60%|█████▉    | 317/532 [05:45<06:25,  1.79s/it]Loading train:  60%|█████▉    | 318/532 [05:47<06:09,  1.73s/it]Loading train:  60%|█████▉    | 319/532 [05:48<05:38,  1.59s/it]Loading train:  60%|██████    | 320/532 [05:50<05:33,  1.57s/it]Loading train:  60%|██████    | 321/532 [05:51<05:21,  1.52s/it]Loading train:  61%|██████    | 322/532 [05:52<05:02,  1.44s/it]Loading train:  61%|██████    | 323/532 [05:54<05:17,  1.52s/it]Loading train:  61%|██████    | 324/532 [05:56<05:25,  1.56s/it]Loading train:  61%|██████    | 325/532 [05:58<05:34,  1.62s/it]Loading train:  61%|██████▏   | 326/532 [05:59<05:35,  1.63s/it]Loading train:  61%|██████▏   | 327/532 [06:01<05:30,  1.61s/it]Loading train:  62%|██████▏   | 328/532 [06:03<05:37,  1.65s/it]Loading train:  62%|██████▏   | 329/532 [06:04<05:30,  1.63s/it]Loading train:  62%|██████▏   | 330/532 [06:06<05:25,  1.61s/it]Loading train:  62%|██████▏   | 331/532 [06:07<05:10,  1.55s/it]Loading train:  62%|██████▏   | 332/532 [06:09<05:03,  1.52s/it]Loading train:  63%|██████▎   | 333/532 [06:10<04:43,  1.42s/it]Loading train:  63%|██████▎   | 334/532 [06:11<04:36,  1.40s/it]Loading train:  63%|██████▎   | 335/532 [06:12<04:34,  1.39s/it]Loading train:  63%|██████▎   | 336/532 [06:14<04:35,  1.41s/it]Loading train:  63%|██████▎   | 337/532 [06:15<04:35,  1.41s/it]Loading train:  64%|██████▎   | 338/532 [06:17<04:41,  1.45s/it]Loading train:  64%|██████▎   | 339/532 [06:18<04:50,  1.50s/it]Loading train:  64%|██████▍   | 340/532 [06:20<04:48,  1.50s/it]Loading train:  64%|██████▍   | 341/532 [06:21<04:34,  1.44s/it]Loading train:  64%|██████▍   | 342/532 [06:23<04:26,  1.40s/it]Loading train:  64%|██████▍   | 343/532 [06:24<04:17,  1.36s/it]Loading train:  65%|██████▍   | 344/532 [06:25<04:10,  1.33s/it]Loading train:  65%|██████▍   | 345/532 [06:27<04:15,  1.37s/it]Loading train:  65%|██████▌   | 346/532 [06:28<04:13,  1.37s/it]Loading train:  65%|██████▌   | 347/532 [06:29<04:19,  1.40s/it]Loading train:  65%|██████▌   | 348/532 [06:31<04:23,  1.43s/it]Loading train:  66%|██████▌   | 349/532 [06:32<04:18,  1.41s/it]Loading train:  66%|██████▌   | 350/532 [06:34<04:10,  1.38s/it]Loading train:  66%|██████▌   | 351/532 [06:35<04:03,  1.35s/it]Loading train:  66%|██████▌   | 352/532 [06:36<04:01,  1.34s/it]Loading train:  66%|██████▋   | 353/532 [06:37<03:54,  1.31s/it]Loading train:  67%|██████▋   | 354/532 [06:39<03:58,  1.34s/it]Loading train:  67%|██████▋   | 355/532 [06:40<03:56,  1.33s/it]Loading train:  67%|██████▋   | 356/532 [06:42<03:59,  1.36s/it]Loading train:  67%|██████▋   | 357/532 [06:43<03:55,  1.34s/it]Loading train:  67%|██████▋   | 358/532 [06:44<03:56,  1.36s/it]Loading train:  67%|██████▋   | 359/532 [06:46<03:48,  1.32s/it]Loading train:  68%|██████▊   | 360/532 [06:47<03:41,  1.29s/it]Loading train:  68%|██████▊   | 361/532 [06:48<03:44,  1.31s/it]Loading train:  68%|██████▊   | 362/532 [06:49<03:44,  1.32s/it]Loading train:  68%|██████▊   | 363/532 [06:51<03:39,  1.30s/it]Loading train:  68%|██████▊   | 364/532 [06:52<03:37,  1.30s/it]Loading train:  69%|██████▊   | 365/532 [06:53<03:34,  1.28s/it]Loading train:  69%|██████▉   | 366/532 [06:55<03:36,  1.30s/it]Loading train:  69%|██████▉   | 367/532 [06:56<03:33,  1.30s/it]Loading train:  69%|██████▉   | 368/532 [06:57<03:27,  1.27s/it]Loading train:  69%|██████▉   | 369/532 [06:58<03:26,  1.27s/it]Loading train:  70%|██████▉   | 370/532 [07:00<03:27,  1.28s/it]Loading train:  70%|██████▉   | 371/532 [07:01<03:49,  1.43s/it]Loading train:  70%|██████▉   | 372/532 [07:03<03:57,  1.48s/it]Loading train:  70%|███████   | 373/532 [07:05<04:02,  1.52s/it]Loading train:  70%|███████   | 374/532 [07:06<04:03,  1.54s/it]Loading train:  70%|███████   | 375/532 [07:08<04:01,  1.54s/it]Loading train:  71%|███████   | 376/532 [07:09<04:03,  1.56s/it]Loading train:  71%|███████   | 377/532 [07:11<03:55,  1.52s/it]Loading train:  71%|███████   | 378/532 [07:12<03:49,  1.49s/it]Loading train:  71%|███████   | 379/532 [07:14<03:40,  1.44s/it]Loading train:  71%|███████▏  | 380/532 [07:15<03:37,  1.43s/it]Loading train:  72%|███████▏  | 381/532 [07:16<03:36,  1.43s/it]Loading train:  72%|███████▏  | 382/532 [07:18<03:21,  1.34s/it]Loading train:  72%|███████▏  | 383/532 [07:19<03:18,  1.33s/it]Loading train:  72%|███████▏  | 384/532 [07:20<03:25,  1.39s/it]Loading train:  72%|███████▏  | 385/532 [07:22<03:23,  1.39s/it]Loading train:  73%|███████▎  | 386/532 [07:23<03:21,  1.38s/it]Loading train:  73%|███████▎  | 387/532 [07:24<03:20,  1.38s/it]Loading train:  73%|███████▎  | 388/532 [07:26<03:23,  1.41s/it]Loading train:  73%|███████▎  | 389/532 [07:27<03:24,  1.43s/it]Loading train:  73%|███████▎  | 390/532 [07:29<03:22,  1.43s/it]Loading train:  73%|███████▎  | 391/532 [07:30<03:24,  1.45s/it]Loading train:  74%|███████▎  | 392/532 [07:32<03:26,  1.48s/it]Loading train:  74%|███████▍  | 393/532 [07:33<03:28,  1.50s/it]Loading train:  74%|███████▍  | 394/532 [07:35<03:29,  1.52s/it]Loading train:  74%|███████▍  | 395/532 [07:36<03:25,  1.50s/it]Loading train:  74%|███████▍  | 396/532 [07:38<03:17,  1.46s/it]Loading train:  75%|███████▍  | 397/532 [07:39<03:16,  1.45s/it]Loading train:  75%|███████▍  | 398/532 [07:41<03:28,  1.56s/it]Loading train:  75%|███████▌  | 399/532 [07:42<03:17,  1.49s/it]Loading train:  75%|███████▌  | 400/532 [07:44<03:13,  1.47s/it]Loading train:  75%|███████▌  | 401/532 [07:45<03:16,  1.50s/it]Loading train:  76%|███████▌  | 402/532 [07:47<03:11,  1.47s/it]Loading train:  76%|███████▌  | 403/532 [07:48<03:12,  1.50s/it]Loading train:  76%|███████▌  | 404/532 [07:50<03:06,  1.46s/it]Loading train:  76%|███████▌  | 405/532 [07:51<03:02,  1.44s/it]Loading train:  76%|███████▋  | 406/532 [07:53<03:10,  1.51s/it]Loading train:  77%|███████▋  | 407/532 [07:54<03:03,  1.47s/it]Loading train:  77%|███████▋  | 408/532 [07:56<03:01,  1.47s/it]Loading train:  77%|███████▋  | 409/532 [07:57<02:51,  1.40s/it]Loading train:  77%|███████▋  | 410/532 [07:58<02:54,  1.43s/it]Loading train:  77%|███████▋  | 411/532 [08:00<02:54,  1.45s/it]Loading train:  77%|███████▋  | 412/532 [08:01<02:56,  1.47s/it]Loading train:  78%|███████▊  | 413/532 [08:03<02:52,  1.45s/it]Loading train:  78%|███████▊  | 414/532 [08:04<02:46,  1.41s/it]Loading train:  78%|███████▊  | 415/532 [08:06<02:49,  1.45s/it]Loading train:  78%|███████▊  | 416/532 [08:07<02:46,  1.43s/it]Loading train:  78%|███████▊  | 417/532 [08:08<02:43,  1.42s/it]Loading train:  79%|███████▊  | 418/532 [08:10<02:31,  1.33s/it]Loading train:  79%|███████▉  | 419/532 [08:11<02:25,  1.29s/it]Loading train:  79%|███████▉  | 420/532 [08:12<02:21,  1.26s/it]Loading train:  79%|███████▉  | 421/532 [08:13<02:20,  1.26s/it]Loading train:  79%|███████▉  | 422/532 [08:14<02:15,  1.23s/it]Loading train:  80%|███████▉  | 423/532 [08:15<02:10,  1.20s/it]Loading train:  80%|███████▉  | 424/532 [08:17<02:06,  1.17s/it]Loading train:  80%|███████▉  | 425/532 [08:18<02:03,  1.15s/it]Loading train:  80%|████████  | 426/532 [08:19<02:01,  1.15s/it]Loading train:  80%|████████  | 427/532 [08:20<02:06,  1.20s/it]Loading train:  80%|████████  | 428/532 [08:22<02:18,  1.33s/it]Loading train:  81%|████████  | 429/532 [08:23<02:23,  1.40s/it]Loading train:  81%|████████  | 430/532 [08:25<02:25,  1.43s/it]Loading train:  81%|████████  | 431/532 [08:26<02:28,  1.47s/it]Loading train:  81%|████████  | 432/532 [08:28<02:25,  1.46s/it]Loading train:  81%|████████▏ | 433/532 [08:29<02:22,  1.44s/it]Loading train:  82%|████████▏ | 434/532 [08:31<02:20,  1.44s/it]Loading train:  82%|████████▏ | 435/532 [08:32<02:17,  1.42s/it]Loading train:  82%|████████▏ | 436/532 [08:34<02:17,  1.43s/it]Loading train:  82%|████████▏ | 437/532 [08:35<02:09,  1.36s/it]Loading train:  82%|████████▏ | 438/532 [08:36<02:07,  1.35s/it]Loading train:  83%|████████▎ | 439/532 [08:37<02:00,  1.30s/it]Loading train:  83%|████████▎ | 440/532 [08:38<01:54,  1.25s/it]Loading train:  83%|████████▎ | 441/532 [08:40<01:53,  1.24s/it]Loading train:  83%|████████▎ | 442/532 [08:41<01:54,  1.27s/it]Loading train:  83%|████████▎ | 443/532 [08:42<01:51,  1.25s/it]Loading train:  83%|████████▎ | 444/532 [08:43<01:50,  1.26s/it]Loading train:  84%|████████▎ | 445/532 [08:45<01:46,  1.23s/it]Loading train:  84%|████████▍ | 446/532 [08:46<01:45,  1.23s/it]Loading train:  84%|████████▍ | 447/532 [08:47<01:47,  1.27s/it]Loading train:  84%|████████▍ | 448/532 [08:48<01:47,  1.28s/it]Loading train:  84%|████████▍ | 449/532 [08:50<01:44,  1.25s/it]Loading train:  85%|████████▍ | 450/532 [08:51<01:44,  1.28s/it]Loading train:  85%|████████▍ | 451/532 [08:52<01:48,  1.34s/it]Loading train:  85%|████████▍ | 452/532 [08:54<01:48,  1.36s/it]Loading train:  85%|████████▌ | 453/532 [08:55<01:45,  1.33s/it]Loading train:  85%|████████▌ | 454/532 [08:56<01:42,  1.32s/it]Loading train:  86%|████████▌ | 455/532 [08:58<01:44,  1.36s/it]Loading train:  86%|████████▌ | 456/532 [08:59<01:45,  1.39s/it]Loading train:  86%|████████▌ | 457/532 [09:01<01:43,  1.38s/it]Loading train:  86%|████████▌ | 458/532 [09:02<01:47,  1.46s/it]Loading train:  86%|████████▋ | 459/532 [09:04<01:48,  1.49s/it]Loading train:  86%|████████▋ | 460/532 [09:05<01:43,  1.44s/it]Loading train:  87%|████████▋ | 461/532 [09:07<01:47,  1.51s/it]Loading train:  87%|████████▋ | 462/532 [09:09<01:54,  1.64s/it]Loading train:  87%|████████▋ | 463/532 [09:11<01:55,  1.67s/it]Loading train:  87%|████████▋ | 464/532 [09:12<01:53,  1.67s/it]Loading train:  87%|████████▋ | 465/532 [09:14<01:51,  1.66s/it]Loading train:  88%|████████▊ | 466/532 [09:15<01:44,  1.58s/it]Loading train:  88%|████████▊ | 467/532 [09:17<01:36,  1.49s/it]Loading train:  88%|████████▊ | 468/532 [09:18<01:30,  1.41s/it]Loading train:  88%|████████▊ | 469/532 [09:19<01:32,  1.46s/it]Loading train:  88%|████████▊ | 470/532 [09:21<01:30,  1.47s/it]Loading train:  89%|████████▊ | 471/532 [09:22<01:28,  1.45s/it]Loading train:  89%|████████▊ | 472/532 [09:23<01:22,  1.38s/it]Loading train:  89%|████████▉ | 473/532 [09:25<01:22,  1.40s/it]Loading train:  89%|████████▉ | 474/532 [09:27<01:24,  1.46s/it]Loading train:  89%|████████▉ | 475/532 [09:28<01:24,  1.48s/it]Loading train:  89%|████████▉ | 476/532 [09:29<01:21,  1.46s/it]Loading train:  90%|████████▉ | 477/532 [09:31<01:20,  1.46s/it]Loading train:  90%|████████▉ | 478/532 [09:32<01:17,  1.44s/it]Loading train:  90%|█████████ | 479/532 [09:34<01:19,  1.50s/it]Loading train:  90%|█████████ | 480/532 [09:36<01:21,  1.58s/it]Loading train:  90%|█████████ | 481/532 [09:37<01:17,  1.51s/it]Loading train:  91%|█████████ | 482/532 [09:38<01:13,  1.48s/it]Loading train:  91%|█████████ | 483/532 [09:40<01:10,  1.44s/it]Loading train:  91%|█████████ | 484/532 [09:41<01:10,  1.46s/it]Loading train:  91%|█████████ | 485/532 [09:43<01:15,  1.61s/it]Loading train:  91%|█████████▏| 486/532 [09:45<01:11,  1.56s/it]Loading train:  92%|█████████▏| 487/532 [09:46<01:09,  1.56s/it]Loading train:  92%|█████████▏| 488/532 [09:48<01:10,  1.60s/it]Loading train:  92%|█████████▏| 489/532 [09:50<01:09,  1.61s/it]Loading train:  92%|█████████▏| 490/532 [09:51<01:08,  1.63s/it]Loading train:  92%|█████████▏| 491/532 [09:53<01:04,  1.58s/it]Loading train:  92%|█████████▏| 492/532 [09:54<01:01,  1.54s/it]Loading train:  93%|█████████▎| 493/532 [09:55<00:57,  1.47s/it]Loading train:  93%|█████████▎| 494/532 [09:57<00:54,  1.44s/it]Loading train:  93%|█████████▎| 495/532 [09:58<00:54,  1.46s/it]Loading train:  93%|█████████▎| 496/532 [10:00<00:50,  1.40s/it]Loading train:  93%|█████████▎| 497/532 [10:01<00:51,  1.46s/it]Loading train:  94%|█████████▎| 498/532 [10:03<00:49,  1.47s/it]Loading train:  94%|█████████▍| 499/532 [10:04<00:46,  1.42s/it]Loading train:  94%|█████████▍| 500/532 [10:05<00:44,  1.40s/it]Loading train:  94%|█████████▍| 501/532 [10:07<00:45,  1.47s/it]Loading train:  94%|█████████▍| 502/532 [10:08<00:44,  1.47s/it]Loading train:  95%|█████████▍| 503/532 [10:10<00:41,  1.44s/it]Loading train:  95%|█████████▍| 504/532 [10:11<00:40,  1.43s/it]Loading train:  95%|█████████▍| 505/532 [10:13<00:38,  1.41s/it]Loading train:  95%|█████████▌| 506/532 [10:14<00:35,  1.36s/it]Loading train:  95%|█████████▌| 507/532 [10:15<00:33,  1.34s/it]Loading train:  95%|█████████▌| 508/532 [10:17<00:33,  1.41s/it]Loading train:  96%|█████████▌| 509/532 [10:18<00:33,  1.45s/it]Loading train:  96%|█████████▌| 510/532 [10:20<00:33,  1.53s/it]Loading train:  96%|█████████▌| 511/532 [10:21<00:31,  1.51s/it]Loading train:  96%|█████████▌| 512/532 [10:23<00:31,  1.57s/it]Loading train:  96%|█████████▋| 513/532 [10:25<00:30,  1.60s/it]Loading train:  97%|█████████▋| 514/532 [10:27<00:29,  1.62s/it]Loading train:  97%|█████████▋| 515/532 [10:28<00:26,  1.58s/it]Loading train:  97%|█████████▋| 516/532 [10:29<00:23,  1.47s/it]Loading train:  97%|█████████▋| 517/532 [10:31<00:21,  1.43s/it]Loading train:  97%|█████████▋| 518/532 [10:32<00:19,  1.42s/it]Loading train:  98%|█████████▊| 519/532 [10:33<00:18,  1.41s/it]Loading train:  98%|█████████▊| 520/532 [10:35<00:16,  1.41s/it]Loading train:  98%|█████████▊| 521/532 [10:36<00:15,  1.45s/it]Loading train:  98%|█████████▊| 522/532 [10:38<00:14,  1.47s/it]Loading train:  98%|█████████▊| 523/532 [10:39<00:13,  1.52s/it]Loading train:  98%|█████████▊| 524/532 [10:41<00:11,  1.50s/it]Loading train:  99%|█████████▊| 525/532 [10:42<00:10,  1.45s/it]Loading train:  99%|█████████▉| 526/532 [10:44<00:08,  1.47s/it]Loading train:  99%|█████████▉| 527/532 [10:45<00:07,  1.45s/it]Loading train:  99%|█████████▉| 528/532 [10:47<00:06,  1.53s/it]Loading train:  99%|█████████▉| 529/532 [10:48<00:04,  1.44s/it]Loading train: 100%|█████████▉| 530/532 [10:49<00:02,  1.41s/it]Loading train: 100%|█████████▉| 531/532 [10:51<00:01,  1.37s/it]Loading train: 100%|██████████| 532/532 [10:52<00:00,  1.33s/it]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   1%|          | 4/532 [00:00<00:15, 33.21it/s]concatenating: train:   2%|▏         | 9/532 [00:00<00:14, 35.98it/s]concatenating: train:   3%|▎         | 14/532 [00:00<00:13, 38.56it/s]concatenating: train:   4%|▍         | 21/532 [00:00<00:11, 43.56it/s]concatenating: train:   5%|▌         | 27/532 [00:00<00:10, 46.15it/s]concatenating: train:   6%|▌         | 32/532 [00:00<00:12, 41.58it/s]concatenating: train:   7%|▋         | 37/532 [00:00<00:12, 41.13it/s]concatenating: train:   9%|▉         | 48/532 [00:00<00:09, 50.37it/s]concatenating: train:  10%|█         | 55/532 [00:01<00:08, 54.56it/s]concatenating: train:  12%|█▏        | 64/532 [00:01<00:07, 59.83it/s]concatenating: train:  13%|█▎        | 71/532 [00:01<00:08, 57.12it/s]concatenating: train:  15%|█▍        | 79/532 [00:01<00:07, 62.41it/s]concatenating: train:  16%|█▋        | 87/532 [00:01<00:06, 66.15it/s]concatenating: train:  19%|█▊        | 99/532 [00:01<00:05, 75.85it/s]concatenating: train:  20%|██        | 108/532 [00:01<00:06, 64.54it/s]concatenating: train:  22%|██▏       | 118/532 [00:01<00:05, 70.98it/s]concatenating: train:  24%|██▎       | 126/532 [00:02<00:05, 71.89it/s]concatenating: train:  26%|██▌       | 137/532 [00:02<00:04, 79.54it/s]concatenating: train:  28%|██▊       | 149/532 [00:02<00:04, 88.47it/s]concatenating: train:  30%|██▉       | 159/532 [00:02<00:04, 75.43it/s]concatenating: train:  32%|███▏      | 168/532 [00:02<00:05, 69.42it/s]concatenating: train:  33%|███▎      | 176/532 [00:02<00:05, 65.23it/s]concatenating: train:  35%|███▍      | 184/532 [00:02<00:05, 63.77it/s]concatenating: train:  37%|███▋      | 196/532 [00:02<00:04, 72.80it/s]concatenating: train:  38%|███▊      | 204/532 [00:03<00:06, 54.47it/s]concatenating: train:  40%|███▉      | 211/532 [00:03<00:05, 54.03it/s]concatenating: train:  41%|████      | 218/532 [00:03<00:05, 57.37it/s]concatenating: train:  43%|████▎     | 227/532 [00:03<00:04, 64.36it/s]concatenating: train:  44%|████▍     | 235/532 [00:03<00:05, 53.60it/s]concatenating: train:  45%|████▌     | 242/532 [00:03<00:05, 55.18it/s]concatenating: train:  47%|████▋     | 249/532 [00:04<00:05, 48.83it/s]concatenating: train:  48%|████▊     | 255/532 [00:04<00:05, 51.19it/s]concatenating: train:  50%|████▉     | 264/532 [00:04<00:04, 58.74it/s]concatenating: train:  51%|█████     | 271/532 [00:04<00:04, 52.31it/s]concatenating: train:  52%|█████▏    | 277/532 [00:04<00:05, 45.49it/s]concatenating: train:  54%|█████▍    | 289/532 [00:04<00:04, 55.63it/s]concatenating: train:  57%|█████▋    | 302/532 [00:04<00:03, 67.13it/s]concatenating: train:  59%|█████▉    | 316/532 [00:04<00:02, 79.53it/s]concatenating: train:  61%|██████▏   | 327/532 [00:04<00:02, 85.91it/s]concatenating: train:  64%|██████▎   | 338/532 [00:05<00:02, 77.24it/s]concatenating: train:  65%|██████▌   | 348/532 [00:05<00:02, 65.28it/s]concatenating: train:  67%|██████▋   | 356/532 [00:05<00:02, 65.76it/s]concatenating: train:  69%|██████▉   | 369/532 [00:05<00:02, 76.66it/s]concatenating: train:  71%|███████   | 379/532 [00:05<00:02, 68.82it/s]concatenating: train:  73%|███████▎  | 387/532 [00:05<00:02, 70.84it/s]concatenating: train:  74%|███████▍  | 395/532 [00:05<00:01, 70.59it/s]concatenating: train:  76%|███████▌  | 403/532 [00:06<00:01, 69.69it/s]concatenating: train:  78%|███████▊  | 416/532 [00:06<00:01, 80.01it/s]concatenating: train:  81%|████████▏ | 433/532 [00:06<00:01, 80.12it/s]concatenating: train:  83%|████████▎ | 442/532 [00:06<00:01, 58.83it/s]concatenating: train:  88%|████████▊ | 468/532 [00:06<00:00, 76.59it/s]concatenating: train:  93%|█████████▎| 494/532 [00:06<00:00, 96.99it/s]concatenating: train:  96%|█████████▌| 511/532 [00:07<00:00, 83.84it/s]concatenating: train:  99%|█████████▊| 525/532 [00:07<00:00, 90.16it/s]concatenating: train: 100%|██████████| 532/532 [00:07<00:00, 72.87it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:01<00:21,  1.53s/it]Loading test:  13%|█▎        | 2/15 [00:02<00:19,  1.47s/it]Loading test:  20%|██        | 3/15 [00:04<00:17,  1.43s/it]Loading test:  27%|██▋       | 4/15 [00:05<00:16,  1.46s/it]Loading test:  33%|███▎      | 5/15 [00:07<00:14,  1.43s/it]Loading test:  40%|████      | 6/15 [00:08<00:12,  1.37s/it]Loading test:  47%|████▋     | 7/15 [00:09<00:10,  1.25s/it]Loading test:  53%|█████▎    | 8/15 [00:10<00:08,  1.25s/it]Loading test:  60%|██████    | 9/15 [00:11<00:07,  1.20s/it]Loading test:  67%|██████▋   | 10/15 [00:12<00:05,  1.10s/it]Loading test:  73%|███████▎  | 11/15 [00:13<00:04,  1.04s/it]Loading test:  80%|████████  | 12/15 [00:14<00:03,  1.06s/it]Loading test:  87%|████████▋ | 13/15 [00:15<00:02,  1.09s/it]Loading test:  93%|█████████▎| 14/15 [00:16<00:01,  1.10s/it]Loading test: 100%|██████████| 15/15 [00:17<00:00,  1.08s/it]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation:  93%|█████████▎| 14/15 [00:00<00:00, 137.11it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 139.26it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 56, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 56, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 56, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 56, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 56, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 56, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 21)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 21)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 28, 21)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 40)   7600        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 61)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 61)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 14, 61)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 14, 80)   44000       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 14, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 14, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 14, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 14, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 14, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 14, 141)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 141)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 40)   22600       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 101)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 40)   36400       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 141)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-06 09:38:46.116115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 09:38:46.116225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 09:38:46.116239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 09:38:46.116248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 09:38:46.116686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 28, 28, 141)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 56, 20)   11300       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 56, 41)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 56, 20)   7400        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 56, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 56, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 56, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 56, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 56, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 56, 61)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 56, 61)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 56, 13)   806         dropout_5[0][0]                  
==================================================================================================
Total params: 225,706
Trainable params: 224,906
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53974061e-02 2.89048015e-02 1.16758472e-01 1.00223856e-02
 3.03440156e-02 5.80063118e-03 6.84518755e-02 1.28261328e-01
 7.55818021e-02 1.22545826e-02 2.73712232e-01 1.84335085e-01
 1.75382711e-04]
Train on 33496 samples, validate on 969 samples
Epoch 1/300
 - 34s - loss: 20.2765 - acc: 0.8148 - mDice: 0.0371 - val_loss: 3.0532 - val_acc: 0.9207 - val_mDice: 0.0768

Epoch 00001: val_mDice improved from -inf to 0.07682, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 26s - loss: 3.2506 - acc: 0.9158 - mDice: 0.1402 - val_loss: 1.9908 - val_acc: 0.9412 - val_mDice: 0.2589

Epoch 00002: val_mDice improved from 0.07682 to 0.25886, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 25s - loss: 2.2833 - acc: 0.9307 - mDice: 0.2994 - val_loss: 1.2848 - val_acc: 0.9628 - val_mDice: 0.4790

Epoch 00003: val_mDice improved from 0.25886 to 0.47899, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 26s - loss: 1.7867 - acc: 0.9447 - mDice: 0.4229 - val_loss: 1.0180 - val_acc: 0.9720 - val_mDice: 0.5959

Epoch 00004: val_mDice improved from 0.47899 to 0.59593, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300
 - 25s - loss: 1.5092 - acc: 0.9515 - mDice: 0.5005 - val_loss: 0.9060 - val_acc: 0.9744 - val_mDice: 0.6424

Epoch 00005: val_mDice improved from 0.59593 to 0.64237, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 24s - loss: 1.3448 - acc: 0.9552 - mDice: 0.5478 - val_loss: 0.8464 - val_acc: 0.9754 - val_mDice: 0.6767

Epoch 00006: val_mDice improved from 0.64237 to 0.67666, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 26s - loss: 1.2300 - acc: 0.9580 - mDice: 0.5822 - val_loss: 0.7865 - val_acc: 0.9774 - val_mDice: 0.6989

Epoch 00007: val_mDice improved from 0.67666 to 0.69894, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300
 - 24s - loss: 1.1518 - acc: 0.9600 - mDice: 0.6046 - val_loss: 0.7608 - val_acc: 0.9770 - val_mDice: 0.7122

Epoch 00008: val_mDice improved from 0.69894 to 0.71218, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 26s - loss: 1.0883 - acc: 0.9614 - mDice: 0.6231 - val_loss: 0.7310 - val_acc: 0.9782 - val_mDice: 0.7195

Epoch 00009: val_mDice improved from 0.71218 to 0.71946, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 26s - loss: 1.0354 - acc: 0.9626 - mDice: 0.6389 - val_loss: 0.7765 - val_acc: 0.9772 - val_mDice: 0.7147

Epoch 00010: val_mDice did not improve from 0.71946
Epoch 11/300
 - 26s - loss: 0.9914 - acc: 0.9636 - mDice: 0.6520 - val_loss: 0.7115 - val_acc: 0.9788 - val_mDice: 0.7325

Epoch 00011: val_mDice improved from 0.71946 to 0.73255, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300
 - 25s - loss: 0.9563 - acc: 0.9644 - mDice: 0.6627 - val_loss: 0.7328 - val_acc: 0.9793 - val_mDice: 0.7263

Epoch 00012: val_mDice did not improve from 0.73255
Epoch 13/300
 - 26s - loss: 0.9266 - acc: 0.9650 - mDice: 0.6717 - val_loss: 0.7009 - val_acc: 0.9791 - val_mDice: 0.7378

Epoch 00013: val_mDice improved from 0.73255 to 0.73776, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 14/300
 - 24s - loss: 0.9009 - acc: 0.9657 - mDice: 0.6798 - val_loss: 0.6833 - val_acc: 0.9792 - val_mDice: 0.7407

Epoch 00014: val_mDice improved from 0.73776 to 0.74068, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 15/300
 - 24s - loss: 0.8829 - acc: 0.9661 - mDice: 0.6851 - val_loss: 0.6902 - val_acc: 0.9796 - val_mDice: 0.7415

Epoch 00015: val_mDice improved from 0.74068 to 0.74154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 16/300
 - 26s - loss: 0.8608 - acc: 0.9665 - mDice: 0.6921 - val_loss: 0.7088 - val_acc: 0.9780 - val_mDice: 0.7386

Epoch 00016: val_mDice did not improve from 0.74154
Epoch 17/300
 - 26s - loss: 0.8459 - acc: 0.9668 - mDice: 0.6971 - val_loss: 0.6962 - val_acc: 0.9784 - val_mDice: 0.7411

Epoch 00017: val_mDice did not improve from 0.74154
Epoch 18/300
 - 26s - loss: 0.8210 - acc: 0.9675 - mDice: 0.7052 - val_loss: 0.7539 - val_acc: 0.9795 - val_mDice: 0.7294

Epoch 00018: val_mDice did not improve from 0.74154
Epoch 19/300
 - 26s - loss: 0.8083 - acc: 0.9679 - mDice: 0.7089 - val_loss: 0.6626 - val_acc: 0.9793 - val_mDice: 0.7518

Epoch 00019: val_mDice improved from 0.74154 to 0.75181, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 20/300
 - 25s - loss: 0.7980 - acc: 0.9681 - mDice: 0.7124 - val_loss: 0.6713 - val_acc: 0.9784 - val_mDice: 0.7540

Epoch 00020: val_mDice improved from 0.75181 to 0.75396, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 21/300
 - 25s - loss: 0.7837 - acc: 0.9685 - mDice: 0.7169 - val_loss: 0.6421 - val_acc: 0.9802 - val_mDice: 0.7559

Epoch 00021: val_mDice improved from 0.75396 to 0.75594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300
 - 26s - loss: 0.7751 - acc: 0.9687 - mDice: 0.7197 - val_loss: 0.6449 - val_acc: 0.9790 - val_mDice: 0.7535

Epoch 00022: val_mDice did not improve from 0.75594
Epoch 23/300
 - 24s - loss: 0.7643 - acc: 0.9690 - mDice: 0.7233 - val_loss: 0.6435 - val_acc: 0.9800 - val_mDice: 0.7566

Epoch 00023: val_mDice improved from 0.75594 to 0.75663, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 24/300
 - 26s - loss: 0.7537 - acc: 0.9692 - mDice: 0.7270 - val_loss: 0.6523 - val_acc: 0.9797 - val_mDice: 0.7560

Epoch 00024: val_mDice did not improve from 0.75663
Epoch 25/300
 - 25s - loss: 0.7448 - acc: 0.9694 - mDice: 0.7295 - val_loss: 0.6686 - val_acc: 0.9801 - val_mDice: 0.7490

Epoch 00025: val_mDice did not improve from 0.75663
Epoch 26/300
 - 24s - loss: 0.7397 - acc: 0.9695 - mDice: 0.7313 - val_loss: 0.6578 - val_acc: 0.9801 - val_mDice: 0.7552

Epoch 00026: val_mDice did not improve from 0.75663
Epoch 27/300
 - 26s - loss: 0.7338 - acc: 0.9697 - mDice: 0.7331 - val_loss: 0.6438 - val_acc: 0.9804 - val_mDice: 0.7585

Epoch 00027: val_mDice improved from 0.75663 to 0.75851, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 28/300
 - 26s - loss: 0.7250 - acc: 0.9699 - mDice: 0.7361 - val_loss: 0.6469 - val_acc: 0.9805 - val_mDice: 0.7580

Epoch 00028: val_mDice did not improve from 0.75851
Epoch 29/300
 - 27s - loss: 0.7175 - acc: 0.9701 - mDice: 0.7385 - val_loss: 0.6496 - val_acc: 0.9805 - val_mDice: 0.7619

Epoch 00029: val_mDice improved from 0.75851 to 0.76192, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 30/300
 - 26s - loss: 0.7084 - acc: 0.9702 - mDice: 0.7419 - val_loss: 0.6370 - val_acc: 0.9806 - val_mDice: 0.7609

Epoch 00030: val_mDice did not improve from 0.76192
Epoch 31/300
 - 25s - loss: 0.7060 - acc: 0.9702 - mDice: 0.7423 - val_loss: 0.6525 - val_acc: 0.9804 - val_mDice: 0.7592

Epoch 00031: val_mDice did not improve from 0.76192
Epoch 32/300
 - 26s - loss: 0.6998 - acc: 0.9704 - mDice: 0.7446 - val_loss: 0.6573 - val_acc: 0.9805 - val_mDice: 0.7600

Epoch 00032: val_mDice did not improve from 0.76192
Epoch 33/300
 - 24s - loss: 0.6943 - acc: 0.9705 - mDice: 0.7463 - val_loss: 0.6456 - val_acc: 0.9811 - val_mDice: 0.7638

Epoch 00033: val_mDice improved from 0.76192 to 0.76385, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 34/300
 - 26s - loss: 0.6885 - acc: 0.9705 - mDice: 0.7482 - val_loss: 0.6344 - val_acc: 0.9805 - val_mDice: 0.7668

Epoch 00034: val_mDice improved from 0.76385 to 0.76679, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 35/300
 - 25s - loss: 0.6851 - acc: 0.9706 - mDice: 0.7496 - val_loss: 0.6649 - val_acc: 0.9810 - val_mDice: 0.7600

Epoch 00035: val_mDice did not improve from 0.76679
Epoch 36/300
 - 26s - loss: 0.6792 - acc: 0.9707 - mDice: 0.7515 - val_loss: 0.6376 - val_acc: 0.9802 - val_mDice: 0.7666

Epoch 00036: val_mDice did not improve from 0.76679
Epoch 37/300
 - 26s - loss: 0.6765 - acc: 0.9708 - mDice: 0.7524 - val_loss: 0.6262 - val_acc: 0.9815 - val_mDice: 0.7675

Epoch 00037: val_mDice improved from 0.76679 to 0.76746, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 38/300
 - 27s - loss: 0.6730 - acc: 0.9708 - mDice: 0.7535 - val_loss: 0.6412 - val_acc: 0.9806 - val_mDice: 0.7651

Epoch 00038: val_mDice did not improve from 0.76746
Epoch 39/300
 - 27s - loss: 0.6691 - acc: 0.9709 - mDice: 0.7549 - val_loss: 0.6470 - val_acc: 0.9814 - val_mDice: 0.7627

Epoch 00039: val_mDice did not improve from 0.76746
Epoch 40/300
 - 26s - loss: 0.6631 - acc: 0.9709 - mDice: 0.7570 - val_loss: 0.6475 - val_acc: 0.9814 - val_mDice: 0.7654

Epoch 00040: val_mDice did not improve from 0.76746
Epoch 41/300
 - 26s - loss: 0.6625 - acc: 0.9709 - mDice: 0.7567 - val_loss: 0.6331 - val_acc: 0.9807 - val_mDice: 0.7682

Epoch 00041: val_mDice improved from 0.76746 to 0.76820, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 42/300
 - 26s - loss: 0.6559 - acc: 0.9711 - mDice: 0.7596 - val_loss: 0.6328 - val_acc: 0.9816 - val_mDice: 0.7666

Epoch 00042: val_mDice did not improve from 0.76820
Epoch 43/300
 - 26s - loss: 0.6543 - acc: 0.9711 - mDice: 0.7599 - val_loss: 0.6429 - val_acc: 0.9801 - val_mDice: 0.7674

Epoch 00043: val_mDice did not improve from 0.76820
Epoch 44/300
 - 24s - loss: 0.6530 - acc: 0.9711 - mDice: 0.7602 - val_loss: 0.6300 - val_acc: 0.9809 - val_mDice: 0.7659

Epoch 00044: val_mDice did not improve from 0.76820
Epoch 45/300
 - 27s - loss: 0.6479 - acc: 0.9712 - mDice: 0.7622 - val_loss: 0.6329 - val_acc: 0.9816 - val_mDice: 0.7681

Epoch 00045: val_mDice did not improve from 0.76820
Epoch 46/300
 - 26s - loss: 0.6466 - acc: 0.9712 - mDice: 0.7624 - val_loss: 0.6383 - val_acc: 0.9797 - val_mDice: 0.7671

Epoch 00046: val_mDice did not improve from 0.76820
Epoch 47/300
 - 25s - loss: 0.6435 - acc: 0.9713 - mDice: 0.7634 - val_loss: 0.6388 - val_acc: 0.9815 - val_mDice: 0.7649

Epoch 00047: val_mDice did not improve from 0.76820
Epoch 48/300
 - 25s - loss: 0.6421 - acc: 0.9713 - mDice: 0.7638 - val_loss: 0.6422 - val_acc: 0.9814 - val_mDice: 0.7666

Epoch 00048: val_mDice did not improve from 0.76820
Epoch 49/300
 - 26s - loss: 0.6391 - acc: 0.9714 - mDice: 0.7649 - val_loss: 0.6315 - val_acc: 0.9809 - val_mDice: 0.7694

Epoch 00049: val_mDice improved from 0.76820 to 0.76942, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 50/300
 - 25s - loss: 0.6355 - acc: 0.9714 - mDice: 0.7660 - val_loss: 0.6398 - val_acc: 0.9815 - val_mDice: 0.7701

Epoch 00050: val_mDice improved from 0.76942 to 0.77013, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 51/300
 - 26s - loss: 0.6327 - acc: 0.9715 - mDice: 0.7670 - val_loss: 0.6251 - val_acc: 0.9815 - val_mDice: 0.7709

Epoch 00051: val_mDice improved from 0.77013 to 0.77088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 52/300
 - 25s - loss: 0.6327 - acc: 0.9715 - mDice: 0.7670 - val_loss: 0.6239 - val_acc: 0.9818 - val_mDice: 0.7732

Epoch 00052: val_mDice improved from 0.77088 to 0.77318, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 53/300
 - 26s - loss: 0.6304 - acc: 0.9715 - mDice: 0.7678 - val_loss: 0.6436 - val_acc: 0.9811 - val_mDice: 0.7702

Epoch 00053: val_mDice did not improve from 0.77318
Epoch 54/300
 - 24s - loss: 0.6296 - acc: 0.9715 - mDice: 0.7680 - val_loss: 0.6271 - val_acc: 0.9820 - val_mDice: 0.7701

Epoch 00054: val_mDice did not improve from 0.77318
Epoch 55/300
 - 25s - loss: 0.6268 - acc: 0.9716 - mDice: 0.7691 - val_loss: 0.6247 - val_acc: 0.9813 - val_mDice: 0.7723

Epoch 00055: val_mDice did not improve from 0.77318
Epoch 56/300
 - 26s - loss: 0.6256 - acc: 0.9716 - mDice: 0.7691 - val_loss: 0.6255 - val_acc: 0.9814 - val_mDice: 0.7736

Epoch 00056: val_mDice improved from 0.77318 to 0.77363, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 57/300
 - 26s - loss: 0.6215 - acc: 0.9716 - mDice: 0.7710 - val_loss: 0.6393 - val_acc: 0.9813 - val_mDice: 0.7693

Epoch 00057: val_mDice did not improve from 0.77363
Epoch 58/300
 - 26s - loss: 0.6214 - acc: 0.9717 - mDice: 0.7708 - val_loss: 0.6386 - val_acc: 0.9814 - val_mDice: 0.7739

Epoch 00058: val_mDice improved from 0.77363 to 0.77386, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 59/300
 - 26s - loss: 0.6187 - acc: 0.9718 - mDice: 0.7717 - val_loss: 0.6422 - val_acc: 0.9809 - val_mDice: 0.7727

Epoch 00059: val_mDice did not improve from 0.77386
Epoch 60/300
 - 26s - loss: 0.6170 - acc: 0.9718 - mDice: 0.7724 - val_loss: 0.6402 - val_acc: 0.9808 - val_mDice: 0.7714

Epoch 00060: val_mDice did not improve from 0.77386
Epoch 61/300
 - 25s - loss: 0.6163 - acc: 0.9718 - mDice: 0.7726 - val_loss: 0.6335 - val_acc: 0.9812 - val_mDice: 0.7733

Epoch 00061: val_mDice did not improve from 0.77386
Epoch 62/300
 - 26s - loss: 0.6142 - acc: 0.9718 - mDice: 0.7735 - val_loss: 0.6298 - val_acc: 0.9810 - val_mDice: 0.7745

Epoch 00062: val_mDice improved from 0.77386 to 0.77449, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 63/300
 - 25s - loss: 0.6119 - acc: 0.9719 - mDice: 0.7743 - val_loss: 0.6272 - val_acc: 0.9811 - val_mDice: 0.7777

Epoch 00063: val_mDice improved from 0.77449 to 0.77771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 64/300
 - 26s - loss: 0.6102 - acc: 0.9719 - mDice: 0.7746 - val_loss: 0.6243 - val_acc: 0.9814 - val_mDice: 0.7760

Epoch 00064: val_mDice did not improve from 0.77771
Epoch 65/300
 - 26s - loss: 0.6093 - acc: 0.9719 - mDice: 0.7749 - val_loss: 0.6260 - val_acc: 0.9819 - val_mDice: 0.7767

Epoch 00065: val_mDice did not improve from 0.77771
Epoch 66/300
 - 25s - loss: 0.6058 - acc: 0.9720 - mDice: 0.7765 - val_loss: 0.6205 - val_acc: 0.9821 - val_mDice: 0.7739

Epoch 00066: val_mDice did not improve from 0.77771
Epoch 67/300
 - 26s - loss: 0.6060 - acc: 0.9720 - mDice: 0.7763 - val_loss: 0.6328 - val_acc: 0.9817 - val_mDice: 0.7745

Epoch 00067: val_mDice did not improve from 0.77771
Epoch 68/300
 - 25s - loss: 0.6063 - acc: 0.9720 - mDice: 0.7764 - val_loss: 0.6391 - val_acc: 0.9820 - val_mDice: 0.7729

Epoch 00068: val_mDice did not improve from 0.77771
Epoch 69/300
 - 25s - loss: 0.6032 - acc: 0.9721 - mDice: 0.7773 - val_loss: 0.6206 - val_acc: 0.9818 - val_mDice: 0.7769

Epoch 00069: val_mDice did not improve from 0.77771
Epoch 70/300
 - 26s - loss: 0.6031 - acc: 0.9720 - mDice: 0.7773 - val_loss: 0.6239 - val_acc: 0.9820 - val_mDice: 0.7747

Epoch 00070: val_mDice did not improve from 0.77771
Epoch 71/300
 - 26s - loss: 0.6011 - acc: 0.9721 - mDice: 0.7780 - val_loss: 0.6301 - val_acc: 0.9814 - val_mDice: 0.7791

Epoch 00071: val_mDice improved from 0.77771 to 0.77912, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 72/300
 - 25s - loss: 0.6002 - acc: 0.9721 - mDice: 0.7781 - val_loss: 0.6381 - val_acc: 0.9820 - val_mDice: 0.7757

Epoch 00072: val_mDice did not improve from 0.77912
Epoch 73/300
 - 25s - loss: 0.6014 - acc: 0.9721 - mDice: 0.7778 - val_loss: 0.6230 - val_acc: 0.9822 - val_mDice: 0.7760

Epoch 00073: val_mDice did not improve from 0.77912
Epoch 74/300
 - 25s - loss: 0.6000 - acc: 0.9721 - mDice: 0.7781 - val_loss: 0.6425 - val_acc: 0.9805 - val_mDice: 0.7740

Epoch 00074: val_mDice did not improve from 0.77912
Epoch 75/300
 - 24s - loss: 0.5963 - acc: 0.9722 - mDice: 0.7795 - val_loss: 0.6272 - val_acc: 0.9816 - val_mDice: 0.7744

Epoch 00075: val_mDice did not improve from 0.77912
Epoch 76/300
 - 25s - loss: 0.5971 - acc: 0.9721 - mDice: 0.7793 - val_loss: 0.6430 - val_acc: 0.9820 - val_mDice: 0.7733

Epoch 00076: val_mDice did not improve from 0.77912
Epoch 77/300
 - 25s - loss: 0.5944 - acc: 0.9722 - mDice: 0.7803 - val_loss: 0.6267 - val_acc: 0.9818 - val_mDice: 0.7787

Epoch 00077: val_mDice did not improve from 0.77912
Epoch 78/300
 - 25s - loss: 0.5949 - acc: 0.9722 - mDice: 0.7801 - val_loss: 0.6354 - val_acc: 0.9815 - val_mDice: 0.7725

Epoch 00078: val_mDice did not improve from 0.77912
Epoch 79/300
 - 26s - loss: 0.5946 - acc: 0.9722 - mDice: 0.7802 - val_loss: 0.6276 - val_acc: 0.9817 - val_mDice: 0.7773

Epoch 00079: val_mDice did not improve from 0.77912
Epoch 80/300
 - 25s - loss: 0.5925 - acc: 0.9722 - mDice: 0.7807 - val_loss: 0.6341 - val_acc: 0.9817 - val_mDice: 0.7778

Epoch 00080: val_mDice did not improve from 0.77912
Epoch 81/300
 - 26s - loss: 0.5901 - acc: 0.9723 - mDice: 0.7819 - val_loss: 0.6386 - val_acc: 0.9817 - val_mDice: 0.7751

Epoch 00081: val_mDice did not improve from 0.77912
Epoch 82/300
 - 25s - loss: 0.5901 - acc: 0.9723 - mDice: 0.7819 - val_loss: 0.6313 - val_acc: 0.9817 - val_mDice: 0.7759

Epoch 00082: val_mDice did not improve from 0.77912
Epoch 83/300
 - 26s - loss: 0.5903 - acc: 0.9723 - mDice: 0.7818 - val_loss: 0.6487 - val_acc: 0.9814 - val_mDice: 0.7762

Epoch 00083: val_mDice did not improve from 0.77912
Epoch 84/300
 - 25s - loss: 0.5869 - acc: 0.9723 - mDice: 0.7829 - val_loss: 0.6402 - val_acc: 0.9813 - val_mDice: 0.7771

Epoch 00084: val_mDice did not improve from 0.77912
Epoch 85/300
 - 26s - loss: 0.5882 - acc: 0.9723 - mDice: 0.7823 - val_loss: 0.6378 - val_acc: 0.9822 - val_mDice: 0.7754

Epoch 00085: val_mDice did not improve from 0.77912
Epoch 86/300
 - 25s - loss: 0.5873 - acc: 0.9723 - mDice: 0.7831 - val_loss: 0.6412 - val_acc: 0.9818 - val_mDice: 0.7778

Epoch 00086: val_mDice did not improve from 0.77912
Epoch 87/300
 - 26s - loss: 0.5875 - acc: 0.9723 - mDice: 0.7826 - val_loss: 0.6419 - val_acc: 0.9817 - val_mDice: 0.7754

Epoch 00087: val_mDice did not improve from 0.77912
Epoch 88/300
 - 25s - loss: 0.5848 - acc: 0.9723 - mDice: 0.7836 - val_loss: 0.6327 - val_acc: 0.9818 - val_mDice: 0.7775

Epoch 00088: val_mDice did not improve from 0.77912
Epoch 89/300
 - 25s - loss: 0.5843 - acc: 0.9724 - mDice: 0.7841 - val_loss: 0.6366 - val_acc: 0.9818 - val_mDice: 0.7791

Epoch 00089: val_mDice did not improve from 0.77912
Epoch 90/300
 - 26s - loss: 0.5831 - acc: 0.9724 - mDice: 0.7842 - val_loss: 0.6358 - val_acc: 0.9820 - val_mDice: 0.7807

Epoch 00090: val_mDice improved from 0.77912 to 0.78071, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 91/300
 - 24s - loss: 0.5833 - acc: 0.9724 - mDice: 0.7842 - val_loss: 0.6434 - val_acc: 0.9816 - val_mDice: 0.7763

Epoch 00091: val_mDice did not improve from 0.78071
Epoch 92/300
 - 26s - loss: 0.5820 - acc: 0.9724 - mDice: 0.7848 - val_loss: 0.6343 - val_acc: 0.9821 - val_mDice: 0.7763

Epoch 00092: val_mDice did not improve from 0.78071
Epoch 93/300
 - 25s - loss: 0.5814 - acc: 0.9724 - mDice: 0.7848 - val_loss: 0.6311 - val_acc: 0.9809 - val_mDice: 0.7786

Epoch 00093: val_mDice did not improve from 0.78071
Epoch 94/300
 - 26s - loss: 0.5800 - acc: 0.9724 - mDice: 0.7853 - val_loss: 0.6254 - val_acc: 0.9820 - val_mDice: 0.7773

Epoch 00094: val_mDice did not improve from 0.78071
Epoch 95/300
 - 27s - loss: 0.5807 - acc: 0.9725 - mDice: 0.7852 - val_loss: 0.6428 - val_acc: 0.9819 - val_mDice: 0.7753

Epoch 00095: val_mDice did not improve from 0.78071
Epoch 96/300
 - 27s - loss: 0.5787 - acc: 0.9725 - mDice: 0.7860 - val_loss: 0.6401 - val_acc: 0.9820 - val_mDice: 0.7798

Epoch 00096: val_mDice did not improve from 0.78071
Epoch 97/300
 - 27s - loss: 0.5800 - acc: 0.9724 - mDice: 0.7855 - val_loss: 0.6406 - val_acc: 0.9822 - val_mDice: 0.7758

Epoch 00097: val_mDice did not improve from 0.78071
Epoch 98/300
 - 26s - loss: 0.5782 - acc: 0.9725 - mDice: 0.7859 - val_loss: 0.6364 - val_acc: 0.9812 - val_mDice: 0.7768

Epoch 00098: val_mDice did not improve from 0.78071
Epoch 99/300
 - 26s - loss: 0.5787 - acc: 0.9725 - mDice: 0.7860 - val_loss: 0.6406 - val_acc: 0.9819 - val_mDice: 0.7778

Epoch 00099: val_mDice did not improve from 0.78071
Epoch 100/300
 - 25s - loss: 0.5763 - acc: 0.9725 - mDice: 0.7866 - val_loss: 0.6404 - val_acc: 0.9818 - val_mDice: 0.7797

Epoch 00100: val_mDice did not improve from 0.78071
Epoch 101/300
 - 25s - loss: 0.5753 - acc: 0.9725 - mDice: 0.7870 - val_loss: 0.6405 - val_acc: 0.9817 - val_mDice: 0.7780

Epoch 00101: val_mDice did not improve from 0.78071
Epoch 102/300
 - 26s - loss: 0.5739 - acc: 0.9726 - mDice: 0.7875 - val_loss: 0.6516 - val_acc: 0.9821 - val_mDice: 0.7742

Epoch 00102: val_mDice did not improve from 0.78071
Epoch 103/300
 - 26s - loss: 0.5725 - acc: 0.9726 - mDice: 0.7878 - val_loss: 0.6459 - val_acc: 0.9817 - val_mDice: 0.7755

Epoch 00103: val_mDice did not improve from 0.78071
Epoch 104/300
 - 26s - loss: 0.5752 - acc: 0.9726 - mDice: 0.7869 - val_loss: 0.6358 - val_acc: 0.9819 - val_mDice: 0.7803

Epoch 00104: val_mDice did not improve from 0.78071
Epoch 105/300
 - 26s - loss: 0.5725 - acc: 0.9726 - mDice: 0.7880 - val_loss: 0.6425 - val_acc: 0.9818 - val_mDice: 0.7787

Epoch 00105: val_mDice did not improve from 0.78071
Epoch 106/300
 - 25s - loss: 0.5726 - acc: 0.9726 - mDice: 0.7881 - val_loss: 0.6366 - val_acc: 0.9820 - val_mDice: 0.7773

Epoch 00106: val_mDice did not improve from 0.78071
Epoch 107/300
 - 26s - loss: 0.5736 - acc: 0.9726 - mDice: 0.7875 - val_loss: 0.6321 - val_acc: 0.9817 - val_mDice: 0.7774

Epoch 00107: val_mDice did not improve from 0.78071
Epoch 108/300
 - 25s - loss: 0.5717 - acc: 0.9726 - mDice: 0.7884 - val_loss: 0.6504 - val_acc: 0.9819 - val_mDice: 0.7753

Epoch 00108: val_mDice did not improve from 0.78071
Epoch 109/300
 - 26s - loss: 0.5717 - acc: 0.9726 - mDice: 0.7884 - val_loss: 0.6389 - val_acc: 0.9820 - val_mDice: 0.7770

Epoch 00109: val_mDice did not improve from 0.78071
Epoch 110/300
 - 24s - loss: 0.5698 - acc: 0.9727 - mDice: 0.7892 - val_loss: 0.6399 - val_acc: 0.9819 - val_mDice: 0.7784

Epoch 00110: val_mDice did not improve from 0.78071
Epoch 111/300
 - 25s - loss: 0.5696 - acc: 0.9727 - mDice: 0.7890 - val_loss: 0.6441 - val_acc: 0.9820 - val_mDice: 0.7795

Epoch 00111: val_mDice did not improve from 0.78071
Epoch 112/300
 - 26s - loss: 0.5689 - acc: 0.9727 - mDice: 0.7894 - val_loss: 0.6338 - val_acc: 0.9824 - val_mDice: 0.7816

Epoch 00112: val_mDice improved from 0.78071 to 0.78162, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 113/300
 - 25s - loss: 0.5686 - acc: 0.9727 - mDice: 0.7893 - val_loss: 0.6515 - val_acc: 0.9817 - val_mDice: 0.7792

Epoch 00113: val_mDice did not improve from 0.78162
Epoch 114/300
 - 26s - loss: 0.5671 - acc: 0.9727 - mDice: 0.7900 - val_loss: 0.6510 - val_acc: 0.9815 - val_mDice: 0.7810

Epoch 00114: val_mDice did not improve from 0.78162
Epoch 115/300
 - 24s - loss: 0.5674 - acc: 0.9727 - mDice: 0.7897 - val_loss: 0.6431 - val_acc: 0.9816 - val_mDice: 0.7783

Epoch 00115: val_mDice did not improve from 0.78162
Epoch 116/300
 - 26s - loss: 0.5675 - acc: 0.9727 - mDice: 0.7898 - val_loss: 0.6641 - val_acc: 0.9815 - val_mDice: 0.7758

Epoch 00116: val_mDice did not improve from 0.78162
Epoch 117/300
 - 26s - loss: 0.5671 - acc: 0.9727 - mDice: 0.7900 - val_loss: 0.6369 - val_acc: 0.9816 - val_mDice: 0.7794

Epoch 00117: val_mDice did not improve from 0.78162
Epoch 118/300
 - 26s - loss: 0.5651 - acc: 0.9727 - mDice: 0.7906 - val_loss: 0.6407 - val_acc: 0.9823 - val_mDice: 0.7822

Epoch 00118: val_mDice improved from 0.78162 to 0.78223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 119/300
 - 24s - loss: 0.5655 - acc: 0.9727 - mDice: 0.7905 - val_loss: 0.6512 - val_acc: 0.9819 - val_mDice: 0.7777

Epoch 00119: val_mDice did not improve from 0.78223
Epoch 120/300
 - 26s - loss: 0.5652 - acc: 0.9727 - mDice: 0.7907 - val_loss: 0.6520 - val_acc: 0.9820 - val_mDice: 0.7806

Epoch 00120: val_mDice did not improve from 0.78223
Epoch 121/300
 - 26s - loss: 0.5656 - acc: 0.9727 - mDice: 0.7904 - val_loss: 0.6580 - val_acc: 0.9818 - val_mDice: 0.7788

Epoch 00121: val_mDice did not improve from 0.78223
Epoch 122/300
 - 26s - loss: 0.5643 - acc: 0.9728 - mDice: 0.7911 - val_loss: 0.6505 - val_acc: 0.9813 - val_mDice: 0.7789

Epoch 00122: val_mDice did not improve from 0.78223
Epoch 123/300
 - 25s - loss: 0.5632 - acc: 0.9728 - mDice: 0.7915 - val_loss: 0.6502 - val_acc: 0.9815 - val_mDice: 0.7771

Epoch 00123: val_mDice did not improve from 0.78223
Epoch 124/300
 - 26s - loss: 0.5627 - acc: 0.9728 - mDice: 0.7915 - val_loss: 0.6346 - val_acc: 0.9816 - val_mDice: 0.7825

Epoch 00124: val_mDice improved from 0.78223 to 0.78249, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 125/300
 - 24s - loss: 0.5630 - acc: 0.9728 - mDice: 0.7915 - val_loss: 0.6514 - val_acc: 0.9821 - val_mDice: 0.7764

Epoch 00125: val_mDice did not improve from 0.78249
Epoch 126/300
 - 25s - loss: 0.5627 - acc: 0.9728 - mDice: 0.7917 - val_loss: 0.6373 - val_acc: 0.9816 - val_mDice: 0.7792

Epoch 00126: val_mDice did not improve from 0.78249
Epoch 127/300
 - 27s - loss: 0.5611 - acc: 0.9728 - mDice: 0.7918 - val_loss: 0.6476 - val_acc: 0.9814 - val_mDice: 0.7805

Epoch 00127: val_mDice did not improve from 0.78249
Epoch 128/300
 - 26s - loss: 0.5616 - acc: 0.9728 - mDice: 0.7920 - val_loss: 0.6669 - val_acc: 0.9810 - val_mDice: 0.7772

Epoch 00128: val_mDice did not improve from 0.78249
Epoch 129/300
 - 25s - loss: 0.5614 - acc: 0.9728 - mDice: 0.7919 - val_loss: 0.6668 - val_acc: 0.9815 - val_mDice: 0.7772

Epoch 00129: val_mDice did not improve from 0.78249
Epoch 130/300
 - 26s - loss: 0.5603 - acc: 0.9729 - mDice: 0.7925 - val_loss: 0.6504 - val_acc: 0.9818 - val_mDice: 0.7798

Epoch 00130: val_mDice did not improve from 0.78249
Epoch 131/300
 - 26s - loss: 0.5610 - acc: 0.9728 - mDice: 0.7922 - val_loss: 0.6427 - val_acc: 0.9812 - val_mDice: 0.7775

Epoch 00131: val_mDice did not improve from 0.78249
Epoch 132/300
 - 26s - loss: 0.5595 - acc: 0.9729 - mDice: 0.7926 - val_loss: 0.6560 - val_acc: 0.9816 - val_mDice: 0.7800

Epoch 00132: val_mDice did not improve from 0.78249
Epoch 133/300
 - 26s - loss: 0.5588 - acc: 0.9729 - mDice: 0.7929 - val_loss: 0.6649 - val_acc: 0.9818 - val_mDice: 0.7794

Epoch 00133: val_mDice did not improve from 0.78249
Epoch 134/300
 - 24s - loss: 0.5587 - acc: 0.9729 - mDice: 0.7930 - val_loss: 0.6467 - val_acc: 0.9813 - val_mDice: 0.7801

Epoch 00134: val_mDice did not improve from 0.78249
Epoch 135/300
 - 26s - loss: 0.5585 - acc: 0.9729 - mDice: 0.7930 - val_loss: 0.6475 - val_acc: 0.9818 - val_mDice: 0.7802

Epoch 00135: val_mDice did not improve from 0.78249
Epoch 136/300
 - 25s - loss: 0.5582 - acc: 0.9729 - mDice: 0.7932 - val_loss: 0.6475 - val_acc: 0.9815 - val_mDice: 0.7789

Epoch 00136: val_mDice did not improve from 0.78249
Epoch 137/300
 - 26s - loss: 0.5576 - acc: 0.9729 - mDice: 0.7934 - val_loss: 0.6441 - val_acc: 0.9819 - val_mDice: 0.7798

Epoch 00137: val_mDice did not improve from 0.78249
Epoch 138/300
 - 25s - loss: 0.5572 - acc: 0.9729 - mDice: 0.7937 - val_loss: 0.6472 - val_acc: 0.9819 - val_mDice: 0.7766

Epoch 00138: val_mDice did not improve from 0.78249
Epoch 139/300
 - 24s - loss: 0.5580 - acc: 0.9729 - mDice: 0.7932 - val_loss: 0.6454 - val_acc: 0.9815 - val_mDice: 0.7801

Epoch 00139: val_mDice did not improve from 0.78249
Epoch 140/300
 - 26s - loss: 0.5563 - acc: 0.9729 - mDice: 0.7938 - val_loss: 0.6613 - val_acc: 0.9815 - val_mDice: 0.7756

Epoch 00140: val_mDice did not improve from 0.78249
Epoch 141/300
 - 26s - loss: 0.5565 - acc: 0.9729 - mDice: 0.7938 - val_loss: 0.6553 - val_acc: 0.9822 - val_mDice: 0.7802

Epoch 00141: val_mDice did not improve from 0.78249
Epoch 142/300
 - 26s - loss: 0.5558 - acc: 0.9729 - mDice: 0.7941 - val_loss: 0.6564 - val_acc: 0.9818 - val_mDice: 0.7785

Epoch 00142: val_mDice did not improve from 0.78249
Epoch 143/300
 - 26s - loss: 0.5552 - acc: 0.9729 - mDice: 0.7944 - val_loss: 0.6453 - val_acc: 0.9817 - val_mDice: 0.7797

Epoch 00143: val_mDice did not improve from 0.78249
Epoch 144/300
 - 24s - loss: 0.5540 - acc: 0.9730 - mDice: 0.7948 - val_loss: 0.6636 - val_acc: 0.9812 - val_mDice: 0.7781

Epoch 00144: val_mDice did not improve from 0.78249
Epoch 145/300
 - 26s - loss: 0.5540 - acc: 0.9730 - mDice: 0.7948 - val_loss: 0.6532 - val_acc: 0.9817 - val_mDice: 0.7802

Epoch 00145: val_mDice did not improve from 0.78249
Epoch 146/300
 - 25s - loss: 0.5550 - acc: 0.9729 - mDice: 0.7942 - val_loss: 0.6434 - val_acc: 0.9815 - val_mDice: 0.7802

Epoch 00146: val_mDice did not improve from 0.78249
Epoch 147/300
 - 24s - loss: 0.5551 - acc: 0.9730 - mDice: 0.7941 - val_loss: 0.6397 - val_acc: 0.9813 - val_mDice: 0.7790

Epoch 00147: val_mDice did not improve from 0.78249
Epoch 148/300
 - 26s - loss: 0.5548 - acc: 0.9730 - mDice: 0.7943 - val_loss: 0.6417 - val_acc: 0.9817 - val_mDice: 0.7806

Epoch 00148: val_mDice did not improve from 0.78249
Epoch 149/300
 - 25s - loss: 0.5534 - acc: 0.9730 - mDice: 0.7950 - val_loss: 0.6658 - val_acc: 0.9814 - val_mDice: 0.7777

Epoch 00149: val_mDice did not improve from 0.78249
Epoch 150/300
 - 25s - loss: 0.5527 - acc: 0.9730 - mDice: 0.7951 - val_loss: 0.6513 - val_acc: 0.9817 - val_mDice: 0.7770

Epoch 00150: val_mDice did not improve from 0.78249
Epoch 151/300
 - 25s - loss: 0.5530 - acc: 0.9730 - mDice: 0.7950 - val_loss: 0.6509 - val_acc: 0.9811 - val_mDice: 0.7827

Epoch 00151: val_mDice improved from 0.78249 to 0.78272, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 152/300
 - 24s - loss: 0.5530 - acc: 0.9730 - mDice: 0.7952 - val_loss: 0.6534 - val_acc: 0.9816 - val_mDice: 0.7792

Epoch 00152: val_mDice did not improve from 0.78272
Epoch 153/300
 - 26s - loss: 0.5507 - acc: 0.9731 - mDice: 0.7960 - val_loss: 0.6540 - val_acc: 0.9813 - val_mDice: 0.7778

Epoch 00153: val_mDice did not improve from 0.78272
Epoch 154/300
 - 25s - loss: 0.5510 - acc: 0.9730 - mDice: 0.7958 - val_loss: 0.6370 - val_acc: 0.9813 - val_mDice: 0.7799

Epoch 00154: val_mDice did not improve from 0.78272
Epoch 155/300
 - 26s - loss: 0.5504 - acc: 0.9730 - mDice: 0.7960 - val_loss: 0.6501 - val_acc: 0.9815 - val_mDice: 0.7807

Epoch 00155: val_mDice did not improve from 0.78272
Epoch 156/300
 - 26s - loss: 0.5514 - acc: 0.9730 - mDice: 0.7957 - val_loss: 0.6470 - val_acc: 0.9818 - val_mDice: 0.7773

Epoch 00156: val_mDice did not improve from 0.78272
Epoch 157/300
 - 26s - loss: 0.5507 - acc: 0.9730 - mDice: 0.7959 - val_loss: 0.6583 - val_acc: 0.9811 - val_mDice: 0.7797

Epoch 00157: val_mDice did not improve from 0.78272
Epoch 158/300
 - 25s - loss: 0.5508 - acc: 0.9730 - mDice: 0.7958 - val_loss: 0.6472 - val_acc: 0.9819 - val_mDice: 0.7806

Epoch 00158: val_mDice did not improve from 0.78272
Epoch 159/300
 - 26s - loss: 0.5492 - acc: 0.9730 - mDice: 0.7964 - val_loss: 0.6556 - val_acc: 0.9817 - val_mDice: 0.7812

Epoch 00159: val_mDice did not improve from 0.78272
Epoch 160/300
 - 26s - loss: 0.5496 - acc: 0.9730 - mDice: 0.7964 - val_loss: 0.6509 - val_acc: 0.9814 - val_mDice: 0.7803

Epoch 00160: val_mDice did not improve from 0.78272
Epoch 161/300
 - 25s - loss: 0.5507 - acc: 0.9731 - mDice: 0.7960 - val_loss: 0.6580 - val_acc: 0.9817 - val_mDice: 0.7788

Epoch 00161: val_mDice did not improve from 0.78272
Epoch 162/300
 - 25s - loss: 0.5486 - acc: 0.9731 - mDice: 0.7968 - val_loss: 0.6620 - val_acc: 0.9816 - val_mDice: 0.7799

Epoch 00162: val_mDice did not improve from 0.78272
Epoch 163/300
 - 25s - loss: 0.5487 - acc: 0.9731 - mDice: 0.7969 - val_loss: 0.6445 - val_acc: 0.9818 - val_mDice: 0.7801

Epoch 00163: val_mDice did not improve from 0.78272
Epoch 164/300
 - 27s - loss: 0.5487 - acc: 0.9731 - mDice: 0.7967 - val_loss: 0.6557 - val_acc: 0.9818 - val_mDice: 0.7806

Epoch 00164: val_mDice did not improve from 0.78272
Epoch 165/300
 - 26s - loss: 0.5485 - acc: 0.9731 - mDice: 0.7967 - val_loss: 0.6665 - val_acc: 0.9815 - val_mDice: 0.7763

Epoch 00165: val_mDice did not improve from 0.78272
Epoch 166/300
 - 26s - loss: 0.5493 - acc: 0.9731 - mDice: 0.7964 - val_loss: 0.6510 - val_acc: 0.9819 - val_mDice: 0.7776

Epoch 00166: val_mDice did not improve from 0.78272
Epoch 167/300
 - 27s - loss: 0.5486 - acc: 0.9731 - mDice: 0.7968 - val_loss: 0.6491 - val_acc: 0.9817 - val_mDice: 0.7794

Epoch 00167: val_mDice did not improve from 0.78272
Epoch 168/300
 - 26s - loss: 0.5474 - acc: 0.9731 - mDice: 0.7972 - val_loss: 0.6543 - val_acc: 0.9818 - val_mDice: 0.7812

Epoch 00168: val_mDice did not improve from 0.78272
Epoch 169/300
 - 26s - loss: 0.5466 - acc: 0.9731 - mDice: 0.7974 - val_loss: 0.6550 - val_acc: 0.9813 - val_mDice: 0.7799

Epoch 00169: val_mDice did not improve from 0.78272
Epoch 170/300
 - 26s - loss: 0.5471 - acc: 0.9731 - mDice: 0.7971 - val_loss: 0.6488 - val_acc: 0.9820 - val_mDice: 0.7816

Epoch 00170: val_mDice did not improve from 0.78272
Epoch 171/300
 - 26s - loss: 0.5455 - acc: 0.9731 - mDice: 0.7978 - val_loss: 0.6683 - val_acc: 0.9809 - val_mDice: 0.7807

Epoch 00171: val_mDice did not improve from 0.78272
Epoch 172/300
 - 26s - loss: 0.5445 - acc: 0.9731 - mDice: 0.7983 - val_loss: 0.6426 - val_acc: 0.9817 - val_mDice: 0.7808

Epoch 00172: val_mDice did not improve from 0.78272
Epoch 173/300
 - 25s - loss: 0.5445 - acc: 0.9731 - mDice: 0.7982 - val_loss: 0.6536 - val_acc: 0.9817 - val_mDice: 0.7810

Epoch 00173: val_mDice did not improve from 0.78272
Epoch 174/300
 - 26s - loss: 0.5461 - acc: 0.9731 - mDice: 0.7976 - val_loss: 0.6616 - val_acc: 0.9821 - val_mDice: 0.7786

Epoch 00174: val_mDice did not improve from 0.78272
Epoch 175/300
 - 25s - loss: 0.5456 - acc: 0.9731 - mDice: 0.7976 - val_loss: 0.6550 - val_acc: 0.9811 - val_mDice: 0.7820

Epoch 00175: val_mDice did not improve from 0.78272
Epoch 176/300
 - 25s - loss: 0.5452 - acc: 0.9731 - mDice: 0.7978 - val_loss: 0.6621 - val_acc: 0.9817 - val_mDice: 0.7815

Epoch 00176: val_mDice did not improve from 0.78272
Epoch 177/300
 - 26s - loss: 0.5461 - acc: 0.9732 - mDice: 0.7977 - val_loss: 0.6533 - val_acc: 0.9817 - val_mDice: 0.7777

Epoch 00177: val_mDice did not improve from 0.78272
Epoch 178/300
 - 25s - loss: 0.5440 - acc: 0.9732 - mDice: 0.7982 - val_loss: 0.6603 - val_acc: 0.9816 - val_mDice: 0.7782

Epoch 00178: val_mDice did not improve from 0.78272
Epoch 179/300
 - 27s - loss: 0.5457 - acc: 0.9731 - mDice: 0.7977 - val_loss: 0.6658 - val_acc: 0.9816 - val_mDice: 0.7790

Epoch 00179: val_mDice did not improve from 0.78272
Epoch 180/300
 - 26s - loss: 0.5454 - acc: 0.9732 - mDice: 0.7980 - val_loss: 0.6566 - val_acc: 0.9817 - val_mDice: 0.7792

Epoch 00180: val_mDice did not improve from 0.78272
Epoch 181/300
 - 25s - loss: 0.5428 - acc: 0.9732 - mDice: 0.7989 - val_loss: 0.6560 - val_acc: 0.9818 - val_mDice: 0.7815

Epoch 00181: val_mDice did not improve from 0.78272
Restoring model weights from the end of the best epoch
Epoch 00181: early stopping
{'val_loss': [3.053183020945058, 1.9907505686930214, 1.284813220286886, 1.0180431611151641, 0.906039790349356, 0.8464255678641415, 0.7864526826407764, 0.7608164459374667, 0.7309728100880504, 0.7764806907981542, 0.7115119867895656, 0.7327971902548098, 0.70087455943519, 0.6833304798824499, 0.6902275534483179, 0.7088177603034166, 0.6961643587263499, 0.7538706954910782, 0.6625879926701203, 0.6712870846653378, 0.642125328658658, 0.6449052480782764, 0.6435092941034197, 0.6523185613785243, 0.6685663829523959, 0.6577565390443654, 0.6437631311554411, 0.6469272568252926, 0.6496368020434621, 0.6370454059358228, 0.6525488533348737, 0.657329345665972, 0.6455617827527663, 0.6344218311910167, 0.6648719346437169, 0.637626612887663, 0.6261907255255893, 0.6412331476669193, 0.6469732884959662, 0.6474639546391395, 0.6331330444913653, 0.6328213852440979, 0.6428661262779906, 0.6299956065090325, 0.6329099353868034, 0.6383348252505094, 0.6388408907365257, 0.6421955795295468, 0.6315040482886681, 0.6398232872333566, 0.6250538384028632, 0.6238982078030136, 0.6435993091117733, 0.6271135255279187, 0.6246552443356705, 0.6255379070992071, 0.6393128976118208, 0.6385745276669108, 0.6422081421772393, 0.6402068124897586, 0.6335176477675837, 0.6297751788445917, 0.6272037178739306, 0.624267235967274, 0.6260393079527884, 0.6204953099859511, 0.6328358324196562, 0.6390678348679045, 0.6206457729310074, 0.6239301071447485, 0.630071736157125, 0.6381105145187693, 0.6229744995218558, 0.6424544712584331, 0.6271907416229031, 0.6429781300487656, 0.6267345544908068, 0.6354475039815755, 0.6275911216457808, 0.6340868787991866, 0.6386310444958316, 0.6312564222067133, 0.6486718235739244, 0.6402313703722998, 0.6378034312351077, 0.6411680214422283, 0.6418621382907694, 0.6327078692438186, 0.6366173194104304, 0.6357769791125267, 0.6434301323688928, 0.6343268724725465, 0.6310818447540173, 0.6254276720916524, 0.6427810010956783, 0.6400537542821945, 0.6405519991036662, 0.636417409091788, 0.6405863055442262, 0.6403522433019152, 0.6405222022865579, 0.6515768244785667, 0.6458745642158639, 0.6358223334185479, 0.6424619996510792, 0.6365645687769564, 0.6320672895148074, 0.6503806222457019, 0.6389022750320572, 0.6398821463894918, 0.6440808435957744, 0.633762446903961, 0.6515122266684277, 0.6509743714541719, 0.6430632777442873, 0.6641300945274601, 0.6368924127766714, 0.6407471822941881, 0.6511802600141157, 0.6519787508698317, 0.6580450392113874, 0.65051140756922, 0.6501597875904127, 0.6346417363767654, 0.6514384103756324, 0.6372933003617022, 0.647630631985187, 0.6669415677171988, 0.6668124426506129, 0.6504013301848873, 0.6426982777222022, 0.6560431783413371, 0.6648514020123103, 0.6466646045292617, 0.6475180272162884, 0.6475142429364601, 0.6440619127853736, 0.6471560906515033, 0.6453501711934959, 0.6613084064302552, 0.6553460401708743, 0.6564220750725552, 0.6452973950942603, 0.6636478408817413, 0.6531982919195488, 0.6434370296535354, 0.6397173714404013, 0.6416805772835502, 0.6658394818643052, 0.6512855576164829, 0.6508951681378702, 0.6534212517049414, 0.6539867613522261, 0.6369566468385474, 0.6500645546536696, 0.6469815249782598, 0.6583089364694491, 0.6471584180929343, 0.6555635651998353, 0.6509493799770579, 0.6579563480535651, 0.662025697152558, 0.644476211015654, 0.6557087070617145, 0.6665010192691972, 0.6510137174397677, 0.6491055020176343, 0.6542950061268112, 0.6549838596822307, 0.6488010664657912, 0.6683101270836082, 0.6425555173088523, 0.6536062748255006, 0.661571464056324, 0.6550367902983576, 0.662071378115407, 0.6532600893878346, 0.6603154850695032, 0.6657929733564734, 0.6566140910051187, 0.6560159573491021], 'val_acc': [0.9207488931369486, 0.9412330735947695, 0.9627554992404146, 0.972033558492198, 0.9744078549561239, 0.975427017184849, 0.9774123466543861, 0.9770256790829394, 0.9781873243388992, 0.977205680312264, 0.9788122475454805, 0.9792851396020352, 0.9790557718867487, 0.9791679859038354, 0.9796033469023966, 0.9779991049884642, 0.9783597746008567, 0.9795483972019947, 0.9792904063398009, 0.9783972723196166, 0.9801729873853079, 0.9789606615116722, 0.979977197322314, 0.9797201736669668, 0.9801154061613683, 0.9801177031238505, 0.9804441450304046, 0.980474431509819, 0.9804645503022477, 0.980642252969791, 0.9804155293879002, 0.9804885765716388, 0.9810779594169435, 0.9805089775376767, 0.9809940390911633, 0.9801726581142414, 0.9814817413334015, 0.980571496904942, 0.981361613308067, 0.981431385557964, 0.980696226851736, 0.9815837528068337, 0.9801101303199005, 0.9808847843308935, 0.9815742180935493, 0.9796813606477743, 0.9815347198362321, 0.9813862957821542, 0.9808568208333263, 0.981460686807662, 0.9815241840847752, 0.9818147701990739, 0.9810921144436264, 0.9819579112148383, 0.981268166203986, 0.9814442223316622, 0.9812938404895204, 0.9813978285120245, 0.9809328281350426, 0.9807692745890779, 0.9812102410692426, 0.9810236647525193, 0.981108893858513, 0.9813839932836369, 0.9818963804604223, 0.9820898691321059, 0.9816624014124167, 0.9819829163782614, 0.9818131176925912, 0.9819954332306412, 0.9813685250479124, 0.9819717366013857, 0.9822047288203756, 0.9804902293241676, 0.9815715725453892, 0.9819661405310419, 0.9818193667690328, 0.9814741729582795, 0.9816508765560186, 0.9816923465029989, 0.9816775375475455, 0.981676227906171, 0.9814399490661543, 0.9813063365510127, 0.9822073550539243, 0.9817765858274249, 0.9817012357145887, 0.9818289220886703, 0.9818147583888657, 0.9820085930012328, 0.9816077797528514, 0.9820757184727397, 0.9809147281297343, 0.9819766769591254, 0.9819371881745794, 0.9819927954329303, 0.9821780662910611, 0.9812257133647261, 0.9818628175093785, 0.9818295706044287, 0.9817426970499588, 0.9820533409576298, 0.9816703018881342, 0.981901647567257, 0.9817525746283516, 0.9819727289049249, 0.9817417068378106, 0.9818861804387394, 0.981999045985648, 0.9818519556239417, 0.9819519940544578, 0.9823863661326122, 0.9816772081534559, 0.9814688984700647, 0.9815880223201401, 0.981513007814055, 0.9816179813738332, 0.982324174498626, 0.9819266466410413, 0.9820326190860894, 0.9817594790483284, 0.9812704672262272, 0.9815037913612783, 0.9815929665531045, 0.9820833001958573, 0.9815992251023173, 0.9813925523014876, 0.9810062168306365, 0.9814682364832875, 0.9817798787226248, 0.981213204201284, 0.9815547995892102, 0.9818131216293272, 0.981315228899689, 0.9817617793939432, 0.9814659373679028, 0.9819292805635277, 0.981877951860674, 0.981498194368262, 0.9815320814234061, 0.9821849675739512, 0.9817621189374304, 0.981715716198625, 0.9812069598612279, 0.9816699680037051, 0.9815034536631361, 0.9813362874851876, 0.9817463241371454, 0.9814103263573504, 0.9817446782123932, 0.9811440961525782, 0.981629163734192, 0.9813405690547482, 0.9813208031334498, 0.9814540912984449, 0.9817762607391405, 0.9810578831086095, 0.9818539236229148, 0.9816920258435425, 0.9813507626792348, 0.9816791885777524, 0.9815850631863463, 0.9817897561164832, 0.9818460299134624, 0.9814830521434945, 0.9819253350928103, 0.9817449895836129, 0.9818348416479995, 0.9813356136266908, 0.9820441320092562, 0.9809246021404601, 0.9816568123543841, 0.9817278732087221, 0.9820974376302508, 0.9811227187529683, 0.9816943181311506, 0.9817137436478007, 0.9815794671775141, 0.9816258721922451, 0.9816739227011477, 0.9817565132712924], 'val_mDice': [0.07681593263542935, 0.25885713752455264, 0.47898698209855084, 0.595925859745565, 0.6423683980301068, 0.6766570890774053, 0.6989395635661941, 0.7121843516642095, 0.7194565545663745, 0.7146685718259821, 0.7325464998851496, 0.7263254010886476, 0.7377575129670378, 0.7406761901543483, 0.7415447999695384, 0.7385818133413238, 0.7411487055144689, 0.7294315919664499, 0.7518060954731685, 0.7539594913414758, 0.7559392065947762, 0.7535167707378281, 0.7566345834141546, 0.7559851471976722, 0.7490308743635321, 0.7551849013880679, 0.7585078279792463, 0.7579540992299839, 0.7619200894461081, 0.7608572370127628, 0.7592083739791492, 0.7600459392471826, 0.7638486556101387, 0.7667850110799044, 0.7599731538563936, 0.76658247049871, 0.7674649496196593, 0.7650762346752903, 0.7627498605918097, 0.7654288261667493, 0.7681981754499815, 0.7665818762360957, 0.7674430429135806, 0.7659112205200274, 0.7681165872097507, 0.7671491130340702, 0.7649302806155477, 0.7665646712231317, 0.7694224418132298, 0.7701306760126592, 0.7708786733379305, 0.7731795320200846, 0.7701780650883883, 0.7700626239934081, 0.7722760296827499, 0.7736349963305289, 0.7692548921110707, 0.7738622978129747, 0.7726532605902452, 0.7713858898579151, 0.7733326951662699, 0.7744936378378617, 0.7777108900072158, 0.7759873362025487, 0.7766598071845323, 0.7739079923083538, 0.7744543817513252, 0.7729264641078756, 0.7769324798579064, 0.7747141021939131, 0.7791240142717942, 0.7757344917001124, 0.7760461529465037, 0.7739631860863929, 0.7744018272472732, 0.7733132644211421, 0.7786510233047453, 0.7725184298398202, 0.7772619568034468, 0.7778100154840294, 0.7751208915306934, 0.7759351341593992, 0.7761859445995098, 0.7771030632457989, 0.775386158465355, 0.7777806617035093, 0.775360249021351, 0.7775003741523183, 0.7791133026589551, 0.7807099430430662, 0.7762673259150502, 0.7763335266226462, 0.7785776080976945, 0.7773472672276452, 0.7753327612537348, 0.7798400795619678, 0.7757905735196959, 0.7767710158576414, 0.7777574360063079, 0.7797271594543576, 0.777980504267233, 0.7742459052487424, 0.7754696406447112, 0.7803308833371252, 0.778746763988176, 0.7773169501155031, 0.7774067686561215, 0.7753107106722545, 0.7769936075889659, 0.7783711792514789, 0.7795306130459434, 0.781617495236136, 0.7791750317879629, 0.7809658670573043, 0.7782714998020845, 0.7757762030797354, 0.7794072066421234, 0.7822265794156629, 0.777696546878362, 0.7806308630573245, 0.7788199312423651, 0.7789153803612795, 0.7770508001217286, 0.7824909012261066, 0.776394101734378, 0.7792073659237447, 0.7804644801668338, 0.7772316594980081, 0.7772026191800987, 0.7797779896064931, 0.7774704577876073, 0.7799530001001584, 0.7793731044812591, 0.7800680549029103, 0.7801829820692969, 0.7789210760187438, 0.7798081066832331, 0.7765783380305681, 0.7800506210671613, 0.7755714641389955, 0.7801875135600874, 0.7785186302551175, 0.7796974966400548, 0.778093044479811, 0.7801929236196512, 0.7802125329941788, 0.7790395188749882, 0.7806252328480976, 0.7776952229048076, 0.7770037922942847, 0.7827181538069088, 0.7791777599460692, 0.7777870561931401, 0.779857870841051, 0.7807333933802704, 0.7772648476594742, 0.77974907532565, 0.7805727415778688, 0.7812316928239553, 0.7803366379841193, 0.7788486566213877, 0.779896361847534, 0.7800586687399015, 0.7806160569314002, 0.7763161473475989, 0.7776296750802866, 0.7793716219310544, 0.7812039300999282, 0.779901101001153, 0.7816150995476943, 0.7807215699470449, 0.78077190677694, 0.7809744485268529, 0.7785572855588206, 0.7820246108914307, 0.7815387012788755, 0.7777063177343, 0.7782214358987208, 0.7790066810830336, 0.7792439103372572, 0.7815057215183758], 'loss': [20.276537314217503, 3.2506335118462086, 2.283311930268199, 1.7866665840091984, 1.5091838335717582, 1.3448001449108693, 1.230042272360592, 1.151848551849491, 1.0882905487630428, 1.0354412133009814, 0.9914211593310304, 0.9563378572876823, 0.9266463609393183, 0.9008725070144784, 0.882866825576554, 0.8607609254279796, 0.8459342511351648, 0.8209610190202491, 0.8082532976360972, 0.7979660701119646, 0.7837396773000535, 0.7750527804353968, 0.7643337849645931, 0.7537033322718993, 0.7448353678920487, 0.7397387275511876, 0.7338126812469199, 0.7249782887526107, 0.717512899056408, 0.7083722299811434, 0.705960550227369, 0.6998298216939457, 0.694276011336069, 0.6884576344877261, 0.6850684472433787, 0.6792473995104763, 0.6764697213395843, 0.6729514600981782, 0.6690603412006247, 0.6630761369952501, 0.6624510968061743, 0.6558859536705878, 0.6543485032741967, 0.6529960271735336, 0.6479355750126345, 0.6465727045530631, 0.6434504667215596, 0.6421151647661419, 0.639067232615913, 0.6354847214288577, 0.6327343640405465, 0.6327447634495498, 0.6303711643338858, 0.6296229898530998, 0.6267866038118766, 0.6256049774001714, 0.6215117635520797, 0.6214364521830014, 0.6187399040088182, 0.6170241570334631, 0.616342655415748, 0.6142227233758769, 0.6118911667708027, 0.610218505551031, 0.6093034832772817, 0.6058131025035085, 0.6059714905972807, 0.6062659647147582, 0.6032312617314468, 0.6031403427857729, 0.6011341151614333, 0.6001751586734352, 0.6014338125967883, 0.600018782079832, 0.5963393590965332, 0.5971389776778614, 0.5943847007930264, 0.5948921432351622, 0.5945713845975137, 0.5924776673160363, 0.5901232525467844, 0.5900721960009953, 0.590280446433668, 0.5869330513910089, 0.5882437510986672, 0.5872904507277374, 0.5875301054804086, 0.584849152145478, 0.5843185656874169, 0.5831166960962751, 0.5832788130961998, 0.5820170473498663, 0.5813533400069965, 0.580037122114991, 0.5806704625818306, 0.5786904626916751, 0.5799572481398996, 0.5782341065193549, 0.5787374927778645, 0.5763439910314179, 0.5752549879920207, 0.5738703198348363, 0.5725102796492213, 0.5752203335568704, 0.572526448320198, 0.5726329986587013, 0.5735870122781425, 0.5717048780614048, 0.5717208536578899, 0.5698481808101359, 0.5695532447886917, 0.5688789244202916, 0.5685763368109459, 0.56711610456284, 0.5673698420232197, 0.5674566627865564, 0.5671356983513088, 0.5651339169992933, 0.5655367694579817, 0.5651937616499222, 0.5656375940635547, 0.5643115417930827, 0.5632142541600436, 0.5626747259518284, 0.5629867396933781, 0.5626837726756272, 0.5610890649926387, 0.561631275594932, 0.561432897290097, 0.5602633935798174, 0.5610240671133294, 0.5594718087921001, 0.5588015623413808, 0.5586567444685453, 0.5585170554067003, 0.5581606309233687, 0.5576382297510916, 0.5572325372029695, 0.5580406758817705, 0.5562697633769935, 0.5564608820261407, 0.5558093427964034, 0.5552299347887866, 0.5539960831382379, 0.553998818641948, 0.5550349362822686, 0.5550503607030584, 0.5547829995668957, 0.5533508054888957, 0.5527274005910449, 0.5529926567629125, 0.5529765744631518, 0.550691036341553, 0.5510326371020396, 0.5504435046791945, 0.5514439419961813, 0.5507194431543578, 0.5508447162911976, 0.5491808972254498, 0.5496270457322255, 0.5507491571125394, 0.5485589009055039, 0.5487337393509565, 0.548717037679644, 0.5485418008852608, 0.5493149610829291, 0.5486047697509989, 0.547402749717563, 0.5465665082139163, 0.5470645452687414, 0.5454610641009203, 0.5444697662493992, 0.5444673545069588, 0.5461463987763241, 0.5456461829596213, 0.5452151440274382, 0.5460986233509211, 0.5439997317257433, 0.5456639571764431, 0.545403879348492, 0.5427567028390043], 'acc': [0.8148484947767125, 0.9157604536293855, 0.9306581552198915, 0.9446520103683954, 0.9514842878841175, 0.955219523858876, 0.9579682722776695, 0.9599631304868457, 0.9613856948913924, 0.9625799339028853, 0.9635518662095895, 0.9643912422918949, 0.9650357662492588, 0.9656751475301141, 0.9660546385020494, 0.9664838627810007, 0.9667539111264486, 0.9675176721115172, 0.9678822735812518, 0.9680621323478458, 0.9684635379133915, 0.9687338444578468, 0.9690100537981826, 0.9692230137131338, 0.9694343824246517, 0.9694842110108384, 0.969651694339928, 0.9698762299979191, 0.9700516433712404, 0.9701937445812416, 0.9702336516561387, 0.9703685691825297, 0.9704727255945022, 0.9705465978786887, 0.9705604522003887, 0.9706637400316116, 0.9707717711156212, 0.9707852148742074, 0.9708505768194556, 0.9709197398313395, 0.9709424741560843, 0.9710847281999682, 0.9711193535374605, 0.9711330240342791, 0.971185077929343, 0.9712468133013716, 0.9713071681005675, 0.9713116631335338, 0.9713659553346186, 0.9714327765758151, 0.971467646083087, 0.9714908652027154, 0.9715083631590553, 0.971502574340865, 0.9715931672653606, 0.9715952992951753, 0.9716369003762144, 0.9717249970803945, 0.9717685868525409, 0.9717702640392685, 0.9718023267898692, 0.971845433525234, 0.9718538758320259, 0.9718805142111786, 0.9719177162929113, 0.9720110967417221, 0.9719678393861175, 0.9719726572550366, 0.972065112799769, 0.9720361831366432, 0.9720944531114226, 0.9720864278805521, 0.9720769948867674, 0.9721299067570254, 0.9721934328735202, 0.9721276098642083, 0.9722024376519915, 0.9721798571860958, 0.972226817382387, 0.9722131950518724, 0.9722790253498346, 0.9723086324374716, 0.9722640417144325, 0.9723211803469306, 0.9722874982276736, 0.9723265393340619, 0.972289924963807, 0.972348311820348, 0.9723759562752257, 0.9723986903010219, 0.9724066281535046, 0.9724049539776158, 0.972428486075013, 0.9724470213116112, 0.9724786753639675, 0.9724747821215317, 0.9724452695514583, 0.9724775343414477, 0.9724863685555409, 0.9725207359075603, 0.9725461709690116, 0.9725760841158941, 0.9726211513202003, 0.9725555761392287, 0.972587306537348, 0.9726021001767167, 0.9726195407989973, 0.9726126006943872, 0.9726121436518931, 0.9726762021886212, 0.9726751072326479, 0.9726770325893714, 0.972690178032903, 0.9726860849860747, 0.9726870769471578, 0.9726828858807971, 0.9727153407522338, 0.9727492317226356, 0.9727375976134519, 0.9727419954383176, 0.9727235367966063, 0.9727945742727835, 0.972803408956653, 0.9727810275762497, 0.9727663084604816, 0.972806637337534, 0.9728499607821196, 0.9728269135223131, 0.9728236569192662, 0.9728607261112546, 0.9728190120561384, 0.9728595646321133, 0.9728565272366428, 0.9728735698730017, 0.9729008722524527, 0.9729012529705372, 0.9729068710108147, 0.9729174071033615, 0.972879957897953, 0.9729310024644471, 0.9729233483568365, 0.9729421414080798, 0.9729494149541981, 0.9729843041636023, 0.9729871619835314, 0.9729356856784018, 0.9729515465427077, 0.9729676749588397, 0.9729999176551084, 0.9730160329958008, 0.9729922811427548, 0.9730067250977447, 0.9730605296212277, 0.9730483922677647, 0.9730321596182415, 0.9730288280146999, 0.9730268198065237, 0.9730052095994706, 0.9730302291153324, 0.9730350159861348, 0.9730595594905059, 0.9731027035161423, 0.9730918125503241, 0.9730813793103109, 0.9730875647335654, 0.9730834080534048, 0.9730780388949052, 0.973104979140906, 0.9731233985776516, 0.9731126704177852, 0.973127472947314, 0.9731403929267033, 0.9731442286144272, 0.9731266073631677, 0.9731223811276465, 0.973141840791429, 0.9731592315747171, 0.9731802151286861, 0.9731411074208336, 0.9731561770964728, 0.9732011286219293], 'mDice': [0.0370691713127649, 0.1402492729547928, 0.29943305871987475, 0.4229488106489409, 0.5005195790589041, 0.5477926153854222, 0.5821819600951907, 0.6045708596450149, 0.6231194902126903, 0.6389094098373902, 0.6520496480912504, 0.662716077760321, 0.6716529265035442, 0.6797548181292604, 0.6851424513425183, 0.6921331475693256, 0.697107014264302, 0.7051625083006451, 0.7089145662521502, 0.7124492451530782, 0.7169038090994571, 0.7196598227792234, 0.7233197294515501, 0.7269645707118133, 0.7294602810994862, 0.7312647018564724, 0.733093559051545, 0.7360574637815723, 0.7384776777047544, 0.7418601640460313, 0.742280862889143, 0.7445701118483018, 0.7463483010168033, 0.7482409986442902, 0.7496054100198793, 0.7515015430999366, 0.7524100459359385, 0.7535143120350917, 0.7549449326116827, 0.7569603631141677, 0.7566545728940168, 0.7596140899278396, 0.7598582542409525, 0.760211636150598, 0.7621788047992559, 0.7623593146117115, 0.7634434467615305, 0.7637699185335639, 0.7649355925706796, 0.7659722670880712, 0.7670307061411413, 0.7669839233636913, 0.7677995446461723, 0.7679690760788327, 0.7691054440908452, 0.7691227754759054, 0.7709663394444423, 0.770762074127498, 0.7716531267108057, 0.7724164960661679, 0.7725728130835977, 0.7734626415936339, 0.7742844556307263, 0.7746475293239432, 0.7749243361342912, 0.7764887326809168, 0.7762814145584166, 0.7763996325061348, 0.7772552462133212, 0.7772854195379658, 0.7779693623089636, 0.7781086760017677, 0.7778342888880265, 0.7780742334997793, 0.7795033650848671, 0.779285419612219, 0.7803359495901623, 0.7801410764747626, 0.780151288877218, 0.7807471027572428, 0.7819128564427911, 0.7818598364741642, 0.781810494177604, 0.7828775005976055, 0.7823383007167432, 0.783109968114873, 0.7825871093591472, 0.783615367777569, 0.7840544859079521, 0.7842076081335047, 0.7842446686915745, 0.7847693753407626, 0.7848181034546636, 0.7852730918306424, 0.7851838287217878, 0.785976410168351, 0.7854799812324639, 0.7859324612593577, 0.7859648999021592, 0.786647611766776, 0.7870384905214153, 0.7875373689419961, 0.7877541083138742, 0.7869369176298501, 0.7880071505302741, 0.7880820624008935, 0.7875292026501212, 0.7883822482968106, 0.788433763643196, 0.7891797030387757, 0.7889761354961969, 0.7894020218794517, 0.7892887074565272, 0.7899849046620486, 0.7896691104136933, 0.7898369036883592, 0.7899824951510456, 0.7905865654488125, 0.7905119246319879, 0.7907077778572964, 0.7903733196607603, 0.7911030046405159, 0.7914855153272509, 0.7915408874728215, 0.7914791829992599, 0.791685952864372, 0.7917861783308034, 0.791970716197251, 0.7919175583136973, 0.792497541722518, 0.7921684164191761, 0.7926270828302304, 0.792900553787697, 0.7930169733606434, 0.7930370089630253, 0.7931635027031981, 0.7934374879945113, 0.7936826488597029, 0.7932400928566898, 0.7937647712125339, 0.7938355376420341, 0.7940741341097531, 0.7943986718555552, 0.7947923695098708, 0.7947763679973943, 0.7941945272853501, 0.7940818718304234, 0.7943251286496517, 0.7949890232766645, 0.7951449960975565, 0.7950383979094919, 0.7951825598571988, 0.7960372292713587, 0.7958436668317757, 0.7960292991559489, 0.7957320883318494, 0.7959050883790146, 0.7957968858622525, 0.7963518910385813, 0.7963932382126597, 0.7960311255820588, 0.7967863073846108, 0.7968616220100928, 0.7967138634551987, 0.7966831185195905, 0.7964092227847188, 0.7967656332850883, 0.7972048454419053, 0.7974379395268428, 0.7971194244339326, 0.7978333316641945, 0.7982659318266605, 0.7981943608412231, 0.79758509967929, 0.7976185848412833, 0.7978450341970049, 0.7977182278990148, 0.7981905883028195, 0.7976685217678448, 0.7979550787207902, 0.7989212188141027]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:02<00:29,  2.10s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:25,  2.00s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:24,  2.02s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:22,  2.03s/it]predicting test subjects:  33%|███▎      | 5/15 [00:10<00:21,  2.15s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:20,  2.23s/it]predicting test subjects:  47%|████▋     | 7/15 [00:14<00:16,  2.05s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:16<00:15,  2.18s/it]predicting test subjects:  60%|██████    | 9/15 [00:18<00:12,  2.13s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:20<00:09,  2.00s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:22<00:07,  1.94s/it]predicting test subjects:  80%|████████  | 12/15 [00:24<00:06,  2.00s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:26<00:04,  2.03s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:28<00:01,  1.99s/it]predicting test subjects: 100%|██████████| 15/15 [00:30<00:00,  1.99s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:46,  2.35s/it]predicting train subjects:   0%|          | 2/532 [00:04<19:15,  2.18s/it]predicting train subjects:   1%|          | 3/532 [00:06<18:24,  2.09s/it]predicting train subjects:   1%|          | 4/532 [00:07<17:44,  2.02s/it]predicting train subjects:   1%|          | 5/532 [00:09<17:33,  2.00s/it]predicting train subjects:   1%|          | 6/532 [00:11<16:58,  1.94s/it]predicting train subjects:   1%|▏         | 7/532 [00:13<16:32,  1.89s/it]predicting train subjects:   2%|▏         | 8/532 [00:15<15:58,  1.83s/it]predicting train subjects:   2%|▏         | 9/532 [00:17<16:44,  1.92s/it]predicting train subjects:   2%|▏         | 10/532 [00:19<16:30,  1.90s/it]predicting train subjects:   2%|▏         | 11/532 [00:20<15:42,  1.81s/it]predicting train subjects:   2%|▏         | 12/532 [00:22<16:45,  1.93s/it]predicting train subjects:   2%|▏         | 13/532 [00:24<15:56,  1.84s/it]predicting train subjects:   3%|▎         | 14/532 [00:25<14:57,  1.73s/it]predicting train subjects:   3%|▎         | 15/532 [00:27<14:52,  1.73s/it]predicting train subjects:   3%|▎         | 16/532 [00:29<15:19,  1.78s/it]predicting train subjects:   3%|▎         | 17/532 [00:31<15:02,  1.75s/it]predicting train subjects:   3%|▎         | 18/532 [00:33<15:44,  1.84s/it]predicting train subjects:   4%|▎         | 19/532 [00:34<14:52,  1.74s/it]predicting train subjects:   4%|▍         | 20/532 [00:36<15:05,  1.77s/it]predicting train subjects:   4%|▍         | 21/532 [00:38<15:49,  1.86s/it]predicting train subjects:   4%|▍         | 22/532 [00:40<15:21,  1.81s/it]predicting train subjects:   4%|▍         | 23/532 [00:42<15:30,  1.83s/it]predicting train subjects:   5%|▍         | 24/532 [00:43<14:42,  1.74s/it]predicting train subjects:   5%|▍         | 25/532 [00:46<16:04,  1.90s/it]predicting train subjects:   5%|▍         | 26/532 [00:47<15:31,  1.84s/it]predicting train subjects:   5%|▌         | 27/532 [00:50<17:02,  2.02s/it]predicting train subjects:   5%|▌         | 28/532 [00:52<16:33,  1.97s/it]predicting train subjects:   5%|▌         | 29/532 [00:54<16:49,  2.01s/it]predicting train subjects:   6%|▌         | 30/532 [00:55<15:40,  1.87s/it]predicting train subjects:   6%|▌         | 31/532 [00:57<15:23,  1.84s/it]predicting train subjects:   6%|▌         | 32/532 [00:59<15:17,  1.83s/it]predicting train subjects:   6%|▌         | 33/532 [01:00<14:34,  1.75s/it]predicting train subjects:   6%|▋         | 34/532 [01:03<15:45,  1.90s/it]predicting train subjects:   7%|▋         | 35/532 [01:04<15:25,  1.86s/it]predicting train subjects:   7%|▋         | 36/532 [01:06<15:45,  1.91s/it]predicting train subjects:   7%|▋         | 37/532 [01:08<15:54,  1.93s/it]predicting train subjects:   7%|▋         | 38/532 [01:11<16:14,  1.97s/it]predicting train subjects:   7%|▋         | 39/532 [01:12<15:48,  1.92s/it]predicting train subjects:   8%|▊         | 40/532 [01:14<15:08,  1.85s/it]predicting train subjects:   8%|▊         | 41/532 [01:16<15:27,  1.89s/it]predicting train subjects:   8%|▊         | 42/532 [01:18<15:34,  1.91s/it]predicting train subjects:   8%|▊         | 43/532 [01:20<14:46,  1.81s/it]predicting train subjects:   8%|▊         | 44/532 [01:21<14:08,  1.74s/it]predicting train subjects:   8%|▊         | 45/532 [01:23<13:53,  1.71s/it]predicting train subjects:   9%|▊         | 46/532 [01:25<14:10,  1.75s/it]predicting train subjects:   9%|▉         | 47/532 [01:27<15:06,  1.87s/it]predicting train subjects:   9%|▉         | 48/532 [01:29<15:13,  1.89s/it]predicting train subjects:   9%|▉         | 49/532 [01:30<14:36,  1.82s/it]predicting train subjects:   9%|▉         | 50/532 [01:32<15:29,  1.93s/it]predicting train subjects:  10%|▉         | 51/532 [01:34<15:07,  1.89s/it]predicting train subjects:  10%|▉         | 52/532 [01:36<15:01,  1.88s/it]predicting train subjects:  10%|▉         | 53/532 [01:38<14:41,  1.84s/it]predicting train subjects:  10%|█         | 54/532 [01:40<15:18,  1.92s/it]predicting train subjects:  10%|█         | 55/532 [01:42<15:25,  1.94s/it]predicting train subjects:  11%|█         | 56/532 [01:44<15:26,  1.95s/it]predicting train subjects:  11%|█         | 57/532 [01:46<15:09,  1.92s/it]predicting train subjects:  11%|█         | 58/532 [01:48<15:09,  1.92s/it]predicting train subjects:  11%|█         | 59/532 [01:50<15:54,  2.02s/it]predicting train subjects:  11%|█▏        | 60/532 [01:51<14:42,  1.87s/it]predicting train subjects:  11%|█▏        | 61/532 [01:53<13:59,  1.78s/it]predicting train subjects:  12%|█▏        | 62/532 [01:55<14:46,  1.89s/it]predicting train subjects:  12%|█▏        | 63/532 [01:57<15:20,  1.96s/it]predicting train subjects:  12%|█▏        | 64/532 [01:59<14:36,  1.87s/it]predicting train subjects:  12%|█▏        | 65/532 [02:01<14:37,  1.88s/it]predicting train subjects:  12%|█▏        | 66/532 [02:03<15:40,  2.02s/it]predicting train subjects:  13%|█▎        | 67/532 [02:06<16:17,  2.10s/it]predicting train subjects:  13%|█▎        | 68/532 [02:08<16:00,  2.07s/it]predicting train subjects:  13%|█▎        | 69/532 [02:09<15:23,  1.99s/it]predicting train subjects:  13%|█▎        | 70/532 [02:11<14:46,  1.92s/it]predicting train subjects:  13%|█▎        | 71/532 [02:13<14:01,  1.83s/it]predicting train subjects:  14%|█▎        | 72/532 [02:14<13:28,  1.76s/it]predicting train subjects:  14%|█▎        | 73/532 [02:16<14:11,  1.85s/it]predicting train subjects:  14%|█▍        | 74/532 [02:19<15:37,  2.05s/it]predicting train subjects:  14%|█▍        | 75/532 [02:22<17:37,  2.31s/it]predicting train subjects:  14%|█▍        | 76/532 [02:24<16:22,  2.15s/it]predicting train subjects:  14%|█▍        | 77/532 [02:26<15:58,  2.11s/it]predicting train subjects:  15%|█▍        | 78/532 [02:28<15:37,  2.07s/it]predicting train subjects:  15%|█▍        | 79/532 [02:29<15:12,  2.01s/it]predicting train subjects:  15%|█▌        | 80/532 [02:31<14:54,  1.98s/it]predicting train subjects:  15%|█▌        | 81/532 [02:33<14:46,  1.96s/it]predicting train subjects:  15%|█▌        | 82/532 [02:35<14:40,  1.96s/it]predicting train subjects:  16%|█▌        | 83/532 [02:37<13:57,  1.87s/it]predicting train subjects:  16%|█▌        | 84/532 [02:39<13:46,  1.85s/it]predicting train subjects:  16%|█▌        | 85/532 [02:40<13:16,  1.78s/it]predicting train subjects:  16%|█▌        | 86/532 [02:42<12:57,  1.74s/it]predicting train subjects:  16%|█▋        | 87/532 [02:44<12:38,  1.70s/it]predicting train subjects:  17%|█▋        | 88/532 [02:45<12:27,  1.68s/it]predicting train subjects:  17%|█▋        | 89/532 [02:47<12:44,  1.73s/it]predicting train subjects:  17%|█▋        | 90/532 [02:49<12:54,  1.75s/it]predicting train subjects:  17%|█▋        | 91/532 [02:51<12:56,  1.76s/it]predicting train subjects:  17%|█▋        | 92/532 [02:52<12:55,  1.76s/it]predicting train subjects:  17%|█▋        | 93/532 [02:54<13:08,  1.80s/it]predicting train subjects:  18%|█▊        | 94/532 [02:56<13:36,  1.86s/it]predicting train subjects:  18%|█▊        | 95/532 [02:59<14:23,  1.97s/it]predicting train subjects:  18%|█▊        | 96/532 [03:01<14:46,  2.03s/it]predicting train subjects:  18%|█▊        | 97/532 [03:03<14:58,  2.07s/it]predicting train subjects:  18%|█▊        | 98/532 [03:05<15:01,  2.08s/it]predicting train subjects:  19%|█▊        | 99/532 [03:07<15:05,  2.09s/it]predicting train subjects:  19%|█▉        | 100/532 [03:09<15:02,  2.09s/it]predicting train subjects:  19%|█▉        | 101/532 [03:11<14:10,  1.97s/it]predicting train subjects:  19%|█▉        | 102/532 [03:12<13:27,  1.88s/it]predicting train subjects:  19%|█▉        | 103/532 [03:14<13:00,  1.82s/it]predicting train subjects:  20%|█▉        | 104/532 [03:16<12:51,  1.80s/it]predicting train subjects:  20%|█▉        | 105/532 [03:18<12:36,  1.77s/it]predicting train subjects:  20%|█▉        | 106/532 [03:19<12:14,  1.72s/it]predicting train subjects:  20%|██        | 107/532 [03:21<12:00,  1.69s/it]predicting train subjects:  20%|██        | 108/532 [03:23<11:58,  1.70s/it]predicting train subjects:  20%|██        | 109/532 [03:24<11:46,  1.67s/it]predicting train subjects:  21%|██        | 110/532 [03:26<11:35,  1.65s/it]predicting train subjects:  21%|██        | 111/532 [03:27<11:34,  1.65s/it]predicting train subjects:  21%|██        | 112/532 [03:29<11:36,  1.66s/it]predicting train subjects:  21%|██        | 113/532 [03:31<12:22,  1.77s/it]predicting train subjects:  21%|██▏       | 114/532 [03:33<12:37,  1.81s/it]predicting train subjects:  22%|██▏       | 115/532 [03:35<12:58,  1.87s/it]predicting train subjects:  22%|██▏       | 116/532 [03:37<12:58,  1.87s/it]predicting train subjects:  22%|██▏       | 117/532 [03:39<13:04,  1.89s/it]predicting train subjects:  22%|██▏       | 118/532 [03:41<13:02,  1.89s/it]predicting train subjects:  22%|██▏       | 119/532 [03:43<12:58,  1.88s/it]predicting train subjects:  23%|██▎       | 120/532 [03:44<12:42,  1.85s/it]predicting train subjects:  23%|██▎       | 121/532 [03:46<12:37,  1.84s/it]predicting train subjects:  23%|██▎       | 122/532 [03:48<12:41,  1.86s/it]predicting train subjects:  23%|██▎       | 123/532 [03:50<12:43,  1.87s/it]predicting train subjects:  23%|██▎       | 124/532 [03:52<12:39,  1.86s/it]predicting train subjects:  23%|██▎       | 125/532 [03:54<13:02,  1.92s/it]predicting train subjects:  24%|██▎       | 126/532 [03:56<13:13,  1.96s/it]predicting train subjects:  24%|██▍       | 127/532 [03:58<13:11,  1.96s/it]predicting train subjects:  24%|██▍       | 128/532 [04:00<13:17,  1.97s/it]predicting train subjects:  24%|██▍       | 129/532 [04:02<13:18,  1.98s/it]predicting train subjects:  24%|██▍       | 130/532 [04:04<13:28,  2.01s/it]predicting train subjects:  25%|██▍       | 131/532 [04:06<14:01,  2.10s/it]predicting train subjects:  25%|██▍       | 132/532 [04:09<14:29,  2.17s/it]predicting train subjects:  25%|██▌       | 133/532 [04:11<14:42,  2.21s/it]predicting train subjects:  25%|██▌       | 134/532 [04:13<15:01,  2.26s/it]predicting train subjects:  25%|██▌       | 135/532 [04:16<15:05,  2.28s/it]predicting train subjects:  26%|██▌       | 136/532 [04:18<15:03,  2.28s/it]predicting train subjects:  26%|██▌       | 137/532 [04:20<15:09,  2.30s/it]predicting train subjects:  26%|██▌       | 138/532 [04:23<15:07,  2.30s/it]predicting train subjects:  26%|██▌       | 139/532 [04:25<15:04,  2.30s/it]predicting train subjects:  26%|██▋       | 140/532 [04:27<15:03,  2.31s/it]predicting train subjects:  27%|██▋       | 141/532 [04:30<15:03,  2.31s/it]predicting train subjects:  27%|██▋       | 142/532 [04:32<15:12,  2.34s/it]predicting train subjects:  27%|██▋       | 143/532 [04:34<14:05,  2.17s/it]predicting train subjects:  27%|██▋       | 144/532 [04:35<13:14,  2.05s/it]predicting train subjects:  27%|██▋       | 145/532 [04:37<12:36,  1.95s/it]predicting train subjects:  27%|██▋       | 146/532 [04:39<12:09,  1.89s/it]predicting train subjects:  28%|██▊       | 147/532 [04:41<11:49,  1.84s/it]predicting train subjects:  28%|██▊       | 148/532 [04:42<11:40,  1.82s/it]predicting train subjects:  28%|██▊       | 149/532 [04:44<11:37,  1.82s/it]predicting train subjects:  28%|██▊       | 150/532 [04:46<11:36,  1.82s/it]predicting train subjects:  28%|██▊       | 151/532 [04:48<11:33,  1.82s/it]predicting train subjects:  29%|██▊       | 152/532 [04:50<11:48,  1.87s/it]predicting train subjects:  29%|██▉       | 153/532 [04:52<11:49,  1.87s/it]predicting train subjects:  29%|██▉       | 154/532 [04:54<11:53,  1.89s/it]predicting train subjects:  29%|██▉       | 155/532 [04:56<12:42,  2.02s/it]predicting train subjects:  29%|██▉       | 156/532 [04:58<13:15,  2.11s/it]predicting train subjects:  30%|██▉       | 157/532 [05:01<13:44,  2.20s/it]predicting train subjects:  30%|██▉       | 158/532 [05:03<14:00,  2.25s/it]predicting train subjects:  30%|██▉       | 159/532 [05:06<14:26,  2.32s/it]predicting train subjects:  30%|███       | 160/532 [05:08<14:31,  2.34s/it]predicting train subjects:  30%|███       | 161/532 [05:10<13:39,  2.21s/it]predicting train subjects:  30%|███       | 162/532 [05:12<13:02,  2.11s/it]predicting train subjects:  31%|███       | 163/532 [05:14<12:28,  2.03s/it]predicting train subjects:  31%|███       | 164/532 [05:15<12:05,  1.97s/it]predicting train subjects:  31%|███       | 165/532 [05:17<11:47,  1.93s/it]predicting train subjects:  31%|███       | 166/532 [05:19<11:35,  1.90s/it]predicting train subjects:  31%|███▏      | 167/532 [05:21<11:35,  1.90s/it]predicting train subjects:  32%|███▏      | 168/532 [05:23<11:35,  1.91s/it]predicting train subjects:  32%|███▏      | 169/532 [05:25<11:32,  1.91s/it]predicting train subjects:  32%|███▏      | 170/532 [05:27<11:29,  1.91s/it]predicting train subjects:  32%|███▏      | 171/532 [05:29<11:28,  1.91s/it]predicting train subjects:  32%|███▏      | 172/532 [05:31<11:24,  1.90s/it]predicting train subjects:  33%|███▎      | 173/532 [05:32<11:05,  1.85s/it]predicting train subjects:  33%|███▎      | 174/532 [05:34<10:54,  1.83s/it]predicting train subjects:  33%|███▎      | 175/532 [05:36<10:52,  1.83s/it]predicting train subjects:  33%|███▎      | 176/532 [05:38<10:34,  1.78s/it]predicting train subjects:  33%|███▎      | 177/532 [05:39<10:20,  1.75s/it]predicting train subjects:  33%|███▎      | 178/532 [05:41<10:16,  1.74s/it]predicting train subjects:  34%|███▎      | 179/532 [05:43<10:21,  1.76s/it]predicting train subjects:  34%|███▍      | 180/532 [05:45<10:20,  1.76s/it]predicting train subjects:  34%|███▍      | 181/532 [05:46<10:20,  1.77s/it]predicting train subjects:  34%|███▍      | 182/532 [05:48<10:21,  1.78s/it]predicting train subjects:  34%|███▍      | 183/532 [05:50<10:16,  1.77s/it]predicting train subjects:  35%|███▍      | 184/532 [05:52<10:11,  1.76s/it]predicting train subjects:  35%|███▍      | 185/532 [05:53<10:04,  1.74s/it]predicting train subjects:  35%|███▍      | 186/532 [05:55<09:57,  1.73s/it]predicting train subjects:  35%|███▌      | 187/532 [05:57<09:49,  1.71s/it]predicting train subjects:  35%|███▌      | 188/532 [05:58<09:51,  1.72s/it]predicting train subjects:  36%|███▌      | 189/532 [06:00<09:49,  1.72s/it]predicting train subjects:  36%|███▌      | 190/532 [06:02<09:50,  1.73s/it]predicting train subjects:  36%|███▌      | 191/532 [06:04<10:53,  1.92s/it]predicting train subjects:  36%|███▌      | 192/532 [06:07<11:58,  2.11s/it]predicting train subjects:  36%|███▋      | 193/532 [06:09<12:34,  2.22s/it]predicting train subjects:  36%|███▋      | 194/532 [06:12<12:49,  2.28s/it]predicting train subjects:  37%|███▋      | 195/532 [06:14<12:58,  2.31s/it]predicting train subjects:  37%|███▋      | 196/532 [06:17<13:07,  2.34s/it]predicting train subjects:  37%|███▋      | 197/532 [06:19<12:39,  2.27s/it]predicting train subjects:  37%|███▋      | 198/532 [06:21<12:19,  2.21s/it]predicting train subjects:  37%|███▋      | 199/532 [06:23<12:11,  2.20s/it]predicting train subjects:  38%|███▊      | 200/532 [06:25<12:12,  2.21s/it]predicting train subjects:  38%|███▊      | 201/532 [06:27<11:53,  2.16s/it]predicting train subjects:  38%|███▊      | 202/532 [06:29<11:44,  2.13s/it]predicting train subjects:  38%|███▊      | 203/532 [06:31<11:08,  2.03s/it]predicting train subjects:  38%|███▊      | 204/532 [06:33<10:45,  1.97s/it]predicting train subjects:  39%|███▊      | 205/532 [06:35<10:24,  1.91s/it]predicting train subjects:  39%|███▊      | 206/532 [06:36<10:12,  1.88s/it]predicting train subjects:  39%|███▉      | 207/532 [06:38<10:01,  1.85s/it]predicting train subjects:  39%|███▉      | 208/532 [06:40<09:49,  1.82s/it]predicting train subjects:  39%|███▉      | 209/532 [06:42<09:28,  1.76s/it]predicting train subjects:  39%|███▉      | 210/532 [06:43<09:14,  1.72s/it]predicting train subjects:  40%|███▉      | 211/532 [06:45<09:00,  1.69s/it]predicting train subjects:  40%|███▉      | 212/532 [06:46<08:53,  1.67s/it]predicting train subjects:  40%|████      | 213/532 [06:48<08:45,  1.65s/it]predicting train subjects:  40%|████      | 214/532 [06:50<08:31,  1.61s/it]predicting train subjects:  40%|████      | 215/532 [06:52<09:27,  1.79s/it]predicting train subjects:  41%|████      | 216/532 [06:54<10:07,  1.92s/it]predicting train subjects:  41%|████      | 217/532 [06:56<10:35,  2.02s/it]predicting train subjects:  41%|████      | 218/532 [06:58<10:52,  2.08s/it]predicting train subjects:  41%|████      | 219/532 [07:01<11:06,  2.13s/it]predicting train subjects:  41%|████▏     | 220/532 [07:03<11:15,  2.17s/it]predicting train subjects:  42%|████▏     | 221/532 [07:05<10:21,  2.00s/it]predicting train subjects:  42%|████▏     | 222/532 [07:06<09:41,  1.88s/it]predicting train subjects:  42%|████▏     | 223/532 [07:08<09:15,  1.80s/it]predicting train subjects:  42%|████▏     | 224/532 [07:09<08:51,  1.72s/it]predicting train subjects:  42%|████▏     | 225/532 [07:11<08:28,  1.66s/it]predicting train subjects:  42%|████▏     | 226/532 [07:12<08:14,  1.61s/it]predicting train subjects:  43%|████▎     | 227/532 [07:14<07:53,  1.55s/it]predicting train subjects:  43%|████▎     | 228/532 [07:15<07:47,  1.54s/it]predicting train subjects:  43%|████▎     | 229/532 [07:17<07:42,  1.53s/it]predicting train subjects:  43%|████▎     | 230/532 [07:18<07:34,  1.51s/it]predicting train subjects:  43%|████▎     | 231/532 [07:20<07:32,  1.50s/it]predicting train subjects:  44%|████▎     | 232/532 [07:21<07:32,  1.51s/it]predicting train subjects:  44%|████▍     | 233/532 [07:23<07:50,  1.57s/it]predicting train subjects:  44%|████▍     | 234/532 [07:25<07:56,  1.60s/it]predicting train subjects:  44%|████▍     | 235/532 [07:26<08:02,  1.62s/it]predicting train subjects:  44%|████▍     | 236/532 [07:28<08:07,  1.65s/it]predicting train subjects:  45%|████▍     | 237/532 [07:30<08:12,  1.67s/it]predicting train subjects:  45%|████▍     | 238/532 [07:31<08:17,  1.69s/it]predicting train subjects:  45%|████▍     | 239/532 [07:33<08:36,  1.76s/it]predicting train subjects:  45%|████▌     | 240/532 [07:35<08:50,  1.82s/it]predicting train subjects:  45%|████▌     | 241/532 [07:37<08:54,  1.84s/it]predicting train subjects:  45%|████▌     | 242/532 [07:39<09:06,  1.88s/it]predicting train subjects:  46%|████▌     | 243/532 [07:41<09:02,  1.88s/it]predicting train subjects:  46%|████▌     | 244/532 [07:43<08:58,  1.87s/it]predicting train subjects:  46%|████▌     | 245/532 [07:44<08:30,  1.78s/it]predicting train subjects:  46%|████▌     | 246/532 [07:46<08:05,  1.70s/it]predicting train subjects:  46%|████▋     | 247/532 [07:47<07:46,  1.64s/it]predicting train subjects:  47%|████▋     | 248/532 [07:49<07:28,  1.58s/it]predicting train subjects:  47%|████▋     | 249/532 [07:50<07:22,  1.56s/it]predicting train subjects:  47%|████▋     | 250/532 [07:52<07:22,  1.57s/it]predicting train subjects:  47%|████▋     | 251/532 [07:54<07:26,  1.59s/it]predicting train subjects:  47%|████▋     | 252/532 [07:55<07:26,  1.59s/it]predicting train subjects:  48%|████▊     | 253/532 [07:57<07:27,  1.60s/it]predicting train subjects:  48%|████▊     | 254/532 [07:59<07:36,  1.64s/it]predicting train subjects:  48%|████▊     | 255/532 [08:00<07:35,  1.64s/it]predicting train subjects:  48%|████▊     | 256/532 [08:02<07:31,  1.63s/it]predicting train subjects:  48%|████▊     | 257/532 [08:04<08:06,  1.77s/it]predicting train subjects:  48%|████▊     | 258/532 [08:06<08:32,  1.87s/it]predicting train subjects:  49%|████▊     | 259/532 [08:08<08:48,  1.94s/it]predicting train subjects:  49%|████▉     | 260/532 [08:10<08:54,  1.97s/it]predicting train subjects:  49%|████▉     | 261/532 [08:12<09:00,  1.99s/it]predicting train subjects:  49%|████▉     | 262/532 [08:14<09:04,  2.02s/it]predicting train subjects:  49%|████▉     | 263/532 [08:16<08:26,  1.88s/it]predicting train subjects:  50%|████▉     | 264/532 [08:17<07:57,  1.78s/it]predicting train subjects:  50%|████▉     | 265/532 [08:19<07:32,  1.70s/it]predicting train subjects:  50%|█████     | 266/532 [08:20<07:17,  1.65s/it]predicting train subjects:  50%|█████     | 267/532 [08:22<07:01,  1.59s/it]predicting train subjects:  50%|█████     | 268/532 [08:23<06:50,  1.55s/it]predicting train subjects:  51%|█████     | 269/532 [08:25<07:15,  1.66s/it]predicting train subjects:  51%|█████     | 270/532 [08:27<07:20,  1.68s/it]predicting train subjects:  51%|█████     | 271/532 [08:29<07:34,  1.74s/it]predicting train subjects:  51%|█████     | 272/532 [08:31<07:37,  1.76s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:33<07:40,  1.78s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:34<07:44,  1.80s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:37<08:25,  1.97s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:39<08:49,  2.07s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:41<09:06,  2.14s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:44<09:16,  2.19s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:46<09:18,  2.21s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:48<09:18,  2.22s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:50<09:05,  2.17s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:52<08:55,  2.14s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:54<08:49,  2.12s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:56<08:45,  2.12s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:59<08:44,  2.12s/it]predicting train subjects:  54%|█████▍    | 286/532 [09:01<08:39,  2.11s/it]predicting train subjects:  54%|█████▍    | 287/532 [09:02<08:05,  1.98s/it]predicting train subjects:  54%|█████▍    | 288/532 [09:04<07:41,  1.89s/it]predicting train subjects:  54%|█████▍    | 289/532 [09:06<07:21,  1.82s/it]predicting train subjects:  55%|█████▍    | 290/532 [09:07<07:12,  1.79s/it]predicting train subjects:  55%|█████▍    | 291/532 [09:09<07:04,  1.76s/it]predicting train subjects:  55%|█████▍    | 292/532 [09:11<06:58,  1.74s/it]predicting train subjects:  55%|█████▌    | 293/532 [09:13<07:11,  1.81s/it]predicting train subjects:  55%|█████▌    | 294/532 [09:15<07:14,  1.82s/it]predicting train subjects:  55%|█████▌    | 295/532 [09:16<07:13,  1.83s/it]predicting train subjects:  56%|█████▌    | 296/532 [09:18<07:10,  1.82s/it]predicting train subjects:  56%|█████▌    | 297/532 [09:20<07:07,  1.82s/it]predicting train subjects:  56%|█████▌    | 298/532 [09:22<07:13,  1.85s/it]predicting train subjects:  56%|█████▌    | 299/532 [09:24<06:56,  1.79s/it]predicting train subjects:  56%|█████▋    | 300/532 [09:25<06:41,  1.73s/it]predicting train subjects:  57%|█████▋    | 301/532 [09:27<06:33,  1.70s/it]predicting train subjects:  57%|█████▋    | 302/532 [09:28<06:25,  1.67s/it]predicting train subjects:  57%|█████▋    | 303/532 [09:30<06:19,  1.66s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:32<06:18,  1.66s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:34<07:05,  1.87s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:37<07:37,  2.02s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:39<07:51,  2.09s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:41<07:57,  2.13s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:43<08:00,  2.15s/it]predicting train subjects:  58%|█████▊    | 310/532 [09:46<08:08,  2.20s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:48<08:52,  2.41s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:51<09:30,  2.59s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:54<09:53,  2.71s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:57<10:00,  2.75s/it]predicting train subjects:  59%|█████▉    | 315/532 [10:00<10:07,  2.80s/it]predicting train subjects:  59%|█████▉    | 316/532 [10:03<10:11,  2.83s/it]predicting train subjects:  60%|█████▉    | 317/532 [10:05<09:03,  2.53s/it]predicting train subjects:  60%|█████▉    | 318/532 [10:07<08:07,  2.28s/it]predicting train subjects:  60%|█████▉    | 319/532 [10:08<07:34,  2.13s/it]predicting train subjects:  60%|██████    | 320/532 [10:10<07:04,  2.00s/it]predicting train subjects:  60%|██████    | 321/532 [10:12<06:45,  1.92s/it]predicting train subjects:  61%|██████    | 322/532 [10:14<06:30,  1.86s/it]predicting train subjects:  61%|██████    | 323/532 [10:16<07:02,  2.02s/it]predicting train subjects:  61%|██████    | 324/532 [10:18<07:19,  2.11s/it]predicting train subjects:  61%|██████    | 325/532 [10:21<07:30,  2.17s/it]predicting train subjects:  61%|██████▏   | 326/532 [10:23<07:41,  2.24s/it]predicting train subjects:  61%|██████▏   | 327/532 [10:25<07:49,  2.29s/it]predicting train subjects:  62%|██████▏   | 328/532 [10:28<07:59,  2.35s/it]predicting train subjects:  62%|██████▏   | 329/532 [10:30<07:25,  2.20s/it]predicting train subjects:  62%|██████▏   | 330/532 [10:32<07:01,  2.09s/it]predicting train subjects:  62%|██████▏   | 331/532 [10:33<06:45,  2.02s/it]predicting train subjects:  62%|██████▏   | 332/532 [10:35<06:31,  1.96s/it]predicting train subjects:  63%|██████▎   | 333/532 [10:37<06:22,  1.92s/it]predicting train subjects:  63%|██████▎   | 334/532 [10:39<06:14,  1.89s/it]predicting train subjects:  63%|██████▎   | 335/532 [10:41<06:20,  1.93s/it]predicting train subjects:  63%|██████▎   | 336/532 [10:43<06:23,  1.96s/it]predicting train subjects:  63%|██████▎   | 337/532 [10:45<06:28,  1.99s/it]predicting train subjects:  64%|██████▎   | 338/532 [10:47<06:27,  2.00s/it]predicting train subjects:  64%|██████▎   | 339/532 [10:49<06:26,  2.00s/it]predicting train subjects:  64%|██████▍   | 340/532 [10:51<06:24,  2.00s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:53<05:59,  1.88s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:54<05:41,  1.80s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:56<05:31,  1.76s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:57<05:22,  1.71s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:59<05:13,  1.68s/it]predicting train subjects:  65%|██████▌   | 346/532 [11:01<05:07,  1.65s/it]predicting train subjects:  65%|██████▌   | 347/532 [11:03<05:16,  1.71s/it]predicting train subjects:  65%|██████▌   | 348/532 [11:04<05:25,  1.77s/it]predicting train subjects:  66%|██████▌   | 349/532 [11:06<05:28,  1.79s/it]predicting train subjects:  66%|██████▌   | 350/532 [11:08<05:33,  1.83s/it]predicting train subjects:  66%|██████▌   | 351/532 [11:10<05:33,  1.84s/it]predicting train subjects:  66%|██████▌   | 352/532 [11:12<05:33,  1.85s/it]predicting train subjects:  66%|██████▋   | 353/532 [11:14<05:30,  1.85s/it]predicting train subjects:  67%|██████▋   | 354/532 [11:16<05:29,  1.85s/it]predicting train subjects:  67%|██████▋   | 355/532 [11:18<05:29,  1.86s/it]predicting train subjects:  67%|██████▋   | 356/532 [11:19<05:24,  1.84s/it]predicting train subjects:  67%|██████▋   | 357/532 [11:21<05:20,  1.83s/it]predicting train subjects:  67%|██████▋   | 358/532 [11:23<05:22,  1.85s/it]predicting train subjects:  67%|██████▋   | 359/532 [11:25<05:06,  1.77s/it]predicting train subjects:  68%|██████▊   | 360/532 [11:26<04:56,  1.72s/it]predicting train subjects:  68%|██████▊   | 361/532 [11:28<04:46,  1.68s/it]predicting train subjects:  68%|██████▊   | 362/532 [11:29<04:38,  1.64s/it]predicting train subjects:  68%|██████▊   | 363/532 [11:31<04:31,  1.61s/it]predicting train subjects:  68%|██████▊   | 364/532 [11:32<04:26,  1.59s/it]predicting train subjects:  69%|██████▊   | 365/532 [11:34<04:24,  1.59s/it]predicting train subjects:  69%|██████▉   | 366/532 [11:36<04:25,  1.60s/it]predicting train subjects:  69%|██████▉   | 367/532 [11:37<04:25,  1.61s/it]predicting train subjects:  69%|██████▉   | 368/532 [11:39<04:24,  1.61s/it]predicting train subjects:  69%|██████▉   | 369/532 [11:41<04:23,  1.62s/it]predicting train subjects:  70%|██████▉   | 370/532 [11:42<04:23,  1.62s/it]predicting train subjects:  70%|██████▉   | 371/532 [11:44<04:53,  1.82s/it]predicting train subjects:  70%|██████▉   | 372/532 [11:47<05:11,  1.95s/it]predicting train subjects:  70%|███████   | 373/532 [11:49<05:22,  2.03s/it]predicting train subjects:  70%|███████   | 374/532 [11:51<05:29,  2.09s/it]predicting train subjects:  70%|███████   | 375/532 [11:53<05:31,  2.11s/it]predicting train subjects:  71%|███████   | 376/532 [11:55<05:32,  2.13s/it]predicting train subjects:  71%|███████   | 377/532 [11:57<05:13,  2.03s/it]predicting train subjects:  71%|███████   | 378/532 [11:59<05:03,  1.97s/it]predicting train subjects:  71%|███████   | 379/532 [12:01<04:57,  1.94s/it]predicting train subjects:  71%|███████▏  | 380/532 [12:03<04:49,  1.90s/it]predicting train subjects:  72%|███████▏  | 381/532 [12:05<04:41,  1.87s/it]predicting train subjects:  72%|███████▏  | 382/532 [12:06<04:36,  1.85s/it]predicting train subjects:  72%|███████▏  | 383/532 [12:08<04:40,  1.88s/it]predicting train subjects:  72%|███████▏  | 384/532 [12:10<04:40,  1.90s/it]predicting train subjects:  72%|███████▏  | 385/532 [12:12<04:41,  1.92s/it]predicting train subjects:  73%|███████▎  | 386/532 [12:14<04:38,  1.91s/it]predicting train subjects:  73%|███████▎  | 387/532 [12:16<04:36,  1.91s/it]predicting train subjects:  73%|███████▎  | 388/532 [12:18<04:39,  1.94s/it]predicting train subjects:  73%|███████▎  | 389/532 [12:20<04:43,  1.98s/it]predicting train subjects:  73%|███████▎  | 390/532 [12:22<04:42,  1.99s/it]predicting train subjects:  73%|███████▎  | 391/532 [12:24<04:41,  2.00s/it]predicting train subjects:  74%|███████▎  | 392/532 [12:26<04:38,  1.99s/it]predicting train subjects:  74%|███████▍  | 393/532 [12:28<04:36,  1.99s/it]predicting train subjects:  74%|███████▍  | 394/532 [12:30<04:32,  1.97s/it]predicting train subjects:  74%|███████▍  | 395/532 [12:32<04:27,  1.95s/it]predicting train subjects:  74%|███████▍  | 396/532 [12:34<04:24,  1.94s/it]predicting train subjects:  75%|███████▍  | 397/532 [12:36<04:22,  1.94s/it]predicting train subjects:  75%|███████▍  | 398/532 [12:38<04:19,  1.94s/it]predicting train subjects:  75%|███████▌  | 399/532 [12:40<04:17,  1.94s/it]predicting train subjects:  75%|███████▌  | 400/532 [12:42<04:15,  1.94s/it]predicting train subjects:  75%|███████▌  | 401/532 [12:44<04:20,  1.99s/it]predicting train subjects:  76%|███████▌  | 402/532 [12:46<04:21,  2.01s/it]predicting train subjects:  76%|███████▌  | 403/532 [12:48<04:19,  2.01s/it]predicting train subjects:  76%|███████▌  | 404/532 [12:50<04:19,  2.03s/it]predicting train subjects:  76%|███████▌  | 405/532 [12:52<04:18,  2.03s/it]predicting train subjects:  76%|███████▋  | 406/532 [12:54<04:17,  2.04s/it]predicting train subjects:  77%|███████▋  | 407/532 [12:56<04:05,  1.97s/it]predicting train subjects:  77%|███████▋  | 408/532 [12:58<03:58,  1.92s/it]predicting train subjects:  77%|███████▋  | 409/532 [12:59<03:52,  1.89s/it]predicting train subjects:  77%|███████▋  | 410/532 [13:01<03:48,  1.87s/it]predicting train subjects:  77%|███████▋  | 411/532 [13:03<03:45,  1.87s/it]predicting train subjects:  77%|███████▋  | 412/532 [13:05<03:47,  1.90s/it]predicting train subjects:  78%|███████▊  | 413/532 [13:07<03:41,  1.86s/it]predicting train subjects:  78%|███████▊  | 414/532 [13:09<03:35,  1.82s/it]predicting train subjects:  78%|███████▊  | 415/532 [13:10<03:29,  1.79s/it]predicting train subjects:  78%|███████▊  | 416/532 [13:12<03:27,  1.79s/it]predicting train subjects:  78%|███████▊  | 417/532 [13:14<03:23,  1.77s/it]predicting train subjects:  79%|███████▊  | 418/532 [13:16<03:21,  1.77s/it]predicting train subjects:  79%|███████▉  | 419/532 [13:18<03:27,  1.84s/it]predicting train subjects:  79%|███████▉  | 420/532 [13:19<03:29,  1.87s/it]predicting train subjects:  79%|███████▉  | 421/532 [13:22<03:32,  1.91s/it]predicting train subjects:  79%|███████▉  | 422/532 [13:24<03:33,  1.94s/it]predicting train subjects:  80%|███████▉  | 423/532 [13:25<03:31,  1.94s/it]predicting train subjects:  80%|███████▉  | 424/532 [13:27<03:27,  1.92s/it]predicting train subjects:  80%|███████▉  | 425/532 [13:29<03:27,  1.94s/it]predicting train subjects:  80%|████████  | 426/532 [13:31<03:26,  1.95s/it]predicting train subjects:  80%|████████  | 427/532 [13:33<03:25,  1.96s/it]predicting train subjects:  80%|████████  | 428/532 [13:35<03:23,  1.96s/it]predicting train subjects:  81%|████████  | 429/532 [13:37<03:22,  1.96s/it]predicting train subjects:  81%|████████  | 430/532 [13:39<03:23,  2.00s/it]predicting train subjects:  81%|████████  | 431/532 [13:41<03:24,  2.03s/it]predicting train subjects:  81%|████████  | 432/532 [13:44<03:27,  2.07s/it]predicting train subjects:  81%|████████▏ | 433/532 [13:46<03:28,  2.11s/it]predicting train subjects:  82%|████████▏ | 434/532 [13:48<03:30,  2.15s/it]predicting train subjects:  82%|████████▏ | 435/532 [13:50<03:28,  2.15s/it]predicting train subjects:  82%|████████▏ | 436/532 [13:52<03:26,  2.15s/it]predicting train subjects:  82%|████████▏ | 437/532 [13:54<03:09,  1.99s/it]predicting train subjects:  82%|████████▏ | 438/532 [13:55<02:55,  1.87s/it]predicting train subjects:  83%|████████▎ | 439/532 [13:57<02:45,  1.78s/it]predicting train subjects:  83%|████████▎ | 440/532 [13:59<02:38,  1.72s/it]predicting train subjects:  83%|████████▎ | 441/532 [14:00<02:32,  1.68s/it]predicting train subjects:  83%|████████▎ | 442/532 [14:02<02:27,  1.64s/it]predicting train subjects:  83%|████████▎ | 443/532 [14:03<02:25,  1.64s/it]predicting train subjects:  83%|████████▎ | 444/532 [14:05<02:23,  1.63s/it]predicting train subjects:  84%|████████▎ | 445/532 [14:07<02:20,  1.62s/it]predicting train subjects:  84%|████████▍ | 446/532 [14:08<02:17,  1.60s/it]predicting train subjects:  84%|████████▍ | 447/532 [14:10<02:16,  1.61s/it]predicting train subjects:  84%|████████▍ | 448/532 [14:11<02:14,  1.60s/it]predicting train subjects:  84%|████████▍ | 449/532 [14:13<02:15,  1.63s/it]predicting train subjects:  85%|████████▍ | 450/532 [14:15<02:15,  1.65s/it]predicting train subjects:  85%|████████▍ | 451/532 [14:16<02:13,  1.65s/it]predicting train subjects:  85%|████████▍ | 452/532 [14:18<02:13,  1.67s/it]predicting train subjects:  85%|████████▌ | 453/532 [14:20<02:11,  1.66s/it]predicting train subjects:  85%|████████▌ | 454/532 [14:21<02:10,  1.67s/it]predicting train subjects:  86%|████████▌ | 455/532 [14:23<02:14,  1.75s/it]predicting train subjects:  86%|████████▌ | 456/532 [14:25<02:17,  1.81s/it]predicting train subjects:  86%|████████▌ | 457/532 [14:27<02:17,  1.84s/it]predicting train subjects:  86%|████████▌ | 458/532 [14:29<02:18,  1.87s/it]predicting train subjects:  86%|████████▋ | 459/532 [14:31<02:17,  1.88s/it]predicting train subjects:  86%|████████▋ | 460/532 [14:33<02:16,  1.89s/it]predicting train subjects:  87%|████████▋ | 461/532 [14:35<02:22,  2.01s/it]predicting train subjects:  87%|████████▋ | 462/532 [14:38<02:26,  2.09s/it]predicting train subjects:  87%|████████▋ | 463/532 [14:40<02:28,  2.15s/it]predicting train subjects:  87%|████████▋ | 464/532 [14:42<02:28,  2.18s/it]predicting train subjects:  87%|████████▋ | 465/532 [14:44<02:27,  2.20s/it]predicting train subjects:  88%|████████▊ | 466/532 [14:47<02:27,  2.24s/it]predicting train subjects:  88%|████████▊ | 467/532 [14:49<02:17,  2.11s/it]predicting train subjects:  88%|████████▊ | 468/532 [14:50<02:09,  2.02s/it]predicting train subjects:  88%|████████▊ | 469/532 [14:52<02:02,  1.94s/it]predicting train subjects:  88%|████████▊ | 470/532 [14:54<01:57,  1.90s/it]predicting train subjects:  89%|████████▊ | 471/532 [14:56<01:54,  1.87s/it]predicting train subjects:  89%|████████▊ | 472/532 [14:57<01:50,  1.84s/it]predicting train subjects:  89%|████████▉ | 473/532 [14:59<01:51,  1.89s/it]predicting train subjects:  89%|████████▉ | 474/532 [15:01<01:51,  1.93s/it]predicting train subjects:  89%|████████▉ | 475/532 [15:03<01:50,  1.94s/it]predicting train subjects:  89%|████████▉ | 476/532 [15:06<01:51,  1.98s/it]predicting train subjects:  90%|████████▉ | 477/532 [15:08<01:50,  2.00s/it]predicting train subjects:  90%|████████▉ | 478/532 [15:10<01:48,  2.02s/it]predicting train subjects:  90%|█████████ | 479/532 [15:11<01:43,  1.95s/it]predicting train subjects:  90%|█████████ | 480/532 [15:13<01:37,  1.88s/it]predicting train subjects:  90%|█████████ | 481/532 [15:15<01:32,  1.82s/it]predicting train subjects:  91%|█████████ | 482/532 [15:17<01:29,  1.79s/it]predicting train subjects:  91%|█████████ | 483/532 [15:18<01:25,  1.75s/it]predicting train subjects:  91%|█████████ | 484/532 [15:20<01:24,  1.75s/it]predicting train subjects:  91%|█████████ | 485/532 [15:22<01:27,  1.86s/it]predicting train subjects:  91%|█████████▏| 486/532 [15:24<01:29,  1.96s/it]predicting train subjects:  92%|█████████▏| 487/532 [15:26<01:30,  2.00s/it]predicting train subjects:  92%|█████████▏| 488/532 [15:29<01:30,  2.06s/it]predicting train subjects:  92%|█████████▏| 489/532 [15:31<01:29,  2.07s/it]predicting train subjects:  92%|█████████▏| 490/532 [15:33<01:29,  2.12s/it]predicting train subjects:  92%|█████████▏| 491/532 [15:35<01:24,  2.06s/it]predicting train subjects:  92%|█████████▏| 492/532 [15:37<01:19,  1.99s/it]predicting train subjects:  93%|█████████▎| 493/532 [15:38<01:15,  1.92s/it]predicting train subjects:  93%|█████████▎| 494/532 [15:40<01:11,  1.89s/it]predicting train subjects:  93%|█████████▎| 495/532 [15:42<01:09,  1.89s/it]predicting train subjects:  93%|█████████▎| 496/532 [15:44<01:06,  1.85s/it]predicting train subjects:  93%|█████████▎| 497/532 [15:46<01:04,  1.85s/it]predicting train subjects:  94%|█████████▎| 498/532 [15:48<01:03,  1.86s/it]predicting train subjects:  94%|█████████▍| 499/532 [15:49<01:00,  1.85s/it]predicting train subjects:  94%|█████████▍| 500/532 [15:51<00:59,  1.84s/it]predicting train subjects:  94%|█████████▍| 501/532 [15:53<00:57,  1.85s/it]predicting train subjects:  94%|█████████▍| 502/532 [15:55<00:55,  1.86s/it]predicting train subjects:  95%|█████████▍| 503/532 [15:57<00:52,  1.82s/it]predicting train subjects:  95%|█████████▍| 504/532 [15:58<00:50,  1.79s/it]predicting train subjects:  95%|█████████▍| 505/532 [16:00<00:47,  1.76s/it]predicting train subjects:  95%|█████████▌| 506/532 [16:02<00:46,  1.77s/it]predicting train subjects:  95%|█████████▌| 507/532 [16:04<00:44,  1.77s/it]predicting train subjects:  95%|█████████▌| 508/532 [16:05<00:42,  1.75s/it]predicting train subjects:  96%|█████████▌| 509/532 [16:08<00:43,  1.87s/it]predicting train subjects:  96%|█████████▌| 510/532 [16:10<00:43,  1.97s/it]predicting train subjects:  96%|█████████▌| 511/532 [16:12<00:42,  2.02s/it]predicting train subjects:  96%|█████████▌| 512/532 [16:14<00:41,  2.06s/it]predicting train subjects:  96%|█████████▋| 513/532 [16:16<00:39,  2.09s/it]predicting train subjects:  97%|█████████▋| 514/532 [16:18<00:37,  2.10s/it]predicting train subjects:  97%|█████████▋| 515/532 [16:20<00:33,  1.99s/it]predicting train subjects:  97%|█████████▋| 516/532 [16:22<00:31,  1.95s/it]predicting train subjects:  97%|█████████▋| 517/532 [16:24<00:28,  1.91s/it]predicting train subjects:  97%|█████████▋| 518/532 [16:26<00:26,  1.87s/it]predicting train subjects:  98%|█████████▊| 519/532 [16:27<00:24,  1.87s/it]predicting train subjects:  98%|█████████▊| 520/532 [16:29<00:22,  1.84s/it]predicting train subjects:  98%|█████████▊| 521/532 [16:31<00:20,  1.90s/it]predicting train subjects:  98%|█████████▊| 522/532 [16:33<00:19,  1.93s/it]predicting train subjects:  98%|█████████▊| 523/532 [16:35<00:17,  1.94s/it]predicting train subjects:  98%|█████████▊| 524/532 [16:37<00:15,  1.96s/it]predicting train subjects:  99%|█████████▊| 525/532 [16:39<00:13,  1.98s/it]predicting train subjects:  99%|█████████▉| 526/532 [16:41<00:11,  2.00s/it]predicting train subjects:  99%|█████████▉| 527/532 [16:43<00:09,  1.94s/it]predicting train subjects:  99%|█████████▉| 528/532 [16:45<00:07,  1.87s/it]predicting train subjects:  99%|█████████▉| 529/532 [16:46<00:05,  1.82s/it]predicting train subjects: 100%|█████████▉| 530/532 [16:48<00:03,  1.81s/it]predicting train subjects: 100%|█████████▉| 531/532 [16:50<00:01,  1.79s/it]predicting train subjects: 100%|██████████| 532/532 [16:52<00:00,  1.78s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<14:10,  1.60s/it]Loading train:   0%|          | 2/532 [00:02<12:21,  1.40s/it]Loading train:   1%|          | 3/532 [00:03<10:54,  1.24s/it]Loading train:   1%|          | 4/532 [00:04<09:49,  1.12s/it]Loading train:   1%|          | 5/532 [00:05<09:14,  1.05s/it]Loading train:   1%|          | 6/532 [00:05<08:35,  1.02it/s]Loading train:   1%|▏         | 7/532 [00:06<08:11,  1.07it/s]Loading train:   2%|▏         | 8/532 [00:07<07:51,  1.11it/s]Loading train:   2%|▏         | 9/532 [00:08<08:08,  1.07it/s]Loading train:   2%|▏         | 10/532 [00:09<07:43,  1.13it/s]Loading train:   2%|▏         | 11/532 [00:10<07:14,  1.20it/s]Loading train:   2%|▏         | 12/532 [00:11<07:49,  1.11it/s]Loading train:   2%|▏         | 13/532 [00:11<07:16,  1.19it/s]Loading train:   3%|▎         | 14/532 [00:12<07:15,  1.19it/s]Loading train:   3%|▎         | 15/532 [00:13<07:14,  1.19it/s]Loading train:   3%|▎         | 16/532 [00:14<07:13,  1.19it/s]Loading train:   3%|▎         | 17/532 [00:15<07:05,  1.21it/s]Loading train:   3%|▎         | 18/532 [00:16<07:26,  1.15it/s]Loading train:   4%|▎         | 19/532 [00:16<07:02,  1.22it/s]Loading train:   4%|▍         | 20/532 [00:17<07:09,  1.19it/s]Loading train:   4%|▍         | 21/532 [00:18<07:35,  1.12it/s]Loading train:   4%|▍         | 22/532 [00:19<07:23,  1.15it/s]Loading train:   4%|▍         | 23/532 [00:20<07:27,  1.14it/s]Loading train:   5%|▍         | 24/532 [00:21<06:57,  1.22it/s]Loading train:   5%|▍         | 25/532 [00:22<07:27,  1.13it/s]Loading train:   5%|▍         | 26/532 [00:22<07:04,  1.19it/s]Loading train:   5%|▌         | 27/532 [00:24<07:53,  1.07it/s]Loading train:   5%|▌         | 28/532 [00:25<07:51,  1.07it/s]Loading train:   5%|▌         | 29/532 [00:26<08:17,  1.01it/s]Loading train:   6%|▌         | 30/532 [00:26<07:54,  1.06it/s]Loading train:   6%|▌         | 31/532 [00:27<07:56,  1.05it/s]Loading train:   6%|▌         | 32/532 [00:28<07:44,  1.08it/s]Loading train:   6%|▌         | 33/532 [00:29<07:27,  1.12it/s]Loading train:   6%|▋         | 34/532 [00:30<07:55,  1.05it/s]Loading train:   7%|▋         | 35/532 [00:31<07:29,  1.11it/s]Loading train:   7%|▋         | 36/532 [00:32<07:28,  1.11it/s]Loading train:   7%|▋         | 37/532 [00:33<07:17,  1.13it/s]Loading train:   7%|▋         | 38/532 [00:34<07:34,  1.09it/s]Loading train:   7%|▋         | 39/532 [00:35<07:15,  1.13it/s]Loading train:   8%|▊         | 40/532 [00:35<06:48,  1.21it/s]Loading train:   8%|▊         | 41/532 [00:36<07:05,  1.15it/s]Loading train:   8%|▊         | 42/532 [00:37<07:14,  1.13it/s]Loading train:   8%|▊         | 43/532 [00:38<07:15,  1.12it/s]Loading train:   8%|▊         | 44/532 [00:39<07:12,  1.13it/s]Loading train:   8%|▊         | 45/532 [00:40<06:56,  1.17it/s]Loading train:   9%|▊         | 46/532 [00:41<07:12,  1.12it/s]Loading train:   9%|▉         | 47/532 [00:42<07:38,  1.06it/s]Loading train:   9%|▉         | 48/532 [00:43<07:30,  1.07it/s]Loading train:   9%|▉         | 49/532 [00:43<07:05,  1.14it/s]Loading train:   9%|▉         | 50/532 [00:44<07:15,  1.11it/s]Loading train:  10%|▉         | 51/532 [00:45<06:59,  1.15it/s]Loading train:  10%|▉         | 52/532 [00:46<06:47,  1.18it/s]Loading train:  10%|▉         | 53/532 [00:47<06:40,  1.20it/s]Loading train:  10%|█         | 54/532 [00:48<07:05,  1.12it/s]Loading train:  10%|█         | 55/532 [00:49<07:00,  1.13it/s]Loading train:  11%|█         | 56/532 [00:50<06:58,  1.14it/s]Loading train:  11%|█         | 57/532 [00:50<06:58,  1.14it/s]Loading train:  11%|█         | 58/532 [00:51<07:03,  1.12it/s]Loading train:  11%|█         | 59/532 [00:52<07:43,  1.02it/s]Loading train:  11%|█▏        | 60/532 [00:53<07:06,  1.11it/s]Loading train:  11%|█▏        | 61/532 [00:54<06:51,  1.15it/s]Loading train:  12%|█▏        | 62/532 [00:55<07:10,  1.09it/s]Loading train:  12%|█▏        | 63/532 [00:56<07:23,  1.06it/s]Loading train:  12%|█▏        | 64/532 [00:57<06:58,  1.12it/s]Loading train:  12%|█▏        | 65/532 [00:58<07:00,  1.11it/s]Loading train:  12%|█▏        | 66/532 [00:59<07:45,  1.00it/s]Loading train:  13%|█▎        | 67/532 [01:00<07:52,  1.02s/it]Loading train:  13%|█▎        | 68/532 [01:01<07:44,  1.00s/it]Loading train:  13%|█▎        | 69/532 [01:02<07:25,  1.04it/s]Loading train:  13%|█▎        | 70/532 [01:03<06:57,  1.11it/s]Loading train:  13%|█▎        | 71/532 [01:03<06:28,  1.19it/s]Loading train:  14%|█▎        | 72/532 [01:04<06:17,  1.22it/s]Loading train:  14%|█▎        | 73/532 [01:05<06:25,  1.19it/s]Loading train:  14%|█▍        | 74/532 [01:06<07:08,  1.07it/s]Loading train:  14%|█▍        | 75/532 [01:08<08:21,  1.10s/it]Loading train:  14%|█▍        | 76/532 [01:08<07:39,  1.01s/it]Loading train:  14%|█▍        | 77/532 [01:09<07:24,  1.02it/s]Loading train:  15%|█▍        | 78/532 [01:10<07:07,  1.06it/s]Loading train:  15%|█▍        | 79/532 [01:11<06:52,  1.10it/s]Loading train:  15%|█▌        | 80/532 [01:12<06:46,  1.11it/s]Loading train:  15%|█▌        | 81/532 [01:13<06:37,  1.13it/s]Loading train:  15%|█▌        | 82/532 [01:14<06:37,  1.13it/s]Loading train:  16%|█▌        | 83/532 [01:14<06:18,  1.19it/s]Loading train:  16%|█▌        | 84/532 [01:15<05:57,  1.25it/s]Loading train:  16%|█▌        | 85/532 [01:16<05:46,  1.29it/s]Loading train:  16%|█▌        | 86/532 [01:17<05:40,  1.31it/s]Loading train:  16%|█▋        | 87/532 [01:17<05:42,  1.30it/s]Loading train:  17%|█▋        | 88/532 [01:18<05:38,  1.31it/s]Loading train:  17%|█▋        | 89/532 [01:19<05:49,  1.27it/s]Loading train:  17%|█▋        | 90/532 [01:20<05:49,  1.27it/s]Loading train:  17%|█▋        | 91/532 [01:20<05:46,  1.27it/s]Loading train:  17%|█▋        | 92/532 [01:21<05:47,  1.27it/s]Loading train:  17%|█▋        | 93/532 [01:22<05:45,  1.27it/s]Loading train:  18%|█▊        | 94/532 [01:23<05:47,  1.26it/s]Loading train:  18%|█▊        | 95/532 [01:24<06:21,  1.15it/s]Loading train:  18%|█▊        | 96/532 [01:25<06:47,  1.07it/s]Loading train:  18%|█▊        | 97/532 [01:26<07:05,  1.02it/s]Loading train:  18%|█▊        | 98/532 [01:27<07:20,  1.01s/it]Loading train:  19%|█▊        | 99/532 [01:28<07:23,  1.02s/it]Loading train:  19%|█▉        | 100/532 [01:29<07:24,  1.03s/it]Loading train:  19%|█▉        | 101/532 [01:30<06:58,  1.03it/s]Loading train:  19%|█▉        | 102/532 [01:31<06:24,  1.12it/s]Loading train:  19%|█▉        | 103/532 [01:32<06:00,  1.19it/s]Loading train:  20%|█▉        | 104/532 [01:32<05:40,  1.26it/s]Loading train:  20%|█▉        | 105/532 [01:33<05:24,  1.32it/s]Loading train:  20%|█▉        | 106/532 [01:34<05:18,  1.34it/s]Loading train:  20%|██        | 107/532 [01:35<05:52,  1.20it/s]Loading train:  20%|██        | 108/532 [01:35<05:53,  1.20it/s]Loading train:  20%|██        | 109/532 [01:36<05:53,  1.20it/s]Loading train:  21%|██        | 110/532 [01:37<05:53,  1.19it/s]Loading train:  21%|██        | 111/532 [01:38<05:45,  1.22it/s]Loading train:  21%|██        | 112/532 [01:39<05:40,  1.23it/s]Loading train:  21%|██        | 113/532 [01:40<05:52,  1.19it/s]Loading train:  21%|██▏       | 114/532 [01:41<05:54,  1.18it/s]Loading train:  22%|██▏       | 115/532 [01:41<05:57,  1.17it/s]Loading train:  22%|██▏       | 116/532 [01:42<06:05,  1.14it/s]Loading train:  22%|██▏       | 117/532 [01:43<06:11,  1.12it/s]Loading train:  22%|██▏       | 118/532 [01:44<06:02,  1.14it/s]Loading train:  22%|██▏       | 119/532 [01:45<06:00,  1.14it/s]Loading train:  23%|██▎       | 120/532 [01:46<06:05,  1.13it/s]Loading train:  23%|██▎       | 121/532 [01:47<05:53,  1.16it/s]Loading train:  23%|██▎       | 122/532 [01:47<05:45,  1.19it/s]Loading train:  23%|██▎       | 123/532 [01:48<05:43,  1.19it/s]Loading train:  23%|██▎       | 124/532 [01:49<05:41,  1.19it/s]Loading train:  23%|██▎       | 125/532 [01:50<05:54,  1.15it/s]Loading train:  24%|██▎       | 126/532 [01:51<06:03,  1.12it/s]Loading train:  24%|██▍       | 127/532 [01:52<06:11,  1.09it/s]Loading train:  24%|██▍       | 128/532 [01:53<06:18,  1.07it/s]Loading train:  24%|██▍       | 129/532 [01:54<06:12,  1.08it/s]Loading train:  24%|██▍       | 130/532 [01:55<06:17,  1.06it/s]Loading train:  25%|██▍       | 131/532 [01:56<06:52,  1.03s/it]Loading train:  25%|██▍       | 132/532 [01:57<07:10,  1.08s/it]Loading train:  25%|██▌       | 133/532 [01:58<07:11,  1.08s/it]Loading train:  25%|██▌       | 134/532 [02:00<07:22,  1.11s/it]Loading train:  25%|██▌       | 135/532 [02:01<07:31,  1.14s/it]Loading train:  26%|██▌       | 136/532 [02:02<07:29,  1.14s/it]Loading train:  26%|██▌       | 137/532 [02:03<07:27,  1.13s/it]Loading train:  26%|██▌       | 138/532 [02:04<07:21,  1.12s/it]Loading train:  26%|██▌       | 139/532 [02:05<07:23,  1.13s/it]Loading train:  26%|██▋       | 140/532 [02:07<07:44,  1.18s/it]Loading train:  27%|██▋       | 141/532 [02:08<07:35,  1.16s/it]Loading train:  27%|██▋       | 142/532 [02:09<07:36,  1.17s/it]Loading train:  27%|██▋       | 143/532 [02:10<07:00,  1.08s/it]Loading train:  27%|██▋       | 144/532 [02:11<06:25,  1.01it/s]Loading train:  27%|██▋       | 145/532 [02:11<05:54,  1.09it/s]Loading train:  27%|██▋       | 146/532 [02:12<05:45,  1.12it/s]Loading train:  28%|██▊       | 147/532 [02:13<05:43,  1.12it/s]Loading train:  28%|██▊       | 148/532 [02:14<05:35,  1.14it/s]Loading train:  28%|██▊       | 149/532 [02:15<05:32,  1.15it/s]Loading train:  28%|██▊       | 150/532 [02:15<05:21,  1.19it/s]Loading train:  28%|██▊       | 151/532 [02:16<05:20,  1.19it/s]Loading train:  29%|██▊       | 152/532 [02:17<05:15,  1.21it/s]Loading train:  29%|██▉       | 153/532 [02:18<05:13,  1.21it/s]Loading train:  29%|██▉       | 154/532 [02:19<05:11,  1.21it/s]Loading train:  29%|██▉       | 155/532 [02:20<05:46,  1.09it/s]Loading train:  29%|██▉       | 156/532 [02:21<06:11,  1.01it/s]Loading train:  30%|██▉       | 157/532 [02:22<06:32,  1.05s/it]Loading train:  30%|██▉       | 158/532 [02:23<06:49,  1.09s/it]Loading train:  30%|██▉       | 159/532 [02:25<07:03,  1.13s/it]Loading train:  30%|███       | 160/532 [02:26<07:16,  1.17s/it]Loading train:  30%|███       | 161/532 [02:27<06:38,  1.07s/it]Loading train:  30%|███       | 162/532 [02:27<06:00,  1.03it/s]Loading train:  31%|███       | 163/532 [02:28<05:40,  1.08it/s]Loading train:  31%|███       | 164/532 [02:29<05:28,  1.12it/s]Loading train:  31%|███       | 165/532 [02:30<05:10,  1.18it/s]Loading train:  31%|███       | 166/532 [02:31<05:00,  1.22it/s]Loading train:  31%|███▏      | 167/532 [02:32<05:20,  1.14it/s]Loading train:  32%|███▏      | 168/532 [02:33<05:19,  1.14it/s]Loading train:  32%|███▏      | 169/532 [02:33<05:19,  1.14it/s]Loading train:  32%|███▏      | 170/532 [02:34<05:19,  1.13it/s]Loading train:  32%|███▏      | 171/532 [02:35<05:15,  1.14it/s]Loading train:  32%|███▏      | 172/532 [02:36<05:11,  1.16it/s]Loading train:  33%|███▎      | 173/532 [02:37<04:57,  1.21it/s]Loading train:  33%|███▎      | 174/532 [02:38<04:51,  1.23it/s]Loading train:  33%|███▎      | 175/532 [02:38<04:45,  1.25it/s]Loading train:  33%|███▎      | 176/532 [02:39<04:41,  1.27it/s]Loading train:  33%|███▎      | 177/532 [02:40<04:37,  1.28it/s]Loading train:  33%|███▎      | 178/532 [02:41<04:32,  1.30it/s]Loading train:  34%|███▎      | 179/532 [02:41<04:40,  1.26it/s]Loading train:  34%|███▍      | 180/532 [02:42<04:43,  1.24it/s]Loading train:  34%|███▍      | 181/532 [02:43<04:54,  1.19it/s]Loading train:  34%|███▍      | 182/532 [02:44<05:04,  1.15it/s]Loading train:  34%|███▍      | 183/532 [02:45<04:59,  1.17it/s]Loading train:  35%|███▍      | 184/532 [02:46<04:54,  1.18it/s]Loading train:  35%|███▍      | 185/532 [02:46<04:44,  1.22it/s]Loading train:  35%|███▍      | 186/532 [02:47<04:34,  1.26it/s]Loading train:  35%|███▌      | 187/532 [02:48<04:30,  1.28it/s]Loading train:  35%|███▌      | 188/532 [02:49<04:24,  1.30it/s]Loading train:  36%|███▌      | 189/532 [02:49<04:17,  1.33it/s]Loading train:  36%|███▌      | 190/532 [02:50<04:16,  1.33it/s]Loading train:  36%|███▌      | 191/532 [02:51<05:08,  1.11it/s]Loading train:  36%|███▌      | 192/532 [02:53<05:46,  1.02s/it]Loading train:  36%|███▋      | 193/532 [02:54<05:57,  1.05s/it]Loading train:  36%|███▋      | 194/532 [02:55<06:02,  1.07s/it]Loading train:  37%|███▋      | 195/532 [02:56<06:10,  1.10s/it]Loading train:  37%|███▋      | 196/532 [02:57<06:15,  1.12s/it]Loading train:  37%|███▋      | 197/532 [02:58<05:58,  1.07s/it]Loading train:  37%|███▋      | 198/532 [02:59<05:41,  1.02s/it]Loading train:  37%|███▋      | 199/532 [03:00<05:30,  1.01it/s]Loading train:  38%|███▊      | 200/532 [03:01<05:21,  1.03it/s]Loading train:  38%|███▊      | 201/532 [03:02<05:19,  1.04it/s]Loading train:  38%|███▊      | 202/532 [03:03<05:11,  1.06it/s]Loading train:  38%|███▊      | 203/532 [03:04<04:50,  1.13it/s]Loading train:  38%|███▊      | 204/532 [03:04<04:33,  1.20it/s]Loading train:  39%|███▊      | 205/532 [03:05<04:19,  1.26it/s]Loading train:  39%|███▊      | 206/532 [03:06<04:12,  1.29it/s]Loading train:  39%|███▉      | 207/532 [03:06<04:08,  1.31it/s]Loading train:  39%|███▉      | 208/532 [03:07<04:05,  1.32it/s]Loading train:  39%|███▉      | 209/532 [03:08<04:01,  1.34it/s]Loading train:  39%|███▉      | 210/532 [03:09<03:52,  1.38it/s]Loading train:  40%|███▉      | 211/532 [03:09<03:46,  1.41it/s]Loading train:  40%|███▉      | 212/532 [03:10<03:43,  1.43it/s]Loading train:  40%|████      | 213/532 [03:11<03:41,  1.44it/s]Loading train:  40%|████      | 214/532 [03:11<03:38,  1.46it/s]Loading train:  40%|████      | 215/532 [03:12<04:11,  1.26it/s]Loading train:  41%|████      | 216/532 [03:13<04:35,  1.15it/s]Loading train:  41%|████      | 217/532 [03:14<04:48,  1.09it/s]Loading train:  41%|████      | 218/532 [03:16<05:01,  1.04it/s]Loading train:  41%|████      | 219/532 [03:17<05:10,  1.01it/s]Loading train:  41%|████▏     | 220/532 [03:18<05:13,  1.00s/it]Loading train:  42%|████▏     | 221/532 [03:18<04:38,  1.11it/s]Loading train:  42%|████▏     | 222/532 [03:19<04:16,  1.21it/s]Loading train:  42%|████▏     | 223/532 [03:20<04:02,  1.27it/s]Loading train:  42%|████▏     | 224/532 [03:20<03:52,  1.32it/s]Loading train:  42%|████▏     | 225/532 [03:21<03:43,  1.37it/s]Loading train:  42%|████▏     | 226/532 [03:22<03:35,  1.42it/s]Loading train:  43%|████▎     | 227/532 [03:22<03:31,  1.44it/s]Loading train:  43%|████▎     | 228/532 [03:23<03:30,  1.45it/s]Loading train:  43%|████▎     | 229/532 [03:24<03:28,  1.45it/s]Loading train:  43%|████▎     | 230/532 [03:24<03:22,  1.49it/s]Loading train:  43%|████▎     | 231/532 [03:25<03:18,  1.52it/s]Loading train:  44%|████▎     | 232/532 [03:26<03:20,  1.50it/s]Loading train:  44%|████▍     | 233/532 [03:26<03:33,  1.40it/s]Loading train:  44%|████▍     | 234/532 [03:27<03:33,  1.40it/s]Loading train:  44%|████▍     | 235/532 [03:28<03:34,  1.39it/s]Loading train:  44%|████▍     | 236/532 [03:29<03:34,  1.38it/s]Loading train:  45%|████▍     | 237/532 [03:29<03:36,  1.36it/s]Loading train:  45%|████▍     | 238/532 [03:30<03:38,  1.35it/s]Loading train:  45%|████▍     | 239/532 [03:31<03:45,  1.30it/s]Loading train:  45%|████▌     | 240/532 [03:32<03:43,  1.31it/s]Loading train:  45%|████▌     | 241/532 [03:32<03:45,  1.29it/s]Loading train:  45%|████▌     | 242/532 [03:33<03:48,  1.27it/s]Loading train:  46%|████▌     | 243/532 [03:34<03:56,  1.22it/s]Loading train:  46%|████▌     | 244/532 [03:35<03:59,  1.20it/s]Loading train:  46%|████▌     | 245/532 [03:36<03:50,  1.25it/s]Loading train:  46%|████▌     | 246/532 [03:37<03:40,  1.30it/s]Loading train:  46%|████▋     | 247/532 [03:37<03:36,  1.32it/s]Loading train:  47%|████▋     | 248/532 [03:38<03:29,  1.36it/s]Loading train:  47%|████▋     | 249/532 [03:39<03:30,  1.34it/s]Loading train:  47%|████▋     | 250/532 [03:39<03:27,  1.36it/s]Loading train:  47%|████▋     | 251/532 [03:40<03:34,  1.31it/s]Loading train:  47%|████▋     | 252/532 [03:41<03:39,  1.28it/s]Loading train:  48%|████▊     | 253/532 [03:42<03:33,  1.31it/s]Loading train:  48%|████▊     | 254/532 [03:43<03:29,  1.33it/s]Loading train:  48%|████▊     | 255/532 [03:43<03:27,  1.33it/s]Loading train:  48%|████▊     | 256/532 [03:44<03:27,  1.33it/s]Loading train:  48%|████▊     | 257/532 [03:45<03:47,  1.21it/s]Loading train:  48%|████▊     | 258/532 [03:46<03:56,  1.16it/s]Loading train:  49%|████▊     | 259/532 [03:47<04:06,  1.11it/s]Loading train:  49%|████▉     | 260/532 [03:48<04:07,  1.10it/s]Loading train:  49%|████▉     | 261/532 [03:49<04:10,  1.08it/s]Loading train:  49%|████▉     | 262/532 [03:50<04:13,  1.07it/s]Loading train:  49%|████▉     | 263/532 [03:51<04:03,  1.11it/s]Loading train:  50%|████▉     | 264/532 [03:51<03:45,  1.19it/s]Loading train:  50%|████▉     | 265/532 [03:52<03:34,  1.25it/s]Loading train:  50%|█████     | 266/532 [03:53<03:24,  1.30it/s]Loading train:  50%|█████     | 267/532 [03:53<03:17,  1.34it/s]Loading train:  50%|█████     | 268/532 [03:54<03:13,  1.36it/s]Loading train:  51%|█████     | 269/532 [03:55<03:24,  1.28it/s]Loading train:  51%|█████     | 270/532 [03:56<03:35,  1.21it/s]Loading train:  51%|█████     | 271/532 [03:57<03:41,  1.18it/s]Loading train:  51%|█████     | 272/532 [03:58<03:53,  1.11it/s]Loading train:  51%|█████▏    | 273/532 [03:59<03:50,  1.12it/s]Loading train:  52%|█████▏    | 274/532 [04:00<03:47,  1.13it/s]Loading train:  52%|█████▏    | 275/532 [04:01<04:05,  1.05it/s]Loading train:  52%|█████▏    | 276/532 [04:02<04:16,  1.00s/it]Loading train:  52%|█████▏    | 277/532 [04:03<04:28,  1.05s/it]Loading train:  52%|█████▏    | 278/532 [04:04<04:30,  1.06s/it]Loading train:  52%|█████▏    | 279/532 [04:05<04:29,  1.07s/it]Loading train:  53%|█████▎    | 280/532 [04:06<04:31,  1.08s/it]Loading train:  53%|█████▎    | 281/532 [04:07<04:24,  1.05s/it]Loading train:  53%|█████▎    | 282/532 [04:08<04:22,  1.05s/it]Loading train:  53%|█████▎    | 283/532 [04:09<04:18,  1.04s/it]Loading train:  53%|█████▎    | 284/532 [04:10<04:18,  1.04s/it]Loading train:  54%|█████▎    | 285/532 [04:11<04:13,  1.02s/it]Loading train:  54%|█████▍    | 286/532 [04:12<04:07,  1.01s/it]Loading train:  54%|█████▍    | 287/532 [04:13<03:52,  1.05it/s]Loading train:  54%|█████▍    | 288/532 [04:14<03:40,  1.11it/s]Loading train:  54%|█████▍    | 289/532 [04:15<03:29,  1.16it/s]Loading train:  55%|█████▍    | 290/532 [04:15<03:22,  1.19it/s]Loading train:  55%|█████▍    | 291/532 [04:16<03:21,  1.20it/s]Loading train:  55%|█████▍    | 292/532 [04:17<03:19,  1.20it/s]Loading train:  55%|█████▌    | 293/532 [04:18<03:25,  1.16it/s]Loading train:  55%|█████▌    | 294/532 [04:19<03:24,  1.16it/s]Loading train:  55%|█████▌    | 295/532 [04:20<03:30,  1.13it/s]Loading train:  56%|█████▌    | 296/532 [04:21<03:34,  1.10it/s]Loading train:  56%|█████▌    | 297/532 [04:22<03:34,  1.09it/s]Loading train:  56%|█████▌    | 298/532 [04:23<03:32,  1.10it/s]Loading train:  56%|█████▌    | 299/532 [04:23<03:19,  1.17it/s]Loading train:  56%|█████▋    | 300/532 [04:24<03:11,  1.21it/s]Loading train:  57%|█████▋    | 301/532 [04:25<03:05,  1.25it/s]Loading train:  57%|█████▋    | 302/532 [04:26<03:00,  1.28it/s]Loading train:  57%|█████▋    | 303/532 [04:26<02:58,  1.29it/s]Loading train:  57%|█████▋    | 304/532 [04:27<02:50,  1.34it/s]Loading train:  57%|█████▋    | 305/532 [04:28<03:18,  1.15it/s]Loading train:  58%|█████▊    | 306/532 [04:29<03:37,  1.04it/s]Loading train:  58%|█████▊    | 307/532 [04:30<03:41,  1.01it/s]Loading train:  58%|█████▊    | 308/532 [04:31<03:43,  1.00it/s]Loading train:  58%|█████▊    | 309/532 [04:33<03:46,  1.01s/it]Loading train:  58%|█████▊    | 310/532 [04:34<03:46,  1.02s/it]Loading train:  58%|█████▊    | 311/532 [04:35<04:19,  1.17s/it]Loading train:  59%|█████▊    | 312/532 [04:37<04:38,  1.27s/it]Loading train:  59%|█████▉    | 313/532 [04:38<04:53,  1.34s/it]Loading train:  59%|█████▉    | 314/532 [04:40<05:02,  1.39s/it]Loading train:  59%|█████▉    | 315/532 [04:41<05:09,  1.43s/it]Loading train:  59%|█████▉    | 316/532 [04:43<05:11,  1.44s/it]Loading train:  60%|█████▉    | 317/532 [04:43<04:28,  1.25s/it]Loading train:  60%|█████▉    | 318/532 [04:44<03:57,  1.11s/it]Loading train:  60%|█████▉    | 319/532 [04:45<03:31,  1.01it/s]Loading train:  60%|██████    | 320/532 [04:46<03:15,  1.09it/s]Loading train:  60%|██████    | 321/532 [04:47<03:19,  1.06it/s]Loading train:  61%|██████    | 322/532 [04:47<03:11,  1.10it/s]Loading train:  61%|██████    | 323/532 [04:49<03:34,  1.03s/it]Loading train:  61%|██████    | 324/532 [04:50<03:44,  1.08s/it]Loading train:  61%|██████    | 325/532 [04:51<03:52,  1.12s/it]Loading train:  61%|██████▏   | 326/532 [04:52<03:52,  1.13s/it]Loading train:  61%|██████▏   | 327/532 [04:54<03:56,  1.16s/it]Loading train:  62%|██████▏   | 328/532 [04:55<03:55,  1.15s/it]Loading train:  62%|██████▏   | 329/532 [04:56<03:35,  1.06s/it]Loading train:  62%|██████▏   | 330/532 [04:56<03:18,  1.02it/s]Loading train:  62%|██████▏   | 331/532 [04:57<03:09,  1.06it/s]Loading train:  62%|██████▏   | 332/532 [04:58<02:56,  1.13it/s]Loading train:  63%|██████▎   | 333/532 [04:59<02:47,  1.19it/s]Loading train:  63%|██████▎   | 334/532 [04:59<02:46,  1.19it/s]Loading train:  63%|██████▎   | 335/532 [05:00<02:54,  1.13it/s]Loading train:  63%|██████▎   | 336/532 [05:01<02:57,  1.10it/s]Loading train:  63%|██████▎   | 337/532 [05:02<03:03,  1.06it/s]Loading train:  64%|██████▎   | 338/532 [05:03<03:05,  1.05it/s]Loading train:  64%|██████▎   | 339/532 [05:04<03:05,  1.04it/s]Loading train:  64%|██████▍   | 340/532 [05:05<03:06,  1.03it/s]Loading train:  64%|██████▍   | 341/532 [05:06<02:52,  1.11it/s]Loading train:  64%|██████▍   | 342/532 [05:07<02:42,  1.17it/s]Loading train:  64%|██████▍   | 343/532 [05:08<02:34,  1.22it/s]Loading train:  65%|██████▍   | 344/532 [05:08<02:30,  1.25it/s]Loading train:  65%|██████▍   | 345/532 [05:09<02:27,  1.27it/s]Loading train:  65%|██████▌   | 346/532 [05:10<02:30,  1.24it/s]Loading train:  65%|██████▌   | 347/532 [05:11<02:29,  1.24it/s]Loading train:  65%|██████▌   | 348/532 [05:12<02:30,  1.22it/s]Loading train:  66%|██████▌   | 349/532 [05:12<02:28,  1.23it/s]Loading train:  66%|██████▌   | 350/532 [05:13<02:27,  1.23it/s]Loading train:  66%|██████▌   | 351/532 [05:14<02:25,  1.24it/s]Loading train:  66%|██████▌   | 352/532 [05:15<02:28,  1.21it/s]Loading train:  66%|██████▋   | 353/532 [05:16<02:26,  1.22it/s]Loading train:  67%|██████▋   | 354/532 [05:17<02:27,  1.21it/s]Loading train:  67%|██████▋   | 355/532 [05:17<02:25,  1.22it/s]Loading train:  67%|██████▋   | 356/532 [05:18<02:28,  1.19it/s]Loading train:  67%|██████▋   | 357/532 [05:19<02:29,  1.17it/s]Loading train:  67%|██████▋   | 358/532 [05:20<02:23,  1.21it/s]Loading train:  67%|██████▋   | 359/532 [05:21<02:20,  1.23it/s]Loading train:  68%|██████▊   | 360/532 [05:22<02:18,  1.24it/s]Loading train:  68%|██████▊   | 361/532 [05:22<02:10,  1.31it/s]Loading train:  68%|██████▊   | 362/532 [05:23<02:11,  1.29it/s]Loading train:  68%|██████▊   | 363/532 [05:24<02:09,  1.31it/s]Loading train:  68%|██████▊   | 364/532 [05:24<02:03,  1.36it/s]Loading train:  69%|██████▊   | 365/532 [05:25<02:00,  1.39it/s]Loading train:  69%|██████▉   | 366/532 [05:26<01:59,  1.39it/s]Loading train:  69%|██████▉   | 367/532 [05:27<01:59,  1.38it/s]Loading train:  69%|██████▉   | 368/532 [05:27<01:58,  1.38it/s]Loading train:  69%|██████▉   | 369/532 [05:28<01:58,  1.38it/s]Loading train:  70%|██████▉   | 370/532 [05:29<02:00,  1.35it/s]Loading train:  70%|██████▉   | 371/532 [05:30<02:16,  1.18it/s]Loading train:  70%|██████▉   | 372/532 [05:31<02:24,  1.11it/s]Loading train:  70%|███████   | 373/532 [05:32<02:32,  1.04it/s]Loading train:  70%|███████   | 374/532 [05:33<02:38,  1.01s/it]Loading train:  70%|███████   | 375/532 [05:34<02:42,  1.03s/it]Loading train:  71%|███████   | 376/532 [05:35<02:44,  1.06s/it]Loading train:  71%|███████   | 377/532 [05:36<02:34,  1.00it/s]Loading train:  71%|███████   | 378/532 [05:37<02:23,  1.08it/s]Loading train:  71%|███████   | 379/532 [05:38<02:13,  1.14it/s]Loading train:  71%|███████▏  | 380/532 [05:38<02:08,  1.19it/s]Loading train:  72%|███████▏  | 381/532 [05:39<02:04,  1.21it/s]Loading train:  72%|███████▏  | 382/532 [05:40<02:01,  1.23it/s]Loading train:  72%|███████▏  | 383/532 [05:41<02:07,  1.17it/s]Loading train:  72%|███████▏  | 384/532 [05:42<02:08,  1.15it/s]Loading train:  72%|███████▏  | 385/532 [05:43<02:09,  1.14it/s]Loading train:  73%|███████▎  | 386/532 [05:44<02:09,  1.13it/s]Loading train:  73%|███████▎  | 387/532 [05:45<02:06,  1.14it/s]Loading train:  73%|███████▎  | 388/532 [05:45<02:05,  1.15it/s]Loading train:  73%|███████▎  | 389/532 [05:46<02:04,  1.15it/s]Loading train:  73%|███████▎  | 390/532 [05:47<02:04,  1.14it/s]Loading train:  73%|███████▎  | 391/532 [05:48<02:07,  1.10it/s]Loading train:  74%|███████▎  | 392/532 [05:49<02:07,  1.10it/s]Loading train:  74%|███████▍  | 393/532 [05:50<02:05,  1.10it/s]Loading train:  74%|███████▍  | 394/532 [05:51<02:04,  1.11it/s]Loading train:  74%|███████▍  | 395/532 [05:52<02:04,  1.10it/s]Loading train:  74%|███████▍  | 396/532 [05:53<02:02,  1.11it/s]Loading train:  75%|███████▍  | 397/532 [05:54<02:03,  1.09it/s]Loading train:  75%|███████▍  | 398/532 [05:54<02:00,  1.12it/s]Loading train:  75%|███████▌  | 399/532 [05:55<01:59,  1.11it/s]Loading train:  75%|███████▌  | 400/532 [05:56<02:00,  1.10it/s]Loading train:  75%|███████▌  | 401/532 [05:57<02:04,  1.05it/s]Loading train:  76%|███████▌  | 402/532 [05:58<02:04,  1.05it/s]Loading train:  76%|███████▌  | 403/532 [05:59<02:03,  1.04it/s]Loading train:  76%|███████▌  | 404/532 [06:00<02:05,  1.02it/s]Loading train:  76%|███████▌  | 405/532 [06:01<02:05,  1.01it/s]Loading train:  76%|███████▋  | 406/532 [06:02<02:03,  1.02it/s]Loading train:  77%|███████▋  | 407/532 [06:03<01:59,  1.05it/s]Loading train:  77%|███████▋  | 408/532 [06:04<01:56,  1.07it/s]Loading train:  77%|███████▋  | 409/532 [06:05<01:49,  1.13it/s]Loading train:  77%|███████▋  | 410/532 [06:06<01:46,  1.15it/s]Loading train:  77%|███████▋  | 411/532 [06:06<01:41,  1.20it/s]Loading train:  77%|███████▋  | 412/532 [06:07<01:39,  1.20it/s]Loading train:  78%|███████▊  | 413/532 [06:08<01:35,  1.25it/s]Loading train:  78%|███████▊  | 414/532 [06:09<01:32,  1.27it/s]Loading train:  78%|███████▊  | 415/532 [06:09<01:29,  1.30it/s]Loading train:  78%|███████▊  | 416/532 [06:10<01:29,  1.29it/s]Loading train:  78%|███████▊  | 417/532 [06:11<01:29,  1.28it/s]Loading train:  79%|███████▊  | 418/532 [06:12<01:28,  1.29it/s]Loading train:  79%|███████▉  | 419/532 [06:13<01:32,  1.22it/s]Loading train:  79%|███████▉  | 420/532 [06:14<01:33,  1.20it/s]Loading train:  79%|███████▉  | 421/532 [06:14<01:35,  1.16it/s]Loading train:  79%|███████▉  | 422/532 [06:15<01:37,  1.13it/s]Loading train:  80%|███████▉  | 423/532 [06:16<01:41,  1.07it/s]Loading train:  80%|███████▉  | 424/532 [06:17<01:42,  1.05it/s]Loading train:  80%|███████▉  | 425/532 [06:18<01:42,  1.05it/s]Loading train:  80%|████████  | 426/532 [06:19<01:38,  1.07it/s]Loading train:  80%|████████  | 427/532 [06:20<01:35,  1.10it/s]Loading train:  80%|████████  | 428/532 [06:21<01:33,  1.11it/s]Loading train:  81%|████████  | 429/532 [06:22<01:29,  1.15it/s]Loading train:  81%|████████  | 430/532 [06:23<01:28,  1.15it/s]Loading train:  81%|████████  | 431/532 [06:24<01:34,  1.07it/s]Loading train:  81%|████████  | 432/532 [06:25<01:35,  1.04it/s]Loading train:  81%|████████▏ | 433/532 [06:26<01:39,  1.00s/it]Loading train:  82%|████████▏ | 434/532 [06:27<01:39,  1.02s/it]Loading train:  82%|████████▏ | 435/532 [06:28<01:39,  1.02s/it]Loading train:  82%|████████▏ | 436/532 [06:29<01:39,  1.04s/it]Loading train:  82%|████████▏ | 437/532 [06:30<01:32,  1.02it/s]Loading train:  82%|████████▏ | 438/532 [06:31<01:26,  1.09it/s]Loading train:  83%|████████▎ | 439/532 [06:31<01:20,  1.15it/s]Loading train:  83%|████████▎ | 440/532 [06:32<01:17,  1.19it/s]Loading train:  83%|████████▎ | 441/532 [06:33<01:17,  1.18it/s]Loading train:  83%|████████▎ | 442/532 [06:34<01:15,  1.19it/s]Loading train:  83%|████████▎ | 443/532 [06:35<01:11,  1.24it/s]Loading train:  83%|████████▎ | 444/532 [06:35<01:09,  1.26it/s]Loading train:  84%|████████▎ | 445/532 [06:36<01:08,  1.28it/s]Loading train:  84%|████████▍ | 446/532 [06:37<01:06,  1.30it/s]Loading train:  84%|████████▍ | 447/532 [06:38<01:03,  1.33it/s]Loading train:  84%|████████▍ | 448/532 [06:38<01:02,  1.35it/s]Loading train:  84%|████████▍ | 449/532 [06:39<01:03,  1.30it/s]Loading train:  85%|████████▍ | 450/532 [06:40<01:03,  1.30it/s]Loading train:  85%|████████▍ | 451/532 [06:41<01:00,  1.33it/s]Loading train:  85%|████████▍ | 452/532 [06:41<01:00,  1.31it/s]Loading train:  85%|████████▌ | 453/532 [06:42<01:00,  1.31it/s]Loading train:  85%|████████▌ | 454/532 [06:43<01:01,  1.28it/s]Loading train:  86%|████████▌ | 455/532 [06:44<01:03,  1.20it/s]Loading train:  86%|████████▌ | 456/532 [06:45<01:10,  1.09it/s]Loading train:  86%|████████▌ | 457/532 [06:46<01:08,  1.10it/s]Loading train:  86%|████████▌ | 458/532 [06:47<01:08,  1.08it/s]Loading train:  86%|████████▋ | 459/532 [06:48<01:06,  1.10it/s]Loading train:  86%|████████▋ | 460/532 [06:49<01:07,  1.07it/s]Loading train:  87%|████████▋ | 461/532 [06:50<01:09,  1.02it/s]Loading train:  87%|████████▋ | 462/532 [06:51<01:08,  1.02it/s]Loading train:  87%|████████▋ | 463/532 [06:52<01:07,  1.02it/s]Loading train:  87%|████████▋ | 464/532 [06:53<01:07,  1.01it/s]Loading train:  87%|████████▋ | 465/532 [06:54<01:06,  1.01it/s]Loading train:  88%|████████▊ | 466/532 [06:55<01:05,  1.01it/s]Loading train:  88%|████████▊ | 467/532 [06:56<01:01,  1.07it/s]Loading train:  88%|████████▊ | 468/532 [06:57<00:58,  1.10it/s]Loading train:  88%|████████▊ | 469/532 [06:57<00:54,  1.16it/s]Loading train:  88%|████████▊ | 470/532 [06:58<00:52,  1.18it/s]Loading train:  89%|████████▊ | 471/532 [06:59<00:50,  1.22it/s]Loading train:  89%|████████▊ | 472/532 [07:00<00:47,  1.26it/s]Loading train:  89%|████████▉ | 473/532 [07:00<00:49,  1.20it/s]Loading train:  89%|████████▉ | 474/532 [07:01<00:49,  1.18it/s]Loading train:  89%|████████▉ | 475/532 [07:02<00:47,  1.19it/s]Loading train:  89%|████████▉ | 476/532 [07:03<00:47,  1.19it/s]Loading train:  90%|████████▉ | 477/532 [07:04<00:46,  1.19it/s]Loading train:  90%|████████▉ | 478/532 [07:05<00:45,  1.19it/s]Loading train:  90%|█████████ | 479/532 [07:05<00:42,  1.24it/s]Loading train:  90%|█████████ | 480/532 [07:06<00:40,  1.29it/s]Loading train:  90%|█████████ | 481/532 [07:07<00:38,  1.33it/s]Loading train:  91%|█████████ | 482/532 [07:08<00:36,  1.35it/s]Loading train:  91%|█████████ | 483/532 [07:08<00:35,  1.38it/s]Loading train:  91%|█████████ | 484/532 [07:09<00:34,  1.40it/s]Loading train:  91%|█████████ | 485/532 [07:10<00:36,  1.27it/s]Loading train:  91%|█████████▏| 486/532 [07:11<00:38,  1.18it/s]Loading train:  92%|█████████▏| 487/532 [07:12<00:41,  1.09it/s]Loading train:  92%|█████████▏| 488/532 [07:13<00:44,  1.01s/it]Loading train:  92%|█████████▏| 489/532 [07:14<00:43,  1.01s/it]Loading train:  92%|█████████▏| 490/532 [07:15<00:42,  1.02s/it]Loading train:  92%|█████████▏| 491/532 [07:16<00:39,  1.04it/s]Loading train:  92%|█████████▏| 492/532 [07:17<00:35,  1.11it/s]Loading train:  93%|█████████▎| 493/532 [07:18<00:33,  1.16it/s]Loading train:  93%|█████████▎| 494/532 [07:18<00:32,  1.18it/s]Loading train:  93%|█████████▎| 495/532 [07:19<00:31,  1.18it/s]Loading train:  93%|█████████▎| 496/532 [07:20<00:30,  1.19it/s]Loading train:  93%|█████████▎| 497/532 [07:21<00:29,  1.18it/s]Loading train:  94%|█████████▎| 498/532 [07:22<00:28,  1.19it/s]Loading train:  94%|█████████▍| 499/532 [07:23<00:27,  1.18it/s]Loading train:  94%|█████████▍| 500/532 [07:24<00:27,  1.16it/s]Loading train:  94%|█████████▍| 501/532 [07:24<00:26,  1.17it/s]Loading train:  94%|█████████▍| 502/532 [07:25<00:25,  1.19it/s]Loading train:  95%|█████████▍| 503/532 [07:26<00:24,  1.17it/s]Loading train:  95%|█████████▍| 504/532 [07:27<00:23,  1.19it/s]Loading train:  95%|█████████▍| 505/532 [07:28<00:22,  1.22it/s]Loading train:  95%|█████████▌| 506/532 [07:28<00:20,  1.25it/s]Loading train:  95%|█████████▌| 507/532 [07:29<00:20,  1.25it/s]Loading train:  95%|█████████▌| 508/532 [07:30<00:18,  1.27it/s]Loading train:  96%|█████████▌| 509/532 [07:31<00:19,  1.19it/s]Loading train:  96%|█████████▌| 510/532 [07:32<00:19,  1.14it/s]Loading train:  96%|█████████▌| 511/532 [07:33<00:18,  1.12it/s]Loading train:  96%|█████████▌| 512/532 [07:34<00:17,  1.11it/s]Loading train:  96%|█████████▋| 513/532 [07:35<00:17,  1.10it/s]Loading train:  97%|█████████▋| 514/532 [07:36<00:16,  1.09it/s]Loading train:  97%|█████████▋| 515/532 [07:36<00:15,  1.13it/s]Loading train:  97%|█████████▋| 516/532 [07:37<00:13,  1.15it/s]Loading train:  97%|█████████▋| 517/532 [07:38<00:12,  1.18it/s]Loading train:  97%|█████████▋| 518/532 [07:39<00:11,  1.19it/s]Loading train:  98%|█████████▊| 519/532 [07:40<00:10,  1.20it/s]Loading train:  98%|█████████▊| 520/532 [07:40<00:09,  1.24it/s]Loading train:  98%|█████████▊| 521/532 [07:41<00:09,  1.13it/s]Loading train:  98%|█████████▊| 522/532 [07:42<00:09,  1.11it/s]Loading train:  98%|█████████▊| 523/532 [07:43<00:08,  1.10it/s]Loading train:  98%|█████████▊| 524/532 [07:44<00:07,  1.11it/s]Loading train:  99%|█████████▊| 525/532 [07:45<00:06,  1.10it/s]Loading train:  99%|█████████▉| 526/532 [07:46<00:05,  1.10it/s]Loading train:  99%|█████████▉| 527/532 [07:47<00:04,  1.10it/s]Loading train:  99%|█████████▉| 528/532 [07:48<00:03,  1.11it/s]Loading train:  99%|█████████▉| 529/532 [07:49<00:02,  1.17it/s]Loading train: 100%|█████████▉| 530/532 [07:49<00:01,  1.21it/s]Loading train: 100%|█████████▉| 531/532 [07:50<00:00,  1.17it/s]Loading train: 100%|██████████| 532/532 [07:51<00:00,  1.17it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 13/532 [00:00<00:04, 127.43it/s]concatenating: train:   7%|▋         | 38/532 [00:00<00:03, 148.92it/s]concatenating: train:  13%|█▎        | 67/532 [00:00<00:02, 174.01it/s]concatenating: train:  17%|█▋        | 93/532 [00:00<00:02, 193.08it/s]concatenating: train:  23%|██▎       | 122/532 [00:00<00:01, 213.49it/s]concatenating: train:  29%|██▊       | 152/532 [00:00<00:01, 232.74it/s]concatenating: train:  34%|███▍      | 181/532 [00:00<00:01, 246.30it/s]concatenating: train:  39%|███▉      | 208/532 [00:00<00:01, 252.92it/s]concatenating: train:  45%|████▍     | 239/532 [00:00<00:01, 266.79it/s]concatenating: train:  51%|█████     | 270/532 [00:01<00:00, 277.82it/s]concatenating: train:  56%|█████▌    | 299/532 [00:01<00:00, 278.82it/s]concatenating: train:  62%|██████▏   | 328/532 [00:01<00:00, 281.42it/s]concatenating: train:  67%|██████▋   | 357/532 [00:01<00:00, 280.57it/s]concatenating: train:  73%|███████▎  | 386/532 [00:01<00:00, 275.22it/s]concatenating: train:  78%|███████▊  | 416/532 [00:01<00:00, 281.36it/s]concatenating: train:  84%|████████▎ | 445/532 [00:01<00:00, 272.55it/s]concatenating: train:  89%|████████▉ | 475/532 [00:01<00:00, 278.98it/s]concatenating: train:  95%|█████████▍| 504/532 [00:01<00:00, 273.46it/s]concatenating: train: 100%|██████████| 532/532 [00:01<00:00, 274.62it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:12,  1.15it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:11,  1.11it/s]Loading test:  20%|██        | 3/15 [00:02<00:10,  1.09it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:10,  1.09it/s]Loading test:  33%|███▎      | 5/15 [00:04<00:09,  1.01it/s]Loading test:  40%|████      | 6/15 [00:06<00:09,  1.05s/it]Loading test:  47%|████▋     | 7/15 [00:06<00:07,  1.03it/s]Loading test:  53%|█████▎    | 8/15 [00:08<00:07,  1.04s/it]Loading test:  60%|██████    | 9/15 [00:09<00:06,  1.04s/it]Loading test:  67%|██████▋   | 10/15 [00:09<00:04,  1.02it/s]Loading test:  73%|███████▎  | 11/15 [00:10<00:03,  1.05it/s]Loading test:  80%|████████  | 12/15 [00:11<00:02,  1.02it/s]Loading test:  87%|████████▋ | 13/15 [00:12<00:02,  1.01s/it]Loading test:  93%|█████████▎| 14/15 [00:13<00:00,  1.00it/s]Loading test: 100%|██████████| 15/15 [00:14<00:00,  1.01it/s]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 349.01it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 56, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 56, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 56, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 56, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 56, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 56, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 84, 56, 21)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 28, 21)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 28, 21)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 28, 40)   7600        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 28, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 28, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 28, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 28, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 28, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 42, 28, 61)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 14, 61)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 14, 61)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 14, 80)   44000       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 14, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 14, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 14, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 14, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 14, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 21, 14, 141)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 14, 141)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 28, 40)   22600       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 28, 101)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 28, 40)   36400       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 28, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 28, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 28, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 28, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 28, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 42, 28, 141)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-06 11:22:23.202729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 11:22:23.202833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 11:22:23.202848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 11:22:23.202857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 11:22:23.203625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 42, 28, 141)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 56, 20)   11300       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 56, 41)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 56, 20)   7400        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 56, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 56, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 56, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 56, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 56, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 84, 56, 61)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 56, 61)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 56, 13)   806         dropout_5[0][0]                  
==================================================================================================
Total params: 225,706
Trainable params: 224,906
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53807809e-02 2.88974534e-02 1.16728790e-01 1.00198377e-02
 3.03363016e-02 5.79915656e-03 6.86746312e-02 1.28228722e-01
 7.55625878e-02 1.22514673e-02 2.73642650e-01 1.84278063e-01
 1.99559502e-04]
Train on 19871 samples, validate on 569 samples
Epoch 1/300
 - 30s - loss: 48.3654 - acc: 0.8415 - mDice: 0.0291 - val_loss: 6.2075 - val_acc: 0.9111 - val_mDice: 0.0598

Epoch 00001: val_mDice improved from -inf to 0.05982, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 20s - loss: 7.0553 - acc: 0.8943 - mDice: 0.0764 - val_loss: 5.0899 - val_acc: 0.9100 - val_mDice: 0.0874

Epoch 00002: val_mDice improved from 0.05982 to 0.08737, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 21s - loss: 5.0299 - acc: 0.9059 - mDice: 0.1777 - val_loss: 3.2123 - val_acc: 0.9401 - val_mDice: 0.2806

Epoch 00003: val_mDice improved from 0.08737 to 0.28060, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 20s - loss: 3.9136 - acc: 0.9230 - mDice: 0.2977 - val_loss: 2.2363 - val_acc: 0.9569 - val_mDice: 0.4609

Epoch 00004: val_mDice improved from 0.28060 to 0.46086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 20s - loss: 3.2438 - acc: 0.9333 - mDice: 0.3922 - val_loss: 1.9782 - val_acc: 0.9611 - val_mDice: 0.5517

Epoch 00005: val_mDice improved from 0.46086 to 0.55166, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 20s - loss: 2.8448 - acc: 0.9399 - mDice: 0.4557 - val_loss: 1.6792 - val_acc: 0.9672 - val_mDice: 0.6128

Epoch 00006: val_mDice improved from 0.55166 to 0.61275, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 21s - loss: 2.5508 - acc: 0.9447 - mDice: 0.5032 - val_loss: 1.5620 - val_acc: 0.9683 - val_mDice: 0.6461

Epoch 00007: val_mDice improved from 0.61275 to 0.64606, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300
 - 20s - loss: 2.3588 - acc: 0.9476 - mDice: 0.5363 - val_loss: 1.4763 - val_acc: 0.9701 - val_mDice: 0.6668

Epoch 00008: val_mDice improved from 0.64606 to 0.66683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300
 - 20s - loss: 2.2184 - acc: 0.9501 - mDice: 0.5595 - val_loss: 1.3934 - val_acc: 0.9717 - val_mDice: 0.6826

Epoch 00009: val_mDice improved from 0.66683 to 0.68265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300
 - 20s - loss: 2.0976 - acc: 0.9521 - mDice: 0.5795 - val_loss: 1.3586 - val_acc: 0.9718 - val_mDice: 0.6935

Epoch 00010: val_mDice improved from 0.68265 to 0.69355, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300
 - 20s - loss: 1.9976 - acc: 0.9537 - mDice: 0.5954 - val_loss: 1.3883 - val_acc: 0.9720 - val_mDice: 0.6957

Epoch 00011: val_mDice improved from 0.69355 to 0.69566, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300
 - 21s - loss: 1.9146 - acc: 0.9551 - mDice: 0.6105 - val_loss: 1.3339 - val_acc: 0.9727 - val_mDice: 0.7030

Epoch 00012: val_mDice improved from 0.69566 to 0.70301, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 13/300
 - 20s - loss: 1.8515 - acc: 0.9559 - mDice: 0.6216 - val_loss: 1.2804 - val_acc: 0.9731 - val_mDice: 0.7171

Epoch 00013: val_mDice improved from 0.70301 to 0.71710, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300
 - 20s - loss: 1.7947 - acc: 0.9567 - mDice: 0.6315 - val_loss: 1.2818 - val_acc: 0.9732 - val_mDice: 0.7165

Epoch 00014: val_mDice did not improve from 0.71710
Epoch 15/300
 - 20s - loss: 1.7331 - acc: 0.9576 - mDice: 0.6426 - val_loss: 1.2766 - val_acc: 0.9745 - val_mDice: 0.7190

Epoch 00015: val_mDice improved from 0.71710 to 0.71899, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 16/300
 - 21s - loss: 1.6976 - acc: 0.9581 - mDice: 0.6491 - val_loss: 1.2416 - val_acc: 0.9745 - val_mDice: 0.7272

Epoch 00016: val_mDice improved from 0.71899 to 0.72722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 17/300
 - 21s - loss: 1.6438 - acc: 0.9589 - mDice: 0.6588 - val_loss: 1.2541 - val_acc: 0.9742 - val_mDice: 0.7268

Epoch 00017: val_mDice did not improve from 0.72722
Epoch 18/300
 - 20s - loss: 1.6144 - acc: 0.9594 - mDice: 0.6639 - val_loss: 1.2215 - val_acc: 0.9743 - val_mDice: 0.7325

Epoch 00018: val_mDice improved from 0.72722 to 0.73253, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300
 - 20s - loss: 1.5895 - acc: 0.9598 - mDice: 0.6678 - val_loss: 1.1998 - val_acc: 0.9748 - val_mDice: 0.7359

Epoch 00019: val_mDice improved from 0.73253 to 0.73587, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 20/300
 - 21s - loss: 1.5645 - acc: 0.9602 - mDice: 0.6726 - val_loss: 1.2043 - val_acc: 0.9752 - val_mDice: 0.7369

Epoch 00020: val_mDice improved from 0.73587 to 0.73695, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 21/300
 - 20s - loss: 1.5246 - acc: 0.9610 - mDice: 0.6794 - val_loss: 1.1804 - val_acc: 0.9745 - val_mDice: 0.7422

Epoch 00021: val_mDice improved from 0.73695 to 0.74215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300
 - 20s - loss: 1.5021 - acc: 0.9613 - mDice: 0.6842 - val_loss: 1.2112 - val_acc: 0.9736 - val_mDice: 0.7397

Epoch 00022: val_mDice did not improve from 0.74215
Epoch 23/300
 - 20s - loss: 1.4777 - acc: 0.9617 - mDice: 0.6888 - val_loss: 1.1553 - val_acc: 0.9762 - val_mDice: 0.7412

Epoch 00023: val_mDice did not improve from 0.74215
Epoch 24/300
 - 21s - loss: 1.4478 - acc: 0.9621 - mDice: 0.6942 - val_loss: 1.1727 - val_acc: 0.9750 - val_mDice: 0.7439

Epoch 00024: val_mDice improved from 0.74215 to 0.74390, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 25/300
 - 20s - loss: 1.4319 - acc: 0.9624 - mDice: 0.6977 - val_loss: 1.1535 - val_acc: 0.9753 - val_mDice: 0.7466

Epoch 00025: val_mDice improved from 0.74390 to 0.74658, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 26/300
 - 20s - loss: 1.4197 - acc: 0.9626 - mDice: 0.6996 - val_loss: 1.1478 - val_acc: 0.9756 - val_mDice: 0.7543

Epoch 00026: val_mDice improved from 0.74658 to 0.75435, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 27/300
 - 20s - loss: 1.4018 - acc: 0.9628 - mDice: 0.7029 - val_loss: 1.1605 - val_acc: 0.9765 - val_mDice: 0.7447

Epoch 00027: val_mDice did not improve from 0.75435
Epoch 28/300
 - 20s - loss: 1.3837 - acc: 0.9631 - mDice: 0.7063 - val_loss: 1.1823 - val_acc: 0.9754 - val_mDice: 0.7480

Epoch 00028: val_mDice did not improve from 0.75435
Epoch 29/300
 - 22s - loss: 1.3703 - acc: 0.9633 - mDice: 0.7090 - val_loss: 1.1461 - val_acc: 0.9762 - val_mDice: 0.7513

Epoch 00029: val_mDice did not improve from 0.75435
Epoch 30/300
 - 22s - loss: 1.3521 - acc: 0.9636 - mDice: 0.7124 - val_loss: 1.1594 - val_acc: 0.9758 - val_mDice: 0.7504

Epoch 00030: val_mDice did not improve from 0.75435
Epoch 31/300
 - 21s - loss: 1.3428 - acc: 0.9637 - mDice: 0.7143 - val_loss: 1.1230 - val_acc: 0.9764 - val_mDice: 0.7535

Epoch 00031: val_mDice did not improve from 0.75435
Epoch 32/300
 - 21s - loss: 1.3216 - acc: 0.9641 - mDice: 0.7182 - val_loss: 1.1626 - val_acc: 0.9765 - val_mDice: 0.7485

Epoch 00032: val_mDice did not improve from 0.75435
Epoch 33/300
 - 20s - loss: 1.3138 - acc: 0.9643 - mDice: 0.7195 - val_loss: 1.1304 - val_acc: 0.9768 - val_mDice: 0.7547

Epoch 00033: val_mDice improved from 0.75435 to 0.75475, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 34/300
 - 20s - loss: 1.3008 - acc: 0.9644 - mDice: 0.7219 - val_loss: 1.1270 - val_acc: 0.9756 - val_mDice: 0.7553

Epoch 00034: val_mDice improved from 0.75475 to 0.75533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 35/300
 - 21s - loss: 1.2894 - acc: 0.9646 - mDice: 0.7245 - val_loss: 1.1404 - val_acc: 0.9764 - val_mDice: 0.7570

Epoch 00035: val_mDice improved from 0.75533 to 0.75702, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 36/300
 - 20s - loss: 1.2860 - acc: 0.9647 - mDice: 0.7249 - val_loss: 1.1339 - val_acc: 0.9769 - val_mDice: 0.7572

Epoch 00036: val_mDice improved from 0.75702 to 0.75723, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 37/300
 - 20s - loss: 1.2687 - acc: 0.9650 - mDice: 0.7281 - val_loss: 1.0963 - val_acc: 0.9765 - val_mDice: 0.7624

Epoch 00037: val_mDice improved from 0.75723 to 0.76240, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300
 - 20s - loss: 1.2579 - acc: 0.9652 - mDice: 0.7305 - val_loss: 1.0909 - val_acc: 0.9775 - val_mDice: 0.7602

Epoch 00038: val_mDice did not improve from 0.76240
Epoch 39/300
 - 21s - loss: 1.2548 - acc: 0.9652 - mDice: 0.7309 - val_loss: 1.1208 - val_acc: 0.9769 - val_mDice: 0.7587

Epoch 00039: val_mDice did not improve from 0.76240
Epoch 40/300
 - 20s - loss: 1.2395 - acc: 0.9655 - mDice: 0.7342 - val_loss: 1.1055 - val_acc: 0.9771 - val_mDice: 0.7612

Epoch 00040: val_mDice did not improve from 0.76240
Epoch 41/300
 - 20s - loss: 1.2336 - acc: 0.9656 - mDice: 0.7350 - val_loss: 1.1142 - val_acc: 0.9772 - val_mDice: 0.7597

Epoch 00041: val_mDice did not improve from 0.76240
Epoch 42/300
 - 20s - loss: 1.2206 - acc: 0.9658 - mDice: 0.7376 - val_loss: 1.1041 - val_acc: 0.9768 - val_mDice: 0.7606

Epoch 00042: val_mDice did not improve from 0.76240
Epoch 43/300
 - 21s - loss: 1.2176 - acc: 0.9658 - mDice: 0.7384 - val_loss: 1.1046 - val_acc: 0.9765 - val_mDice: 0.7613

Epoch 00043: val_mDice did not improve from 0.76240
Epoch 44/300
 - 20s - loss: 1.2079 - acc: 0.9660 - mDice: 0.7403 - val_loss: 1.1252 - val_acc: 0.9768 - val_mDice: 0.7575

Epoch 00044: val_mDice did not improve from 0.76240
Epoch 45/300
 - 20s - loss: 1.2024 - acc: 0.9661 - mDice: 0.7413 - val_loss: 1.0876 - val_acc: 0.9777 - val_mDice: 0.7628

Epoch 00045: val_mDice improved from 0.76240 to 0.76284, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 46/300
 - 20s - loss: 1.1964 - acc: 0.9661 - mDice: 0.7424 - val_loss: 1.0973 - val_acc: 0.9768 - val_mDice: 0.7648

Epoch 00046: val_mDice improved from 0.76284 to 0.76479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 47/300
 - 21s - loss: 1.1881 - acc: 0.9662 - mDice: 0.7442 - val_loss: 1.0935 - val_acc: 0.9779 - val_mDice: 0.7685

Epoch 00047: val_mDice improved from 0.76479 to 0.76852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 48/300
 - 20s - loss: 1.1831 - acc: 0.9663 - mDice: 0.7449 - val_loss: 1.0946 - val_acc: 0.9765 - val_mDice: 0.7691

Epoch 00048: val_mDice improved from 0.76852 to 0.76915, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 49/300
 - 20s - loss: 1.1725 - acc: 0.9664 - mDice: 0.7472 - val_loss: 1.0868 - val_acc: 0.9777 - val_mDice: 0.7673

Epoch 00049: val_mDice did not improve from 0.76915
Epoch 50/300
 - 20s - loss: 1.1710 - acc: 0.9664 - mDice: 0.7476 - val_loss: 1.0691 - val_acc: 0.9783 - val_mDice: 0.7703

Epoch 00050: val_mDice improved from 0.76915 to 0.77030, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 51/300
 - 20s - loss: 1.1616 - acc: 0.9666 - mDice: 0.7494 - val_loss: 1.0776 - val_acc: 0.9778 - val_mDice: 0.7688

Epoch 00051: val_mDice did not improve from 0.77030
Epoch 52/300
 - 21s - loss: 1.1562 - acc: 0.9667 - mDice: 0.7504 - val_loss: 1.0912 - val_acc: 0.9778 - val_mDice: 0.7655

Epoch 00052: val_mDice did not improve from 0.77030
Epoch 53/300
 - 20s - loss: 1.1527 - acc: 0.9667 - mDice: 0.7512 - val_loss: 1.0829 - val_acc: 0.9777 - val_mDice: 0.7660

Epoch 00053: val_mDice did not improve from 0.77030
Epoch 54/300
 - 20s - loss: 1.1481 - acc: 0.9667 - mDice: 0.7519 - val_loss: 1.0820 - val_acc: 0.9779 - val_mDice: 0.7645

Epoch 00054: val_mDice did not improve from 0.77030
Epoch 55/300
 - 20s - loss: 1.1386 - acc: 0.9669 - mDice: 0.7535 - val_loss: 1.0950 - val_acc: 0.9777 - val_mDice: 0.7697

Epoch 00055: val_mDice did not improve from 0.77030
Epoch 56/300
 - 21s - loss: 1.1395 - acc: 0.9669 - mDice: 0.7539 - val_loss: 1.0746 - val_acc: 0.9770 - val_mDice: 0.7702

Epoch 00056: val_mDice did not improve from 0.77030
Epoch 57/300
 - 20s - loss: 1.1373 - acc: 0.9669 - mDice: 0.7539 - val_loss: 1.0705 - val_acc: 0.9785 - val_mDice: 0.7662

Epoch 00057: val_mDice did not improve from 0.77030
Epoch 58/300
 - 20s - loss: 1.1268 - acc: 0.9670 - mDice: 0.7562 - val_loss: 1.0743 - val_acc: 0.9776 - val_mDice: 0.7727

Epoch 00058: val_mDice improved from 0.77030 to 0.77267, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 59/300
 - 21s - loss: 1.1203 - acc: 0.9671 - mDice: 0.7573 - val_loss: 1.0615 - val_acc: 0.9786 - val_mDice: 0.7716

Epoch 00059: val_mDice did not improve from 0.77267
Epoch 60/300
 - 20s - loss: 1.1202 - acc: 0.9671 - mDice: 0.7575 - val_loss: 1.0896 - val_acc: 0.9779 - val_mDice: 0.7670

Epoch 00060: val_mDice did not improve from 0.77267
Epoch 61/300
 - 20s - loss: 1.1124 - acc: 0.9672 - mDice: 0.7589 - val_loss: 1.0603 - val_acc: 0.9776 - val_mDice: 0.7711

Epoch 00061: val_mDice did not improve from 0.77267
Epoch 62/300
 - 20s - loss: 1.1142 - acc: 0.9672 - mDice: 0.7585 - val_loss: 1.0580 - val_acc: 0.9781 - val_mDice: 0.7754

Epoch 00062: val_mDice improved from 0.77267 to 0.77543, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 63/300
 - 20s - loss: 1.1073 - acc: 0.9673 - mDice: 0.7599 - val_loss: 1.0723 - val_acc: 0.9779 - val_mDice: 0.7706

Epoch 00063: val_mDice did not improve from 0.77543
Epoch 64/300
 - 20s - loss: 1.1048 - acc: 0.9673 - mDice: 0.7607 - val_loss: 1.0630 - val_acc: 0.9785 - val_mDice: 0.7702

Epoch 00064: val_mDice did not improve from 0.77543
Epoch 65/300
 - 21s - loss: 1.0999 - acc: 0.9674 - mDice: 0.7616 - val_loss: 1.0449 - val_acc: 0.9788 - val_mDice: 0.7712

Epoch 00065: val_mDice did not improve from 0.77543
Epoch 66/300
 - 20s - loss: 1.0977 - acc: 0.9674 - mDice: 0.7621 - val_loss: 1.0772 - val_acc: 0.9784 - val_mDice: 0.7702

Epoch 00066: val_mDice did not improve from 0.77543
Epoch 67/300
 - 20s - loss: 1.0937 - acc: 0.9674 - mDice: 0.7627 - val_loss: 1.0675 - val_acc: 0.9790 - val_mDice: 0.7705

Epoch 00067: val_mDice did not improve from 0.77543
Epoch 68/300
 - 20s - loss: 1.0888 - acc: 0.9675 - mDice: 0.7637 - val_loss: 1.0541 - val_acc: 0.9785 - val_mDice: 0.7702

Epoch 00068: val_mDice did not improve from 0.77543
Epoch 69/300
 - 20s - loss: 1.0874 - acc: 0.9675 - mDice: 0.7643 - val_loss: 1.0680 - val_acc: 0.9779 - val_mDice: 0.7694

Epoch 00069: val_mDice did not improve from 0.77543
Epoch 70/300
 - 21s - loss: 1.0833 - acc: 0.9675 - mDice: 0.7644 - val_loss: 1.0755 - val_acc: 0.9785 - val_mDice: 0.7692

Epoch 00070: val_mDice did not improve from 0.77543
Epoch 71/300
 - 20s - loss: 1.0782 - acc: 0.9677 - mDice: 0.7660 - val_loss: 1.0517 - val_acc: 0.9787 - val_mDice: 0.7737

Epoch 00071: val_mDice did not improve from 0.77543
Epoch 72/300
 - 20s - loss: 1.0748 - acc: 0.9677 - mDice: 0.7664 - val_loss: 1.0609 - val_acc: 0.9786 - val_mDice: 0.7699

Epoch 00072: val_mDice did not improve from 0.77543
Epoch 73/300
 - 20s - loss: 1.0713 - acc: 0.9677 - mDice: 0.7669 - val_loss: 1.0909 - val_acc: 0.9770 - val_mDice: 0.7699

Epoch 00073: val_mDice did not improve from 0.77543
Epoch 74/300
 - 20s - loss: 1.0701 - acc: 0.9677 - mDice: 0.7673 - val_loss: 1.0510 - val_acc: 0.9779 - val_mDice: 0.7759

Epoch 00074: val_mDice improved from 0.77543 to 0.77586, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 75/300
 - 21s - loss: 1.0668 - acc: 0.9678 - mDice: 0.7680 - val_loss: 1.0542 - val_acc: 0.9785 - val_mDice: 0.7764

Epoch 00075: val_mDice improved from 0.77586 to 0.77635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 76/300
 - 20s - loss: 1.0660 - acc: 0.9678 - mDice: 0.7681 - val_loss: 1.0689 - val_acc: 0.9780 - val_mDice: 0.7742

Epoch 00076: val_mDice did not improve from 0.77635
Epoch 77/300
 - 20s - loss: 1.0629 - acc: 0.9679 - mDice: 0.7687 - val_loss: 1.0585 - val_acc: 0.9785 - val_mDice: 0.7750

Epoch 00077: val_mDice did not improve from 0.77635
Epoch 78/300
 - 20s - loss: 1.0578 - acc: 0.9680 - mDice: 0.7698 - val_loss: 1.0623 - val_acc: 0.9792 - val_mDice: 0.7770

Epoch 00078: val_mDice improved from 0.77635 to 0.77703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 79/300
 - 20s - loss: 1.0548 - acc: 0.9680 - mDice: 0.7705 - val_loss: 1.0735 - val_acc: 0.9773 - val_mDice: 0.7749

Epoch 00079: val_mDice did not improve from 0.77703
Epoch 80/300
 - 21s - loss: 1.0550 - acc: 0.9680 - mDice: 0.7705 - val_loss: 1.0501 - val_acc: 0.9792 - val_mDice: 0.7777

Epoch 00080: val_mDice improved from 0.77703 to 0.77772, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 81/300
 - 20s - loss: 1.0486 - acc: 0.9681 - mDice: 0.7720 - val_loss: 1.0660 - val_acc: 0.9780 - val_mDice: 0.7758

Epoch 00081: val_mDice did not improve from 0.77772
Epoch 82/300
 - 20s - loss: 1.0486 - acc: 0.9681 - mDice: 0.7718 - val_loss: 1.0504 - val_acc: 0.9778 - val_mDice: 0.7760

Epoch 00082: val_mDice did not improve from 0.77772
Epoch 83/300
 - 20s - loss: 1.0471 - acc: 0.9681 - mDice: 0.7721 - val_loss: 1.0377 - val_acc: 0.9793 - val_mDice: 0.7777

Epoch 00083: val_mDice did not improve from 0.77772
Epoch 84/300
 - 20s - loss: 1.0429 - acc: 0.9681 - mDice: 0.7730 - val_loss: 1.0568 - val_acc: 0.9787 - val_mDice: 0.7727

Epoch 00084: val_mDice did not improve from 0.77772
Epoch 85/300
 - 21s - loss: 1.0446 - acc: 0.9681 - mDice: 0.7728 - val_loss: 1.0330 - val_acc: 0.9790 - val_mDice: 0.7738

Epoch 00085: val_mDice did not improve from 0.77772
Epoch 86/300
 - 20s - loss: 1.0374 - acc: 0.9682 - mDice: 0.7742 - val_loss: 1.0450 - val_acc: 0.9786 - val_mDice: 0.7766

Epoch 00086: val_mDice did not improve from 0.77772
Epoch 87/300
 - 20s - loss: 1.0374 - acc: 0.9682 - mDice: 0.7741 - val_loss: 1.0446 - val_acc: 0.9787 - val_mDice: 0.7757

Epoch 00087: val_mDice did not improve from 0.77772
Epoch 88/300
 - 20s - loss: 1.0336 - acc: 0.9682 - mDice: 0.7748 - val_loss: 1.0623 - val_acc: 0.9783 - val_mDice: 0.7791

Epoch 00088: val_mDice improved from 0.77772 to 0.77912, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 89/300
 - 20s - loss: 1.0343 - acc: 0.9682 - mDice: 0.7749 - val_loss: 1.0579 - val_acc: 0.9782 - val_mDice: 0.7774

Epoch 00089: val_mDice did not improve from 0.77912
Epoch 90/300
 - 21s - loss: 1.0288 - acc: 0.9683 - mDice: 0.7758 - val_loss: 1.0442 - val_acc: 0.9786 - val_mDice: 0.7793

Epoch 00090: val_mDice improved from 0.77912 to 0.77925, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 91/300
 - 20s - loss: 1.0281 - acc: 0.9683 - mDice: 0.7761 - val_loss: 1.0590 - val_acc: 0.9782 - val_mDice: 0.7784

Epoch 00091: val_mDice did not improve from 0.77925
Epoch 92/300
 - 20s - loss: 1.0259 - acc: 0.9683 - mDice: 0.7765 - val_loss: 1.0517 - val_acc: 0.9787 - val_mDice: 0.7757

Epoch 00092: val_mDice did not improve from 0.77925
Epoch 93/300
 - 20s - loss: 1.0269 - acc: 0.9683 - mDice: 0.7761 - val_loss: 1.0413 - val_acc: 0.9789 - val_mDice: 0.7769

Epoch 00093: val_mDice did not improve from 0.77925
Epoch 94/300
 - 21s - loss: 1.0239 - acc: 0.9684 - mDice: 0.7768 - val_loss: 1.0467 - val_acc: 0.9789 - val_mDice: 0.7795

Epoch 00094: val_mDice improved from 0.77925 to 0.77945, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 95/300
 - 21s - loss: 1.0204 - acc: 0.9684 - mDice: 0.7775 - val_loss: 1.0644 - val_acc: 0.9785 - val_mDice: 0.7787

Epoch 00095: val_mDice did not improve from 0.77945
Epoch 96/300
 - 20s - loss: 1.0169 - acc: 0.9685 - mDice: 0.7783 - val_loss: 1.0438 - val_acc: 0.9785 - val_mDice: 0.7813

Epoch 00096: val_mDice improved from 0.77945 to 0.78133, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 97/300
 - 20s - loss: 1.0171 - acc: 0.9684 - mDice: 0.7785 - val_loss: 1.0535 - val_acc: 0.9781 - val_mDice: 0.7805

Epoch 00097: val_mDice did not improve from 0.78133
Epoch 98/300
 - 21s - loss: 1.0141 - acc: 0.9685 - mDice: 0.7787 - val_loss: 1.0410 - val_acc: 0.9792 - val_mDice: 0.7813

Epoch 00098: val_mDice did not improve from 0.78133
Epoch 99/300
 - 20s - loss: 1.0115 - acc: 0.9686 - mDice: 0.7795 - val_loss: 1.0488 - val_acc: 0.9781 - val_mDice: 0.7818

Epoch 00099: val_mDice improved from 0.78133 to 0.78176, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 100/300
 - 20s - loss: 1.0121 - acc: 0.9685 - mDice: 0.7793 - val_loss: 1.0404 - val_acc: 0.9782 - val_mDice: 0.7822

Epoch 00100: val_mDice improved from 0.78176 to 0.78218, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 101/300
 - 21s - loss: 1.0088 - acc: 0.9685 - mDice: 0.7802 - val_loss: 1.0374 - val_acc: 0.9789 - val_mDice: 0.7788

Epoch 00101: val_mDice did not improve from 0.78218
Epoch 102/300
 - 20s - loss: 1.0100 - acc: 0.9685 - mDice: 0.7795 - val_loss: 1.0671 - val_acc: 0.9781 - val_mDice: 0.7818

Epoch 00102: val_mDice did not improve from 0.78218
Epoch 103/300
 - 20s - loss: 1.0062 - acc: 0.9686 - mDice: 0.7807 - val_loss: 1.0513 - val_acc: 0.9790 - val_mDice: 0.7816

Epoch 00103: val_mDice did not improve from 0.78218
Epoch 104/300
 - 20s - loss: 1.0037 - acc: 0.9686 - mDice: 0.7813 - val_loss: 1.0532 - val_acc: 0.9790 - val_mDice: 0.7822

Epoch 00104: val_mDice did not improve from 0.78218
Epoch 105/300
 - 20s - loss: 1.0046 - acc: 0.9686 - mDice: 0.7812 - val_loss: 1.0526 - val_acc: 0.9787 - val_mDice: 0.7861

Epoch 00105: val_mDice improved from 0.78218 to 0.78606, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 106/300
 - 21s - loss: 1.0032 - acc: 0.9686 - mDice: 0.7812 - val_loss: 1.0588 - val_acc: 0.9786 - val_mDice: 0.7839

Epoch 00106: val_mDice did not improve from 0.78606
Epoch 107/300
 - 20s - loss: 1.0020 - acc: 0.9687 - mDice: 0.7816 - val_loss: 1.0482 - val_acc: 0.9778 - val_mDice: 0.7782

Epoch 00107: val_mDice did not improve from 0.78606
Epoch 108/300
 - 20s - loss: 0.9966 - acc: 0.9687 - mDice: 0.7826 - val_loss: 1.0274 - val_acc: 0.9792 - val_mDice: 0.7835

Epoch 00108: val_mDice did not improve from 0.78606
Epoch 109/300
 - 20s - loss: 0.9971 - acc: 0.9687 - mDice: 0.7826 - val_loss: 1.0323 - val_acc: 0.9791 - val_mDice: 0.7849

Epoch 00109: val_mDice did not improve from 0.78606
Epoch 110/300
 - 20s - loss: 0.9938 - acc: 0.9687 - mDice: 0.7834 - val_loss: 1.0390 - val_acc: 0.9788 - val_mDice: 0.7827

Epoch 00110: val_mDice did not improve from 0.78606
Epoch 111/300
 - 21s - loss: 0.9952 - acc: 0.9687 - mDice: 0.7830 - val_loss: 1.0471 - val_acc: 0.9792 - val_mDice: 0.7803

Epoch 00111: val_mDice did not improve from 0.78606
Epoch 112/300
 - 20s - loss: 0.9916 - acc: 0.9688 - mDice: 0.7837 - val_loss: 1.0533 - val_acc: 0.9790 - val_mDice: 0.7838

Epoch 00112: val_mDice did not improve from 0.78606
Epoch 113/300
 - 20s - loss: 0.9882 - acc: 0.9688 - mDice: 0.7845 - val_loss: 1.0446 - val_acc: 0.9785 - val_mDice: 0.7821

Epoch 00113: val_mDice did not improve from 0.78606
Epoch 114/300
 - 20s - loss: 0.9852 - acc: 0.9689 - mDice: 0.7851 - val_loss: 1.0414 - val_acc: 0.9786 - val_mDice: 0.7889

Epoch 00114: val_mDice improved from 0.78606 to 0.78889, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 115/300
 - 21s - loss: 0.9892 - acc: 0.9689 - mDice: 0.7841 - val_loss: 1.0399 - val_acc: 0.9777 - val_mDice: 0.7839

Epoch 00115: val_mDice did not improve from 0.78889
Epoch 116/300
 - 21s - loss: 0.9862 - acc: 0.9689 - mDice: 0.7848 - val_loss: 1.0535 - val_acc: 0.9789 - val_mDice: 0.7824

Epoch 00116: val_mDice did not improve from 0.78889
Epoch 117/300
 - 20s - loss: 0.9861 - acc: 0.9689 - mDice: 0.7849 - val_loss: 1.0267 - val_acc: 0.9787 - val_mDice: 0.7857

Epoch 00117: val_mDice did not improve from 0.78889
Epoch 118/300
 - 20s - loss: 0.9846 - acc: 0.9689 - mDice: 0.7854 - val_loss: 1.0357 - val_acc: 0.9792 - val_mDice: 0.7854

Epoch 00118: val_mDice did not improve from 0.78889
Epoch 119/300
 - 21s - loss: 0.9807 - acc: 0.9689 - mDice: 0.7861 - val_loss: 1.0261 - val_acc: 0.9789 - val_mDice: 0.7848

Epoch 00119: val_mDice did not improve from 0.78889
Epoch 120/300
 - 20s - loss: 0.9829 - acc: 0.9689 - mDice: 0.7856 - val_loss: 1.0307 - val_acc: 0.9786 - val_mDice: 0.7877

Epoch 00120: val_mDice did not improve from 0.78889
Epoch 121/300
 - 20s - loss: 0.9785 - acc: 0.9690 - mDice: 0.7865 - val_loss: 1.0539 - val_acc: 0.9787 - val_mDice: 0.7827

Epoch 00121: val_mDice did not improve from 0.78889
Epoch 122/300
 - 20s - loss: 0.9796 - acc: 0.9690 - mDice: 0.7863 - val_loss: 1.0254 - val_acc: 0.9791 - val_mDice: 0.7856

Epoch 00122: val_mDice did not improve from 0.78889
Epoch 123/300
 - 21s - loss: 0.9756 - acc: 0.9690 - mDice: 0.7871 - val_loss: 1.0420 - val_acc: 0.9792 - val_mDice: 0.7835

Epoch 00123: val_mDice did not improve from 0.78889
Epoch 124/300
 - 20s - loss: 0.9761 - acc: 0.9690 - mDice: 0.7872 - val_loss: 1.0384 - val_acc: 0.9787 - val_mDice: 0.7870

Epoch 00124: val_mDice did not improve from 0.78889
Epoch 125/300
 - 20s - loss: 0.9755 - acc: 0.9690 - mDice: 0.7873 - val_loss: 1.0457 - val_acc: 0.9781 - val_mDice: 0.7878

Epoch 00125: val_mDice did not improve from 0.78889
Epoch 126/300
 - 20s - loss: 0.9725 - acc: 0.9691 - mDice: 0.7879 - val_loss: 1.0412 - val_acc: 0.9791 - val_mDice: 0.7817

Epoch 00126: val_mDice did not improve from 0.78889
Epoch 127/300
 - 21s - loss: 0.9723 - acc: 0.9691 - mDice: 0.7879 - val_loss: 1.0380 - val_acc: 0.9790 - val_mDice: 0.7840

Epoch 00127: val_mDice did not improve from 0.78889
Epoch 128/300
 - 20s - loss: 0.9690 - acc: 0.9691 - mDice: 0.7886 - val_loss: 1.0424 - val_acc: 0.9789 - val_mDice: 0.7888

Epoch 00128: val_mDice did not improve from 0.78889
Epoch 129/300
 - 20s - loss: 0.9698 - acc: 0.9691 - mDice: 0.7884 - val_loss: 1.0427 - val_acc: 0.9787 - val_mDice: 0.7846

Epoch 00129: val_mDice did not improve from 0.78889
Epoch 130/300
 - 20s - loss: 0.9697 - acc: 0.9691 - mDice: 0.7882 - val_loss: 1.0303 - val_acc: 0.9790 - val_mDice: 0.7849

Epoch 00130: val_mDice did not improve from 0.78889
Epoch 131/300
 - 20s - loss: 0.9683 - acc: 0.9691 - mDice: 0.7887 - val_loss: 1.0230 - val_acc: 0.9793 - val_mDice: 0.7880

Epoch 00131: val_mDice did not improve from 0.78889
Epoch 132/300
 - 20s - loss: 0.9667 - acc: 0.9692 - mDice: 0.7891 - val_loss: 1.0503 - val_acc: 0.9790 - val_mDice: 0.7813

Epoch 00132: val_mDice did not improve from 0.78889
Epoch 133/300
 - 21s - loss: 0.9631 - acc: 0.9692 - mDice: 0.7897 - val_loss: 1.0491 - val_acc: 0.9796 - val_mDice: 0.7896

Epoch 00133: val_mDice improved from 0.78889 to 0.78956, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 134/300
 - 20s - loss: 0.9633 - acc: 0.9692 - mDice: 0.7898 - val_loss: 1.0429 - val_acc: 0.9795 - val_mDice: 0.7872

Epoch 00134: val_mDice did not improve from 0.78956
Epoch 135/300
 - 20s - loss: 0.9635 - acc: 0.9692 - mDice: 0.7899 - val_loss: 1.0286 - val_acc: 0.9795 - val_mDice: 0.7881

Epoch 00135: val_mDice did not improve from 0.78956
Epoch 136/300
 - 20s - loss: 0.9611 - acc: 0.9692 - mDice: 0.7902 - val_loss: 1.0267 - val_acc: 0.9797 - val_mDice: 0.7859

Epoch 00136: val_mDice did not improve from 0.78956
Epoch 137/300
 - 21s - loss: 0.9612 - acc: 0.9692 - mDice: 0.7906 - val_loss: 1.0441 - val_acc: 0.9786 - val_mDice: 0.7847

Epoch 00137: val_mDice did not improve from 0.78956
Epoch 138/300
 - 20s - loss: 0.9581 - acc: 0.9693 - mDice: 0.7910 - val_loss: 1.0220 - val_acc: 0.9797 - val_mDice: 0.7871

Epoch 00138: val_mDice did not improve from 0.78956
Epoch 139/300
 - 20s - loss: 0.9574 - acc: 0.9693 - mDice: 0.7914 - val_loss: 1.0558 - val_acc: 0.9780 - val_mDice: 0.7852

Epoch 00139: val_mDice did not improve from 0.78956
Epoch 140/300
 - 20s - loss: 0.9545 - acc: 0.9693 - mDice: 0.7920 - val_loss: 1.0323 - val_acc: 0.9783 - val_mDice: 0.7879

Epoch 00140: val_mDice did not improve from 0.78956
Epoch 141/300
 - 20s - loss: 0.9550 - acc: 0.9693 - mDice: 0.7918 - val_loss: 1.0244 - val_acc: 0.9791 - val_mDice: 0.7881

Epoch 00141: val_mDice did not improve from 0.78956
Epoch 142/300
 - 21s - loss: 0.9567 - acc: 0.9693 - mDice: 0.7914 - val_loss: 1.0318 - val_acc: 0.9790 - val_mDice: 0.7884

Epoch 00142: val_mDice did not improve from 0.78956
Epoch 143/300
 - 20s - loss: 0.9545 - acc: 0.9693 - mDice: 0.7919 - val_loss: 1.0298 - val_acc: 0.9791 - val_mDice: 0.7859

Epoch 00143: val_mDice did not improve from 0.78956
Epoch 144/300
 - 20s - loss: 0.9521 - acc: 0.9694 - mDice: 0.7925 - val_loss: 1.0261 - val_acc: 0.9789 - val_mDice: 0.7875

Epoch 00144: val_mDice did not improve from 0.78956
Epoch 145/300
 - 20s - loss: 0.9497 - acc: 0.9694 - mDice: 0.7932 - val_loss: 1.0393 - val_acc: 0.9791 - val_mDice: 0.7917

Epoch 00145: val_mDice improved from 0.78956 to 0.79165, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 146/300
 - 21s - loss: 0.9468 - acc: 0.9694 - mDice: 0.7935 - val_loss: 1.0483 - val_acc: 0.9796 - val_mDice: 0.7844

Epoch 00146: val_mDice did not improve from 0.79165
Epoch 147/300
 - 20s - loss: 0.9480 - acc: 0.9694 - mDice: 0.7933 - val_loss: 1.0467 - val_acc: 0.9789 - val_mDice: 0.7890

Epoch 00147: val_mDice did not improve from 0.79165
Epoch 148/300
 - 20s - loss: 0.9464 - acc: 0.9694 - mDice: 0.7938 - val_loss: 1.0194 - val_acc: 0.9796 - val_mDice: 0.7901

Epoch 00148: val_mDice did not improve from 0.79165
Epoch 149/300
 - 20s - loss: 0.9460 - acc: 0.9694 - mDice: 0.7939 - val_loss: 1.0241 - val_acc: 0.9793 - val_mDice: 0.7887

Epoch 00149: val_mDice did not improve from 0.79165
Epoch 150/300
 - 20s - loss: 0.9460 - acc: 0.9694 - mDice: 0.7937 - val_loss: 1.0265 - val_acc: 0.9789 - val_mDice: 0.7913

Epoch 00150: val_mDice did not improve from 0.79165
Epoch 151/300
 - 21s - loss: 0.9430 - acc: 0.9695 - mDice: 0.7945 - val_loss: 1.0422 - val_acc: 0.9788 - val_mDice: 0.7893

Epoch 00151: val_mDice did not improve from 0.79165
Epoch 152/300
 - 20s - loss: 0.9425 - acc: 0.9695 - mDice: 0.7945 - val_loss: 1.0140 - val_acc: 0.9786 - val_mDice: 0.7916

Epoch 00152: val_mDice did not improve from 0.79165
Epoch 153/300
 - 20s - loss: 0.9397 - acc: 0.9695 - mDice: 0.7950 - val_loss: 1.0202 - val_acc: 0.9795 - val_mDice: 0.7898

Epoch 00153: val_mDice did not improve from 0.79165
Epoch 154/300
 - 20s - loss: 0.9421 - acc: 0.9695 - mDice: 0.7946 - val_loss: 1.0428 - val_acc: 0.9789 - val_mDice: 0.7912

Epoch 00154: val_mDice did not improve from 0.79165
Epoch 155/300
 - 20s - loss: 0.9421 - acc: 0.9695 - mDice: 0.7950 - val_loss: 1.0228 - val_acc: 0.9792 - val_mDice: 0.7884

Epoch 00155: val_mDice did not improve from 0.79165
Epoch 156/300
 - 21s - loss: 0.9375 - acc: 0.9696 - mDice: 0.7954 - val_loss: 1.0199 - val_acc: 0.9791 - val_mDice: 0.7909

Epoch 00156: val_mDice did not improve from 0.79165
Epoch 157/300
 - 20s - loss: 0.9389 - acc: 0.9695 - mDice: 0.7955 - val_loss: 1.0297 - val_acc: 0.9796 - val_mDice: 0.7877

Epoch 00157: val_mDice did not improve from 0.79165
Epoch 158/300
 - 20s - loss: 0.9355 - acc: 0.9695 - mDice: 0.7961 - val_loss: 1.0203 - val_acc: 0.9799 - val_mDice: 0.7877

Epoch 00158: val_mDice did not improve from 0.79165
Epoch 159/300
 - 20s - loss: 0.9324 - acc: 0.9696 - mDice: 0.7967 - val_loss: 1.0158 - val_acc: 0.9799 - val_mDice: 0.7928

Epoch 00159: val_mDice improved from 0.79165 to 0.79276, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 160/300
 - 20s - loss: 0.9337 - acc: 0.9696 - mDice: 0.7965 - val_loss: 1.0427 - val_acc: 0.9784 - val_mDice: 0.7934

Epoch 00160: val_mDice improved from 0.79276 to 0.79343, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 161/300
 - 21s - loss: 0.9313 - acc: 0.9696 - mDice: 0.7970 - val_loss: 1.0284 - val_acc: 0.9793 - val_mDice: 0.7931

Epoch 00161: val_mDice did not improve from 0.79343
Epoch 162/300
 - 20s - loss: 0.9289 - acc: 0.9696 - mDice: 0.7976 - val_loss: 1.0180 - val_acc: 0.9793 - val_mDice: 0.7940

Epoch 00162: val_mDice improved from 0.79343 to 0.79395, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 163/300
 - 20s - loss: 0.9283 - acc: 0.9697 - mDice: 0.7977 - val_loss: 1.0423 - val_acc: 0.9796 - val_mDice: 0.7871

Epoch 00163: val_mDice did not improve from 0.79395
Epoch 164/300
 - 20s - loss: 0.9282 - acc: 0.9697 - mDice: 0.7979 - val_loss: 1.0097 - val_acc: 0.9797 - val_mDice: 0.7931

Epoch 00164: val_mDice did not improve from 0.79395
Epoch 165/300
 - 20s - loss: 0.9307 - acc: 0.9697 - mDice: 0.7971 - val_loss: 1.0290 - val_acc: 0.9789 - val_mDice: 0.7926

Epoch 00165: val_mDice did not improve from 0.79395
Epoch 166/300
 - 21s - loss: 0.9266 - acc: 0.9697 - mDice: 0.7982 - val_loss: 1.0250 - val_acc: 0.9790 - val_mDice: 0.7923

Epoch 00166: val_mDice did not improve from 0.79395
Epoch 167/300
 - 20s - loss: 0.9263 - acc: 0.9697 - mDice: 0.7983 - val_loss: 1.0171 - val_acc: 0.9791 - val_mDice: 0.7937

Epoch 00167: val_mDice did not improve from 0.79395
Epoch 168/300
 - 20s - loss: 0.9261 - acc: 0.9697 - mDice: 0.7982 - val_loss: 1.0192 - val_acc: 0.9802 - val_mDice: 0.7939

Epoch 00168: val_mDice did not improve from 0.79395
Epoch 169/300
 - 20s - loss: 0.9247 - acc: 0.9698 - mDice: 0.7986 - val_loss: 1.0371 - val_acc: 0.9791 - val_mDice: 0.7870

Epoch 00169: val_mDice did not improve from 0.79395
Epoch 170/300
 - 21s - loss: 0.9251 - acc: 0.9698 - mDice: 0.7983 - val_loss: 1.0322 - val_acc: 0.9793 - val_mDice: 0.7915

Epoch 00170: val_mDice did not improve from 0.79395
Epoch 171/300
 - 21s - loss: 0.9208 - acc: 0.9698 - mDice: 0.7994 - val_loss: 1.0647 - val_acc: 0.9783 - val_mDice: 0.7890

Epoch 00171: val_mDice did not improve from 0.79395
Epoch 172/300
 - 20s - loss: 0.9217 - acc: 0.9698 - mDice: 0.7992 - val_loss: 1.0276 - val_acc: 0.9800 - val_mDice: 0.7923

Epoch 00172: val_mDice did not improve from 0.79395
Epoch 173/300
 - 20s - loss: 0.9218 - acc: 0.9698 - mDice: 0.7992 - val_loss: 1.0134 - val_acc: 0.9800 - val_mDice: 0.7912

Epoch 00173: val_mDice did not improve from 0.79395
Epoch 174/300
 - 20s - loss: 0.9219 - acc: 0.9698 - mDice: 0.7993 - val_loss: 1.0367 - val_acc: 0.9793 - val_mDice: 0.7905

Epoch 00174: val_mDice did not improve from 0.79395
Epoch 175/300
 - 21s - loss: 0.9174 - acc: 0.9698 - mDice: 0.8002 - val_loss: 1.0577 - val_acc: 0.9785 - val_mDice: 0.7921

Epoch 00175: val_mDice did not improve from 0.79395
Epoch 176/300
 - 20s - loss: 0.9186 - acc: 0.9698 - mDice: 0.7995 - val_loss: 1.0227 - val_acc: 0.9796 - val_mDice: 0.7955

Epoch 00176: val_mDice improved from 0.79395 to 0.79548, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 177/300
 - 20s - loss: 0.9151 - acc: 0.9699 - mDice: 0.8006 - val_loss: 1.0310 - val_acc: 0.9798 - val_mDice: 0.7948

Epoch 00177: val_mDice did not improve from 0.79548
Epoch 178/300
 - 20s - loss: 0.9175 - acc: 0.9699 - mDice: 0.8003 - val_loss: 1.0144 - val_acc: 0.9790 - val_mDice: 0.7973

Epoch 00178: val_mDice improved from 0.79548 to 0.79734, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 179/300
 - 20s - loss: 0.9168 - acc: 0.9699 - mDice: 0.8006 - val_loss: 1.0262 - val_acc: 0.9782 - val_mDice: 0.7933

Epoch 00179: val_mDice did not improve from 0.79734
Epoch 180/300
 - 20s - loss: 0.9142 - acc: 0.9699 - mDice: 0.8010 - val_loss: 1.0294 - val_acc: 0.9794 - val_mDice: 0.7943

Epoch 00180: val_mDice did not improve from 0.79734
Epoch 181/300
 - 21s - loss: 0.9146 - acc: 0.9699 - mDice: 0.8011 - val_loss: 1.0102 - val_acc: 0.9800 - val_mDice: 0.7960

Epoch 00181: val_mDice did not improve from 0.79734
Epoch 182/300
 - 20s - loss: 0.9114 - acc: 0.9699 - mDice: 0.8015 - val_loss: 1.0236 - val_acc: 0.9796 - val_mDice: 0.7962

Epoch 00182: val_mDice did not improve from 0.79734
Epoch 183/300
 - 20s - loss: 0.9093 - acc: 0.9699 - mDice: 0.8019 - val_loss: 1.0394 - val_acc: 0.9792 - val_mDice: 0.7927

Epoch 00183: val_mDice did not improve from 0.79734
Epoch 184/300
 - 20s - loss: 0.9109 - acc: 0.9700 - mDice: 0.8016 - val_loss: 1.0074 - val_acc: 0.9794 - val_mDice: 0.7930

Epoch 00184: val_mDice did not improve from 0.79734
Epoch 185/300
 - 20s - loss: 0.9118 - acc: 0.9699 - mDice: 0.8015 - val_loss: 1.0191 - val_acc: 0.9794 - val_mDice: 0.7956

Epoch 00185: val_mDice did not improve from 0.79734
Epoch 186/300
 - 21s - loss: 0.9075 - acc: 0.9700 - mDice: 0.8024 - val_loss: 0.9931 - val_acc: 0.9803 - val_mDice: 0.7949

Epoch 00186: val_mDice did not improve from 0.79734
Epoch 187/300
 - 20s - loss: 0.9061 - acc: 0.9700 - mDice: 0.8026 - val_loss: 1.0314 - val_acc: 0.9789 - val_mDice: 0.7937

Epoch 00187: val_mDice did not improve from 0.79734
Epoch 188/300
 - 20s - loss: 0.9080 - acc: 0.9700 - mDice: 0.8024 - val_loss: 1.0230 - val_acc: 0.9794 - val_mDice: 0.7929

Epoch 00188: val_mDice did not improve from 0.79734
Epoch 189/300
 - 20s - loss: 0.9081 - acc: 0.9700 - mDice: 0.8025 - val_loss: 1.0158 - val_acc: 0.9791 - val_mDice: 0.7940

Epoch 00189: val_mDice did not improve from 0.79734
Epoch 190/300
 - 20s - loss: 0.9042 - acc: 0.9700 - mDice: 0.8032 - val_loss: 1.0283 - val_acc: 0.9799 - val_mDice: 0.7954

Epoch 00190: val_mDice did not improve from 0.79734
Epoch 191/300
 - 21s - loss: 0.9033 - acc: 0.9701 - mDice: 0.8031 - val_loss: 1.0208 - val_acc: 0.9799 - val_mDice: 0.7962

Epoch 00191: val_mDice did not improve from 0.79734
Epoch 192/300
 - 20s - loss: 0.9052 - acc: 0.9701 - mDice: 0.8030 - val_loss: 0.9915 - val_acc: 0.9801 - val_mDice: 0.7972

Epoch 00192: val_mDice did not improve from 0.79734
Epoch 193/300
 - 20s - loss: 0.9056 - acc: 0.9700 - mDice: 0.8028 - val_loss: 1.0219 - val_acc: 0.9797 - val_mDice: 0.7961

Epoch 00193: val_mDice did not improve from 0.79734
Epoch 194/300
 - 20s - loss: 0.9027 - acc: 0.9700 - mDice: 0.8034 - val_loss: 1.0099 - val_acc: 0.9795 - val_mDice: 0.7953

Epoch 00194: val_mDice did not improve from 0.79734
Epoch 195/300
 - 20s - loss: 0.9022 - acc: 0.9701 - mDice: 0.8037 - val_loss: 1.0309 - val_acc: 0.9793 - val_mDice: 0.7933

Epoch 00195: val_mDice did not improve from 0.79734
Epoch 196/300
 - 21s - loss: 0.8998 - acc: 0.9701 - mDice: 0.8039 - val_loss: 1.0088 - val_acc: 0.9797 - val_mDice: 0.7948

Epoch 00196: val_mDice did not improve from 0.79734
Epoch 197/300
 - 20s - loss: 0.9028 - acc: 0.9700 - mDice: 0.8035 - val_loss: 1.0094 - val_acc: 0.9792 - val_mDice: 0.7928

Epoch 00197: val_mDice did not improve from 0.79734
Epoch 198/300
 - 20s - loss: 0.9014 - acc: 0.9701 - mDice: 0.8038 - val_loss: 1.0038 - val_acc: 0.9799 - val_mDice: 0.7968

Epoch 00198: val_mDice did not improve from 0.79734
Epoch 199/300
 - 20s - loss: 0.8993 - acc: 0.9701 - mDice: 0.8043 - val_loss: 1.0110 - val_acc: 0.9796 - val_mDice: 0.7974

Epoch 00199: val_mDice improved from 0.79734 to 0.79735, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 200/300
 - 20s - loss: 0.8954 - acc: 0.9702 - mDice: 0.8048 - val_loss: 1.0120 - val_acc: 0.9794 - val_mDice: 0.7953

Epoch 00200: val_mDice did not improve from 0.79735
Epoch 201/300
 - 20s - loss: 0.8954 - acc: 0.9701 - mDice: 0.8051 - val_loss: 1.0361 - val_acc: 0.9787 - val_mDice: 0.7966

Epoch 00201: val_mDice did not improve from 0.79735
Epoch 202/300
 - 21s - loss: 0.8973 - acc: 0.9701 - mDice: 0.8047 - val_loss: 1.0087 - val_acc: 0.9794 - val_mDice: 0.7953

Epoch 00202: val_mDice did not improve from 0.79735
Epoch 203/300
 - 20s - loss: 0.8945 - acc: 0.9702 - mDice: 0.8054 - val_loss: 1.0137 - val_acc: 0.9799 - val_mDice: 0.7947

Epoch 00203: val_mDice did not improve from 0.79735
Epoch 204/300
 - 20s - loss: 0.8958 - acc: 0.9702 - mDice: 0.8050 - val_loss: 0.9873 - val_acc: 0.9797 - val_mDice: 0.7973

Epoch 00204: val_mDice did not improve from 0.79735
Epoch 205/300
 - 20s - loss: 0.8950 - acc: 0.9702 - mDice: 0.8049 - val_loss: 1.0275 - val_acc: 0.9796 - val_mDice: 0.7907

Epoch 00205: val_mDice did not improve from 0.79735
Epoch 206/300
 - 20s - loss: 0.8953 - acc: 0.9702 - mDice: 0.8051 - val_loss: 1.0318 - val_acc: 0.9787 - val_mDice: 0.7951

Epoch 00206: val_mDice did not improve from 0.79735
Epoch 207/300
 - 21s - loss: 0.8929 - acc: 0.9702 - mDice: 0.8056 - val_loss: 1.0103 - val_acc: 0.9798 - val_mDice: 0.7958

Epoch 00207: val_mDice did not improve from 0.79735
Epoch 208/300
 - 20s - loss: 0.8958 - acc: 0.9702 - mDice: 0.8050 - val_loss: 1.0139 - val_acc: 0.9797 - val_mDice: 0.7948

Epoch 00208: val_mDice did not improve from 0.79735
Epoch 209/300
 - 20s - loss: 0.8916 - acc: 0.9702 - mDice: 0.8058 - val_loss: 1.0270 - val_acc: 0.9797 - val_mDice: 0.7978

Epoch 00209: val_mDice improved from 0.79735 to 0.79778, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 210/300
 - 20s - loss: 0.8920 - acc: 0.9702 - mDice: 0.8057 - val_loss: 0.9970 - val_acc: 0.9800 - val_mDice: 0.7988

Epoch 00210: val_mDice improved from 0.79778 to 0.79878, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 211/300
 - 20s - loss: 0.8915 - acc: 0.9702 - mDice: 0.8059 - val_loss: 1.0208 - val_acc: 0.9796 - val_mDice: 0.7947

Epoch 00211: val_mDice did not improve from 0.79878
Epoch 212/300
 - 21s - loss: 0.8932 - acc: 0.9702 - mDice: 0.8055 - val_loss: 1.0144 - val_acc: 0.9797 - val_mDice: 0.7946

Epoch 00212: val_mDice did not improve from 0.79878
Epoch 213/300
 - 20s - loss: 0.8899 - acc: 0.9702 - mDice: 0.8062 - val_loss: 1.0398 - val_acc: 0.9797 - val_mDice: 0.7955

Epoch 00213: val_mDice did not improve from 0.79878
Epoch 214/300
 - 20s - loss: 0.8896 - acc: 0.9703 - mDice: 0.8063 - val_loss: 1.0322 - val_acc: 0.9800 - val_mDice: 0.7977

Epoch 00214: val_mDice did not improve from 0.79878
Epoch 215/300
 - 20s - loss: 0.8890 - acc: 0.9703 - mDice: 0.8064 - val_loss: 1.0288 - val_acc: 0.9795 - val_mDice: 0.7982

Epoch 00215: val_mDice did not improve from 0.79878
Epoch 216/300
 - 20s - loss: 0.8905 - acc: 0.9702 - mDice: 0.8063 - val_loss: 1.0316 - val_acc: 0.9794 - val_mDice: 0.7973

Epoch 00216: val_mDice did not improve from 0.79878
Epoch 217/300
 - 21s - loss: 0.8907 - acc: 0.9703 - mDice: 0.8061 - val_loss: 0.9959 - val_acc: 0.9800 - val_mDice: 0.7964

Epoch 00217: val_mDice did not improve from 0.79878
Epoch 218/300
 - 20s - loss: 0.8911 - acc: 0.9702 - mDice: 0.8060 - val_loss: 1.0072 - val_acc: 0.9791 - val_mDice: 0.7988

Epoch 00218: val_mDice improved from 0.79878 to 0.79884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 219/300
 - 20s - loss: 0.8846 - acc: 0.9703 - mDice: 0.8071 - val_loss: 1.0064 - val_acc: 0.9805 - val_mDice: 0.7970

Epoch 00219: val_mDice did not improve from 0.79884
Epoch 220/300
 - 20s - loss: 0.8880 - acc: 0.9703 - mDice: 0.8067 - val_loss: 1.0098 - val_acc: 0.9797 - val_mDice: 0.7967

Epoch 00220: val_mDice did not improve from 0.79884
Epoch 221/300
 - 21s - loss: 0.8874 - acc: 0.9703 - mDice: 0.8066 - val_loss: 1.0118 - val_acc: 0.9792 - val_mDice: 0.8003

Epoch 00221: val_mDice improved from 0.79884 to 0.80029, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 222/300
 - 20s - loss: 0.8844 - acc: 0.9704 - mDice: 0.8075 - val_loss: 1.0182 - val_acc: 0.9802 - val_mDice: 0.7977

Epoch 00222: val_mDice did not improve from 0.80029
Epoch 223/300
 - 20s - loss: 0.8871 - acc: 0.9704 - mDice: 0.8069 - val_loss: 1.0201 - val_acc: 0.9801 - val_mDice: 0.7963

Epoch 00223: val_mDice did not improve from 0.80029
Epoch 224/300
 - 20s - loss: 0.8856 - acc: 0.9703 - mDice: 0.8071 - val_loss: 1.0136 - val_acc: 0.9796 - val_mDice: 0.7959

Epoch 00224: val_mDice did not improve from 0.80029
Epoch 225/300
 - 20s - loss: 0.8826 - acc: 0.9704 - mDice: 0.8078 - val_loss: 1.0182 - val_acc: 0.9787 - val_mDice: 0.7972

Epoch 00225: val_mDice did not improve from 0.80029
Epoch 226/300
 - 21s - loss: 0.8852 - acc: 0.9703 - mDice: 0.8071 - val_loss: 1.0138 - val_acc: 0.9792 - val_mDice: 0.7970

Epoch 00226: val_mDice did not improve from 0.80029
Epoch 227/300
 - 20s - loss: 0.8832 - acc: 0.9703 - mDice: 0.8077 - val_loss: 1.0716 - val_acc: 0.9786 - val_mDice: 0.7888

Epoch 00227: val_mDice did not improve from 0.80029
Epoch 228/300
 - 20s - loss: 0.8852 - acc: 0.9703 - mDice: 0.8074 - val_loss: 0.9974 - val_acc: 0.9801 - val_mDice: 0.7972

Epoch 00228: val_mDice did not improve from 0.80029
Epoch 229/300
 - 20s - loss: 0.8799 - acc: 0.9704 - mDice: 0.8085 - val_loss: 1.0080 - val_acc: 0.9799 - val_mDice: 0.7988

Epoch 00229: val_mDice did not improve from 0.80029
Epoch 230/300
 - 20s - loss: 0.8826 - acc: 0.9704 - mDice: 0.8077 - val_loss: 1.0008 - val_acc: 0.9799 - val_mDice: 0.7985

Epoch 00230: val_mDice did not improve from 0.80029
Epoch 231/300
 - 21s - loss: 0.8810 - acc: 0.9704 - mDice: 0.8083 - val_loss: 1.0151 - val_acc: 0.9800 - val_mDice: 0.7970

Epoch 00231: val_mDice did not improve from 0.80029
Epoch 232/300
 - 20s - loss: 0.8794 - acc: 0.9704 - mDice: 0.8088 - val_loss: 1.0125 - val_acc: 0.9798 - val_mDice: 0.7986

Epoch 00232: val_mDice did not improve from 0.80029
Epoch 233/300
 - 20s - loss: 0.8803 - acc: 0.9704 - mDice: 0.8082 - val_loss: 1.0632 - val_acc: 0.9791 - val_mDice: 0.7940

Epoch 00233: val_mDice did not improve from 0.80029
Epoch 234/300
 - 20s - loss: 0.8811 - acc: 0.9704 - mDice: 0.8084 - val_loss: 1.0735 - val_acc: 0.9792 - val_mDice: 0.7894

Epoch 00234: val_mDice did not improve from 0.80029
Epoch 235/300
 - 20s - loss: 0.8808 - acc: 0.9704 - mDice: 0.8083 - val_loss: 1.0254 - val_acc: 0.9803 - val_mDice: 0.7947

Epoch 00235: val_mDice did not improve from 0.80029
Epoch 236/300
 - 21s - loss: 0.8811 - acc: 0.9704 - mDice: 0.8083 - val_loss: 1.0497 - val_acc: 0.9795 - val_mDice: 0.7876

Epoch 00236: val_mDice did not improve from 0.80029
Epoch 237/300
 - 20s - loss: 0.8828 - acc: 0.9704 - mDice: 0.8080 - val_loss: 1.0128 - val_acc: 0.9803 - val_mDice: 0.7979

Epoch 00237: val_mDice did not improve from 0.80029
Epoch 238/300
 - 20s - loss: 0.8801 - acc: 0.9704 - mDice: 0.8083 - val_loss: 1.0194 - val_acc: 0.9800 - val_mDice: 0.7955

Epoch 00238: val_mDice did not improve from 0.80029
Epoch 239/300
 - 20s - loss: 0.8787 - acc: 0.9704 - mDice: 0.8085 - val_loss: 1.0002 - val_acc: 0.9801 - val_mDice: 0.8007

Epoch 00239: val_mDice improved from 0.80029 to 0.80065, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 240/300
 - 20s - loss: 0.8768 - acc: 0.9705 - mDice: 0.8091 - val_loss: 1.0330 - val_acc: 0.9802 - val_mDice: 0.7965

Epoch 00240: val_mDice did not improve from 0.80065
Epoch 241/300
 - 20s - loss: 0.8798 - acc: 0.9704 - mDice: 0.8086 - val_loss: 1.0165 - val_acc: 0.9800 - val_mDice: 0.7955

Epoch 00241: val_mDice did not improve from 0.80065
Epoch 242/300
 - 21s - loss: 0.8768 - acc: 0.9705 - mDice: 0.8091 - val_loss: 1.0130 - val_acc: 0.9799 - val_mDice: 0.7984

Epoch 00242: val_mDice did not improve from 0.80065
Epoch 243/300
 - 20s - loss: 0.8781 - acc: 0.9704 - mDice: 0.8088 - val_loss: 1.0135 - val_acc: 0.9800 - val_mDice: 0.7975

Epoch 00243: val_mDice did not improve from 0.80065
Epoch 244/300
 - 20s - loss: 0.8760 - acc: 0.9705 - mDice: 0.8093 - val_loss: 1.0051 - val_acc: 0.9797 - val_mDice: 0.7966

Epoch 00244: val_mDice did not improve from 0.80065
Epoch 245/300
 - 20s - loss: 0.8787 - acc: 0.9705 - mDice: 0.8086 - val_loss: 1.0176 - val_acc: 0.9797 - val_mDice: 0.7971

Epoch 00245: val_mDice did not improve from 0.80065
Epoch 246/300
 - 20s - loss: 0.8777 - acc: 0.9705 - mDice: 0.8089 - val_loss: 1.0164 - val_acc: 0.9799 - val_mDice: 0.7992

Epoch 00246: val_mDice did not improve from 0.80065
Epoch 247/300
 - 21s - loss: 0.8752 - acc: 0.9705 - mDice: 0.8094 - val_loss: 1.0152 - val_acc: 0.9795 - val_mDice: 0.8003

Epoch 00247: val_mDice did not improve from 0.80065
Epoch 248/300
 - 20s - loss: 0.8746 - acc: 0.9705 - mDice: 0.8097 - val_loss: 1.0016 - val_acc: 0.9799 - val_mDice: 0.8007

Epoch 00248: val_mDice improved from 0.80065 to 0.80068, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 249/300
 - 20s - loss: 0.8776 - acc: 0.9705 - mDice: 0.8088 - val_loss: 0.9979 - val_acc: 0.9798 - val_mDice: 0.7983

Epoch 00249: val_mDice did not improve from 0.80068
Epoch 250/300
 - 20s - loss: 0.8783 - acc: 0.9704 - mDice: 0.8089 - val_loss: 1.0111 - val_acc: 0.9799 - val_mDice: 0.7967

Epoch 00250: val_mDice did not improve from 0.80068
Epoch 251/300
 - 20s - loss: 0.8738 - acc: 0.9705 - mDice: 0.8097 - val_loss: 1.0161 - val_acc: 0.9796 - val_mDice: 0.8008

Epoch 00251: val_mDice improved from 0.80068 to 0.80082, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 252/300
 - 21s - loss: 0.8753 - acc: 0.9705 - mDice: 0.8092 - val_loss: 1.0156 - val_acc: 0.9795 - val_mDice: 0.7981

Epoch 00252: val_mDice did not improve from 0.80082
Epoch 253/300
 - 20s - loss: 0.8746 - acc: 0.9705 - mDice: 0.8095 - val_loss: 1.0133 - val_acc: 0.9796 - val_mDice: 0.7992

Epoch 00253: val_mDice did not improve from 0.80082
Epoch 254/300
 - 20s - loss: 0.8726 - acc: 0.9705 - mDice: 0.8100 - val_loss: 0.9950 - val_acc: 0.9796 - val_mDice: 0.7970

Epoch 00254: val_mDice did not improve from 0.80082
Epoch 255/300
 - 20s - loss: 0.8728 - acc: 0.9706 - mDice: 0.8098 - val_loss: 1.0276 - val_acc: 0.9795 - val_mDice: 0.7972

Epoch 00255: val_mDice did not improve from 0.80082
Epoch 256/300
 - 20s - loss: 0.8738 - acc: 0.9705 - mDice: 0.8095 - val_loss: 0.9926 - val_acc: 0.9803 - val_mDice: 0.8016

Epoch 00256: val_mDice improved from 0.80082 to 0.80164, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 257/300
 - 21s - loss: 0.8697 - acc: 0.9706 - mDice: 0.8107 - val_loss: 1.0118 - val_acc: 0.9803 - val_mDice: 0.7977

Epoch 00257: val_mDice did not improve from 0.80164
Epoch 258/300
 - 20s - loss: 0.8712 - acc: 0.9705 - mDice: 0.8104 - val_loss: 1.0225 - val_acc: 0.9800 - val_mDice: 0.7980

Epoch 00258: val_mDice did not improve from 0.80164
Epoch 259/300
 - 20s - loss: 0.8736 - acc: 0.9705 - mDice: 0.8100 - val_loss: 1.0008 - val_acc: 0.9802 - val_mDice: 0.7999

Epoch 00259: val_mDice did not improve from 0.80164
Epoch 260/300
 - 20s - loss: 0.8697 - acc: 0.9706 - mDice: 0.8104 - val_loss: 1.0194 - val_acc: 0.9806 - val_mDice: 0.7983

Epoch 00260: val_mDice did not improve from 0.80164
Epoch 261/300
 - 21s - loss: 0.8702 - acc: 0.9705 - mDice: 0.8105 - val_loss: 0.9971 - val_acc: 0.9795 - val_mDice: 0.7963

Epoch 00261: val_mDice did not improve from 0.80164
Epoch 262/300
 - 20s - loss: 0.8695 - acc: 0.9706 - mDice: 0.8107 - val_loss: 0.9894 - val_acc: 0.9798 - val_mDice: 0.8002

Epoch 00262: val_mDice did not improve from 0.80164
Epoch 263/300
 - 20s - loss: 0.8702 - acc: 0.9706 - mDice: 0.8106 - val_loss: 1.0117 - val_acc: 0.9799 - val_mDice: 0.8009

Epoch 00263: val_mDice did not improve from 0.80164
Epoch 264/300
 - 20s - loss: 0.8692 - acc: 0.9706 - mDice: 0.8109 - val_loss: 1.0241 - val_acc: 0.9798 - val_mDice: 0.7956

Epoch 00264: val_mDice did not improve from 0.80164
Epoch 265/300
 - 20s - loss: 0.8712 - acc: 0.9706 - mDice: 0.8104 - val_loss: 0.9954 - val_acc: 0.9802 - val_mDice: 0.8015

Epoch 00265: val_mDice did not improve from 0.80164
Epoch 266/300
 - 21s - loss: 0.8678 - acc: 0.9706 - mDice: 0.8110 - val_loss: 1.0158 - val_acc: 0.9795 - val_mDice: 0.7967

Epoch 00266: val_mDice did not improve from 0.80164
Epoch 267/300
 - 20s - loss: 0.8687 - acc: 0.9706 - mDice: 0.8108 - val_loss: 1.0703 - val_acc: 0.9787 - val_mDice: 0.7909

Epoch 00267: val_mDice did not improve from 0.80164
Epoch 268/300
 - 20s - loss: 0.8679 - acc: 0.9706 - mDice: 0.8111 - val_loss: 1.0197 - val_acc: 0.9798 - val_mDice: 0.7959

Epoch 00268: val_mDice did not improve from 0.80164
Epoch 269/300
 - 20s - loss: 0.8676 - acc: 0.9706 - mDice: 0.8111 - val_loss: 1.0078 - val_acc: 0.9798 - val_mDice: 0.7979

Epoch 00269: val_mDice did not improve from 0.80164
Epoch 270/300
 - 20s - loss: 0.8670 - acc: 0.9706 - mDice: 0.8111 - val_loss: 1.0046 - val_acc: 0.9793 - val_mDice: 0.8004

Epoch 00270: val_mDice did not improve from 0.80164
Epoch 271/300
 - 21s - loss: 0.8681 - acc: 0.9706 - mDice: 0.8109 - val_loss: 1.0129 - val_acc: 0.9794 - val_mDice: 0.7965

Epoch 00271: val_mDice did not improve from 0.80164
Epoch 272/300
 - 20s - loss: 0.8666 - acc: 0.9706 - mDice: 0.8112 - val_loss: 1.0134 - val_acc: 0.9800 - val_mDice: 0.7982

Epoch 00272: val_mDice did not improve from 0.80164
Epoch 273/300
 - 20s - loss: 0.8666 - acc: 0.9706 - mDice: 0.8112 - val_loss: 1.0128 - val_acc: 0.9797 - val_mDice: 0.8005

Epoch 00273: val_mDice did not improve from 0.80164
Epoch 274/300
 - 20s - loss: 0.8660 - acc: 0.9706 - mDice: 0.8115 - val_loss: 0.9896 - val_acc: 0.9798 - val_mDice: 0.8010

Epoch 00274: val_mDice did not improve from 0.80164
Epoch 275/300
 - 20s - loss: 0.8670 - acc: 0.9706 - mDice: 0.8114 - val_loss: 0.9968 - val_acc: 0.9802 - val_mDice: 0.7982

Epoch 00275: val_mDice did not improve from 0.80164
Epoch 276/300
 - 21s - loss: 0.8640 - acc: 0.9707 - mDice: 0.8117 - val_loss: 1.0190 - val_acc: 0.9796 - val_mDice: 0.7998

Epoch 00276: val_mDice did not improve from 0.80164
Epoch 277/300
 - 20s - loss: 0.8676 - acc: 0.9706 - mDice: 0.8112 - val_loss: 1.0206 - val_acc: 0.9789 - val_mDice: 0.7943

Epoch 00277: val_mDice did not improve from 0.80164
Epoch 278/300
 - 20s - loss: 0.8644 - acc: 0.9706 - mDice: 0.8118 - val_loss: 1.0302 - val_acc: 0.9795 - val_mDice: 0.7971

Epoch 00278: val_mDice did not improve from 0.80164
Epoch 279/300
 - 20s - loss: 0.8664 - acc: 0.9707 - mDice: 0.8113 - val_loss: 1.0163 - val_acc: 0.9797 - val_mDice: 0.7988

Epoch 00279: val_mDice did not improve from 0.80164
Epoch 280/300
 - 20s - loss: 0.8628 - acc: 0.9707 - mDice: 0.8123 - val_loss: 1.0436 - val_acc: 0.9789 - val_mDice: 0.7949

Epoch 00280: val_mDice did not improve from 0.80164
Epoch 281/300
 - 21s - loss: 0.8647 - acc: 0.9706 - mDice: 0.8118 - val_loss: 1.0221 - val_acc: 0.9798 - val_mDice: 0.7982

Epoch 00281: val_mDice did not improve from 0.80164
Epoch 282/300
 - 20s - loss: 0.8660 - acc: 0.9706 - mDice: 0.8116 - val_loss: 0.9883 - val_acc: 0.9805 - val_mDice: 0.8001

Epoch 00282: val_mDice did not improve from 0.80164
Epoch 283/300
 - 20s - loss: 0.8639 - acc: 0.9706 - mDice: 0.8118 - val_loss: 1.0125 - val_acc: 0.9796 - val_mDice: 0.7966

Epoch 00283: val_mDice did not improve from 0.80164
Epoch 284/300
 - 20s - loss: 0.8650 - acc: 0.9706 - mDice: 0.8117 - val_loss: 1.0010 - val_acc: 0.9800 - val_mDice: 0.8001

Epoch 00284: val_mDice did not improve from 0.80164
Epoch 285/300
 - 20s - loss: 0.8625 - acc: 0.9706 - mDice: 0.8123 - val_loss: 1.0290 - val_acc: 0.9801 - val_mDice: 0.7988

Epoch 00285: val_mDice did not improve from 0.80164
Epoch 286/300
 - 20s - loss: 0.8612 - acc: 0.9707 - mDice: 0.8124 - val_loss: 1.0219 - val_acc: 0.9797 - val_mDice: 0.7985

Epoch 00286: val_mDice did not improve from 0.80164
Restoring model weights from the end of the best epoch
Epoch 00286: early stopping
{'val_loss': [6.207490813753098, 5.089904026742141, 3.212329578734869, 2.2362915828483594, 1.978191270350572, 1.6792466135561361, 1.5619659128423735, 1.476335398761166, 1.3934108967940293, 1.358590762309324, 1.3883177898889685, 1.3339334062197505, 1.2803718525202705, 1.281794255039604, 1.276594056710208, 1.2416137197105663, 1.2541232196015089, 1.2214846816339475, 1.1998298787274553, 1.2042542936931804, 1.1804215881862206, 1.2112388674739376, 1.155297136160108, 1.172712653089701, 1.153483078328922, 1.1478418487865603, 1.1605247574447537, 1.182337671465949, 1.1460659469368168, 1.1593618909378254, 1.1229943530421265, 1.1626156923641011, 1.1303648573652512, 1.1270072258000634, 1.1404453183938414, 1.1339440504989222, 1.0962752118471009, 1.090903586473951, 1.12082912810867, 1.105466408344061, 1.1142152742677498, 1.1041110590597867, 1.1046157143028004, 1.1252075247270152, 1.087578786487948, 1.0972983396116167, 1.0934760332736064, 1.0945987133652968, 1.0867599480390968, 1.069066016870978, 1.0776077837851847, 1.0912085688805537, 1.0829496613076786, 1.0819673859800731, 1.0950292523799756, 1.0745817011186234, 1.0704590262344307, 1.0742667624317699, 1.0615189071279838, 1.0895515272822656, 1.060278529442048, 1.057960213697229, 1.0723087951043369, 1.063022141925899, 1.0448918887307648, 1.0771773265828566, 1.0674599514904257, 1.0540963730828834, 1.0679830093794422, 1.0755469541348557, 1.0516733483187135, 1.060891859036967, 1.0909235377931845, 1.051014298504392, 1.0541914245784807, 1.0688614856798746, 1.0584693366697677, 1.0623272750624873, 1.0735179889809687, 1.0501491231532842, 1.0659583080003467, 1.0503500623527018, 1.0376691400271518, 1.056807685924959, 1.0330181743013418, 1.0450119640160949, 1.0445859704579117, 1.0622618176396577, 1.0579133770168352, 1.0442151882945967, 1.0589733427563954, 1.0516808250876428, 1.0412930576998236, 1.0467132873610578, 1.0643934811774791, 1.0438086528769697, 1.053540621051051, 1.0410068004747266, 1.0488459838714668, 1.040374305734316, 1.0374150417601078, 1.0670778516725832, 1.0513286771799433, 1.0531998796287028, 1.0526254306987635, 1.0588295300522252, 1.048194515369898, 1.027369915077263, 1.0322615054873046, 1.0390057556449006, 1.04714929313777, 1.0533014294971272, 1.0445574530398698, 1.0414273954652944, 1.039890152603545, 1.0535203279217014, 1.0266988557126693, 1.035690774083766, 1.0261117673506845, 1.0307490905265606, 1.0538657929859596, 1.0254245112357114, 1.0419853143197582, 1.0383704515249323, 1.0456974402672377, 1.0412331553460843, 1.0379703814828334, 1.0424333582444014, 1.0427447530213474, 1.0302619506688444, 1.0230312374647976, 1.0503185296100557, 1.0491202699907636, 1.0428798545433473, 1.0285989627477783, 1.026705404052835, 1.0440905631531312, 1.0219576454120696, 1.0558222687307268, 1.0322500644333543, 1.0244117797154324, 1.0318059432066597, 1.0297663835104824, 1.0261420014662985, 1.0392606143163672, 1.048291529419133, 1.0466990604970492, 1.0194436693023923, 1.0241158032668705, 1.0264870995796418, 1.042203952956074, 1.0139822792294575, 1.02020116357267, 1.0427651380193674, 1.0228368691275116, 1.0198790642624371, 1.0296750326357742, 1.0202960126848337, 1.0157758363101939, 1.0427314763747955, 1.0284313086047114, 1.0179726882642937, 1.0423439884646706, 1.009705461810469, 1.02901922064213, 1.0250390506167824, 1.0171123677062652, 1.0192223018418296, 1.0370936238702864, 1.0321759026373207, 1.0647138376018075, 1.0276155830058147, 1.0133975442557845, 1.0367406169852809, 1.0577483070336662, 1.0227215084753054, 1.0309795701650408, 1.0144183519435475, 1.0261787017954673, 1.0294246972130976, 1.0102090697296893, 1.0235758084404447, 1.0394209360405933, 1.0073576049980673, 1.0191380163697241, 0.9931427839560751, 1.0313717308069574, 1.0229651425132014, 1.0158362897712023, 1.0282942624418931, 1.0208371644070362, 0.9915319923147795, 1.0218914056913715, 1.0099360899891712, 1.0309067925795101, 1.0087636042982078, 1.0093768659082154, 1.003802401529255, 1.0109860490621287, 1.011969168282561, 1.0361064793983956, 1.0086555386679035, 1.0137084197704738, 0.9872508436598552, 1.0274797615141777, 1.031803793354068, 1.0103425769060064, 1.0138649537190403, 1.026997514894013, 0.9970413433855992, 1.0207854527161704, 1.0143507979875708, 1.0398352586112667, 1.0321837160834528, 1.0288193965419106, 1.0316189583030861, 0.9958557013049067, 1.007249311110257, 1.0064406762642684, 1.0097529650363017, 1.0117678752263946, 1.0182262174064958, 1.0201271008732868, 1.0135525815935251, 1.018243583832558, 1.0138339950246424, 1.0716300829852192, 0.9973509344149558, 1.0079895571162285, 1.0007506390028553, 1.0151450743993682, 1.0125152598575464, 1.0631528016971996, 1.0734849671697366, 1.0254499844172298, 1.049690623903526, 1.0128222465096124, 1.0194217123759024, 1.0001596161057953, 1.0329943927813499, 1.016545444255554, 1.0129866419860893, 1.0135389638701306, 1.0051319136141892, 1.0175899999422433, 1.0164400804231373, 1.015220079983475, 1.0015721429001887, 0.9979017306714988, 1.011056770549298, 1.0160714255904482, 1.0155658120639715, 1.013271554492689, 0.9950476799991722, 1.027574564115863, 0.9926189425121502, 1.0118385314522393, 1.0225387937364763, 1.0008207098670918, 1.0194068206425082, 0.997063423397252, 0.9894423479564579, 1.011691491716985, 1.0240607937316692, 0.9954354505337814, 1.0158083878837276, 1.0702657190483778, 1.0196757798454674, 1.0078474541959108, 1.004599988879554, 1.0128626120530448, 1.0134459416560422, 1.012843022237553, 0.9895981578500074, 0.9967994131606157, 1.0190152100603274, 1.020606194313465, 1.030197683662857, 1.0162785672345354, 1.0436170832344014, 1.0220737231008197, 0.9882705586656326, 1.0125479559906756, 1.001000434422116, 1.0290028924263215, 1.0218710460436575], 'val_acc': [0.9111185355638997, 0.9100414158380304, 0.9400611170984948, 0.9568938050412545, 0.9611141225994158, 0.9672189501551207, 0.9683382767784574, 0.9701241470598378, 0.9717314131230588, 0.9718012891461015, 0.9719507328655264, 0.9727409204405933, 0.973058483303117, 0.9731985959101646, 0.9745305324061684, 0.9745245545526381, 0.9741894044440325, 0.974333624533903, 0.9748241824927775, 0.9752489771490985, 0.9744856879246047, 0.9736222714866611, 0.9762229794777131, 0.9749773574210638, 0.9753494762368696, 0.9755941857772049, 0.9764815324848691, 0.9754372803733303, 0.9762039278848309, 0.9758232117537246, 0.9763911096199116, 0.976540922699578, 0.9768357155402642, 0.9755710306821264, 0.9763724350342642, 0.9768910009957366, 0.9765364524559732, 0.9775044605895379, 0.9769212544907167, 0.9771189054831051, 0.9771790549499498, 0.9767856434485196, 0.9765121486358777, 0.9767983390283501, 0.9776647387363789, 0.9767546340535102, 0.9779117166053222, 0.9764695680832821, 0.9776905303680206, 0.9783320225814525, 0.9778126932824642, 0.97783100259325, 0.9777051017657316, 0.9779240282641982, 0.9776871577298075, 0.9769911373227259, 0.9785311449391142, 0.9775627595888081, 0.9786226985115158, 0.9779371089172698, 0.9775649916727639, 0.9780966525337608, 0.9778866754265367, 0.9784881903123772, 0.9787500971021468, 0.9783790949568388, 0.9790205864369974, 0.9784975237293277, 0.9778567886729651, 0.9784803405168396, 0.9787310375480116, 0.9785718665391154, 0.976953768143545, 0.9779464462100935, 0.9785020047625255, 0.977993148177705, 0.9784721149711073, 0.9791633165155018, 0.9773281320327195, 0.9792249496037595, 0.9780114617833773, 0.9778377363468096, 0.9793366536314752, 0.9787321605037093, 0.9790067721963557, 0.9785703746421894, 0.978673131059469, 0.978326773811099, 0.9781881799178299, 0.9786058786883296, 0.9782475852170184, 0.9786667646758376, 0.978899528251172, 0.9789245736200906, 0.9784960348702483, 0.9784807271613807, 0.9781448413701385, 0.9792126100804978, 0.978073114565261, 0.9782386145608496, 0.9788576932280261, 0.9780857979937052, 0.979044488109059, 0.9789847108307957, 0.9786529592344757, 0.978644365061775, 0.9778126992534041, 0.9792021655449759, 0.9791005416159471, 0.9788180886127827, 0.9792141184236965, 0.9790224607883312, 0.9785352728488366, 0.9786424849490081, 0.9777234214470969, 0.9788576927042594, 0.9787388853532358, 0.9792036512614554, 0.9788980348876993, 0.978605496338675, 0.9786753694286245, 0.9791233231606509, 0.9792301775282003, 0.9786690117395197, 0.9780891579567653, 0.9790785060197063, 0.9789757431077203, 0.9788561944173803, 0.9787030189653273, 0.9790403924633623, 0.9792510987585165, 0.9790075043173792, 0.9795772749636001, 0.9795376511784974, 0.9795002956172494, 0.9797181171775912, 0.9786039941759227, 0.9796620832595758, 0.9780121989325605, 0.9783492005562741, 0.9790736434748806, 0.978957452024014, 0.9791431409193887, 0.9789066435163595, 0.9791490948891598, 0.9796198602300956, 0.978890196300768, 0.9796280797001348, 0.9792992924135473, 0.9789447465973705, 0.9788150920390245, 0.9785733753013276, 0.9795088951323698, 0.9788831006244532, 0.979190194858189, 0.9791334256765503, 0.9796093910775411, 0.9799116578588167, 0.9798817590586121, 0.9783734932725165, 0.9793045288230078, 0.9792511140525026, 0.9796407651188922, 0.979700557167375, 0.9788980404396258, 0.9789992741625003, 0.9791125008846209, 0.9801574806127901, 0.9790646710379053, 0.979338169935927, 0.9783301517917318, 0.9799960851669312, 0.9800192495850562, 0.9792832160247441, 0.9785300299446696, 0.9796034249563838, 0.9798186198269755, 0.9790388981571096, 0.9782191828181329, 0.9793912173365038, 0.9800218603523, 0.979554852303805, 0.9792062558482527, 0.9793997959009583, 0.9794128694308034, 0.9803390639109016, 0.978918592833467, 0.979379635808338, 0.9790534495469346, 0.9799131399089297, 0.9799135139830711, 0.9801077940133, 0.9797140032000617, 0.9794595871114145, 0.979323208751075, 0.9797046582602449, 0.9792234557165203, 0.979886245853243, 0.9795526075446962, 0.9793893377475035, 0.9786701218105578, 0.9794091540396946, 0.9798795222607564, 0.9797375401210282, 0.9795563559331039, 0.9787048871362146, 0.9798279483205196, 0.9797330770006498, 0.9796867365367802, 0.9799624667854845, 0.9796213623928479, 0.9796908434958366, 0.9796919495862482, 0.9799759182653444, 0.9794752613521838, 0.9794390231947069, 0.9800087893365347, 0.9790672974133953, 0.9804832675544993, 0.9797405361710199, 0.9791644339192731, 0.9801608496893898, 0.9800752994255777, 0.9796131322379691, 0.9787254255978629, 0.9792010423797715, 0.9786230465020781, 0.9801193772175191, 0.9798925964191215, 0.9798959508302552, 0.9799520076892497, 0.979826828926435, 0.9790773802356686, 0.9792425049000758, 0.9802934866797945, 0.9794707979175453, 0.980344293930409, 0.9799549893880352, 0.9800973418307849, 0.9802213815058891, 0.9800196195738178, 0.9798634506906063, 0.9799859850603583, 0.979724469105263, 0.9797285779498792, 0.9798821271618138, 0.9795305610541091, 0.9799168807550976, 0.979761832942024, 0.9799456486383515, 0.9796325616761126, 0.9795174895145772, 0.979643765673277, 0.9795503598524942, 0.9795305793859418, 0.980312913918118, 0.9803461581206698, 0.9800065440536593, 0.980195216114366, 0.9806315969079995, 0.9794625537257203, 0.9797588574236852, 0.9798802734468859, 0.9798059187999718, 0.9801507576488233, 0.9794726528895131, 0.9786966674566688, 0.9798414195987587, 0.9797734013760236, 0.9792780037085494, 0.9794222104947471, 0.9799635876461157, 0.9796785212568743, 0.9797581013141491, 0.9802060648929884, 0.9795735318128589, 0.978945875838268, 0.9794536039154643, 0.979734162035642, 0.978884589588286, 0.9798223560639969, 0.9805030750474109, 0.9795735182996793, 0.9799516279584285, 0.9800603341553459, 0.9796736559884619], 'val_mDice': [0.05981859911776594, 0.0873729163790629, 0.280601450135712, 0.46086432492795854, 0.5516639872468838, 0.6127545004988806, 0.6460579424206109, 0.6668309799816152, 0.6826464814544771, 0.6935459714782259, 0.6956589867234858, 0.7030117703447861, 0.7170959230257045, 0.7164840916129114, 0.71899267093578, 0.7272158955438275, 0.7267885918357461, 0.7325334890446051, 0.7358675285885748, 0.7369470129113624, 0.7421518574699576, 0.7396882727401747, 0.7411898371414896, 0.7438981317258677, 0.7465813109661028, 0.7543467461329563, 0.7447275336052706, 0.7480402786828093, 0.7512620998602131, 0.7504408265249591, 0.7534841752638926, 0.7485163425519303, 0.754747074182507, 0.7553281972613611, 0.7570209552407893, 0.7572304902889607, 0.7623995225123655, 0.7602170727793487, 0.7586536411660836, 0.7611771627134514, 0.7597291012849455, 0.7606084551040234, 0.7613303795430698, 0.7574958637854337, 0.7628360729016402, 0.7647867728620505, 0.7685233204771219, 0.7691478008545136, 0.7672937341649838, 0.7702966614222065, 0.7688014895928555, 0.7655087780659144, 0.7660162720194181, 0.764508907216714, 0.7696631157963892, 0.770204442247146, 0.7661684263360102, 0.7726686971887344, 0.7716004181201512, 0.7670426937523961, 0.7711436160838457, 0.7754342495242824, 0.7705881217452051, 0.7702059765691707, 0.7712416672958012, 0.7702292428075325, 0.7704707196601036, 0.7701545551497823, 0.7693607203780243, 0.7692150955459983, 0.7736564780371051, 0.7698531002068143, 0.7698867622703994, 0.7758577811697036, 0.7763538485252166, 0.7741705803544324, 0.775034026018555, 0.7770335577912825, 0.774927249901743, 0.7777228020197059, 0.7758001789895848, 0.7760476251059132, 0.7776821848587747, 0.7726775026489017, 0.7737562687828586, 0.7765904390539562, 0.7757152321049534, 0.7791163213223691, 0.7774026615967441, 0.7792532740452168, 0.7783893653713756, 0.7756850949909231, 0.776927965806113, 0.7794524464749704, 0.7787243389706201, 0.7813291727763279, 0.780539670916559, 0.7812857171144133, 0.7817632674332872, 0.7821785364502972, 0.778777689213074, 0.7817765367471061, 0.7815748203827333, 0.7821505504458985, 0.786055799423077, 0.7838990054566328, 0.7781505729276392, 0.7834521594910714, 0.7849107787982023, 0.7826932062164133, 0.7803086561980692, 0.7837852144283234, 0.7820876153785021, 0.7888868045723082, 0.7839432091620768, 0.7824475191180023, 0.7857201537264671, 0.7854264733661457, 0.784846389021312, 0.7877131572716685, 0.7826516081452998, 0.7855987316275732, 0.7835213909668747, 0.7869917414100392, 0.7878060175371087, 0.7816935859371782, 0.7840244028396472, 0.7887996129285682, 0.7845590076882307, 0.7849212972895752, 0.7879769613537512, 0.7813335909575276, 0.7895567031652521, 0.7871659834900304, 0.7880710516327923, 0.7858772222312767, 0.7847298367999769, 0.7871389317805821, 0.7851923285134019, 0.7878947198286207, 0.7880561666036113, 0.7883509079056591, 0.7859389936986833, 0.7874827941188913, 0.7916506831591494, 0.7843987148759235, 0.7890410578313737, 0.7901080262262917, 0.7887074417305328, 0.7912966530436162, 0.7893108133691475, 0.791564614471107, 0.7897644845379258, 0.7911987756174352, 0.7883875461161032, 0.7908931326363334, 0.7876557113835179, 0.7877094332907866, 0.7927555061601796, 0.7934285319542843, 0.7930642062624644, 0.7939507737101067, 0.7870568634127062, 0.7930794830690997, 0.7925568815903421, 0.7922787490335207, 0.7936610574252995, 0.7939038825579813, 0.7869631885853718, 0.7914807147007625, 0.7890185838424468, 0.7922625972223198, 0.7912121576458373, 0.7905018803105412, 0.7920944171127828, 0.7954779241122764, 0.7948409043422692, 0.797344246208144, 0.7932683325819475, 0.7943327770501323, 0.7959776427498602, 0.7961720912234436, 0.7927370271489993, 0.7930106016789253, 0.7955882091513836, 0.7949278881554025, 0.7937020746391981, 0.7928806884846076, 0.7940211090765016, 0.7953970699402486, 0.796204171302239, 0.797231397121778, 0.7960545939175651, 0.7952812392598506, 0.7933103441144544, 0.794823904867122, 0.7928074300812921, 0.7968084361724987, 0.7973515286596462, 0.7953028803550506, 0.7966408513761363, 0.7953000527484136, 0.7946880812804183, 0.7972641438507657, 0.7907213326707246, 0.7951010408007617, 0.7958144017598332, 0.7947867648882271, 0.7977762967086215, 0.7987809954292955, 0.7947463138660773, 0.7946012727196061, 0.7954689655027406, 0.7977033065785842, 0.7981551757805796, 0.7973178793759673, 0.7963770644735997, 0.7988407601371591, 0.7970477784664644, 0.7967229729587039, 0.8002928168576715, 0.7977137982531046, 0.7963304004266937, 0.7958730536730722, 0.7971679389162516, 0.7969791711110012, 0.7887959051844525, 0.7971829965789204, 0.7988326414398234, 0.7985005554708738, 0.7969671216497103, 0.7986335306050278, 0.7939780684472807, 0.7894272988626114, 0.7946778545270695, 0.7875791889711926, 0.7978648920469837, 0.7954505854834572, 0.800653958676779, 0.7965347173972372, 0.795478671841546, 0.7983749355917865, 0.7974586191412015, 0.7965730928369062, 0.7970676760472396, 0.7992494779856637, 0.8003380168720373, 0.8006757520833209, 0.7983102880169511, 0.796714993269037, 0.8008151342453982, 0.7981197402431173, 0.799248690240622, 0.7970161178200023, 0.79724814351288, 0.8016387447321981, 0.7977461651465176, 0.7980334264322827, 0.7998597407173398, 0.79830076614876, 0.7962688225434409, 0.8002292082263631, 0.8008968267583261, 0.795600259660208, 0.8015212999077379, 0.7966592302431331, 0.7909103871858393, 0.7958538207521757, 0.7978792816976671, 0.8003799391547071, 0.7964925780656678, 0.7981587711243722, 0.8005029287824312, 0.8009874634248301, 0.7981924347592574, 0.799776869624696, 0.7942674199809927, 0.7971007151427714, 0.7987887866677216, 0.7948941518217063, 0.7982165612319652, 0.8001369209616381, 0.7966117544626728, 0.8001306704351898, 0.7987566308522686, 0.7985239459466851], 'loss': [48.365423058078896, 7.055251774481466, 5.029917131858001, 3.913634542321998, 3.2437920824617894, 2.8447751494012143, 2.5508495200940753, 2.3588166911782835, 2.218372310949472, 2.097598800349143, 1.9975854795812138, 1.91460217070036, 1.8514772666417911, 1.7947007742653278, 1.7330535143684305, 1.6976233217989898, 1.6437562165241837, 1.614429667559803, 1.5895147885429644, 1.564453800907253, 1.5246133726812154, 1.5020549006313557, 1.477674810865146, 1.447753143609905, 1.431898524888262, 1.4196999434087951, 1.4018031279170022, 1.3836554233611882, 1.370342244272125, 1.352081531724478, 1.3427565487904156, 1.3216225096032495, 1.3138366872816416, 1.300813087006171, 1.2893705885649436, 1.286006478654151, 1.2686714931514866, 1.2579397075303782, 1.254817850984137, 1.2395065124228895, 1.2335859865788563, 1.2206243103040397, 1.2176152890998329, 1.2078506312542596, 1.2023569534987677, 1.1963744351556824, 1.1881199809656753, 1.1830845996913706, 1.1724992667259084, 1.17100671364913, 1.1616055445236502, 1.1562149320357364, 1.1527355123247727, 1.148059836916832, 1.1385853854778771, 1.1395358654310608, 1.137324436253471, 1.126834090303975, 1.1202643795669975, 1.1202464619836379, 1.112363038407521, 1.114153936638049, 1.1072682242503638, 1.1047774503565921, 1.0999277752764745, 1.0977024292587112, 1.0936963464160203, 1.0887849929228228, 1.0873641562110303, 1.0832636955639754, 1.0781668803603197, 1.07483986185515, 1.071341348863633, 1.0701310541056153, 1.0667875925383914, 1.0659774513208873, 1.062867254551209, 1.057821238105715, 1.0548147315120164, 1.0549502113548033, 1.0485710900254965, 1.0485704643941958, 1.0470916281216314, 1.0429362257121704, 1.0445521135502016, 1.0374180508856143, 1.0373672964012466, 1.033590169745071, 1.034274134153713, 1.0287691537096026, 1.0281011675841663, 1.0259397493311746, 1.0269364878999456, 1.023930740514823, 1.020391573505081, 1.0168872267753224, 1.0171229786202398, 1.014061979781123, 1.0115169907430897, 1.0121178390767276, 1.0087807148261554, 1.009960387440706, 1.0061670760852595, 1.0037159357407595, 1.0045588064983215, 1.0031762006664233, 1.00203344721564, 0.9966421738003732, 0.997084266309991, 0.9938434602827798, 0.9952054211246392, 0.9916271009752827, 0.9882151315523127, 0.9852003445173448, 0.9892420283555805, 0.9862479285477308, 0.9860877141781659, 0.9846114492711541, 0.9806952246193749, 0.9829041589446867, 0.9785406320415777, 0.9795702751488757, 0.9756377616104305, 0.9760898699560545, 0.9754746490501265, 0.9724554043040291, 0.9722501193803376, 0.9690419359551745, 0.9698481972705532, 0.9696701102153349, 0.9683136853944505, 0.9666731880881108, 0.9630748563669245, 0.9633319660009625, 0.9635133267942765, 0.9610677120682097, 0.9612009407475588, 0.9580964112244604, 0.9573972430600415, 0.9545348065919662, 0.9549614577521124, 0.9566606481670044, 0.9545301177152408, 0.9520970905956759, 0.9496618901667601, 0.9468439431112742, 0.9480083188271043, 0.9464246319522256, 0.9460212448259874, 0.946011990199277, 0.9429531853546117, 0.9424521185168622, 0.9396816248454839, 0.9420909974038838, 0.9421261139493374, 0.9374876732759282, 0.9388780328589257, 0.9355332517856668, 0.9323757984372931, 0.933724023724567, 0.9312745533479754, 0.9289107875425391, 0.928326029976339, 0.9281886538443691, 0.9306882539679028, 0.9266368870910104, 0.9263426256878111, 0.926123827835327, 0.9246895014204861, 0.9250770088087048, 0.9208331452474987, 0.9217404840960908, 0.9217817319950672, 0.9218513579260846, 0.9174413811691957, 0.918641338395302, 0.9150899504686565, 0.917491517941153, 0.9168000384459564, 0.9141806670976426, 0.9145726144844738, 0.9114043167502379, 0.9092548903902538, 0.9109303779087563, 0.9118373042011794, 0.9074784283470984, 0.9061180276765383, 0.9079917193352981, 0.9080592832893581, 0.9041619381321004, 0.9033097002306004, 0.9052342069112422, 0.9055946059294421, 0.9026555401720618, 0.9021642627080634, 0.8998279776104526, 0.9027744091443174, 0.9014447876542115, 0.8992941804407564, 0.8953736668881842, 0.8954268942349126, 0.8972515421684948, 0.8944752641199556, 0.8958292762440456, 0.8950210576790292, 0.8953257097366933, 0.892869887756453, 0.8958442096567185, 0.8915853008765122, 0.8920225500880511, 0.8914768430848443, 0.893158825856859, 0.8899296812409814, 0.8895641911324375, 0.8890045609089516, 0.8905030514944254, 0.8906549462752088, 0.8910865574680705, 0.8846307184518503, 0.888034855666719, 0.887438395771921, 0.8844209946033957, 0.8870696549110592, 0.8855710074085815, 0.8826214425679356, 0.8851603892979639, 0.8832411738336131, 0.8852433562698688, 0.8798517914004573, 0.8826365251047047, 0.8810198105375622, 0.8793630586867424, 0.8803365686406404, 0.8811234224365148, 0.8808086869143245, 0.8811012044369764, 0.88277352360461, 0.8800744520664623, 0.878651027332452, 0.876823084579729, 0.8797756678372611, 0.8767546169453446, 0.8780921097010769, 0.8760485904433096, 0.8786602517533966, 0.8776659666996982, 0.8751667302841809, 0.8745747664681343, 0.8776485248736482, 0.8783473799479553, 0.8738209114245803, 0.8752838699898406, 0.8746031352584708, 0.8726465913527242, 0.8728020676584586, 0.8737947095574902, 0.8697226164139521, 0.8711809704686608, 0.8736136947626749, 0.8696624918250612, 0.8702332308443448, 0.8694908757597564, 0.8701621671548686, 0.8692399197361212, 0.8711934255237435, 0.8678139194731419, 0.868747733621495, 0.8678819306855065, 0.8676122453892923, 0.8670222925903465, 0.868124079730681, 0.8665815003894547, 0.8665718804769491, 0.8659662119149982, 0.8670116971245914, 0.8639555372491612, 0.8676301550897568, 0.8643956268885362, 0.8663611486077962, 0.8627937800732708, 0.8647130716159571, 0.8659729614818614, 0.8638732206160934, 0.8649641472658234, 0.8625439265510899, 0.8612073179934131], 'acc': [0.8414625101438988, 0.8943086724043504, 0.905857032318191, 0.9229795481992964, 0.9332844159826408, 0.9398760398688899, 0.9446868735196984, 0.9475769091939885, 0.9500507660291496, 0.9521160219649257, 0.9536588259536615, 0.9551419154320225, 0.95586632740383, 0.9567280484130125, 0.957639638499677, 0.9580867064808463, 0.9589087502368833, 0.9594253676387949, 0.9598046341262018, 0.960236241662576, 0.9610322538886454, 0.9613395229804607, 0.9617200594088504, 0.9621349028801679, 0.9623600092413417, 0.9625690816111188, 0.9628280039450572, 0.9631054312910109, 0.9632963582323671, 0.9635900070464375, 0.9636975677328593, 0.9640836012836768, 0.9642503057114822, 0.9644341110091319, 0.9646461155908792, 0.9647204053611318, 0.9650205131577987, 0.965171958208624, 0.9652372785191922, 0.9655079146221248, 0.9655669469471265, 0.9657552177638113, 0.9658160799793037, 0.9659527794721232, 0.9660529586072717, 0.9661432404228009, 0.9662127915593027, 0.9662883628879098, 0.9664248304952343, 0.9664340821553606, 0.9665934788785411, 0.9666777493927189, 0.9667035617224133, 0.9667473940981046, 0.9668734632130628, 0.9668650326248408, 0.9669160405226859, 0.9670009509641603, 0.9670964365462389, 0.9671192334436289, 0.9672394904233172, 0.967202968226904, 0.9673149487517643, 0.9673149788255486, 0.9673554806631061, 0.9673781016771509, 0.9674473711741324, 0.9675173701598561, 0.967524504062844, 0.9675408945692653, 0.9676751050859829, 0.9676976459992561, 0.9677378259639329, 0.9677384175230096, 0.9678041441656192, 0.9678053235972895, 0.967909642251171, 0.9679833431919642, 0.9679680535342288, 0.9679740468201055, 0.9680579599763768, 0.9680820008713864, 0.9680704135886616, 0.9681045111529535, 0.9681266016793607, 0.9681762394952603, 0.9682104342819238, 0.9682319240945376, 0.9682190128244059, 0.9683148921372924, 0.9682809333755436, 0.9683130739121665, 0.9683386504899081, 0.9683842807475769, 0.9684194677321566, 0.9684737066630839, 0.9684471544631024, 0.968507994940642, 0.9685516880458557, 0.9685295955637225, 0.9685464234028397, 0.9685438347477107, 0.968592907031885, 0.968630736367067, 0.9686132972646351, 0.9686365441529465, 0.9686732390831464, 0.9687384451937228, 0.9687200127235387, 0.9687396453734847, 0.9687135189158238, 0.9687970091523789, 0.9687998331005251, 0.9688505310548173, 0.9688802939087376, 0.9688725790411877, 0.9688828402937919, 0.9688507567581788, 0.9689407417113213, 0.9689294849132701, 0.9689963831376504, 0.9689645194982128, 0.9690269778619612, 0.9690325946256179, 0.9689886244732399, 0.9690761036226394, 0.9690575950591219, 0.9691139428804304, 0.9691123165744018, 0.9690810917824121, 0.9691279072789372, 0.9691738944175793, 0.969188382188721, 0.969211929742886, 0.9691939896266849, 0.9691986519961261, 0.9692254743322405, 0.9693058155502169, 0.9692976557430375, 0.9692986489698096, 0.9693164074014786, 0.9693028014137307, 0.9693192230467887, 0.9693794960128805, 0.9693850061594825, 0.9694282271458766, 0.9694131645743089, 0.9694031403994717, 0.9694343476679177, 0.9694100169745312, 0.9694961621029304, 0.9695059299936929, 0.9695427099169786, 0.9695374348294267, 0.9694966089382943, 0.9695550840494047, 0.9695383209412138, 0.9695467525162974, 0.9696332801870728, 0.9695888390857113, 0.9696273970757522, 0.9696342960366737, 0.9697059546657528, 0.9697149601053936, 0.9696615997033164, 0.9697192955506622, 0.9697277218674829, 0.9697215627408533, 0.969789848204774, 0.969752791198329, 0.9698122281636093, 0.9697647166266534, 0.9697965997303626, 0.9697513461328946, 0.9698297299032608, 0.9698200287411446, 0.9698628662432945, 0.9698981365052006, 0.969871028954067, 0.9699064490569687, 0.9698807402877574, 0.9699126969111607, 0.9699289365627285, 0.9699582093963385, 0.9699356681591107, 0.9699961431468836, 0.9700156032160216, 0.9699738263082003, 0.9699957474543508, 0.9700214766177034, 0.9700606112619105, 0.9700567826255987, 0.9700351063831182, 0.9700298737396166, 0.9700688169326608, 0.9700568771843437, 0.9700283871300057, 0.9700686109875296, 0.9700858906653091, 0.9701642670867056, 0.9701429656776819, 0.9701217314832362, 0.9701938045522203, 0.970199077486074, 0.9702003929426767, 0.9701913317708483, 0.9701979649450271, 0.9701903704116092, 0.9702278253032796, 0.9702200804219369, 0.9702278807055134, 0.9702304576052903, 0.970248824432676, 0.9702541418083, 0.9702549420691216, 0.9702479480635231, 0.9702630220184952, 0.9702303810590203, 0.9703151743999096, 0.9702856059697355, 0.9702962930246518, 0.9703594112928966, 0.9703551661529692, 0.9703269013250596, 0.9703816668681677, 0.9703072816842899, 0.970339897603275, 0.9702817960058063, 0.9704179228039046, 0.9703783904364494, 0.9704129478272839, 0.9704088797015432, 0.9704095251240696, 0.9704324817070753, 0.9704270480677488, 0.9704112453529323, 0.9703689560569363, 0.9704154720484451, 0.9704405286160804, 0.9704784197006064, 0.9704468164006751, 0.9704733395377305, 0.970436848824904, 0.9704937948613562, 0.9704624591239187, 0.9704792337025018, 0.9704873945145402, 0.970494501211342, 0.9704849261575481, 0.9704492687279143, 0.9705158659204924, 0.9705012925863445, 0.9704977427909457, 0.9705018295410757, 0.9705606800555631, 0.9705250118844349, 0.9705506452292191, 0.970549637577469, 0.9705220687299986, 0.9705542073018969, 0.9705479063161527, 0.9705696439750241, 0.9705750055554516, 0.97058273013564, 0.9705650991681034, 0.9705935145779049, 0.970593138952559, 0.970616942916781, 0.9706088531677811, 0.970636744731014, 0.9706100309136877, 0.9706299392759851, 0.970629962111784, 0.9706287421587939, 0.9706119878663672, 0.9706517638146779, 0.9705955445853437, 0.9706183222794256, 0.9706691599301355, 0.9706672438557258, 0.970644841948967, 0.9706259618365323, 0.9706286118570593, 0.9706471327668461, 0.9706488859340917, 0.9706764359981951], 'mDice': [0.02908506816703284, 0.07635142953313595, 0.17766956161162278, 0.2977157278069552, 0.39219574483506037, 0.4557110506556221, 0.5031574555230689, 0.5363144442813277, 0.55945503301058, 0.5795236803806096, 0.5954187999919427, 0.6105236259345937, 0.6216422599153019, 0.6315029388848827, 0.6425967874269494, 0.6491249433899674, 0.6587549068806462, 0.6639265326475511, 0.6678358270167992, 0.6726326377952977, 0.6794124061439437, 0.6842326324621124, 0.6887584136498364, 0.6941522617301933, 0.6976503360032087, 0.6995557827706198, 0.7029222988901161, 0.7063219802625934, 0.7089957440093217, 0.7123837568014703, 0.7142811617141677, 0.7181586200491087, 0.7195250043768953, 0.7218571658175494, 0.7244559576716434, 0.7248654437889409, 0.7280693667853199, 0.7304886738131315, 0.7309080179675549, 0.7342147606679342, 0.7349802291505464, 0.7376305723333088, 0.7383770875917348, 0.7402592987702505, 0.7412604513342791, 0.7424038960981558, 0.7441778426009331, 0.7448971232790321, 0.7472153366388921, 0.7475887678102784, 0.7493831771086341, 0.7504478572155364, 0.7511925209145092, 0.7518569062546944, 0.7535232931568976, 0.7538713992352899, 0.7539103982385107, 0.7561824191865033, 0.7572747108158939, 0.7575457114277305, 0.7589038118377353, 0.758531388339096, 0.7598883484589436, 0.760697409692837, 0.7615840254933322, 0.7621239176676465, 0.7626579160971309, 0.763700049789531, 0.764272467400514, 0.7644004278853449, 0.7660081568250694, 0.7664410699284326, 0.7669407544489518, 0.7672667069047814, 0.767981614069948, 0.7680747170170182, 0.7687009693895839, 0.7698296127582819, 0.7704534252158686, 0.7705143818011324, 0.7719594354262608, 0.771803860383367, 0.7721239193223346, 0.7729630714201686, 0.7728210266466368, 0.7741562162832143, 0.7740551517171832, 0.7748139399622138, 0.774851628566123, 0.775810813985997, 0.7760811274031629, 0.7764550262329941, 0.7760822363867085, 0.7768049806462916, 0.7775073590687048, 0.7783244486262085, 0.7784841978389779, 0.7787250376486777, 0.7795455054616264, 0.7793061500148525, 0.7802220026600025, 0.7794754780638854, 0.7806747955726657, 0.7812736910030565, 0.7812199812519502, 0.7811984586749293, 0.7816145863652283, 0.7826262382884839, 0.7825910209391607, 0.7834172703936538, 0.7829975630501413, 0.7836704820980934, 0.7844691906851939, 0.7850737383522689, 0.7841266009835303, 0.7848180805177215, 0.7849375972041822, 0.7853784151774613, 0.7860982538883367, 0.7856098928615195, 0.7865211516146294, 0.7862869574063666, 0.78706482559582, 0.7872319935666472, 0.7872881934246967, 0.7879375967530214, 0.7879160908026698, 0.7885636847306315, 0.7883869816664091, 0.7882395528126036, 0.7886984029021874, 0.789136009477632, 0.7896874616504957, 0.7898435629666178, 0.7899052500094881, 0.7901988641034254, 0.7905553034726785, 0.7910491630167278, 0.7914148504256147, 0.7920101434899759, 0.7917786753715768, 0.7913559769013526, 0.7918660884102435, 0.7925285425911336, 0.7931559446905335, 0.7934796983765809, 0.7933474472182055, 0.7937919328523615, 0.7938963161681788, 0.7937397584120393, 0.7945308536913536, 0.7945043377802852, 0.7950283933955274, 0.7945794891587598, 0.7949838269987294, 0.7953907820738217, 0.7954956633198406, 0.7960718896319298, 0.7967088500084751, 0.7965413169618234, 0.797000379283657, 0.7975940746849745, 0.7977014543677688, 0.797908512282927, 0.7970800946213675, 0.7982037670568781, 0.7982614571240341, 0.7982330214760694, 0.7986250099271445, 0.7982816257568474, 0.7993821278803123, 0.7991876286317128, 0.7992475683895128, 0.7992936920780137, 0.8001516413649066, 0.799544135614488, 0.8005990188186947, 0.8002620924641577, 0.8005816738040739, 0.8009764583342017, 0.8010833375491504, 0.8014826817611138, 0.8019248070621351, 0.8015749548505113, 0.8015055236791929, 0.8024136796762392, 0.8025914286398502, 0.8023952235003782, 0.8024899408061518, 0.8032146704454294, 0.8030745461796858, 0.8029930696118599, 0.802772115760598, 0.8034461199334401, 0.8037176898954667, 0.8038777845668107, 0.8035386398102435, 0.8037568629642875, 0.8043213897193902, 0.8048488325198255, 0.805136307086386, 0.8046526046705368, 0.8054026962406448, 0.8050126031938769, 0.8048530791085498, 0.8050860761426294, 0.8055611505379321, 0.8049573959386869, 0.8058404602130258, 0.8057445482437171, 0.8059230397532399, 0.8054926354681969, 0.8062359764815805, 0.8062845740642973, 0.8064201923976708, 0.8062741086333143, 0.8061120388738331, 0.806045957305976, 0.807085812400784, 0.8066657448527641, 0.8065767841120502, 0.8075343920290545, 0.8068800527876333, 0.8071445606716038, 0.8078380837623345, 0.8071189376543719, 0.8076952164171086, 0.8073571705145071, 0.8085032309507257, 0.8077439475471052, 0.8083180874476621, 0.8087693774569983, 0.8082094032807269, 0.8083591764049185, 0.8083299913150088, 0.8083143614719597, 0.8079982957772295, 0.8082563636959376, 0.8085391641227068, 0.80909834921459, 0.8085806902026885, 0.8090925525211555, 0.8087546663101688, 0.8093320406183347, 0.808620635738227, 0.8088773834280012, 0.8094011765890335, 0.8097344067536929, 0.8087510590518249, 0.8089380930668815, 0.8096943269959694, 0.8092400773705045, 0.8095252383973709, 0.809972945183994, 0.8097570918807505, 0.809526942950431, 0.8107016660157966, 0.8104187920843311, 0.8100306195543502, 0.810437893451751, 0.8105277750453821, 0.8106984452352773, 0.8106147336777464, 0.8108558119720161, 0.8104061709755255, 0.8110499190165936, 0.8108154185992543, 0.8110693608992809, 0.8110706605510991, 0.8110680089287966, 0.810872087804512, 0.8111633790120656, 0.8112040849020195, 0.8115311962753949, 0.8114053200753945, 0.8117299219375468, 0.8111728183528832, 0.8117868278336258, 0.8113318994153123, 0.8122822437725036, 0.8117798409460295, 0.8115688128114065, 0.8118435060556195, 0.8116939593186935, 0.8123063922234363, 0.8124294608949889]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:01<00:25,  1.83s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:22,  1.75s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:21,  1.76s/it]predicting test subjects:  27%|██▋       | 4/15 [00:06<00:19,  1.75s/it]predicting test subjects:  33%|███▎      | 5/15 [00:08<00:18,  1.85s/it]predicting test subjects:  40%|████      | 6/15 [00:11<00:17,  1.94s/it]predicting test subjects:  47%|████▋     | 7/15 [00:12<00:14,  1.76s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:14<00:13,  1.88s/it]predicting test subjects:  60%|██████    | 9/15 [00:16<00:11,  1.84s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:17<00:08,  1.71s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:19<00:06,  1.66s/it]predicting test subjects:  80%|████████  | 12/15 [00:21<00:05,  1.76s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:23<00:03,  1.84s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:25<00:01,  1.82s/it]predicting test subjects: 100%|██████████| 15/15 [00:27<00:00,  1.85s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<19:30,  2.20s/it]predicting train subjects:   0%|          | 2/532 [00:03<17:53,  2.03s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:17,  1.96s/it]predicting train subjects:   1%|          | 4/532 [00:07<16:24,  1.86s/it]predicting train subjects:   1%|          | 5/532 [00:08<15:57,  1.82s/it]predicting train subjects:   1%|          | 6/532 [00:10<15:14,  1.74s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<15:01,  1.72s/it]predicting train subjects:   2%|▏         | 8/532 [00:13<14:41,  1.68s/it]predicting train subjects:   2%|▏         | 9/532 [00:15<15:04,  1.73s/it]predicting train subjects:   2%|▏         | 10/532 [00:17<14:48,  1.70s/it]predicting train subjects:   2%|▏         | 11/532 [00:18<13:47,  1.59s/it]predicting train subjects:   2%|▏         | 12/532 [00:20<14:48,  1.71s/it]predicting train subjects:   2%|▏         | 13/532 [00:21<13:44,  1.59s/it]predicting train subjects:   3%|▎         | 14/532 [00:23<13:12,  1.53s/it]predicting train subjects:   3%|▎         | 15/532 [00:24<13:24,  1.56s/it]predicting train subjects:   3%|▎         | 16/532 [00:26<13:44,  1.60s/it]predicting train subjects:   3%|▎         | 17/532 [00:28<13:16,  1.55s/it]predicting train subjects:   3%|▎         | 18/532 [00:29<14:01,  1.64s/it]predicting train subjects:   4%|▎         | 19/532 [00:31<13:06,  1.53s/it]predicting train subjects:   4%|▍         | 20/532 [00:32<13:11,  1.55s/it]predicting train subjects:   4%|▍         | 21/532 [00:34<14:00,  1.64s/it]predicting train subjects:   4%|▍         | 22/532 [00:36<13:41,  1.61s/it]predicting train subjects:   4%|▍         | 23/532 [00:37<13:59,  1.65s/it]predicting train subjects:   5%|▍         | 24/532 [00:39<13:14,  1.56s/it]predicting train subjects:   5%|▍         | 25/532 [00:41<14:29,  1.72s/it]predicting train subjects:   5%|▍         | 26/532 [00:42<13:51,  1.64s/it]predicting train subjects:   5%|▌         | 27/532 [00:44<15:02,  1.79s/it]predicting train subjects:   5%|▌         | 28/532 [00:46<14:27,  1.72s/it]predicting train subjects:   5%|▌         | 29/532 [00:48<14:51,  1.77s/it]predicting train subjects:   6%|▌         | 30/532 [00:49<13:45,  1.64s/it]predicting train subjects:   6%|▌         | 31/532 [00:51<13:29,  1.62s/it]predicting train subjects:   6%|▌         | 32/532 [00:52<13:19,  1.60s/it]predicting train subjects:   6%|▌         | 33/532 [00:54<12:45,  1.53s/it]predicting train subjects:   6%|▋         | 34/532 [00:56<14:02,  1.69s/it]predicting train subjects:   7%|▋         | 35/532 [00:57<13:48,  1.67s/it]predicting train subjects:   7%|▋         | 36/532 [00:59<13:51,  1.68s/it]predicting train subjects:   7%|▋         | 37/532 [01:01<13:42,  1.66s/it]predicting train subjects:   7%|▋         | 38/532 [01:03<14:24,  1.75s/it]predicting train subjects:   7%|▋         | 39/532 [01:04<13:55,  1.69s/it]predicting train subjects:   8%|▊         | 40/532 [01:06<13:39,  1.66s/it]predicting train subjects:   8%|▊         | 41/532 [01:08<13:47,  1.69s/it]predicting train subjects:   8%|▊         | 42/532 [01:09<13:58,  1.71s/it]predicting train subjects:   8%|▊         | 43/532 [01:11<13:05,  1.61s/it]predicting train subjects:   8%|▊         | 44/532 [01:12<12:35,  1.55s/it]predicting train subjects:   8%|▊         | 45/532 [01:14<12:27,  1.53s/it]predicting train subjects:   9%|▊         | 46/532 [01:15<12:49,  1.58s/it]predicting train subjects:   9%|▉         | 47/532 [01:17<13:56,  1.72s/it]predicting train subjects:   9%|▉         | 48/532 [01:19<14:03,  1.74s/it]predicting train subjects:   9%|▉         | 49/532 [01:21<13:27,  1.67s/it]predicting train subjects:   9%|▉         | 50/532 [01:23<14:01,  1.75s/it]predicting train subjects:  10%|▉         | 51/532 [01:24<13:49,  1.73s/it]predicting train subjects:  10%|▉         | 52/532 [01:26<13:34,  1.70s/it]predicting train subjects:  10%|▉         | 53/532 [01:27<13:06,  1.64s/it]predicting train subjects:  10%|█         | 54/532 [01:29<13:33,  1.70s/it]predicting train subjects:  10%|█         | 55/532 [01:31<13:45,  1.73s/it]predicting train subjects:  11%|█         | 56/532 [01:33<13:31,  1.71s/it]predicting train subjects:  11%|█         | 57/532 [01:34<13:18,  1.68s/it]predicting train subjects:  11%|█         | 58/532 [01:36<13:29,  1.71s/it]predicting train subjects:  11%|█         | 59/532 [01:38<14:24,  1.83s/it]predicting train subjects:  11%|█▏        | 60/532 [01:39<13:09,  1.67s/it]predicting train subjects:  11%|█▏        | 61/532 [01:41<12:28,  1.59s/it]predicting train subjects:  12%|█▏        | 62/532 [01:43<13:08,  1.68s/it]predicting train subjects:  12%|█▏        | 63/532 [01:45<13:39,  1.75s/it]predicting train subjects:  12%|█▏        | 64/532 [01:46<12:50,  1.65s/it]predicting train subjects:  12%|█▏        | 65/532 [01:48<12:53,  1.66s/it]predicting train subjects:  12%|█▏        | 66/532 [01:50<14:07,  1.82s/it]predicting train subjects:  13%|█▎        | 67/532 [01:52<14:24,  1.86s/it]predicting train subjects:  13%|█▎        | 68/532 [01:54<13:55,  1.80s/it]predicting train subjects:  13%|█▎        | 69/532 [01:55<13:10,  1.71s/it]predicting train subjects:  13%|█▎        | 70/532 [01:57<12:34,  1.63s/it]predicting train subjects:  13%|█▎        | 71/532 [01:58<11:59,  1.56s/it]predicting train subjects:  14%|█▎        | 72/532 [01:59<11:49,  1.54s/it]predicting train subjects:  14%|█▎        | 73/532 [02:01<12:04,  1.58s/it]predicting train subjects:  14%|█▍        | 74/532 [02:03<13:09,  1.72s/it]predicting train subjects:  14%|█▍        | 75/532 [02:06<15:12,  2.00s/it]predicting train subjects:  14%|█▍        | 76/532 [02:07<13:59,  1.84s/it]predicting train subjects:  14%|█▍        | 77/532 [02:09<13:44,  1.81s/it]predicting train subjects:  15%|█▍        | 78/532 [02:11<13:23,  1.77s/it]predicting train subjects:  15%|█▍        | 79/532 [02:12<13:00,  1.72s/it]predicting train subjects:  15%|█▌        | 80/532 [02:14<12:45,  1.69s/it]predicting train subjects:  15%|█▌        | 81/532 [02:16<12:48,  1.70s/it]predicting train subjects:  15%|█▌        | 82/532 [02:17<12:42,  1.69s/it]predicting train subjects:  16%|█▌        | 83/532 [02:19<12:03,  1.61s/it]predicting train subjects:  16%|█▌        | 84/532 [02:20<11:34,  1.55s/it]predicting train subjects:  16%|█▌        | 85/532 [02:22<11:10,  1.50s/it]predicting train subjects:  16%|█▌        | 86/532 [02:23<10:53,  1.46s/it]predicting train subjects:  16%|█▋        | 87/532 [02:24<10:40,  1.44s/it]predicting train subjects:  17%|█▋        | 88/532 [02:26<10:31,  1.42s/it]predicting train subjects:  17%|█▋        | 89/532 [02:27<10:51,  1.47s/it]predicting train subjects:  17%|█▋        | 90/532 [02:29<11:12,  1.52s/it]predicting train subjects:  17%|█▋        | 91/532 [02:30<11:20,  1.54s/it]predicting train subjects:  17%|█▋        | 92/532 [02:32<11:22,  1.55s/it]predicting train subjects:  17%|█▋        | 93/532 [02:34<11:20,  1.55s/it]predicting train subjects:  18%|█▊        | 94/532 [02:35<11:20,  1.55s/it]predicting train subjects:  18%|█▊        | 95/532 [02:37<11:56,  1.64s/it]predicting train subjects:  18%|█▊        | 96/532 [02:39<12:30,  1.72s/it]predicting train subjects:  18%|█▊        | 97/532 [02:41<12:43,  1.75s/it]predicting train subjects:  18%|█▊        | 98/532 [02:43<12:54,  1.79s/it]predicting train subjects:  19%|█▊        | 99/532 [02:44<12:53,  1.79s/it]predicting train subjects:  19%|█▉        | 100/532 [02:46<13:00,  1.81s/it]predicting train subjects:  19%|█▉        | 101/532 [02:48<12:05,  1.68s/it]predicting train subjects:  19%|█▉        | 102/532 [02:49<11:20,  1.58s/it]predicting train subjects:  19%|█▉        | 103/532 [02:50<10:49,  1.51s/it]predicting train subjects:  20%|█▉        | 104/532 [02:52<10:29,  1.47s/it]predicting train subjects:  20%|█▉        | 105/532 [02:53<10:23,  1.46s/it]predicting train subjects:  20%|█▉        | 106/532 [02:55<10:08,  1.43s/it]predicting train subjects:  20%|██        | 107/532 [02:56<10:11,  1.44s/it]predicting train subjects:  20%|██        | 108/532 [02:57<10:03,  1.42s/it]predicting train subjects:  20%|██        | 109/532 [02:59<09:51,  1.40s/it]predicting train subjects:  21%|██        | 110/532 [03:00<09:53,  1.41s/it]predicting train subjects:  21%|██        | 111/532 [03:02<10:03,  1.43s/it]predicting train subjects:  21%|██        | 112/532 [03:03<10:12,  1.46s/it]predicting train subjects:  21%|██        | 113/532 [03:05<10:46,  1.54s/it]predicting train subjects:  21%|██▏       | 114/532 [03:07<11:03,  1.59s/it]predicting train subjects:  22%|██▏       | 115/532 [03:08<11:14,  1.62s/it]predicting train subjects:  22%|██▏       | 116/532 [03:10<11:21,  1.64s/it]predicting train subjects:  22%|██▏       | 117/532 [03:12<11:19,  1.64s/it]predicting train subjects:  22%|██▏       | 118/532 [03:13<11:22,  1.65s/it]predicting train subjects:  22%|██▏       | 119/532 [03:15<11:25,  1.66s/it]predicting train subjects:  23%|██▎       | 120/532 [03:17<11:16,  1.64s/it]predicting train subjects:  23%|██▎       | 121/532 [03:18<11:06,  1.62s/it]predicting train subjects:  23%|██▎       | 122/532 [03:20<10:58,  1.61s/it]predicting train subjects:  23%|██▎       | 123/532 [03:21<11:04,  1.62s/it]predicting train subjects:  23%|██▎       | 124/532 [03:23<11:00,  1.62s/it]predicting train subjects:  23%|██▎       | 125/532 [03:25<11:18,  1.67s/it]predicting train subjects:  24%|██▎       | 126/532 [03:26<11:22,  1.68s/it]predicting train subjects:  24%|██▍       | 127/532 [03:28<11:31,  1.71s/it]predicting train subjects:  24%|██▍       | 128/532 [03:30<11:24,  1.69s/it]predicting train subjects:  24%|██▍       | 129/532 [03:32<11:21,  1.69s/it]predicting train subjects:  24%|██▍       | 130/532 [03:33<11:21,  1.70s/it]predicting train subjects:  25%|██▍       | 131/532 [03:35<12:05,  1.81s/it]predicting train subjects:  25%|██▍       | 132/532 [03:37<12:32,  1.88s/it]predicting train subjects:  25%|██▌       | 133/532 [03:39<12:44,  1.92s/it]predicting train subjects:  25%|██▌       | 134/532 [03:41<12:51,  1.94s/it]predicting train subjects:  25%|██▌       | 135/532 [03:43<12:59,  1.96s/it]predicting train subjects:  26%|██▌       | 136/532 [03:45<13:11,  2.00s/it]predicting train subjects:  26%|██▌       | 137/532 [03:48<13:25,  2.04s/it]predicting train subjects:  26%|██▌       | 138/532 [03:50<13:18,  2.03s/it]predicting train subjects:  26%|██▌       | 139/532 [03:52<13:16,  2.03s/it]predicting train subjects:  26%|██▋       | 140/532 [03:54<13:14,  2.03s/it]predicting train subjects:  27%|██▋       | 141/532 [03:56<13:26,  2.06s/it]predicting train subjects:  27%|██▋       | 142/532 [03:58<13:31,  2.08s/it]predicting train subjects:  27%|██▋       | 143/532 [03:59<12:24,  1.91s/it]predicting train subjects:  27%|██▋       | 144/532 [04:01<11:25,  1.77s/it]predicting train subjects:  27%|██▋       | 145/532 [04:02<10:51,  1.68s/it]predicting train subjects:  27%|██▋       | 146/532 [04:04<10:24,  1.62s/it]predicting train subjects:  28%|██▊       | 147/532 [04:05<10:08,  1.58s/it]predicting train subjects:  28%|██▊       | 148/532 [04:07<09:58,  1.56s/it]predicting train subjects:  28%|██▊       | 149/532 [04:09<10:08,  1.59s/it]predicting train subjects:  28%|██▊       | 150/532 [04:10<10:08,  1.59s/it]predicting train subjects:  28%|██▊       | 151/532 [04:12<10:09,  1.60s/it]predicting train subjects:  29%|██▊       | 152/532 [04:13<10:08,  1.60s/it]predicting train subjects:  29%|██▉       | 153/532 [04:15<10:02,  1.59s/it]predicting train subjects:  29%|██▉       | 154/532 [04:16<10:01,  1.59s/it]predicting train subjects:  29%|██▉       | 155/532 [04:19<11:01,  1.76s/it]predicting train subjects:  29%|██▉       | 156/532 [04:21<11:43,  1.87s/it]predicting train subjects:  30%|██▉       | 157/532 [04:23<12:22,  1.98s/it]predicting train subjects:  30%|██▉       | 158/532 [04:25<12:32,  2.01s/it]predicting train subjects:  30%|██▉       | 159/532 [04:27<12:41,  2.04s/it]predicting train subjects:  30%|███       | 160/532 [04:30<13:09,  2.12s/it]predicting train subjects:  30%|███       | 161/532 [04:31<12:04,  1.95s/it]predicting train subjects:  30%|███       | 162/532 [04:33<11:20,  1.84s/it]predicting train subjects:  31%|███       | 163/532 [04:34<10:48,  1.76s/it]predicting train subjects:  31%|███       | 164/532 [04:36<10:19,  1.68s/it]predicting train subjects:  31%|███       | 165/532 [04:37<09:57,  1.63s/it]predicting train subjects:  31%|███       | 166/532 [04:39<09:39,  1.58s/it]predicting train subjects:  31%|███▏      | 167/532 [04:40<09:54,  1.63s/it]predicting train subjects:  32%|███▏      | 168/532 [04:42<10:05,  1.66s/it]predicting train subjects:  32%|███▏      | 169/532 [04:44<10:04,  1.67s/it]predicting train subjects:  32%|███▏      | 170/532 [04:46<10:15,  1.70s/it]predicting train subjects:  32%|███▏      | 171/532 [04:47<10:17,  1.71s/it]predicting train subjects:  32%|███▏      | 172/532 [04:49<10:15,  1.71s/it]predicting train subjects:  33%|███▎      | 173/532 [04:51<10:00,  1.67s/it]predicting train subjects:  33%|███▎      | 174/532 [04:52<09:51,  1.65s/it]predicting train subjects:  33%|███▎      | 175/532 [04:54<09:42,  1.63s/it]predicting train subjects:  33%|███▎      | 176/532 [04:55<09:24,  1.59s/it]predicting train subjects:  33%|███▎      | 177/532 [04:57<09:12,  1.56s/it]predicting train subjects:  33%|███▎      | 178/532 [04:58<09:04,  1.54s/it]predicting train subjects:  34%|███▎      | 179/532 [05:00<09:13,  1.57s/it]predicting train subjects:  34%|███▍      | 180/532 [05:02<09:16,  1.58s/it]predicting train subjects:  34%|███▍      | 181/532 [05:03<09:14,  1.58s/it]predicting train subjects:  34%|███▍      | 182/532 [05:05<09:07,  1.56s/it]predicting train subjects:  34%|███▍      | 183/532 [05:06<08:57,  1.54s/it]predicting train subjects:  35%|███▍      | 184/532 [05:08<08:57,  1.54s/it]predicting train subjects:  35%|███▍      | 185/532 [05:09<08:38,  1.49s/it]predicting train subjects:  35%|███▍      | 186/532 [05:10<08:30,  1.48s/it]predicting train subjects:  35%|███▌      | 187/532 [05:12<08:26,  1.47s/it]predicting train subjects:  35%|███▌      | 188/532 [05:14<08:39,  1.51s/it]predicting train subjects:  36%|███▌      | 189/532 [05:15<08:26,  1.48s/it]predicting train subjects:  36%|███▌      | 190/532 [05:16<08:16,  1.45s/it]predicting train subjects:  36%|███▌      | 191/532 [05:19<09:31,  1.68s/it]predicting train subjects:  36%|███▌      | 192/532 [05:21<10:21,  1.83s/it]predicting train subjects:  36%|███▋      | 193/532 [05:23<10:52,  1.92s/it]predicting train subjects:  36%|███▋      | 194/532 [05:25<11:06,  1.97s/it]predicting train subjects:  37%|███▋      | 195/532 [05:27<11:18,  2.01s/it]predicting train subjects:  37%|███▋      | 196/532 [05:29<11:36,  2.07s/it]predicting train subjects:  37%|███▋      | 197/532 [05:31<11:08,  2.00s/it]predicting train subjects:  37%|███▋      | 198/532 [05:33<10:40,  1.92s/it]predicting train subjects:  37%|███▋      | 199/532 [05:35<10:27,  1.89s/it]predicting train subjects:  38%|███▊      | 200/532 [05:37<10:27,  1.89s/it]predicting train subjects:  38%|███▊      | 201/532 [05:38<10:19,  1.87s/it]predicting train subjects:  38%|███▊      | 202/532 [05:40<10:08,  1.84s/it]predicting train subjects:  38%|███▊      | 203/532 [05:42<09:32,  1.74s/it]predicting train subjects:  38%|███▊      | 204/532 [05:43<09:07,  1.67s/it]predicting train subjects:  39%|███▊      | 205/532 [05:45<08:53,  1.63s/it]predicting train subjects:  39%|███▊      | 206/532 [05:46<08:44,  1.61s/it]predicting train subjects:  39%|███▉      | 207/532 [05:48<08:35,  1.59s/it]predicting train subjects:  39%|███▉      | 208/532 [05:49<08:26,  1.56s/it]predicting train subjects:  39%|███▉      | 209/532 [05:51<08:06,  1.51s/it]predicting train subjects:  39%|███▉      | 210/532 [05:52<07:51,  1.47s/it]predicting train subjects:  40%|███▉      | 211/532 [05:53<07:35,  1.42s/it]predicting train subjects:  40%|███▉      | 212/532 [05:55<07:29,  1.40s/it]predicting train subjects:  40%|████      | 213/532 [05:56<07:20,  1.38s/it]predicting train subjects:  40%|████      | 214/532 [05:57<07:14,  1.37s/it]predicting train subjects:  40%|████      | 215/532 [05:59<08:16,  1.57s/it]predicting train subjects:  41%|████      | 216/532 [06:01<09:00,  1.71s/it]predicting train subjects:  41%|████      | 217/532 [06:03<09:20,  1.78s/it]predicting train subjects:  41%|████      | 218/532 [06:05<09:39,  1.84s/it]predicting train subjects:  41%|████      | 219/532 [06:07<09:49,  1.88s/it]predicting train subjects:  41%|████▏     | 220/532 [06:09<09:56,  1.91s/it]predicting train subjects:  42%|████▏     | 221/532 [06:11<08:58,  1.73s/it]predicting train subjects:  42%|████▏     | 222/532 [06:12<08:13,  1.59s/it]predicting train subjects:  42%|████▏     | 223/532 [06:13<07:47,  1.51s/it]predicting train subjects:  42%|████▏     | 224/532 [06:15<07:24,  1.44s/it]predicting train subjects:  42%|████▏     | 225/532 [06:16<07:14,  1.41s/it]predicting train subjects:  42%|████▏     | 226/532 [06:17<07:01,  1.38s/it]predicting train subjects:  43%|████▎     | 227/532 [06:18<06:50,  1.35s/it]predicting train subjects:  43%|████▎     | 228/532 [06:20<06:43,  1.33s/it]predicting train subjects:  43%|████▎     | 229/532 [06:21<06:40,  1.32s/it]predicting train subjects:  43%|████▎     | 230/532 [06:22<06:30,  1.29s/it]predicting train subjects:  43%|████▎     | 231/532 [06:24<06:30,  1.30s/it]predicting train subjects:  44%|████▎     | 232/532 [06:25<06:32,  1.31s/it]predicting train subjects:  44%|████▍     | 233/532 [06:27<06:56,  1.39s/it]predicting train subjects:  44%|████▍     | 234/532 [06:28<07:00,  1.41s/it]predicting train subjects:  44%|████▍     | 235/532 [06:29<07:04,  1.43s/it]predicting train subjects:  44%|████▍     | 236/532 [06:31<07:10,  1.45s/it]predicting train subjects:  45%|████▍     | 237/532 [06:32<07:12,  1.47s/it]predicting train subjects:  45%|████▍     | 238/532 [06:34<07:16,  1.48s/it]predicting train subjects:  45%|████▍     | 239/532 [06:36<07:26,  1.52s/it]predicting train subjects:  45%|████▌     | 240/532 [06:37<07:44,  1.59s/it]predicting train subjects:  45%|████▌     | 241/532 [06:39<07:47,  1.61s/it]predicting train subjects:  45%|████▌     | 242/532 [06:41<07:47,  1.61s/it]predicting train subjects:  46%|████▌     | 243/532 [06:42<07:48,  1.62s/it]predicting train subjects:  46%|████▌     | 244/532 [06:44<07:46,  1.62s/it]predicting train subjects:  46%|████▌     | 245/532 [06:45<07:22,  1.54s/it]predicting train subjects:  46%|████▌     | 246/532 [06:47<07:06,  1.49s/it]predicting train subjects:  46%|████▋     | 247/532 [06:48<06:47,  1.43s/it]predicting train subjects:  47%|████▋     | 248/532 [06:49<06:32,  1.38s/it]predicting train subjects:  47%|████▋     | 249/532 [06:50<06:22,  1.35s/it]predicting train subjects:  47%|████▋     | 250/532 [06:52<06:16,  1.33s/it]predicting train subjects:  47%|████▋     | 251/532 [06:53<06:27,  1.38s/it]predicting train subjects:  47%|████▋     | 252/532 [06:55<06:29,  1.39s/it]predicting train subjects:  48%|████▊     | 253/532 [06:56<06:33,  1.41s/it]predicting train subjects:  48%|████▊     | 254/532 [06:58<06:35,  1.42s/it]predicting train subjects:  48%|████▊     | 255/532 [06:59<06:32,  1.42s/it]predicting train subjects:  48%|████▊     | 256/532 [07:00<06:33,  1.43s/it]predicting train subjects:  48%|████▊     | 257/532 [07:02<07:05,  1.55s/it]predicting train subjects:  48%|████▊     | 258/532 [07:04<07:16,  1.59s/it]predicting train subjects:  49%|████▊     | 259/532 [07:06<07:29,  1.65s/it]predicting train subjects:  49%|████▉     | 260/532 [07:07<07:41,  1.70s/it]predicting train subjects:  49%|████▉     | 261/532 [07:09<07:44,  1.71s/it]predicting train subjects:  49%|████▉     | 262/532 [07:11<07:42,  1.71s/it]predicting train subjects:  49%|████▉     | 263/532 [07:12<07:12,  1.61s/it]predicting train subjects:  50%|████▉     | 264/532 [07:14<06:45,  1.51s/it]predicting train subjects:  50%|████▉     | 265/532 [07:15<06:19,  1.42s/it]predicting train subjects:  50%|█████     | 266/532 [07:16<06:08,  1.38s/it]predicting train subjects:  50%|█████     | 267/532 [07:17<06:00,  1.36s/it]predicting train subjects:  50%|█████     | 268/532 [07:19<05:50,  1.33s/it]predicting train subjects:  51%|█████     | 269/532 [07:20<06:12,  1.42s/it]predicting train subjects:  51%|█████     | 270/532 [07:22<06:24,  1.47s/it]predicting train subjects:  51%|█████     | 271/532 [07:23<06:31,  1.50s/it]predicting train subjects:  51%|█████     | 272/532 [07:25<06:42,  1.55s/it]predicting train subjects:  51%|█████▏    | 273/532 [07:27<06:50,  1.59s/it]predicting train subjects:  52%|█████▏    | 274/532 [07:29<07:08,  1.66s/it]predicting train subjects:  52%|█████▏    | 275/532 [07:31<07:32,  1.76s/it]predicting train subjects:  52%|█████▏    | 276/532 [07:33<07:44,  1.82s/it]predicting train subjects:  52%|█████▏    | 277/532 [07:35<07:58,  1.88s/it]predicting train subjects:  52%|█████▏    | 278/532 [07:37<08:02,  1.90s/it]predicting train subjects:  52%|█████▏    | 279/532 [07:39<08:09,  1.93s/it]predicting train subjects:  53%|█████▎    | 280/532 [07:41<08:10,  1.95s/it]predicting train subjects:  53%|█████▎    | 281/532 [07:42<07:59,  1.91s/it]predicting train subjects:  53%|█████▎    | 282/532 [07:44<07:56,  1.91s/it]predicting train subjects:  53%|█████▎    | 283/532 [07:46<07:46,  1.87s/it]predicting train subjects:  53%|█████▎    | 284/532 [07:48<07:43,  1.87s/it]predicting train subjects:  54%|█████▎    | 285/532 [07:50<07:40,  1.86s/it]predicting train subjects:  54%|█████▍    | 286/532 [07:52<07:39,  1.87s/it]predicting train subjects:  54%|█████▍    | 287/532 [07:53<07:01,  1.72s/it]predicting train subjects:  54%|█████▍    | 288/532 [07:54<06:39,  1.64s/it]predicting train subjects:  54%|█████▍    | 289/532 [07:56<06:24,  1.58s/it]predicting train subjects:  55%|█████▍    | 290/532 [07:57<06:16,  1.55s/it]predicting train subjects:  55%|█████▍    | 291/532 [07:59<06:10,  1.54s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:00<06:06,  1.53s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:02<06:15,  1.57s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:04<06:22,  1.61s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:05<06:21,  1.61s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:07<06:16,  1.60s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:09<06:15,  1.60s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:10<06:09,  1.58s/it]predicting train subjects:  56%|█████▌    | 299/532 [08:11<05:49,  1.50s/it]predicting train subjects:  56%|█████▋    | 300/532 [08:13<05:32,  1.43s/it]predicting train subjects:  57%|█████▋    | 301/532 [08:14<05:27,  1.42s/it]predicting train subjects:  57%|█████▋    | 302/532 [08:15<05:15,  1.37s/it]predicting train subjects:  57%|█████▋    | 303/532 [08:17<05:11,  1.36s/it]predicting train subjects:  57%|█████▋    | 304/532 [08:18<05:07,  1.35s/it]predicting train subjects:  57%|█████▋    | 305/532 [08:20<05:50,  1.54s/it]predicting train subjects:  58%|█████▊    | 306/532 [08:22<06:15,  1.66s/it]predicting train subjects:  58%|█████▊    | 307/532 [08:24<06:30,  1.73s/it]predicting train subjects:  58%|█████▊    | 308/532 [08:26<06:35,  1.77s/it]predicting train subjects:  58%|█████▊    | 309/532 [08:28<06:42,  1.80s/it]predicting train subjects:  58%|█████▊    | 310/532 [08:30<06:52,  1.86s/it]predicting train subjects:  58%|█████▊    | 311/532 [08:32<07:54,  2.15s/it]predicting train subjects:  59%|█████▊    | 312/532 [08:35<08:38,  2.36s/it]predicting train subjects:  59%|█████▉    | 313/532 [08:38<09:07,  2.50s/it]predicting train subjects:  59%|█████▉    | 314/532 [08:41<09:22,  2.58s/it]predicting train subjects:  59%|█████▉    | 315/532 [08:44<09:43,  2.69s/it]predicting train subjects:  59%|█████▉    | 316/532 [08:47<09:47,  2.72s/it]predicting train subjects:  60%|█████▉    | 317/532 [08:48<08:23,  2.34s/it]predicting train subjects:  60%|█████▉    | 318/532 [08:49<07:19,  2.05s/it]predicting train subjects:  60%|█████▉    | 319/532 [08:51<06:42,  1.89s/it]predicting train subjects:  60%|██████    | 320/532 [08:52<06:16,  1.78s/it]predicting train subjects:  60%|██████    | 321/532 [08:54<05:54,  1.68s/it]predicting train subjects:  61%|██████    | 322/532 [08:55<05:39,  1.62s/it]predicting train subjects:  61%|██████    | 323/532 [08:57<06:07,  1.76s/it]predicting train subjects:  61%|██████    | 324/532 [08:59<06:25,  1.85s/it]predicting train subjects:  61%|██████    | 325/532 [09:02<06:35,  1.91s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:04<06:44,  1.96s/it]predicting train subjects:  61%|██████▏   | 327/532 [09:06<06:48,  1.99s/it]predicting train subjects:  62%|██████▏   | 328/532 [09:08<06:50,  2.01s/it]predicting train subjects:  62%|██████▏   | 329/532 [09:09<06:19,  1.87s/it]predicting train subjects:  62%|██████▏   | 330/532 [09:11<05:57,  1.77s/it]predicting train subjects:  62%|██████▏   | 331/532 [09:12<05:41,  1.70s/it]predicting train subjects:  62%|██████▏   | 332/532 [09:14<05:27,  1.64s/it]predicting train subjects:  63%|██████▎   | 333/532 [09:15<05:20,  1.61s/it]predicting train subjects:  63%|██████▎   | 334/532 [09:17<05:14,  1.59s/it]predicting train subjects:  63%|██████▎   | 335/532 [09:19<05:32,  1.69s/it]predicting train subjects:  63%|██████▎   | 336/532 [09:21<05:41,  1.74s/it]predicting train subjects:  63%|██████▎   | 337/532 [09:23<05:44,  1.77s/it]predicting train subjects:  64%|██████▎   | 338/532 [09:24<05:46,  1.79s/it]predicting train subjects:  64%|██████▎   | 339/532 [09:26<05:45,  1.79s/it]predicting train subjects:  64%|██████▍   | 340/532 [09:28<05:51,  1.83s/it]predicting train subjects:  64%|██████▍   | 341/532 [09:30<05:27,  1.72s/it]predicting train subjects:  64%|██████▍   | 342/532 [09:31<05:07,  1.62s/it]predicting train subjects:  64%|██████▍   | 343/532 [09:32<04:50,  1.54s/it]predicting train subjects:  65%|██████▍   | 344/532 [09:34<04:37,  1.48s/it]predicting train subjects:  65%|██████▍   | 345/532 [09:35<04:29,  1.44s/it]predicting train subjects:  65%|██████▌   | 346/532 [09:36<04:24,  1.42s/it]predicting train subjects:  65%|██████▌   | 347/532 [09:38<04:31,  1.47s/it]predicting train subjects:  65%|██████▌   | 348/532 [09:39<04:36,  1.50s/it]predicting train subjects:  66%|██████▌   | 349/532 [09:41<04:42,  1.54s/it]predicting train subjects:  66%|██████▌   | 350/532 [09:43<04:43,  1.56s/it]predicting train subjects:  66%|██████▌   | 351/532 [09:44<04:43,  1.57s/it]predicting train subjects:  66%|██████▌   | 352/532 [09:46<04:46,  1.59s/it]predicting train subjects:  66%|██████▋   | 353/532 [09:48<04:43,  1.58s/it]predicting train subjects:  67%|██████▋   | 354/532 [09:49<04:42,  1.59s/it]predicting train subjects:  67%|██████▋   | 355/532 [09:51<04:44,  1.61s/it]predicting train subjects:  67%|██████▋   | 356/532 [09:52<04:40,  1.59s/it]predicting train subjects:  67%|██████▋   | 357/532 [09:54<04:37,  1.58s/it]predicting train subjects:  67%|██████▋   | 358/532 [09:55<04:35,  1.58s/it]predicting train subjects:  67%|██████▋   | 359/532 [09:57<04:23,  1.52s/it]predicting train subjects:  68%|██████▊   | 360/532 [09:58<04:15,  1.49s/it]predicting train subjects:  68%|██████▊   | 361/532 [10:00<04:10,  1.46s/it]predicting train subjects:  68%|██████▊   | 362/532 [10:01<04:04,  1.44s/it]predicting train subjects:  68%|██████▊   | 363/532 [10:02<03:57,  1.41s/it]predicting train subjects:  68%|██████▊   | 364/532 [10:04<03:53,  1.39s/it]predicting train subjects:  69%|██████▊   | 365/532 [10:05<03:49,  1.38s/it]predicting train subjects:  69%|██████▉   | 366/532 [10:06<03:47,  1.37s/it]predicting train subjects:  69%|██████▉   | 367/532 [10:08<03:51,  1.40s/it]predicting train subjects:  69%|██████▉   | 368/532 [10:09<03:48,  1.39s/it]predicting train subjects:  69%|██████▉   | 369/532 [10:11<03:47,  1.39s/it]predicting train subjects:  70%|██████▉   | 370/532 [10:12<03:46,  1.40s/it]predicting train subjects:  70%|██████▉   | 371/532 [10:14<04:10,  1.56s/it]predicting train subjects:  70%|██████▉   | 372/532 [10:16<04:25,  1.66s/it]predicting train subjects:  70%|███████   | 373/532 [10:18<04:32,  1.71s/it]predicting train subjects:  70%|███████   | 374/532 [10:20<04:40,  1.77s/it]predicting train subjects:  70%|███████   | 375/532 [10:22<04:48,  1.84s/it]predicting train subjects:  71%|███████   | 376/532 [10:23<04:47,  1.84s/it]predicting train subjects:  71%|███████   | 377/532 [10:25<04:29,  1.74s/it]predicting train subjects:  71%|███████   | 378/532 [10:27<04:20,  1.69s/it]predicting train subjects:  71%|███████   | 379/532 [10:28<04:11,  1.64s/it]predicting train subjects:  71%|███████▏  | 380/532 [10:30<04:04,  1.61s/it]predicting train subjects:  72%|███████▏  | 381/532 [10:31<03:59,  1.59s/it]predicting train subjects:  72%|███████▏  | 382/532 [10:33<03:55,  1.57s/it]predicting train subjects:  72%|███████▏  | 383/532 [10:34<03:57,  1.59s/it]predicting train subjects:  72%|███████▏  | 384/532 [10:36<04:01,  1.63s/it]predicting train subjects:  72%|███████▏  | 385/532 [10:38<03:58,  1.62s/it]predicting train subjects:  73%|███████▎  | 386/532 [10:39<03:58,  1.63s/it]predicting train subjects:  73%|███████▎  | 387/532 [10:41<03:57,  1.64s/it]predicting train subjects:  73%|███████▎  | 388/532 [10:43<03:56,  1.64s/it]predicting train subjects:  73%|███████▎  | 389/532 [10:44<03:58,  1.67s/it]predicting train subjects:  73%|███████▎  | 390/532 [10:46<03:58,  1.68s/it]predicting train subjects:  73%|███████▎  | 391/532 [10:48<03:56,  1.67s/it]predicting train subjects:  74%|███████▎  | 392/532 [10:49<03:53,  1.67s/it]predicting train subjects:  74%|███████▍  | 393/532 [10:51<03:51,  1.67s/it]predicting train subjects:  74%|███████▍  | 394/532 [10:53<03:56,  1.72s/it]predicting train subjects:  74%|███████▍  | 395/532 [10:55<03:54,  1.71s/it]predicting train subjects:  74%|███████▍  | 396/532 [10:56<03:52,  1.71s/it]predicting train subjects:  75%|███████▍  | 397/532 [10:58<03:53,  1.73s/it]predicting train subjects:  75%|███████▍  | 398/532 [11:00<03:49,  1.71s/it]predicting train subjects:  75%|███████▌  | 399/532 [11:01<03:45,  1.69s/it]predicting train subjects:  75%|███████▌  | 400/532 [11:03<03:47,  1.73s/it]predicting train subjects:  75%|███████▌  | 401/532 [11:05<03:52,  1.77s/it]predicting train subjects:  76%|███████▌  | 402/532 [11:07<03:51,  1.78s/it]predicting train subjects:  76%|███████▌  | 403/532 [11:09<03:54,  1.82s/it]predicting train subjects:  76%|███████▌  | 404/532 [11:11<03:52,  1.82s/it]predicting train subjects:  76%|███████▌  | 405/532 [11:12<03:49,  1.81s/it]predicting train subjects:  76%|███████▋  | 406/532 [11:14<03:46,  1.79s/it]predicting train subjects:  77%|███████▋  | 407/532 [11:16<03:34,  1.71s/it]predicting train subjects:  77%|███████▋  | 408/532 [11:17<03:25,  1.66s/it]predicting train subjects:  77%|███████▋  | 409/532 [11:19<03:22,  1.64s/it]predicting train subjects:  77%|███████▋  | 410/532 [11:20<03:16,  1.61s/it]predicting train subjects:  77%|███████▋  | 411/532 [11:22<03:18,  1.64s/it]predicting train subjects:  77%|███████▋  | 412/532 [11:24<03:14,  1.62s/it]predicting train subjects:  78%|███████▊  | 413/532 [11:25<03:08,  1.58s/it]predicting train subjects:  78%|███████▊  | 414/532 [11:27<03:04,  1.56s/it]predicting train subjects:  78%|███████▊  | 415/532 [11:28<02:57,  1.51s/it]predicting train subjects:  78%|███████▊  | 416/532 [11:29<02:54,  1.50s/it]predicting train subjects:  78%|███████▊  | 417/532 [11:31<02:50,  1.49s/it]predicting train subjects:  79%|███████▊  | 418/532 [11:32<02:50,  1.49s/it]predicting train subjects:  79%|███████▉  | 419/532 [11:34<02:57,  1.57s/it]predicting train subjects:  79%|███████▉  | 420/532 [11:36<02:59,  1.60s/it]predicting train subjects:  79%|███████▉  | 421/532 [11:38<03:02,  1.64s/it]predicting train subjects:  79%|███████▉  | 422/532 [11:39<03:03,  1.67s/it]predicting train subjects:  80%|███████▉  | 423/532 [11:41<03:04,  1.69s/it]predicting train subjects:  80%|███████▉  | 424/532 [11:43<03:02,  1.69s/it]predicting train subjects:  80%|███████▉  | 425/532 [11:44<03:02,  1.70s/it]predicting train subjects:  80%|████████  | 426/532 [11:46<02:59,  1.69s/it]predicting train subjects:  80%|████████  | 427/532 [11:48<02:57,  1.69s/it]predicting train subjects:  80%|████████  | 428/532 [11:49<02:54,  1.68s/it]predicting train subjects:  81%|████████  | 429/532 [11:51<02:50,  1.66s/it]predicting train subjects:  81%|████████  | 430/532 [11:53<02:50,  1.67s/it]predicting train subjects:  81%|████████  | 431/532 [11:55<02:54,  1.73s/it]predicting train subjects:  81%|████████  | 432/532 [11:57<02:56,  1.77s/it]predicting train subjects:  81%|████████▏ | 433/532 [11:58<02:59,  1.81s/it]predicting train subjects:  82%|████████▏ | 434/532 [12:00<03:00,  1.85s/it]predicting train subjects:  82%|████████▏ | 435/532 [12:02<03:00,  1.86s/it]predicting train subjects:  82%|████████▏ | 436/532 [12:04<02:59,  1.87s/it]predicting train subjects:  82%|████████▏ | 437/532 [12:05<02:42,  1.71s/it]predicting train subjects:  82%|████████▏ | 438/532 [12:07<02:31,  1.62s/it]predicting train subjects:  83%|████████▎ | 439/532 [12:08<02:21,  1.52s/it]predicting train subjects:  83%|████████▎ | 440/532 [12:10<02:17,  1.49s/it]predicting train subjects:  83%|████████▎ | 441/532 [12:11<02:12,  1.46s/it]predicting train subjects:  83%|████████▎ | 442/532 [12:12<02:09,  1.43s/it]predicting train subjects:  83%|████████▎ | 443/532 [12:14<02:03,  1.39s/it]predicting train subjects:  83%|████████▎ | 444/532 [12:15<01:59,  1.36s/it]predicting train subjects:  84%|████████▎ | 445/532 [12:16<01:57,  1.35s/it]predicting train subjects:  84%|████████▍ | 446/532 [12:18<01:54,  1.34s/it]predicting train subjects:  84%|████████▍ | 447/532 [12:19<01:54,  1.35s/it]predicting train subjects:  84%|████████▍ | 448/532 [12:20<01:50,  1.32s/it]predicting train subjects:  84%|████████▍ | 449/532 [12:22<01:53,  1.37s/it]predicting train subjects:  85%|████████▍ | 450/532 [12:23<01:56,  1.42s/it]predicting train subjects:  85%|████████▍ | 451/532 [12:25<01:56,  1.44s/it]predicting train subjects:  85%|████████▍ | 452/532 [12:26<01:57,  1.47s/it]predicting train subjects:  85%|████████▌ | 453/532 [12:28<01:58,  1.50s/it]predicting train subjects:  85%|████████▌ | 454/532 [12:29<01:57,  1.51s/it]predicting train subjects:  86%|████████▌ | 455/532 [12:31<01:59,  1.56s/it]predicting train subjects:  86%|████████▌ | 456/532 [12:33<02:00,  1.59s/it]predicting train subjects:  86%|████████▌ | 457/532 [12:34<02:01,  1.62s/it]predicting train subjects:  86%|████████▌ | 458/532 [12:36<02:02,  1.65s/it]predicting train subjects:  86%|████████▋ | 459/532 [12:38<02:02,  1.68s/it]predicting train subjects:  86%|████████▋ | 460/532 [12:40<02:02,  1.71s/it]predicting train subjects:  87%|████████▋ | 461/532 [12:42<02:06,  1.78s/it]predicting train subjects:  87%|████████▋ | 462/532 [12:44<02:08,  1.83s/it]predicting train subjects:  87%|████████▋ | 463/532 [12:45<02:08,  1.87s/it]predicting train subjects:  87%|████████▋ | 464/532 [12:47<02:07,  1.88s/it]predicting train subjects:  87%|████████▋ | 465/532 [12:49<02:06,  1.88s/it]predicting train subjects:  88%|████████▊ | 466/532 [12:51<02:06,  1.91s/it]predicting train subjects:  88%|████████▊ | 467/532 [12:53<01:56,  1.79s/it]predicting train subjects:  88%|████████▊ | 468/532 [12:54<01:48,  1.70s/it]predicting train subjects:  88%|████████▊ | 469/532 [12:56<01:43,  1.64s/it]predicting train subjects:  88%|████████▊ | 470/532 [12:57<01:39,  1.61s/it]predicting train subjects:  89%|████████▊ | 471/532 [12:59<01:36,  1.59s/it]predicting train subjects:  89%|████████▊ | 472/532 [13:00<01:34,  1.57s/it]predicting train subjects:  89%|████████▉ | 473/532 [13:02<01:35,  1.61s/it]predicting train subjects:  89%|████████▉ | 474/532 [13:04<01:35,  1.65s/it]predicting train subjects:  89%|████████▉ | 475/532 [13:06<01:35,  1.67s/it]predicting train subjects:  89%|████████▉ | 476/532 [13:07<01:33,  1.67s/it]predicting train subjects:  90%|████████▉ | 477/532 [13:09<01:32,  1.68s/it]predicting train subjects:  90%|████████▉ | 478/532 [13:11<01:30,  1.67s/it]predicting train subjects:  90%|█████████ | 479/532 [13:12<01:25,  1.62s/it]predicting train subjects:  90%|█████████ | 480/532 [13:13<01:20,  1.55s/it]predicting train subjects:  90%|█████████ | 481/532 [13:15<01:16,  1.51s/it]predicting train subjects:  91%|█████████ | 482/532 [13:16<01:15,  1.50s/it]predicting train subjects:  91%|█████████ | 483/532 [13:18<01:15,  1.54s/it]predicting train subjects:  91%|█████████ | 484/532 [13:19<01:13,  1.53s/it]predicting train subjects:  91%|█████████ | 485/532 [13:21<01:17,  1.65s/it]predicting train subjects:  91%|█████████▏| 486/532 [13:23<01:21,  1.76s/it]predicting train subjects:  92%|█████████▏| 487/532 [13:25<01:22,  1.83s/it]predicting train subjects:  92%|█████████▏| 488/532 [13:27<01:22,  1.88s/it]predicting train subjects:  92%|█████████▏| 489/532 [13:29<01:22,  1.91s/it]predicting train subjects:  92%|█████████▏| 490/532 [13:31<01:19,  1.89s/it]predicting train subjects:  92%|█████████▏| 491/532 [13:33<01:14,  1.83s/it]predicting train subjects:  92%|█████████▏| 492/532 [13:34<01:10,  1.76s/it]predicting train subjects:  93%|█████████▎| 493/532 [13:36<01:06,  1.72s/it]predicting train subjects:  93%|█████████▎| 494/532 [13:38<01:04,  1.69s/it]predicting train subjects:  93%|█████████▎| 495/532 [13:39<01:01,  1.67s/it]predicting train subjects:  93%|█████████▎| 496/532 [13:41<00:58,  1.62s/it]predicting train subjects:  93%|█████████▎| 497/532 [13:42<00:56,  1.61s/it]predicting train subjects:  94%|█████████▎| 498/532 [13:44<00:54,  1.60s/it]predicting train subjects:  94%|█████████▍| 499/532 [13:46<00:52,  1.60s/it]predicting train subjects:  94%|█████████▍| 500/532 [13:47<00:52,  1.63s/it]predicting train subjects:  94%|█████████▍| 501/532 [13:49<00:49,  1.61s/it]predicting train subjects:  94%|█████████▍| 502/532 [13:50<00:48,  1.60s/it]predicting train subjects:  95%|█████████▍| 503/532 [13:52<00:45,  1.58s/it]predicting train subjects:  95%|█████████▍| 504/532 [13:53<00:43,  1.56s/it]predicting train subjects:  95%|█████████▍| 505/532 [13:55<00:41,  1.54s/it]predicting train subjects:  95%|█████████▌| 506/532 [13:56<00:39,  1.51s/it]predicting train subjects:  95%|█████████▌| 507/532 [13:58<00:37,  1.52s/it]predicting train subjects:  95%|█████████▌| 508/532 [13:59<00:36,  1.50s/it]predicting train subjects:  96%|█████████▌| 509/532 [14:01<00:36,  1.59s/it]predicting train subjects:  96%|█████████▌| 510/532 [14:03<00:36,  1.68s/it]predicting train subjects:  96%|█████████▌| 511/532 [14:05<00:36,  1.71s/it]predicting train subjects:  96%|█████████▌| 512/532 [14:07<00:34,  1.72s/it]predicting train subjects:  96%|█████████▋| 513/532 [14:08<00:33,  1.74s/it]predicting train subjects:  97%|█████████▋| 514/532 [14:10<00:31,  1.74s/it]predicting train subjects:  97%|█████████▋| 515/532 [14:12<00:28,  1.68s/it]predicting train subjects:  97%|█████████▋| 516/532 [14:13<00:26,  1.65s/it]predicting train subjects:  97%|█████████▋| 517/532 [14:15<00:24,  1.62s/it]predicting train subjects:  97%|█████████▋| 518/532 [14:16<00:22,  1.60s/it]predicting train subjects:  98%|█████████▊| 519/532 [14:18<00:20,  1.58s/it]predicting train subjects:  98%|█████████▊| 520/532 [14:20<00:19,  1.60s/it]predicting train subjects:  98%|█████████▊| 521/532 [14:21<00:18,  1.64s/it]predicting train subjects:  98%|█████████▊| 522/532 [14:23<00:16,  1.67s/it]predicting train subjects:  98%|█████████▊| 523/532 [14:25<00:14,  1.65s/it]predicting train subjects:  98%|█████████▊| 524/532 [14:26<00:13,  1.65s/it]predicting train subjects:  99%|█████████▊| 525/532 [14:28<00:11,  1.68s/it]predicting train subjects:  99%|█████████▉| 526/532 [14:30<00:10,  1.67s/it]predicting train subjects:  99%|█████████▉| 527/532 [14:31<00:07,  1.60s/it]predicting train subjects:  99%|█████████▉| 528/532 [14:33<00:06,  1.60s/it]predicting train subjects:  99%|█████████▉| 529/532 [14:34<00:04,  1.55s/it]predicting train subjects: 100%|█████████▉| 530/532 [14:36<00:03,  1.54s/it]predicting train subjects: 100%|█████████▉| 531/532 [14:37<00:01,  1.53s/it]predicting train subjects: 100%|██████████| 532/532 [14:39<00:00,  1.53s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss’: File exists

Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<12:39,  1.43s/it]Loading train:   0%|          | 2/532 [00:02<11:32,  1.31s/it]Loading train:   1%|          | 3/532 [00:03<11:05,  1.26s/it]Loading train:   1%|          | 4/532 [00:04<11:18,  1.28s/it]Loading train:   1%|          | 5/532 [00:05<10:34,  1.20s/it]Loading train:   1%|          | 6/532 [00:06<09:32,  1.09s/it]Loading train:   1%|▏         | 7/532 [00:07<08:46,  1.00s/it]Loading train:   2%|▏         | 8/532 [00:08<08:23,  1.04it/s]Loading train:   2%|▏         | 9/532 [00:09<09:04,  1.04s/it]Loading train:   2%|▏         | 10/532 [00:10<09:25,  1.08s/it]Loading train:   2%|▏         | 11/532 [00:11<09:15,  1.07s/it]Loading train:   2%|▏         | 12/532 [00:13<09:39,  1.11s/it]Loading train:   2%|▏         | 13/532 [00:14<09:22,  1.08s/it]Loading train:   3%|▎         | 14/532 [00:15<08:55,  1.03s/it]Loading train:   3%|▎         | 15/532 [00:15<08:31,  1.01it/s]Loading train:   3%|▎         | 16/532 [00:17<08:55,  1.04s/it]Loading train:   3%|▎         | 17/532 [00:17<08:37,  1.00s/it]Loading train:   3%|▎         | 18/532 [00:18<08:35,  1.00s/it]Loading train:   4%|▎         | 19/532 [00:19<07:51,  1.09it/s]Loading train:   4%|▍         | 20/532 [00:20<07:47,  1.10it/s]Loading train:   4%|▍         | 21/532 [00:21<08:05,  1.05it/s]Loading train:   4%|▍         | 22/532 [00:22<07:58,  1.06it/s]Loading train:   4%|▍         | 23/532 [00:23<08:20,  1.02it/s]Loading train:   5%|▍         | 24/532 [00:24<07:51,  1.08it/s]Loading train:   5%|▍         | 25/532 [00:25<08:21,  1.01it/s]Loading train:   5%|▍         | 26/532 [00:26<08:20,  1.01it/s]Loading train:   5%|▌         | 27/532 [00:27<08:53,  1.06s/it]Loading train:   5%|▌         | 28/532 [00:28<08:26,  1.00s/it]Loading train:   5%|▌         | 29/532 [00:29<08:48,  1.05s/it]Loading train:   6%|▌         | 30/532 [00:30<08:26,  1.01s/it]Loading train:   6%|▌         | 31/532 [00:31<08:16,  1.01it/s]Loading train:   6%|▌         | 32/532 [00:32<08:04,  1.03it/s]Loading train:   6%|▌         | 33/532 [00:33<07:42,  1.08it/s]Loading train:   6%|▋         | 34/532 [00:34<08:30,  1.02s/it]Loading train:   7%|▋         | 35/532 [00:35<08:31,  1.03s/it]Loading train:   7%|▋         | 36/532 [00:36<08:55,  1.08s/it]Loading train:   7%|▋         | 37/532 [00:38<09:04,  1.10s/it]Loading train:   7%|▋         | 38/532 [00:39<09:13,  1.12s/it]Loading train:   7%|▋         | 39/532 [00:40<09:19,  1.13s/it]Loading train:   8%|▊         | 40/532 [00:41<08:45,  1.07s/it]Loading train:   8%|▊         | 41/532 [00:42<08:41,  1.06s/it]Loading train:   8%|▊         | 42/532 [00:43<08:52,  1.09s/it]Loading train:   8%|▊         | 43/532 [00:44<08:10,  1.00s/it]Loading train:   8%|▊         | 44/532 [00:45<07:31,  1.08it/s]Loading train:   8%|▊         | 45/532 [00:45<07:16,  1.12it/s]Loading train:   9%|▊         | 46/532 [00:46<07:30,  1.08it/s]Loading train:   9%|▉         | 47/532 [00:48<08:15,  1.02s/it]Loading train:   9%|▉         | 48/532 [00:49<08:04,  1.00s/it]Loading train:   9%|▉         | 49/532 [00:49<07:44,  1.04it/s]Loading train:   9%|▉         | 50/532 [00:51<08:01,  1.00it/s]Loading train:  10%|▉         | 51/532 [00:52<07:54,  1.01it/s]Loading train:  10%|▉         | 52/532 [00:52<07:51,  1.02it/s]Loading train:  10%|▉         | 53/532 [00:53<07:43,  1.03it/s]Loading train:  10%|█         | 54/532 [00:54<07:53,  1.01it/s]Loading train:  10%|█         | 55/532 [00:55<07:52,  1.01it/s]Loading train:  11%|█         | 56/532 [00:57<08:12,  1.04s/it]Loading train:  11%|█         | 57/532 [00:57<07:52,  1.00it/s]Loading train:  11%|█         | 58/532 [00:58<07:50,  1.01it/s]Loading train:  11%|█         | 59/532 [01:00<08:07,  1.03s/it]Loading train:  11%|█▏        | 60/532 [01:00<07:35,  1.04it/s]Loading train:  11%|█▏        | 61/532 [01:01<07:35,  1.03it/s]Loading train:  12%|█▏        | 62/532 [01:02<07:53,  1.01s/it]Loading train:  12%|█▏        | 63/532 [01:04<08:11,  1.05s/it]Loading train:  12%|█▏        | 64/532 [01:05<07:57,  1.02s/it]Loading train:  12%|█▏        | 65/532 [01:06<07:46,  1.00it/s]Loading train:  12%|█▏        | 66/532 [01:07<08:15,  1.06s/it]Loading train:  13%|█▎        | 67/532 [01:08<08:21,  1.08s/it]Loading train:  13%|█▎        | 68/532 [01:09<08:02,  1.04s/it]Loading train:  13%|█▎        | 69/532 [01:10<07:44,  1.00s/it]Loading train:  13%|█▎        | 70/532 [01:11<07:21,  1.05it/s]Loading train:  13%|█▎        | 71/532 [01:11<07:00,  1.10it/s]Loading train:  14%|█▎        | 72/532 [01:12<06:35,  1.16it/s]Loading train:  14%|█▎        | 73/532 [01:13<06:47,  1.13it/s]Loading train:  14%|█▍        | 74/532 [01:14<07:31,  1.01it/s]Loading train:  14%|█▍        | 75/532 [01:16<08:32,  1.12s/it]Loading train:  14%|█▍        | 76/532 [01:17<08:09,  1.07s/it]Loading train:  14%|█▍        | 77/532 [01:18<07:48,  1.03s/it]Loading train:  15%|█▍        | 78/532 [01:19<07:33,  1.00it/s]Loading train:  15%|█▍        | 79/532 [01:20<07:29,  1.01it/s]Loading train:  15%|█▌        | 80/532 [01:20<07:18,  1.03it/s]Loading train:  15%|█▌        | 81/532 [01:21<07:06,  1.06it/s]Loading train:  15%|█▌        | 82/532 [01:22<07:09,  1.05it/s]Loading train:  16%|█▌        | 83/532 [01:23<07:00,  1.07it/s]Loading train:  16%|█▌        | 84/532 [01:24<06:49,  1.10it/s]Loading train:  16%|█▌        | 85/532 [01:25<06:39,  1.12it/s]Loading train:  16%|█▌        | 86/532 [01:26<06:28,  1.15it/s]Loading train:  16%|█▋        | 87/532 [01:27<06:23,  1.16it/s]Loading train:  17%|█▋        | 88/532 [01:27<06:16,  1.18it/s]Loading train:  17%|█▋        | 89/532 [01:28<06:30,  1.13it/s]Loading train:  17%|█▋        | 90/532 [01:29<06:22,  1.16it/s]Loading train:  17%|█▋        | 91/532 [01:30<06:12,  1.18it/s]Loading train:  17%|█▋        | 92/532 [01:31<06:15,  1.17it/s]Loading train:  17%|█▋        | 93/532 [01:32<06:17,  1.16it/s]Loading train:  18%|█▊        | 94/532 [01:33<06:13,  1.17it/s]Loading train:  18%|█▊        | 95/532 [01:34<06:52,  1.06it/s]Loading train:  18%|█▊        | 96/532 [01:35<06:57,  1.05it/s]Loading train:  18%|█▊        | 97/532 [01:36<07:06,  1.02it/s]Loading train:  18%|█▊        | 98/532 [01:37<07:25,  1.03s/it]Loading train:  19%|█▊        | 99/532 [01:38<07:31,  1.04s/it]Loading train:  19%|█▉        | 100/532 [01:39<07:51,  1.09s/it]Loading train:  19%|█▉        | 101/532 [01:40<07:28,  1.04s/it]Loading train:  19%|█▉        | 102/532 [01:41<07:01,  1.02it/s]Loading train:  19%|█▉        | 103/532 [01:42<06:28,  1.11it/s]Loading train:  20%|█▉        | 104/532 [01:42<06:07,  1.16it/s]Loading train:  20%|█▉        | 105/532 [01:43<05:54,  1.20it/s]Loading train:  20%|█▉        | 106/532 [01:44<05:44,  1.24it/s]Loading train:  20%|██        | 107/532 [01:45<05:50,  1.21it/s]Loading train:  20%|██        | 108/532 [01:46<05:39,  1.25it/s]Loading train:  20%|██        | 109/532 [01:46<05:51,  1.20it/s]Loading train:  21%|██        | 110/532 [01:47<05:44,  1.23it/s]Loading train:  21%|██        | 111/532 [01:48<05:31,  1.27it/s]Loading train:  21%|██        | 112/532 [01:49<05:19,  1.31it/s]Loading train:  21%|██        | 113/532 [01:50<05:44,  1.22it/s]Loading train:  21%|██▏       | 114/532 [01:50<05:52,  1.19it/s]Loading train:  22%|██▏       | 115/532 [01:51<05:53,  1.18it/s]Loading train:  22%|██▏       | 116/532 [01:52<05:49,  1.19it/s]Loading train:  22%|██▏       | 117/532 [01:53<06:03,  1.14it/s]Loading train:  22%|██▏       | 118/532 [01:54<06:01,  1.15it/s]Loading train:  22%|██▏       | 119/532 [01:55<06:22,  1.08it/s]Loading train:  23%|██▎       | 120/532 [01:56<06:33,  1.05it/s]Loading train:  23%|██▎       | 121/532 [01:57<06:42,  1.02it/s]Loading train:  23%|██▎       | 122/532 [01:58<06:51,  1.00s/it]Loading train:  23%|██▎       | 123/532 [01:59<06:54,  1.01s/it]Loading train:  23%|██▎       | 124/532 [02:00<06:50,  1.01s/it]Loading train:  23%|██▎       | 125/532 [02:01<06:55,  1.02s/it]Loading train:  24%|██▎       | 126/532 [02:02<06:41,  1.01it/s]Loading train:  24%|██▍       | 127/532 [02:03<06:40,  1.01it/s]Loading train:  24%|██▍       | 128/532 [02:04<06:35,  1.02it/s]Loading train:  24%|██▍       | 129/532 [02:05<06:31,  1.03it/s]Loading train:  24%|██▍       | 130/532 [02:06<06:41,  1.00it/s]Loading train:  25%|██▍       | 131/532 [02:07<07:19,  1.10s/it]Loading train:  25%|██▍       | 132/532 [02:09<07:20,  1.10s/it]Loading train:  25%|██▌       | 133/532 [02:10<07:31,  1.13s/it]Loading train:  25%|██▌       | 134/532 [02:11<07:27,  1.13s/it]Loading train:  25%|██▌       | 135/532 [02:12<07:24,  1.12s/it]Loading train:  26%|██▌       | 136/532 [02:13<07:20,  1.11s/it]Loading train:  26%|██▌       | 137/532 [02:14<07:26,  1.13s/it]Loading train:  26%|██▌       | 138/532 [02:15<07:25,  1.13s/it]Loading train:  26%|██▌       | 139/532 [02:16<07:25,  1.13s/it]Loading train:  26%|██▋       | 140/532 [02:18<07:22,  1.13s/it]Loading train:  27%|██▋       | 141/532 [02:19<07:20,  1.13s/it]Loading train:  27%|██▋       | 142/532 [02:20<07:18,  1.12s/it]Loading train:  27%|██▋       | 143/532 [02:21<06:55,  1.07s/it]Loading train:  27%|██▋       | 144/532 [02:22<06:37,  1.02s/it]Loading train:  27%|██▋       | 145/532 [02:23<06:17,  1.02it/s]Loading train:  27%|██▋       | 146/532 [02:23<06:06,  1.05it/s]Loading train:  28%|██▊       | 147/532 [02:24<05:53,  1.09it/s]Loading train:  28%|██▊       | 148/532 [02:25<05:58,  1.07it/s]Loading train:  28%|██▊       | 149/532 [02:26<05:53,  1.08it/s]Loading train:  28%|██▊       | 150/532 [02:27<05:41,  1.12it/s]Loading train:  28%|██▊       | 151/532 [02:28<05:30,  1.15it/s]Loading train:  29%|██▊       | 152/532 [02:29<05:23,  1.18it/s]Loading train:  29%|██▉       | 153/532 [02:30<05:29,  1.15it/s]Loading train:  29%|██▉       | 154/532 [02:30<05:28,  1.15it/s]Loading train:  29%|██▉       | 155/532 [02:32<06:14,  1.01it/s]Loading train:  29%|██▉       | 156/532 [02:33<06:38,  1.06s/it]Loading train:  30%|██▉       | 157/532 [02:34<06:45,  1.08s/it]Loading train:  30%|██▉       | 158/532 [02:35<07:00,  1.12s/it]Loading train:  30%|██▉       | 159/532 [02:36<06:59,  1.13s/it]Loading train:  30%|███       | 160/532 [02:38<07:06,  1.15s/it]Loading train:  30%|███       | 161/532 [02:39<06:47,  1.10s/it]Loading train:  30%|███       | 162/532 [02:39<06:24,  1.04s/it]Loading train:  31%|███       | 163/532 [02:40<06:07,  1.01it/s]Loading train:  31%|███       | 164/532 [02:41<05:58,  1.03it/s]Loading train:  31%|███       | 165/532 [02:42<05:52,  1.04it/s]Loading train:  31%|███       | 166/532 [02:43<05:44,  1.06it/s]Loading train:  31%|███▏      | 167/532 [02:44<05:48,  1.05it/s]Loading train:  32%|███▏      | 168/532 [02:45<05:50,  1.04it/s]Loading train:  32%|███▏      | 169/532 [02:46<05:45,  1.05it/s]Loading train:  32%|███▏      | 170/532 [02:47<05:52,  1.03it/s]Loading train:  32%|███▏      | 171/532 [02:48<05:51,  1.03it/s]Loading train:  32%|███▏      | 172/532 [02:49<05:47,  1.03it/s]Loading train:  33%|███▎      | 173/532 [02:50<05:41,  1.05it/s]Loading train:  33%|███▎      | 174/532 [02:51<05:26,  1.10it/s]Loading train:  33%|███▎      | 175/532 [02:51<05:08,  1.16it/s]Loading train:  33%|███▎      | 176/532 [02:52<05:00,  1.18it/s]Loading train:  33%|███▎      | 177/532 [02:53<04:51,  1.22it/s]Loading train:  33%|███▎      | 178/532 [02:54<04:50,  1.22it/s]Loading train:  34%|███▎      | 179/532 [02:55<06:09,  1.05s/it]Loading train:  34%|███▍      | 180/532 [02:56<05:42,  1.03it/s]Loading train:  34%|███▍      | 181/532 [02:57<05:25,  1.08it/s]Loading train:  34%|███▍      | 182/532 [02:58<05:12,  1.12it/s]Loading train:  34%|███▍      | 183/532 [02:59<05:09,  1.13it/s]Loading train:  35%|███▍      | 184/532 [03:00<05:01,  1.15it/s]Loading train:  35%|███▍      | 185/532 [03:01<05:14,  1.10it/s]Loading train:  35%|███▍      | 186/532 [03:02<05:30,  1.05it/s]Loading train:  35%|███▌      | 187/532 [03:03<05:44,  1.00it/s]Loading train:  35%|███▌      | 188/532 [03:04<05:34,  1.03it/s]Loading train:  36%|███▌      | 189/532 [03:05<05:43,  1.00s/it]Loading train:  36%|███▌      | 190/532 [03:06<05:55,  1.04s/it]Loading train:  36%|███▌      | 191/532 [03:07<06:27,  1.14s/it]Loading train:  36%|███▌      | 192/532 [03:08<06:25,  1.13s/it]Loading train:  36%|███▋      | 193/532 [03:09<06:20,  1.12s/it]Loading train:  36%|███▋      | 194/532 [03:10<06:11,  1.10s/it]Loading train:  37%|███▋      | 195/532 [03:12<06:07,  1.09s/it]Loading train:  37%|███▋      | 196/532 [03:13<06:09,  1.10s/it]Loading train:  37%|███▋      | 197/532 [03:14<06:08,  1.10s/it]Loading train:  37%|███▋      | 198/532 [03:15<05:59,  1.08s/it]Loading train:  37%|███▋      | 199/532 [03:16<05:47,  1.04s/it]Loading train:  38%|███▊      | 200/532 [03:17<05:38,  1.02s/it]Loading train:  38%|███▊      | 201/532 [03:18<05:28,  1.01it/s]Loading train:  38%|███▊      | 202/532 [03:19<05:23,  1.02it/s]Loading train:  38%|███▊      | 203/532 [03:20<05:32,  1.01s/it]Loading train:  38%|███▊      | 204/532 [03:20<05:11,  1.05it/s]Loading train:  39%|███▊      | 205/532 [03:21<04:55,  1.10it/s]Loading train:  39%|███▊      | 206/532 [03:22<04:45,  1.14it/s]Loading train:  39%|███▉      | 207/532 [03:23<04:31,  1.20it/s]Loading train:  39%|███▉      | 208/532 [03:24<04:17,  1.26it/s]Loading train:  39%|███▉      | 209/532 [03:24<04:15,  1.27it/s]Loading train:  39%|███▉      | 210/532 [03:25<04:04,  1.31it/s]Loading train:  40%|███▉      | 211/532 [03:26<03:58,  1.35it/s]Loading train:  40%|███▉      | 212/532 [03:26<04:01,  1.32it/s]Loading train:  40%|████      | 213/532 [03:27<04:02,  1.31it/s]Loading train:  40%|████      | 214/532 [03:28<04:02,  1.31it/s]Loading train:  40%|████      | 215/532 [03:29<04:40,  1.13it/s]Loading train:  41%|████      | 216/532 [03:30<04:59,  1.06it/s]Loading train:  41%|████      | 217/532 [03:31<05:09,  1.02it/s]Loading train:  41%|████      | 218/532 [03:32<05:17,  1.01s/it]Loading train:  41%|████      | 219/532 [03:33<05:24,  1.04s/it]Loading train:  41%|████▏     | 220/532 [03:35<05:28,  1.05s/it]Loading train:  42%|████▏     | 221/532 [03:35<05:07,  1.01it/s]Loading train:  42%|████▏     | 222/532 [03:36<04:41,  1.10it/s]Loading train:  42%|████▏     | 223/532 [03:37<04:23,  1.17it/s]Loading train:  42%|████▏     | 224/532 [03:38<04:16,  1.20it/s]Loading train:  42%|████▏     | 225/532 [03:38<04:07,  1.24it/s]Loading train:  42%|████▏     | 226/532 [03:39<04:03,  1.26it/s]Loading train:  43%|████▎     | 227/532 [03:40<03:54,  1.30it/s]Loading train:  43%|████▎     | 228/532 [03:41<03:44,  1.35it/s]Loading train:  43%|████▎     | 229/532 [03:41<03:45,  1.34it/s]Loading train:  43%|████▎     | 230/532 [03:42<03:43,  1.35it/s]Loading train:  43%|████▎     | 231/532 [03:43<03:43,  1.35it/s]Loading train:  44%|████▎     | 232/532 [03:43<03:37,  1.38it/s]Loading train:  44%|████▍     | 233/532 [03:44<03:51,  1.29it/s]Loading train:  44%|████▍     | 234/532 [03:45<03:51,  1.29it/s]Loading train:  44%|████▍     | 235/532 [03:46<03:52,  1.28it/s]Loading train:  44%|████▍     | 236/532 [03:47<03:56,  1.25it/s]Loading train:  45%|████▍     | 237/532 [03:48<03:53,  1.26it/s]Loading train:  45%|████▍     | 238/532 [03:48<04:03,  1.21it/s]Loading train:  45%|████▍     | 239/532 [03:49<04:18,  1.13it/s]Loading train:  45%|████▌     | 240/532 [03:50<04:20,  1.12it/s]Loading train:  45%|████▌     | 241/532 [03:51<04:24,  1.10it/s]Loading train:  45%|████▌     | 242/532 [03:52<04:38,  1.04it/s]Loading train:  46%|████▌     | 243/532 [03:53<04:33,  1.06it/s]Loading train:  46%|████▌     | 244/532 [03:54<04:21,  1.10it/s]Loading train:  46%|████▌     | 245/532 [03:55<04:03,  1.18it/s]Loading train:  46%|████▌     | 246/532 [03:55<03:45,  1.27it/s]Loading train:  46%|████▋     | 247/532 [03:56<03:31,  1.34it/s]Loading train:  47%|████▋     | 248/532 [03:57<03:30,  1.35it/s]Loading train:  47%|████▋     | 249/532 [03:58<03:24,  1.38it/s]Loading train:  47%|████▋     | 250/532 [03:58<03:20,  1.41it/s]Loading train:  47%|████▋     | 251/532 [03:59<03:41,  1.27it/s]Loading train:  47%|████▋     | 252/532 [04:00<03:41,  1.26it/s]Loading train:  48%|████▊     | 253/532 [04:01<03:47,  1.22it/s]Loading train:  48%|████▊     | 254/532 [04:02<03:44,  1.24it/s]Loading train:  48%|████▊     | 255/532 [04:02<03:40,  1.25it/s]Loading train:  48%|████▊     | 256/532 [04:03<03:38,  1.27it/s]Loading train:  48%|████▊     | 257/532 [04:04<04:09,  1.10it/s]Loading train:  48%|████▊     | 258/532 [04:05<04:14,  1.08it/s]Loading train:  49%|████▊     | 259/532 [04:06<04:19,  1.05it/s]Loading train:  49%|████▉     | 260/532 [04:07<04:19,  1.05it/s]Loading train:  49%|████▉     | 261/532 [04:08<04:17,  1.05it/s]Loading train:  49%|████▉     | 262/532 [04:09<04:26,  1.01it/s]Loading train:  49%|████▉     | 263/532 [04:10<04:15,  1.05it/s]Loading train:  50%|████▉     | 264/532 [04:11<04:09,  1.07it/s]Loading train:  50%|████▉     | 265/532 [04:12<04:02,  1.10it/s]Loading train:  50%|█████     | 266/532 [04:13<03:51,  1.15it/s]Loading train:  50%|█████     | 267/532 [04:13<03:41,  1.20it/s]Loading train:  50%|█████     | 268/532 [04:14<03:33,  1.24it/s]Loading train:  51%|█████     | 269/532 [04:15<03:43,  1.18it/s]Loading train:  51%|█████     | 270/532 [04:16<03:44,  1.17it/s]Loading train:  51%|█████     | 271/532 [04:17<03:47,  1.15it/s]Loading train:  51%|█████     | 272/532 [04:18<03:46,  1.15it/s]Loading train:  51%|█████▏    | 273/532 [04:19<03:41,  1.17it/s]Loading train:  52%|█████▏    | 274/532 [04:20<03:43,  1.16it/s]Loading train:  52%|█████▏    | 275/532 [04:21<04:04,  1.05it/s]Loading train:  52%|█████▏    | 276/532 [04:22<04:15,  1.00it/s]Loading train:  52%|█████▏    | 277/532 [04:23<04:21,  1.02s/it]Loading train:  52%|█████▏    | 278/532 [04:24<04:23,  1.04s/it]Loading train:  52%|█████▏    | 279/532 [04:25<04:27,  1.06s/it]Loading train:  53%|█████▎    | 280/532 [04:26<04:28,  1.07s/it]Loading train:  53%|█████▎    | 281/532 [04:27<04:32,  1.09s/it]Loading train:  53%|█████▎    | 282/532 [04:28<04:30,  1.08s/it]Loading train:  53%|█████▎    | 283/532 [04:29<04:23,  1.06s/it]Loading train:  53%|█████▎    | 284/532 [04:30<04:22,  1.06s/it]Loading train:  54%|█████▎    | 285/532 [04:31<04:23,  1.07s/it]Loading train:  54%|█████▍    | 286/532 [04:33<04:23,  1.07s/it]Loading train:  54%|█████▍    | 287/532 [04:33<04:01,  1.01it/s]Loading train:  54%|█████▍    | 288/532 [04:34<03:48,  1.07it/s]Loading train:  54%|█████▍    | 289/532 [04:35<03:38,  1.11it/s]Loading train:  55%|█████▍    | 290/532 [04:36<03:27,  1.16it/s]Loading train:  55%|█████▍    | 291/532 [04:36<03:16,  1.23it/s]Loading train:  55%|█████▍    | 292/532 [04:37<03:16,  1.22it/s]Loading train:  55%|█████▌    | 293/532 [04:38<03:25,  1.16it/s]Loading train:  55%|█████▌    | 294/532 [04:39<03:35,  1.11it/s]Loading train:  55%|█████▌    | 295/532 [04:40<03:36,  1.09it/s]Loading train:  56%|█████▌    | 296/532 [04:41<03:34,  1.10it/s]Loading train:  56%|█████▌    | 297/532 [04:42<03:36,  1.08it/s]Loading train:  56%|█████▌    | 298/532 [04:43<03:37,  1.08it/s]Loading train:  56%|█████▌    | 299/532 [04:44<03:29,  1.11it/s]Loading train:  56%|█████▋    | 300/532 [04:45<03:18,  1.17it/s]Loading train:  57%|█████▋    | 301/532 [04:45<03:14,  1.19it/s]Loading train:  57%|█████▋    | 302/532 [04:46<03:12,  1.19it/s]Loading train:  57%|█████▋    | 303/532 [04:47<03:10,  1.20it/s]Loading train:  57%|█████▋    | 304/532 [04:48<03:12,  1.18it/s]Loading train:  57%|█████▋    | 305/532 [04:49<03:33,  1.06it/s]Loading train:  58%|█████▊    | 306/532 [04:50<03:45,  1.00it/s]Loading train:  58%|█████▊    | 307/532 [04:51<03:53,  1.04s/it]Loading train:  58%|█████▊    | 308/532 [04:52<03:56,  1.06s/it]Loading train:  58%|█████▊    | 309/532 [04:54<03:59,  1.07s/it]Loading train:  58%|█████▊    | 310/532 [04:55<04:03,  1.10s/it]Loading train:  58%|█████▊    | 311/532 [04:56<04:40,  1.27s/it]Loading train:  59%|█████▊    | 312/532 [04:58<04:55,  1.34s/it]Loading train:  59%|█████▉    | 313/532 [04:59<05:09,  1.41s/it]Loading train:  59%|█████▉    | 314/532 [05:01<05:09,  1.42s/it]Loading train:  59%|█████▉    | 315/532 [05:02<05:08,  1.42s/it]Loading train:  59%|█████▉    | 316/532 [05:04<05:07,  1.42s/it]Loading train:  60%|█████▉    | 317/532 [05:05<04:35,  1.28s/it]Loading train:  60%|█████▉    | 318/532 [05:06<04:04,  1.14s/it]Loading train:  60%|█████▉    | 319/532 [05:06<03:43,  1.05s/it]Loading train:  60%|██████    | 320/532 [05:07<03:24,  1.03it/s]Loading train:  60%|██████    | 321/532 [05:08<03:17,  1.07it/s]Loading train:  61%|██████    | 322/532 [05:09<03:09,  1.11it/s]Loading train:  61%|██████    | 323/532 [05:10<03:30,  1.01s/it]Loading train:  61%|██████    | 324/532 [05:11<03:37,  1.04s/it]Loading train:  61%|██████    | 325/532 [05:12<03:41,  1.07s/it]Loading train:  61%|██████▏   | 326/532 [05:13<03:46,  1.10s/it]Loading train:  61%|██████▏   | 327/532 [05:15<03:51,  1.13s/it]Loading train:  62%|██████▏   | 328/532 [05:16<03:49,  1.13s/it]Loading train:  62%|██████▏   | 329/532 [05:17<03:36,  1.06s/it]Loading train:  62%|██████▏   | 330/532 [05:18<03:18,  1.02it/s]Loading train:  62%|██████▏   | 331/532 [05:18<03:08,  1.06it/s]Loading train:  62%|██████▏   | 332/532 [05:19<03:00,  1.11it/s]Loading train:  63%|██████▎   | 333/532 [05:20<02:52,  1.16it/s]Loading train:  63%|██████▎   | 334/532 [05:21<02:45,  1.19it/s]Loading train:  63%|██████▎   | 335/532 [05:22<03:04,  1.07it/s]Loading train:  63%|██████▎   | 336/532 [05:23<03:12,  1.02it/s]Loading train:  63%|██████▎   | 337/532 [05:24<03:14,  1.00it/s]Loading train:  64%|██████▎   | 338/532 [05:25<03:14,  1.00s/it]Loading train:  64%|██████▎   | 339/532 [05:26<03:14,  1.01s/it]Loading train:  64%|██████▍   | 340/532 [05:27<03:17,  1.03s/it]Loading train:  64%|██████▍   | 341/532 [05:28<03:10,  1.00it/s]Loading train:  64%|██████▍   | 342/532 [05:29<02:53,  1.09it/s]Loading train:  64%|██████▍   | 343/532 [05:30<02:43,  1.16it/s]Loading train:  65%|██████▍   | 344/532 [05:30<02:35,  1.21it/s]Loading train:  65%|██████▍   | 345/532 [05:31<02:39,  1.17it/s]Loading train:  65%|██████▌   | 346/532 [05:32<02:32,  1.22it/s]Loading train:  65%|██████▌   | 347/532 [05:33<02:35,  1.19it/s]Loading train:  65%|██████▌   | 348/532 [05:34<02:38,  1.16it/s]Loading train:  66%|██████▌   | 349/532 [05:35<02:39,  1.15it/s]Loading train:  66%|██████▌   | 350/532 [05:35<02:38,  1.15it/s]Loading train:  66%|██████▌   | 351/532 [05:37<02:45,  1.10it/s]Loading train:  66%|██████▌   | 352/532 [05:37<02:44,  1.10it/s]Loading train:  66%|██████▋   | 353/532 [05:38<02:46,  1.08it/s]Loading train:  67%|██████▋   | 354/532 [05:39<02:41,  1.10it/s]Loading train:  67%|██████▋   | 355/532 [05:40<02:38,  1.12it/s]Loading train:  67%|██████▋   | 356/532 [05:41<02:42,  1.09it/s]Loading train:  67%|██████▋   | 357/532 [05:42<02:37,  1.11it/s]Loading train:  67%|██████▋   | 358/532 [05:43<02:35,  1.12it/s]Loading train:  67%|██████▋   | 359/532 [05:44<02:36,  1.10it/s]Loading train:  68%|██████▊   | 360/532 [05:45<02:36,  1.10it/s]Loading train:  68%|██████▊   | 361/532 [05:46<02:32,  1.12it/s]Loading train:  68%|██████▊   | 362/532 [05:46<02:35,  1.10it/s]Loading train:  68%|██████▊   | 363/532 [05:47<02:25,  1.16it/s]Loading train:  68%|██████▊   | 364/532 [05:48<02:21,  1.19it/s]Loading train:  69%|██████▊   | 365/532 [05:49<02:14,  1.25it/s]Loading train:  69%|██████▉   | 366/532 [05:49<02:07,  1.30it/s]Loading train:  69%|██████▉   | 367/532 [05:50<02:06,  1.30it/s]Loading train:  69%|██████▉   | 368/532 [05:51<02:03,  1.33it/s]Loading train:  69%|██████▉   | 369/532 [05:52<02:00,  1.35it/s]Loading train:  70%|██████▉   | 370/532 [05:52<01:57,  1.38it/s]Loading train:  70%|██████▉   | 371/532 [05:53<02:18,  1.16it/s]Loading train:  70%|██████▉   | 372/532 [05:54<02:21,  1.13it/s]Loading train:  70%|███████   | 373/532 [05:55<02:26,  1.08it/s]Loading train:  70%|███████   | 374/532 [05:56<02:26,  1.08it/s]Loading train:  70%|███████   | 375/532 [05:57<02:25,  1.08it/s]Loading train:  71%|███████   | 376/532 [05:58<02:28,  1.05it/s]Loading train:  71%|███████   | 377/532 [05:59<02:26,  1.06it/s]Loading train:  71%|███████   | 378/532 [06:00<02:22,  1.08it/s]Loading train:  71%|███████   | 379/532 [06:01<02:17,  1.11it/s]Loading train:  71%|███████▏  | 380/532 [06:02<02:16,  1.11it/s]Loading train:  72%|███████▏  | 381/532 [06:03<02:16,  1.10it/s]Loading train:  72%|███████▏  | 382/532 [06:04<02:16,  1.10it/s]Loading train:  72%|███████▏  | 383/532 [06:05<02:20,  1.06it/s]Loading train:  72%|███████▏  | 384/532 [06:06<02:20,  1.05it/s]Loading train:  72%|███████▏  | 385/532 [06:07<02:17,  1.07it/s]Loading train:  73%|███████▎  | 386/532 [06:07<02:12,  1.10it/s]Loading train:  73%|███████▎  | 387/532 [06:08<02:10,  1.12it/s]Loading train:  73%|███████▎  | 388/532 [06:09<02:06,  1.14it/s]Loading train:  73%|███████▎  | 389/532 [06:10<02:15,  1.06it/s]Loading train:  73%|███████▎  | 390/532 [06:11<02:16,  1.04it/s]Loading train:  73%|███████▎  | 391/532 [06:12<02:18,  1.02it/s]Loading train:  74%|███████▎  | 392/532 [06:13<02:18,  1.01it/s]Loading train:  74%|███████▍  | 393/532 [06:14<02:14,  1.04it/s]Loading train:  74%|███████▍  | 394/532 [06:15<02:12,  1.04it/s]Loading train:  74%|███████▍  | 395/532 [06:16<02:12,  1.03it/s]Loading train:  74%|███████▍  | 396/532 [06:17<02:13,  1.02it/s]Loading train:  75%|███████▍  | 397/532 [06:18<02:11,  1.03it/s]Loading train:  75%|███████▍  | 398/532 [06:19<02:07,  1.05it/s]Loading train:  75%|███████▌  | 399/532 [06:20<02:06,  1.05it/s]Loading train:  75%|███████▌  | 400/532 [06:21<02:07,  1.03it/s]Loading train:  75%|███████▌  | 401/532 [06:22<02:12,  1.01s/it]Loading train:  76%|███████▌  | 402/532 [06:23<02:11,  1.01s/it]Loading train:  76%|███████▌  | 403/532 [06:24<02:10,  1.01s/it]Loading train:  76%|███████▌  | 404/532 [06:25<02:10,  1.02s/it]Loading train:  76%|███████▌  | 405/532 [06:26<02:08,  1.01s/it]Loading train:  76%|███████▋  | 406/532 [06:27<02:05,  1.00it/s]Loading train:  77%|███████▋  | 407/532 [06:28<02:05,  1.00s/it]Loading train:  77%|███████▋  | 408/532 [06:29<01:56,  1.07it/s]Loading train:  77%|███████▋  | 409/532 [06:30<01:49,  1.12it/s]Loading train:  77%|███████▋  | 410/532 [06:31<01:49,  1.11it/s]Loading train:  77%|███████▋  | 411/532 [06:31<01:45,  1.15it/s]Loading train:  77%|███████▋  | 412/532 [06:32<01:42,  1.17it/s]Loading train:  78%|███████▊  | 413/532 [06:33<01:47,  1.11it/s]Loading train:  78%|███████▊  | 414/532 [06:34<01:47,  1.10it/s]Loading train:  78%|███████▊  | 415/532 [06:35<01:45,  1.11it/s]Loading train:  78%|███████▊  | 416/532 [06:36<01:42,  1.13it/s]Loading train:  78%|███████▊  | 417/532 [06:37<01:42,  1.12it/s]Loading train:  79%|███████▊  | 418/532 [06:38<01:39,  1.15it/s]Loading train:  79%|███████▉  | 419/532 [06:39<01:44,  1.08it/s]Loading train:  79%|███████▉  | 420/532 [06:40<01:43,  1.08it/s]Loading train:  79%|███████▉  | 421/532 [06:41<01:43,  1.07it/s]Loading train:  79%|███████▉  | 422/532 [06:41<01:43,  1.06it/s]Loading train:  80%|███████▉  | 423/532 [06:43<01:45,  1.03it/s]Loading train:  80%|███████▉  | 424/532 [06:43<01:44,  1.04it/s]Loading train:  80%|███████▉  | 425/532 [06:44<01:43,  1.03it/s]Loading train:  80%|████████  | 426/532 [06:45<01:43,  1.02it/s]Loading train:  80%|████████  | 427/532 [06:46<01:44,  1.00it/s]Loading train:  80%|████████  | 428/532 [06:47<01:40,  1.04it/s]Loading train:  81%|████████  | 429/532 [06:48<01:39,  1.04it/s]Loading train:  81%|████████  | 430/532 [06:49<01:39,  1.02it/s]Loading train:  81%|████████  | 431/532 [06:50<01:40,  1.01it/s]Loading train:  81%|████████  | 432/532 [06:51<01:40,  1.01s/it]Loading train:  81%|████████▏ | 433/532 [06:52<01:40,  1.02s/it]Loading train:  82%|████████▏ | 434/532 [06:53<01:38,  1.01s/it]Loading train:  82%|████████▏ | 435/532 [06:54<01:36,  1.00it/s]Loading train:  82%|████████▏ | 436/532 [06:55<01:36,  1.00s/it]Loading train:  82%|████████▏ | 437/532 [06:56<01:30,  1.06it/s]Loading train:  82%|████████▏ | 438/532 [06:57<01:23,  1.12it/s]Loading train:  83%|████████▎ | 439/532 [06:58<01:18,  1.19it/s]Loading train:  83%|████████▎ | 440/532 [06:59<01:15,  1.21it/s]Loading train:  83%|████████▎ | 441/532 [06:59<01:14,  1.22it/s]Loading train:  83%|████████▎ | 442/532 [07:00<01:13,  1.22it/s]Loading train:  83%|████████▎ | 443/532 [07:01<01:13,  1.21it/s]Loading train:  83%|████████▎ | 444/532 [07:02<01:10,  1.25it/s]Loading train:  84%|████████▎ | 445/532 [07:03<01:10,  1.24it/s]Loading train:  84%|████████▍ | 446/532 [07:03<01:06,  1.30it/s]Loading train:  84%|████████▍ | 447/532 [07:04<01:03,  1.34it/s]Loading train:  84%|████████▍ | 448/532 [07:05<01:02,  1.34it/s]Loading train:  84%|████████▍ | 449/532 [07:06<01:04,  1.29it/s]Loading train:  85%|████████▍ | 450/532 [07:06<01:03,  1.28it/s]Loading train:  85%|████████▍ | 451/532 [07:07<01:06,  1.22it/s]Loading train:  85%|████████▍ | 452/532 [07:08<01:05,  1.21it/s]Loading train:  85%|████████▌ | 453/532 [07:09<01:04,  1.23it/s]Loading train:  85%|████████▌ | 454/532 [07:10<01:03,  1.22it/s]Loading train:  86%|████████▌ | 455/532 [07:10<01:03,  1.22it/s]Loading train:  86%|████████▌ | 456/532 [07:11<01:03,  1.21it/s]Loading train:  86%|████████▌ | 457/532 [07:12<01:03,  1.19it/s]Loading train:  86%|████████▌ | 458/532 [07:13<01:04,  1.14it/s]Loading train:  86%|████████▋ | 459/532 [07:14<01:04,  1.13it/s]Loading train:  86%|████████▋ | 460/532 [07:15<01:04,  1.11it/s]Loading train:  87%|████████▋ | 461/532 [07:16<01:11,  1.01s/it]Loading train:  87%|████████▋ | 462/532 [07:17<01:11,  1.03s/it]Loading train:  87%|████████▋ | 463/532 [07:18<01:13,  1.06s/it]Loading train:  87%|████████▋ | 464/532 [07:20<01:13,  1.08s/it]Loading train:  87%|████████▋ | 465/532 [07:21<01:15,  1.13s/it]Loading train:  88%|████████▊ | 466/532 [07:22<01:16,  1.16s/it]Loading train:  88%|████████▊ | 467/532 [07:23<01:10,  1.09s/it]Loading train:  88%|████████▊ | 468/532 [07:24<01:08,  1.06s/it]Loading train:  88%|████████▊ | 469/532 [07:25<01:03,  1.01s/it]Loading train:  88%|████████▊ | 470/532 [07:26<01:03,  1.02s/it]Loading train:  89%|████████▊ | 471/532 [07:27<00:59,  1.02it/s]Loading train:  89%|████████▊ | 472/532 [07:28<00:57,  1.04it/s]Loading train:  89%|████████▉ | 473/532 [07:29<00:57,  1.03it/s]Loading train:  89%|████████▉ | 474/532 [07:30<00:54,  1.07it/s]Loading train:  89%|████████▉ | 475/532 [07:31<00:54,  1.05it/s]Loading train:  89%|████████▉ | 476/532 [07:32<00:52,  1.06it/s]Loading train:  90%|████████▉ | 477/532 [07:33<00:53,  1.03it/s]Loading train:  90%|████████▉ | 478/532 [07:34<00:52,  1.02it/s]Loading train:  90%|█████████ | 479/532 [07:34<00:48,  1.09it/s]Loading train:  90%|█████████ | 480/532 [07:35<00:47,  1.10it/s]Loading train:  90%|█████████ | 481/532 [07:36<00:44,  1.14it/s]Loading train:  91%|█████████ | 482/532 [07:37<00:41,  1.20it/s]Loading train:  91%|█████████ | 483/532 [07:38<00:40,  1.21it/s]Loading train:  91%|█████████ | 484/532 [07:38<00:40,  1.17it/s]Loading train:  91%|█████████ | 485/532 [07:40<00:45,  1.04it/s]Loading train:  91%|█████████▏| 486/532 [07:41<00:45,  1.01it/s]Loading train:  92%|█████████▏| 487/532 [07:42<00:45,  1.01s/it]Loading train:  92%|█████████▏| 488/532 [07:43<00:45,  1.04s/it]Loading train:  92%|█████████▏| 489/532 [07:44<00:45,  1.05s/it]Loading train:  92%|█████████▏| 490/532 [07:45<00:44,  1.06s/it]Loading train:  92%|█████████▏| 491/532 [07:46<00:40,  1.00it/s]Loading train:  92%|█████████▏| 492/532 [07:47<00:37,  1.06it/s]Loading train:  93%|█████████▎| 493/532 [07:48<00:35,  1.09it/s]Loading train:  93%|█████████▎| 494/532 [07:48<00:33,  1.13it/s]Loading train:  93%|█████████▎| 495/532 [07:49<00:32,  1.15it/s]Loading train:  93%|█████████▎| 496/532 [07:50<00:32,  1.11it/s]Loading train:  93%|█████████▎| 497/532 [07:51<00:32,  1.09it/s]Loading train:  94%|█████████▎| 498/532 [07:52<00:32,  1.05it/s]Loading train:  94%|█████████▍| 499/532 [07:53<00:31,  1.05it/s]Loading train:  94%|█████████▍| 500/532 [07:54<00:30,  1.04it/s]Loading train:  94%|█████████▍| 501/532 [07:55<00:32,  1.05s/it]Loading train:  94%|█████████▍| 502/532 [07:56<00:30,  1.01s/it]Loading train:  95%|█████████▍| 503/532 [07:57<00:29,  1.00s/it]Loading train:  95%|█████████▍| 504/532 [07:58<00:27,  1.02it/s]Loading train:  95%|█████████▍| 505/532 [07:59<00:25,  1.07it/s]Loading train:  95%|█████████▌| 506/532 [08:00<00:23,  1.09it/s]Loading train:  95%|█████████▌| 507/532 [08:01<00:22,  1.12it/s]Loading train:  95%|█████████▌| 508/532 [08:02<00:21,  1.13it/s]Loading train:  96%|█████████▌| 509/532 [08:03<00:21,  1.06it/s]Loading train:  96%|█████████▌| 510/532 [08:04<00:21,  1.01it/s]Loading train:  96%|█████████▌| 511/532 [08:05<00:21,  1.02s/it]Loading train:  96%|█████████▌| 512/532 [08:06<00:20,  1.04s/it]Loading train:  96%|█████████▋| 513/532 [08:07<00:20,  1.08s/it]Loading train:  97%|█████████▋| 514/532 [08:08<00:19,  1.06s/it]Loading train:  97%|█████████▋| 515/532 [08:09<00:17,  1.02s/it]Loading train:  97%|█████████▋| 516/532 [08:10<00:15,  1.02it/s]Loading train:  97%|█████████▋| 517/532 [08:11<00:14,  1.05it/s]Loading train:  97%|█████████▋| 518/532 [08:12<00:12,  1.08it/s]Loading train:  98%|█████████▊| 519/532 [08:13<00:11,  1.10it/s]Loading train:  98%|█████████▊| 520/532 [08:13<00:10,  1.11it/s]Loading train:  98%|█████████▊| 521/532 [08:14<00:10,  1.08it/s]Loading train:  98%|█████████▊| 522/532 [08:15<00:09,  1.08it/s]Loading train:  98%|█████████▊| 523/532 [08:16<00:08,  1.06it/s]Loading train:  98%|█████████▊| 524/532 [08:17<00:07,  1.08it/s]Loading train:  99%|█████████▊| 525/532 [08:18<00:06,  1.07it/s]Loading train:  99%|█████████▉| 526/532 [08:19<00:05,  1.10it/s]Loading train:  99%|█████████▉| 527/532 [08:20<00:04,  1.11it/s]Loading train:  99%|█████████▉| 528/532 [08:21<00:03,  1.16it/s]Loading train:  99%|█████████▉| 529/532 [08:22<00:02,  1.17it/s]Loading train: 100%|█████████▉| 530/532 [08:22<00:01,  1.18it/s]Loading train: 100%|█████████▉| 531/532 [08:23<00:00,  1.22it/s]Loading train: 100%|██████████| 532/532 [08:24<00:00,  1.22it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   4%|▎         | 19/532 [00:00<00:02, 180.19it/s]concatenating: train:   6%|▌         | 33/532 [00:00<00:03, 157.69it/s]concatenating: train:   9%|▉         | 50/532 [00:00<00:03, 160.47it/s]concatenating: train:  14%|█▎        | 72/532 [00:00<00:02, 173.71it/s]concatenating: train:  18%|█▊        | 96/532 [00:00<00:02, 187.18it/s]concatenating: train:  22%|██▏       | 119/532 [00:00<00:02, 196.44it/s]concatenating: train:  26%|██▌       | 139/532 [00:00<00:01, 196.92it/s]concatenating: train:  31%|███       | 163/532 [00:00<00:01, 207.40it/s]concatenating: train:  36%|███▌      | 190/532 [00:00<00:01, 221.19it/s]concatenating: train:  40%|████      | 213/532 [00:01<00:01, 221.78it/s]concatenating: train:  44%|████▍     | 236/532 [00:01<00:01, 212.18it/s]concatenating: train:  49%|████▉     | 260/532 [00:01<00:01, 218.54it/s]concatenating: train:  54%|█████▎    | 285/532 [00:01<00:01, 226.62it/s]concatenating: train:  58%|█████▊    | 308/532 [00:01<00:00, 225.55it/s]concatenating: train:  63%|██████▎   | 334/532 [00:01<00:00, 234.89it/s]concatenating: train:  67%|██████▋   | 358/532 [00:01<00:00, 232.42it/s]concatenating: train:  72%|███████▏  | 382/532 [00:01<00:00, 222.51it/s]concatenating: train:  77%|███████▋  | 409/532 [00:01<00:00, 233.16it/s]concatenating: train:  82%|████████▏ | 434/532 [00:01<00:00, 235.54it/s]concatenating: train:  86%|████████▌ | 458/532 [00:02<00:00, 228.64it/s]concatenating: train:  91%|█████████ | 482/532 [00:02<00:00, 227.05it/s]concatenating: train:  96%|█████████▌| 509/532 [00:02<00:00, 236.62it/s]concatenating: train: 100%|██████████| 532/532 [00:02<00:00, 222.85it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:10,  1.32it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:09,  1.31it/s]Loading test:  20%|██        | 3/15 [00:02<00:09,  1.26it/s]Loading test:  27%|██▋       | 4/15 [00:03<00:09,  1.15it/s]Loading test:  33%|███▎      | 5/15 [00:04<00:09,  1.07it/s]Loading test:  40%|████      | 6/15 [00:05<00:09,  1.01s/it]Loading test:  47%|████▋     | 7/15 [00:06<00:07,  1.08it/s]Loading test:  53%|█████▎    | 8/15 [00:07<00:06,  1.00it/s]Loading test:  60%|██████    | 9/15 [00:08<00:05,  1.03it/s]Loading test:  67%|██████▋   | 10/15 [00:09<00:04,  1.08it/s]Loading test:  73%|███████▎  | 11/15 [00:10<00:03,  1.10it/s]Loading test:  80%|████████  | 12/15 [00:11<00:02,  1.06it/s]Loading test:  87%|████████▋ | 13/15 [00:12<00:02,  1.01s/it]Loading test:  93%|█████████▎| 14/15 [00:13<00:00,  1.03it/s]Loading test: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation:  53%|█████▎    | 8/15 [00:00<00:00, 79.63it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 131.91it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 84, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 84, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 84, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 84, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 84, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 84, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 84, 31)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 42, 31)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 42, 31)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 42, 60)   16800       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 42, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 42, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 42, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 42, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 42, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 42, 91)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 21, 91)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 21, 91)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 21, 120)  98400       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 21, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 21, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 21, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 21, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 21, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 21, 211)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 21, 211)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 42, 60)   50700       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 42, 151)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 42, 60)   81600       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 42, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________2019-07-06 13:23:25.063253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 13:23:25.063384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 13:23:25.063400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 13:23:25.063409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 13:23:25.063800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

activation_7 (Activation)       (None, 28, 42, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 42, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 42, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 42, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 42, 211)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 28, 42, 211)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 84, 30)   25350       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 84, 61)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 84, 30)   16500       concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 84, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 84, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 84, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 84, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 84, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 84, 91)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 84, 91)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 84, 13)   1196        dropout_5[0][0]                  
==================================================================================================
Total params: 504,146
Trainable params: 502,946
Non-trainable params: 1,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53383120e-02 2.88786827e-02 1.16652967e-01 1.00133292e-02
 3.03165963e-02 5.79538965e-03 6.85058777e-02 1.28145429e-01
 7.55135052e-02 1.22435092e-02 2.73464902e-01 1.84941443e-01
 1.90056842e-04]
Train on 20749 samples, validate on 584 samples
Epoch 1/300
 - 34s - loss: 21.3843 - acc: 0.7788 - mDice: 0.0422 - val_loss: 4.5454 - val_acc: 0.9135 - val_mDice: 0.0316

Epoch 00001: val_mDice improved from -inf to 0.03155, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 28s - loss: 3.0885 - acc: 0.9170 - mDice: 0.2107 - val_loss: 1.9435 - val_acc: 0.9460 - val_mDice: 0.3422

Epoch 00002: val_mDice improved from 0.03155 to 0.34223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 27s - loss: 1.9466 - acc: 0.9400 - mDice: 0.4319 - val_loss: 1.1778 - val_acc: 0.9635 - val_mDice: 0.6037

Epoch 00003: val_mDice improved from 0.34223 to 0.60370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 28s - loss: 1.5333 - acc: 0.9521 - mDice: 0.5371 - val_loss: 0.9489 - val_acc: 0.9689 - val_mDice: 0.6881

Epoch 00004: val_mDice improved from 0.60370 to 0.68810, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 26s - loss: 1.3259 - acc: 0.9573 - mDice: 0.5946 - val_loss: 0.8892 - val_acc: 0.9711 - val_mDice: 0.7136

Epoch 00005: val_mDice improved from 0.68810 to 0.71355, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 27s - loss: 1.2148 - acc: 0.9599 - mDice: 0.6255 - val_loss: 0.8479 - val_acc: 0.9729 - val_mDice: 0.7233

Epoch 00006: val_mDice improved from 0.71355 to 0.72331, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 28s - loss: 1.1345 - acc: 0.9616 - mDice: 0.6470 - val_loss: 0.8004 - val_acc: 0.9749 - val_mDice: 0.7370

Epoch 00007: val_mDice improved from 0.72331 to 0.73703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300
 - 26s - loss: 1.0697 - acc: 0.9629 - mDice: 0.6637 - val_loss: 0.7821 - val_acc: 0.9761 - val_mDice: 0.7439

Epoch 00008: val_mDice improved from 0.73703 to 0.74393, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300
 - 27s - loss: 1.0142 - acc: 0.9641 - mDice: 0.6788 - val_loss: 0.7568 - val_acc: 0.9761 - val_mDice: 0.7522

Epoch 00009: val_mDice improved from 0.74393 to 0.75225, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300
 - 28s - loss: 0.9781 - acc: 0.9648 - mDice: 0.6889 - val_loss: 0.7645 - val_acc: 0.9759 - val_mDice: 0.7528

Epoch 00010: val_mDice improved from 0.75225 to 0.75282, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 26s - loss: 0.9425 - acc: 0.9655 - mDice: 0.6980 - val_loss: 0.7381 - val_acc: 0.9768 - val_mDice: 0.7592

Epoch 00011: val_mDice improved from 0.75282 to 0.75920, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 26s - loss: 0.9122 - acc: 0.9661 - mDice: 0.7062 - val_loss: 0.7389 - val_acc: 0.9771 - val_mDice: 0.7615

Epoch 00012: val_mDice improved from 0.75920 to 0.76149, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300
 - 28s - loss: 0.8810 - acc: 0.9667 - mDice: 0.7145 - val_loss: 0.7221 - val_acc: 0.9773 - val_mDice: 0.7631

Epoch 00013: val_mDice improved from 0.76149 to 0.76306, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 27s - loss: 0.8569 - acc: 0.9672 - mDice: 0.7211 - val_loss: 0.7133 - val_acc: 0.9781 - val_mDice: 0.7694

Epoch 00014: val_mDice improved from 0.76306 to 0.76945, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 26s - loss: 0.8414 - acc: 0.9674 - mDice: 0.7254 - val_loss: 0.7236 - val_acc: 0.9787 - val_mDice: 0.7637

Epoch 00015: val_mDice did not improve from 0.76945
Epoch 16/300
 - 26s - loss: 0.8240 - acc: 0.9678 - mDice: 0.7298 - val_loss: 0.7255 - val_acc: 0.9785 - val_mDice: 0.7654

Epoch 00016: val_mDice did not improve from 0.76945
Epoch 17/300
 - 28s - loss: 0.8035 - acc: 0.9682 - mDice: 0.7356 - val_loss: 0.7178 - val_acc: 0.9782 - val_mDice: 0.7685

Epoch 00017: val_mDice did not improve from 0.76945
Epoch 18/300
 - 27s - loss: 0.7903 - acc: 0.9684 - mDice: 0.7388 - val_loss: 0.7049 - val_acc: 0.9784 - val_mDice: 0.7702

Epoch 00018: val_mDice improved from 0.76945 to 0.77024, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 19/300
 - 26s - loss: 0.7780 - acc: 0.9686 - mDice: 0.7422 - val_loss: 0.6948 - val_acc: 0.9791 - val_mDice: 0.7692

Epoch 00019: val_mDice did not improve from 0.77024
Epoch 20/300
 - 26s - loss: 0.7619 - acc: 0.9689 - mDice: 0.7467 - val_loss: 0.6882 - val_acc: 0.9794 - val_mDice: 0.7715

Epoch 00020: val_mDice improved from 0.77024 to 0.77150, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300
 - 28s - loss: 0.7532 - acc: 0.9691 - mDice: 0.7491 - val_loss: 0.6956 - val_acc: 0.9783 - val_mDice: 0.7718

Epoch 00021: val_mDice improved from 0.77150 to 0.77178, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300
 - 26s - loss: 0.7403 - acc: 0.9693 - mDice: 0.7523 - val_loss: 0.7038 - val_acc: 0.9787 - val_mDice: 0.7710

Epoch 00022: val_mDice did not improve from 0.77178
Epoch 23/300
 - 26s - loss: 0.7312 - acc: 0.9695 - mDice: 0.7551 - val_loss: 0.6947 - val_acc: 0.9784 - val_mDice: 0.7756

Epoch 00023: val_mDice improved from 0.77178 to 0.77560, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 24/300
 - 27s - loss: 0.7209 - acc: 0.9696 - mDice: 0.7582 - val_loss: 0.6840 - val_acc: 0.9799 - val_mDice: 0.7725

Epoch 00024: val_mDice did not improve from 0.77560
Epoch 25/300
 - 28s - loss: 0.7112 - acc: 0.9698 - mDice: 0.7610 - val_loss: 0.6762 - val_acc: 0.9800 - val_mDice: 0.7758

Epoch 00025: val_mDice improved from 0.77560 to 0.77582, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300
 - 26s - loss: 0.7051 - acc: 0.9699 - mDice: 0.7630 - val_loss: 0.6778 - val_acc: 0.9793 - val_mDice: 0.7757

Epoch 00026: val_mDice did not improve from 0.77582
Epoch 27/300
 - 26s - loss: 0.6971 - acc: 0.9700 - mDice: 0.7651 - val_loss: 0.6813 - val_acc: 0.9786 - val_mDice: 0.7768

Epoch 00027: val_mDice improved from 0.77582 to 0.77681, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300
 - 27s - loss: 0.6902 - acc: 0.9702 - mDice: 0.7671 - val_loss: 0.6911 - val_acc: 0.9794 - val_mDice: 0.7767

Epoch 00028: val_mDice did not improve from 0.77681
Epoch 29/300
 - 28s - loss: 0.6839 - acc: 0.9703 - mDice: 0.7688 - val_loss: 0.6786 - val_acc: 0.9781 - val_mDice: 0.7769

Epoch 00029: val_mDice improved from 0.77681 to 0.77691, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 26s - loss: 0.6774 - acc: 0.9703 - mDice: 0.7707 - val_loss: 0.7165 - val_acc: 0.9789 - val_mDice: 0.7739

Epoch 00030: val_mDice did not improve from 0.77691
Epoch 31/300
 - 26s - loss: 0.6730 - acc: 0.9704 - mDice: 0.7720 - val_loss: 0.6722 - val_acc: 0.9794 - val_mDice: 0.7770

Epoch 00031: val_mDice improved from 0.77691 to 0.77704, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 32/300
 - 28s - loss: 0.6648 - acc: 0.9706 - mDice: 0.7745 - val_loss: 0.6756 - val_acc: 0.9792 - val_mDice: 0.7777

Epoch 00032: val_mDice improved from 0.77704 to 0.77770, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 33/300
 - 27s - loss: 0.6588 - acc: 0.9707 - mDice: 0.7763 - val_loss: 0.6808 - val_acc: 0.9805 - val_mDice: 0.7775

Epoch 00033: val_mDice did not improve from 0.77770
Epoch 34/300
 - 26s - loss: 0.6566 - acc: 0.9707 - mDice: 0.7771 - val_loss: 0.6710 - val_acc: 0.9800 - val_mDice: 0.7792

Epoch 00034: val_mDice improved from 0.77770 to 0.77924, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 35/300
 - 27s - loss: 0.6514 - acc: 0.9708 - mDice: 0.7787 - val_loss: 0.6751 - val_acc: 0.9799 - val_mDice: 0.7820

Epoch 00035: val_mDice improved from 0.77924 to 0.78205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 36/300
 - 28s - loss: 0.6463 - acc: 0.9709 - mDice: 0.7801 - val_loss: 0.6809 - val_acc: 0.9793 - val_mDice: 0.7776

Epoch 00036: val_mDice did not improve from 0.78205
Epoch 37/300
 - 27s - loss: 0.6391 - acc: 0.9711 - mDice: 0.7823 - val_loss: 0.6910 - val_acc: 0.9797 - val_mDice: 0.7777

Epoch 00037: val_mDice did not improve from 0.78205
Epoch 38/300
 - 26s - loss: 0.6375 - acc: 0.9711 - mDice: 0.7829 - val_loss: 0.6746 - val_acc: 0.9799 - val_mDice: 0.7798

Epoch 00038: val_mDice did not improve from 0.78205
Epoch 39/300
 - 26s - loss: 0.6314 - acc: 0.9712 - mDice: 0.7845 - val_loss: 0.6751 - val_acc: 0.9803 - val_mDice: 0.7807

Epoch 00039: val_mDice did not improve from 0.78205
Epoch 40/300
 - 28s - loss: 0.6273 - acc: 0.9712 - mDice: 0.7860 - val_loss: 0.6668 - val_acc: 0.9808 - val_mDice: 0.7821

Epoch 00040: val_mDice improved from 0.78205 to 0.78206, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 41/300
 - 27s - loss: 0.6243 - acc: 0.9713 - mDice: 0.7867 - val_loss: 0.6772 - val_acc: 0.9799 - val_mDice: 0.7827

Epoch 00041: val_mDice improved from 0.78206 to 0.78273, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 42/300
 - 26s - loss: 0.6188 - acc: 0.9714 - mDice: 0.7887 - val_loss: 0.6883 - val_acc: 0.9801 - val_mDice: 0.7784

Epoch 00042: val_mDice did not improve from 0.78273
Epoch 43/300
 - 27s - loss: 0.6161 - acc: 0.9715 - mDice: 0.7893 - val_loss: 0.6750 - val_acc: 0.9803 - val_mDice: 0.7869

Epoch 00043: val_mDice improved from 0.78273 to 0.78688, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 44/300
 - 28s - loss: 0.6105 - acc: 0.9716 - mDice: 0.7910 - val_loss: 0.6751 - val_acc: 0.9799 - val_mDice: 0.7837

Epoch 00044: val_mDice did not improve from 0.78688
Epoch 45/300
 - 27s - loss: 0.6066 - acc: 0.9717 - mDice: 0.7923 - val_loss: 0.6721 - val_acc: 0.9801 - val_mDice: 0.7847

Epoch 00045: val_mDice did not improve from 0.78688
Epoch 46/300
 - 26s - loss: 0.6048 - acc: 0.9717 - mDice: 0.7930 - val_loss: 0.6859 - val_acc: 0.9798 - val_mDice: 0.7807

Epoch 00046: val_mDice did not improve from 0.78688
Epoch 47/300
 - 27s - loss: 0.6044 - acc: 0.9717 - mDice: 0.7928 - val_loss: 0.6822 - val_acc: 0.9801 - val_mDice: 0.7867

Epoch 00047: val_mDice did not improve from 0.78688
Epoch 48/300
 - 28s - loss: 0.5995 - acc: 0.9718 - mDice: 0.7945 - val_loss: 0.6757 - val_acc: 0.9795 - val_mDice: 0.7843

Epoch 00048: val_mDice did not improve from 0.78688
Epoch 49/300
 - 26s - loss: 0.5972 - acc: 0.9718 - mDice: 0.7956 - val_loss: 0.6952 - val_acc: 0.9797 - val_mDice: 0.7806

Epoch 00049: val_mDice did not improve from 0.78688
Epoch 50/300
 - 26s - loss: 0.5954 - acc: 0.9718 - mDice: 0.7958 - val_loss: 0.6741 - val_acc: 0.9801 - val_mDice: 0.7847

Epoch 00050: val_mDice did not improve from 0.78688
Epoch 51/300
 - 28s - loss: 0.5900 - acc: 0.9719 - mDice: 0.7976 - val_loss: 0.6767 - val_acc: 0.9806 - val_mDice: 0.7857

Epoch 00051: val_mDice did not improve from 0.78688
Epoch 52/300
 - 27s - loss: 0.5882 - acc: 0.9720 - mDice: 0.7981 - val_loss: 0.6824 - val_acc: 0.9805 - val_mDice: 0.7846

Epoch 00052: val_mDice did not improve from 0.78688
Epoch 53/300
 - 27s - loss: 0.5854 - acc: 0.9720 - mDice: 0.7991 - val_loss: 0.6744 - val_acc: 0.9805 - val_mDice: 0.7888

Epoch 00053: val_mDice improved from 0.78688 to 0.78880, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 54/300
 - 27s - loss: 0.5829 - acc: 0.9721 - mDice: 0.7999 - val_loss: 0.6705 - val_acc: 0.9807 - val_mDice: 0.7859

Epoch 00054: val_mDice did not improve from 0.78880
Epoch 55/300
 - 28s - loss: 0.5804 - acc: 0.9721 - mDice: 0.8005 - val_loss: 0.6793 - val_acc: 0.9804 - val_mDice: 0.7862

Epoch 00055: val_mDice did not improve from 0.78880
Epoch 56/300
 - 26s - loss: 0.5790 - acc: 0.9721 - mDice: 0.8012 - val_loss: 0.6773 - val_acc: 0.9791 - val_mDice: 0.7900

Epoch 00056: val_mDice improved from 0.78880 to 0.79001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 57/300
 - 26s - loss: 0.5746 - acc: 0.9722 - mDice: 0.8025 - val_loss: 0.6910 - val_acc: 0.9806 - val_mDice: 0.7847

Epoch 00057: val_mDice did not improve from 0.79001
Epoch 58/300
 - 28s - loss: 0.5720 - acc: 0.9722 - mDice: 0.8032 - val_loss: 0.6808 - val_acc: 0.9801 - val_mDice: 0.7919

Epoch 00058: val_mDice improved from 0.79001 to 0.79194, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 59/300
 - 27s - loss: 0.5703 - acc: 0.9722 - mDice: 0.8040 - val_loss: 0.6664 - val_acc: 0.9802 - val_mDice: 0.7926

Epoch 00059: val_mDice improved from 0.79194 to 0.79262, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 60/300
 - 26s - loss: 0.5660 - acc: 0.9723 - mDice: 0.8052 - val_loss: 0.6732 - val_acc: 0.9804 - val_mDice: 0.7865

Epoch 00060: val_mDice did not improve from 0.79262
Epoch 61/300
 - 27s - loss: 0.5650 - acc: 0.9723 - mDice: 0.8059 - val_loss: 0.6692 - val_acc: 0.9803 - val_mDice: 0.7898

Epoch 00061: val_mDice did not improve from 0.79262
Epoch 62/300
 - 28s - loss: 0.5615 - acc: 0.9724 - mDice: 0.8071 - val_loss: 0.6866 - val_acc: 0.9806 - val_mDice: 0.7899

Epoch 00062: val_mDice did not improve from 0.79262
Epoch 63/300
 - 26s - loss: 0.5615 - acc: 0.9723 - mDice: 0.8069 - val_loss: 0.6613 - val_acc: 0.9813 - val_mDice: 0.7905

Epoch 00063: val_mDice did not improve from 0.79262
Epoch 64/300
 - 26s - loss: 0.5564 - acc: 0.9725 - mDice: 0.8086 - val_loss: 0.6607 - val_acc: 0.9796 - val_mDice: 0.7931

Epoch 00064: val_mDice improved from 0.79262 to 0.79314, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 65/300
 - 28s - loss: 0.5532 - acc: 0.9725 - mDice: 0.8098 - val_loss: 0.6661 - val_acc: 0.9802 - val_mDice: 0.7934

Epoch 00065: val_mDice improved from 0.79314 to 0.79341, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 66/300
 - 27s - loss: 0.5523 - acc: 0.9725 - mDice: 0.8101 - val_loss: 0.6747 - val_acc: 0.9799 - val_mDice: 0.7894

Epoch 00066: val_mDice did not improve from 0.79341
Epoch 67/300
 - 26s - loss: 0.5490 - acc: 0.9726 - mDice: 0.8112 - val_loss: 0.6832 - val_acc: 0.9797 - val_mDice: 0.7933

Epoch 00067: val_mDice did not improve from 0.79341
Epoch 68/300
 - 27s - loss: 0.5479 - acc: 0.9725 - mDice: 0.8119 - val_loss: 0.6651 - val_acc: 0.9804 - val_mDice: 0.7946

Epoch 00068: val_mDice improved from 0.79341 to 0.79461, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 69/300
 - 28s - loss: 0.5467 - acc: 0.9726 - mDice: 0.8121 - val_loss: 0.6624 - val_acc: 0.9809 - val_mDice: 0.7975

Epoch 00069: val_mDice improved from 0.79461 to 0.79751, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 70/300
 - 27s - loss: 0.5427 - acc: 0.9727 - mDice: 0.8137 - val_loss: 0.6778 - val_acc: 0.9803 - val_mDice: 0.7939

Epoch 00070: val_mDice did not improve from 0.79751
Epoch 71/300
 - 26s - loss: 0.5396 - acc: 0.9727 - mDice: 0.8147 - val_loss: 0.6612 - val_acc: 0.9808 - val_mDice: 0.7971

Epoch 00071: val_mDice did not improve from 0.79751
Epoch 72/300
 - 27s - loss: 0.5366 - acc: 0.9728 - mDice: 0.8158 - val_loss: 0.6779 - val_acc: 0.9808 - val_mDice: 0.7954

Epoch 00072: val_mDice did not improve from 0.79751
Epoch 73/300
 - 28s - loss: 0.5361 - acc: 0.9728 - mDice: 0.8160 - val_loss: 0.6625 - val_acc: 0.9805 - val_mDice: 0.7971

Epoch 00073: val_mDice did not improve from 0.79751
Epoch 74/300
 - 27s - loss: 0.5317 - acc: 0.9728 - mDice: 0.8175 - val_loss: 0.6640 - val_acc: 0.9805 - val_mDice: 0.7970

Epoch 00074: val_mDice did not improve from 0.79751
Epoch 75/300
 - 27s - loss: 0.5317 - acc: 0.9729 - mDice: 0.8176 - val_loss: 0.6622 - val_acc: 0.9808 - val_mDice: 0.7973

Epoch 00075: val_mDice did not improve from 0.79751
Epoch 76/300
 - 27s - loss: 0.5292 - acc: 0.9729 - mDice: 0.8184 - val_loss: 0.6667 - val_acc: 0.9811 - val_mDice: 0.7958

Epoch 00076: val_mDice did not improve from 0.79751
Epoch 77/300
 - 28s - loss: 0.5270 - acc: 0.9729 - mDice: 0.8194 - val_loss: 0.6587 - val_acc: 0.9810 - val_mDice: 0.8027

Epoch 00077: val_mDice improved from 0.79751 to 0.80267, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 78/300
 - 26s - loss: 0.5232 - acc: 0.9730 - mDice: 0.8206 - val_loss: 0.6654 - val_acc: 0.9806 - val_mDice: 0.7962

Epoch 00078: val_mDice did not improve from 0.80267
Epoch 79/300
 - 26s - loss: 0.5227 - acc: 0.9730 - mDice: 0.8208 - val_loss: 0.6551 - val_acc: 0.9813 - val_mDice: 0.7980

Epoch 00079: val_mDice did not improve from 0.80267
Epoch 80/300
 - 27s - loss: 0.5177 - acc: 0.9731 - mDice: 0.8226 - val_loss: 0.6571 - val_acc: 0.9807 - val_mDice: 0.8006

Epoch 00080: val_mDice did not improve from 0.80267
Epoch 81/300
 - 28s - loss: 0.5174 - acc: 0.9731 - mDice: 0.8227 - val_loss: 0.6460 - val_acc: 0.9811 - val_mDice: 0.8019

Epoch 00081: val_mDice did not improve from 0.80267
Epoch 82/300
 - 26s - loss: 0.5146 - acc: 0.9731 - mDice: 0.8238 - val_loss: 0.6574 - val_acc: 0.9813 - val_mDice: 0.8064

Epoch 00082: val_mDice improved from 0.80267 to 0.80642, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 83/300
 - 26s - loss: 0.5125 - acc: 0.9732 - mDice: 0.8244 - val_loss: 0.6580 - val_acc: 0.9810 - val_mDice: 0.8001

Epoch 00083: val_mDice did not improve from 0.80642
Epoch 84/300
 - 27s - loss: 0.5093 - acc: 0.9732 - mDice: 0.8255 - val_loss: 0.6481 - val_acc: 0.9815 - val_mDice: 0.8066

Epoch 00084: val_mDice improved from 0.80642 to 0.80660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 85/300
 - 28s - loss: 0.5062 - acc: 0.9733 - mDice: 0.8268 - val_loss: 0.6454 - val_acc: 0.9815 - val_mDice: 0.8027

Epoch 00085: val_mDice did not improve from 0.80660
Epoch 86/300
 - 26s - loss: 0.5077 - acc: 0.9732 - mDice: 0.8263 - val_loss: 0.6650 - val_acc: 0.9812 - val_mDice: 0.8082

Epoch 00086: val_mDice improved from 0.80660 to 0.80822, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 87/300
 - 27s - loss: 0.5040 - acc: 0.9733 - mDice: 0.8275 - val_loss: 0.6530 - val_acc: 0.9811 - val_mDice: 0.8040

Epoch 00087: val_mDice did not improve from 0.80822
Epoch 88/300
 - 28s - loss: 0.5032 - acc: 0.9733 - mDice: 0.8277 - val_loss: 0.6635 - val_acc: 0.9811 - val_mDice: 0.8038

Epoch 00088: val_mDice did not improve from 0.80822
Epoch 89/300
 - 27s - loss: 0.5012 - acc: 0.9734 - mDice: 0.8286 - val_loss: 0.6489 - val_acc: 0.9809 - val_mDice: 0.8075

Epoch 00089: val_mDice did not improve from 0.80822
Epoch 90/300
 - 27s - loss: 0.4986 - acc: 0.9734 - mDice: 0.8295 - val_loss: 0.6562 - val_acc: 0.9812 - val_mDice: 0.8035

Epoch 00090: val_mDice did not improve from 0.80822
Epoch 91/300
 - 28s - loss: 0.4989 - acc: 0.9734 - mDice: 0.8293 - val_loss: 0.6432 - val_acc: 0.9812 - val_mDice: 0.8072

Epoch 00091: val_mDice did not improve from 0.80822
Epoch 92/300
 - 27s - loss: 0.4970 - acc: 0.9734 - mDice: 0.8300 - val_loss: 0.6580 - val_acc: 0.9809 - val_mDice: 0.8061

Epoch 00092: val_mDice did not improve from 0.80822
Epoch 93/300
 - 27s - loss: 0.4958 - acc: 0.9734 - mDice: 0.8302 - val_loss: 0.6628 - val_acc: 0.9809 - val_mDice: 0.8075

Epoch 00093: val_mDice did not improve from 0.80822
Epoch 94/300
 - 28s - loss: 0.4942 - acc: 0.9735 - mDice: 0.8311 - val_loss: 0.6621 - val_acc: 0.9813 - val_mDice: 0.8049

Epoch 00094: val_mDice did not improve from 0.80822
Epoch 95/300
 - 27s - loss: 0.4934 - acc: 0.9735 - mDice: 0.8312 - val_loss: 0.6568 - val_acc: 0.9807 - val_mDice: 0.8104

Epoch 00095: val_mDice improved from 0.80822 to 0.81036, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 96/300
 - 27s - loss: 0.4898 - acc: 0.9735 - mDice: 0.8324 - val_loss: 0.6642 - val_acc: 0.9810 - val_mDice: 0.8098

Epoch 00096: val_mDice did not improve from 0.81036
Epoch 97/300
 - 28s - loss: 0.4885 - acc: 0.9735 - mDice: 0.8328 - val_loss: 0.6626 - val_acc: 0.9802 - val_mDice: 0.8095

Epoch 00097: val_mDice did not improve from 0.81036
Epoch 98/300
 - 28s - loss: 0.4886 - acc: 0.9735 - mDice: 0.8330 - val_loss: 0.6514 - val_acc: 0.9815 - val_mDice: 0.8101

Epoch 00098: val_mDice did not improve from 0.81036
Epoch 99/300
 - 28s - loss: 0.4873 - acc: 0.9736 - mDice: 0.8334 - val_loss: 0.6546 - val_acc: 0.9806 - val_mDice: 0.8112

Epoch 00099: val_mDice improved from 0.81036 to 0.81117, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 100/300
 - 27s - loss: 0.4842 - acc: 0.9736 - mDice: 0.8344 - val_loss: 0.6559 - val_acc: 0.9811 - val_mDice: 0.8083

Epoch 00100: val_mDice did not improve from 0.81117
Epoch 101/300
 - 28s - loss: 0.4835 - acc: 0.9736 - mDice: 0.8347 - val_loss: 0.6616 - val_acc: 0.9810 - val_mDice: 0.8103

Epoch 00101: val_mDice did not improve from 0.81117
Epoch 102/300
 - 27s - loss: 0.4833 - acc: 0.9736 - mDice: 0.8346 - val_loss: 0.6474 - val_acc: 0.9818 - val_mDice: 0.8092

Epoch 00102: val_mDice did not improve from 0.81117
Epoch 103/300
 - 28s - loss: 0.4817 - acc: 0.9736 - mDice: 0.8352 - val_loss: 0.6611 - val_acc: 0.9815 - val_mDice: 0.8052

Epoch 00103: val_mDice did not improve from 0.81117
Epoch 104/300
 - 27s - loss: 0.4811 - acc: 0.9736 - mDice: 0.8354 - val_loss: 0.6488 - val_acc: 0.9812 - val_mDice: 0.8111

Epoch 00104: val_mDice did not improve from 0.81117
Epoch 105/300
 - 28s - loss: 0.4784 - acc: 0.9737 - mDice: 0.8365 - val_loss: 0.6621 - val_acc: 0.9817 - val_mDice: 0.8082

Epoch 00105: val_mDice did not improve from 0.81117
Epoch 106/300
 - 27s - loss: 0.4771 - acc: 0.9737 - mDice: 0.8368 - val_loss: 0.6660 - val_acc: 0.9806 - val_mDice: 0.8101

Epoch 00106: val_mDice did not improve from 0.81117
Epoch 107/300
 - 28s - loss: 0.4773 - acc: 0.9737 - mDice: 0.8367 - val_loss: 0.6632 - val_acc: 0.9803 - val_mDice: 0.8094

Epoch 00107: val_mDice did not improve from 0.81117
Epoch 108/300
 - 28s - loss: 0.4751 - acc: 0.9738 - mDice: 0.8374 - val_loss: 0.6605 - val_acc: 0.9812 - val_mDice: 0.8075

Epoch 00108: val_mDice did not improve from 0.81117
Epoch 109/300
 - 28s - loss: 0.4733 - acc: 0.9738 - mDice: 0.8382 - val_loss: 0.6666 - val_acc: 0.9813 - val_mDice: 0.8127

Epoch 00109: val_mDice improved from 0.81117 to 0.81274, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 110/300
 - 28s - loss: 0.4754 - acc: 0.9737 - mDice: 0.8375 - val_loss: 0.6570 - val_acc: 0.9812 - val_mDice: 0.8138

Epoch 00110: val_mDice improved from 0.81274 to 0.81378, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 111/300
 - 27s - loss: 0.4729 - acc: 0.9738 - mDice: 0.8384 - val_loss: 0.6542 - val_acc: 0.9816 - val_mDice: 0.8086

Epoch 00111: val_mDice did not improve from 0.81378
Epoch 112/300
 - 27s - loss: 0.4715 - acc: 0.9738 - mDice: 0.8387 - val_loss: 0.6582 - val_acc: 0.9811 - val_mDice: 0.8093

Epoch 00112: val_mDice did not improve from 0.81378
Epoch 113/300
 - 28s - loss: 0.4701 - acc: 0.9738 - mDice: 0.8393 - val_loss: 0.6743 - val_acc: 0.9811 - val_mDice: 0.8075

Epoch 00113: val_mDice did not improve from 0.81378
Epoch 114/300
 - 27s - loss: 0.4692 - acc: 0.9738 - mDice: 0.8397 - val_loss: 0.6639 - val_acc: 0.9810 - val_mDice: 0.8029

Epoch 00114: val_mDice did not improve from 0.81378
Epoch 115/300
 - 28s - loss: 0.4676 - acc: 0.9739 - mDice: 0.8400 - val_loss: 0.6521 - val_acc: 0.9813 - val_mDice: 0.8119

Epoch 00115: val_mDice did not improve from 0.81378
Epoch 116/300
 - 27s - loss: 0.4666 - acc: 0.9739 - mDice: 0.8404 - val_loss: 0.6671 - val_acc: 0.9814 - val_mDice: 0.8051

Epoch 00116: val_mDice did not improve from 0.81378
Epoch 117/300
 - 27s - loss: 0.4653 - acc: 0.9739 - mDice: 0.8411 - val_loss: 0.6628 - val_acc: 0.9815 - val_mDice: 0.8125

Epoch 00117: val_mDice did not improve from 0.81378
Epoch 118/300
 - 28s - loss: 0.4646 - acc: 0.9739 - mDice: 0.8410 - val_loss: 0.6441 - val_acc: 0.9823 - val_mDice: 0.8142

Epoch 00118: val_mDice improved from 0.81378 to 0.81425, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 119/300
 - 27s - loss: 0.4643 - acc: 0.9739 - mDice: 0.8413 - val_loss: 0.6582 - val_acc: 0.9820 - val_mDice: 0.8124

Epoch 00119: val_mDice did not improve from 0.81425
Epoch 120/300
 - 28s - loss: 0.4619 - acc: 0.9740 - mDice: 0.8420 - val_loss: 0.6668 - val_acc: 0.9813 - val_mDice: 0.8121

Epoch 00120: val_mDice did not improve from 0.81425
Epoch 121/300
 - 28s - loss: 0.4615 - acc: 0.9740 - mDice: 0.8425 - val_loss: 0.6578 - val_acc: 0.9816 - val_mDice: 0.8122

Epoch 00121: val_mDice did not improve from 0.81425
Epoch 122/300
 - 28s - loss: 0.4629 - acc: 0.9740 - mDice: 0.8417 - val_loss: 0.6492 - val_acc: 0.9823 - val_mDice: 0.8132

Epoch 00122: val_mDice did not improve from 0.81425
Epoch 123/300
 - 27s - loss: 0.4611 - acc: 0.9740 - mDice: 0.8424 - val_loss: 0.6533 - val_acc: 0.9814 - val_mDice: 0.8163

Epoch 00123: val_mDice improved from 0.81425 to 0.81635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 124/300
 - 28s - loss: 0.4601 - acc: 0.9740 - mDice: 0.8425 - val_loss: 0.6536 - val_acc: 0.9817 - val_mDice: 0.8112

Epoch 00124: val_mDice did not improve from 0.81635
Epoch 125/300
 - 27s - loss: 0.4596 - acc: 0.9740 - mDice: 0.8428 - val_loss: 0.6527 - val_acc: 0.9818 - val_mDice: 0.8143

Epoch 00125: val_mDice did not improve from 0.81635
Epoch 126/300
 - 28s - loss: 0.4579 - acc: 0.9741 - mDice: 0.8434 - val_loss: 0.6713 - val_acc: 0.9814 - val_mDice: 0.8146

Epoch 00126: val_mDice did not improve from 0.81635
Epoch 127/300
 - 28s - loss: 0.4582 - acc: 0.9740 - mDice: 0.8433 - val_loss: 0.6695 - val_acc: 0.9812 - val_mDice: 0.8137

Epoch 00127: val_mDice did not improve from 0.81635
Epoch 128/300
 - 28s - loss: 0.4575 - acc: 0.9740 - mDice: 0.8436 - val_loss: 0.6645 - val_acc: 0.9819 - val_mDice: 0.8144

Epoch 00128: val_mDice did not improve from 0.81635
Epoch 129/300
 - 27s - loss: 0.4550 - acc: 0.9741 - mDice: 0.8445 - val_loss: 0.6645 - val_acc: 0.9818 - val_mDice: 0.8131

Epoch 00129: val_mDice did not improve from 0.81635
Epoch 130/300
 - 28s - loss: 0.4537 - acc: 0.9741 - mDice: 0.8449 - val_loss: 0.6680 - val_acc: 0.9816 - val_mDice: 0.8154

Epoch 00130: val_mDice did not improve from 0.81635
Epoch 131/300
 - 27s - loss: 0.4548 - acc: 0.9741 - mDice: 0.8446 - val_loss: 0.6602 - val_acc: 0.9819 - val_mDice: 0.8144

Epoch 00131: val_mDice did not improve from 0.81635
Epoch 132/300
 - 28s - loss: 0.4534 - acc: 0.9741 - mDice: 0.8449 - val_loss: 0.6840 - val_acc: 0.9819 - val_mDice: 0.8129

Epoch 00132: val_mDice did not improve from 0.81635
Epoch 133/300
 - 27s - loss: 0.4535 - acc: 0.9741 - mDice: 0.8449 - val_loss: 0.6625 - val_acc: 0.9817 - val_mDice: 0.8153

Epoch 00133: val_mDice did not improve from 0.81635
Epoch 134/300
 - 28s - loss: 0.4521 - acc: 0.9741 - mDice: 0.8455 - val_loss: 0.6667 - val_acc: 0.9810 - val_mDice: 0.8136

Epoch 00134: val_mDice did not improve from 0.81635
Epoch 135/300
 - 28s - loss: 0.4520 - acc: 0.9741 - mDice: 0.8455 - val_loss: 0.6718 - val_acc: 0.9815 - val_mDice: 0.8152

Epoch 00135: val_mDice did not improve from 0.81635
Epoch 136/300
 - 28s - loss: 0.4504 - acc: 0.9742 - mDice: 0.8461 - val_loss: 0.6666 - val_acc: 0.9812 - val_mDice: 0.8135

Epoch 00136: val_mDice did not improve from 0.81635
Epoch 137/300
 - 28s - loss: 0.4506 - acc: 0.9742 - mDice: 0.8461 - val_loss: 0.6695 - val_acc: 0.9814 - val_mDice: 0.8137

Epoch 00137: val_mDice did not improve from 0.81635
Epoch 138/300
 - 28s - loss: 0.4493 - acc: 0.9742 - mDice: 0.8463 - val_loss: 0.6739 - val_acc: 0.9821 - val_mDice: 0.8109

Epoch 00138: val_mDice did not improve from 0.81635
Epoch 139/300
 - 28s - loss: 0.4472 - acc: 0.9742 - mDice: 0.8470 - val_loss: 0.6676 - val_acc: 0.9814 - val_mDice: 0.8131

Epoch 00139: val_mDice did not improve from 0.81635
Epoch 140/300
 - 28s - loss: 0.4483 - acc: 0.9742 - mDice: 0.8470 - val_loss: 0.6797 - val_acc: 0.9808 - val_mDice: 0.8134

Epoch 00140: val_mDice did not improve from 0.81635
Epoch 141/300
 - 28s - loss: 0.4472 - acc: 0.9742 - mDice: 0.8471 - val_loss: 0.6722 - val_acc: 0.9818 - val_mDice: 0.8130

Epoch 00141: val_mDice did not improve from 0.81635
Epoch 142/300
 - 28s - loss: 0.4470 - acc: 0.9742 - mDice: 0.8473 - val_loss: 0.6718 - val_acc: 0.9818 - val_mDice: 0.8124

Epoch 00142: val_mDice did not improve from 0.81635
Epoch 143/300
 - 28s - loss: 0.4473 - acc: 0.9742 - mDice: 0.8472 - val_loss: 0.6718 - val_acc: 0.9815 - val_mDice: 0.8140

Epoch 00143: val_mDice did not improve from 0.81635
Epoch 144/300
 - 28s - loss: 0.4440 - acc: 0.9743 - mDice: 0.8483 - val_loss: 0.6706 - val_acc: 0.9813 - val_mDice: 0.8156

Epoch 00144: val_mDice did not improve from 0.81635
Epoch 145/300
 - 28s - loss: 0.4456 - acc: 0.9742 - mDice: 0.8478 - val_loss: 0.6867 - val_acc: 0.9822 - val_mDice: 0.8136

Epoch 00145: val_mDice did not improve from 0.81635
Epoch 146/300
 - 27s - loss: 0.4450 - acc: 0.9742 - mDice: 0.8480 - val_loss: 0.6644 - val_acc: 0.9820 - val_mDice: 0.8146

Epoch 00146: val_mDice did not improve from 0.81635
Epoch 147/300
 - 28s - loss: 0.4440 - acc: 0.9742 - mDice: 0.8483 - val_loss: 0.6646 - val_acc: 0.9817 - val_mDice: 0.8118

Epoch 00147: val_mDice did not improve from 0.81635
Epoch 148/300
 - 28s - loss: 0.4426 - acc: 0.9743 - mDice: 0.8487 - val_loss: 0.6803 - val_acc: 0.9814 - val_mDice: 0.8157

Epoch 00148: val_mDice did not improve from 0.81635
Epoch 149/300
 - 28s - loss: 0.4430 - acc: 0.9743 - mDice: 0.8487 - val_loss: 0.6798 - val_acc: 0.9810 - val_mDice: 0.8174

Epoch 00149: val_mDice improved from 0.81635 to 0.81738, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 150/300
 - 28s - loss: 0.4420 - acc: 0.9743 - mDice: 0.8490 - val_loss: 0.6763 - val_acc: 0.9815 - val_mDice: 0.8173

Epoch 00150: val_mDice did not improve from 0.81738
Epoch 151/300
 - 28s - loss: 0.4410 - acc: 0.9743 - mDice: 0.8492 - val_loss: 0.6578 - val_acc: 0.9818 - val_mDice: 0.8127

Epoch 00151: val_mDice did not improve from 0.81738
Epoch 152/300
 - 27s - loss: 0.4403 - acc: 0.9743 - mDice: 0.8496 - val_loss: 0.6721 - val_acc: 0.9812 - val_mDice: 0.8151

Epoch 00152: val_mDice did not improve from 0.81738
Epoch 153/300
 - 27s - loss: 0.4404 - acc: 0.9743 - mDice: 0.8495 - val_loss: 0.6764 - val_acc: 0.9811 - val_mDice: 0.8170

Epoch 00153: val_mDice did not improve from 0.81738
Epoch 154/300
 - 28s - loss: 0.4392 - acc: 0.9743 - mDice: 0.8499 - val_loss: 0.6794 - val_acc: 0.9815 - val_mDice: 0.8147

Epoch 00154: val_mDice did not improve from 0.81738
Epoch 155/300
 - 27s - loss: 0.4400 - acc: 0.9743 - mDice: 0.8497 - val_loss: 0.6679 - val_acc: 0.9817 - val_mDice: 0.8128

Epoch 00155: val_mDice did not improve from 0.81738
Epoch 156/300
 - 28s - loss: 0.4403 - acc: 0.9743 - mDice: 0.8495 - val_loss: 0.6737 - val_acc: 0.9812 - val_mDice: 0.8176

Epoch 00156: val_mDice improved from 0.81738 to 0.81756, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 157/300
 - 27s - loss: 0.4376 - acc: 0.9744 - mDice: 0.8504 - val_loss: 0.6898 - val_acc: 0.9814 - val_mDice: 0.8152

Epoch 00157: val_mDice did not improve from 0.81756
Epoch 158/300
 - 27s - loss: 0.4373 - acc: 0.9744 - mDice: 0.8506 - val_loss: 0.6708 - val_acc: 0.9819 - val_mDice: 0.8138

Epoch 00158: val_mDice did not improve from 0.81756
Epoch 159/300
 - 28s - loss: 0.4359 - acc: 0.9744 - mDice: 0.8512 - val_loss: 0.6774 - val_acc: 0.9816 - val_mDice: 0.8142

Epoch 00159: val_mDice did not improve from 0.81756
Epoch 160/300
 - 27s - loss: 0.4367 - acc: 0.9744 - mDice: 0.8508 - val_loss: 0.6682 - val_acc: 0.9818 - val_mDice: 0.8133

Epoch 00160: val_mDice did not improve from 0.81756
Epoch 161/300
 - 27s - loss: 0.4363 - acc: 0.9744 - mDice: 0.8509 - val_loss: 0.6850 - val_acc: 0.9813 - val_mDice: 0.8140

Epoch 00161: val_mDice did not improve from 0.81756
Epoch 162/300
 - 28s - loss: 0.4372 - acc: 0.9744 - mDice: 0.8505 - val_loss: 0.6667 - val_acc: 0.9819 - val_mDice: 0.8168

Epoch 00162: val_mDice did not improve from 0.81756
Epoch 163/300
 - 27s - loss: 0.4338 - acc: 0.9744 - mDice: 0.8518 - val_loss: 0.6756 - val_acc: 0.9821 - val_mDice: 0.8169

Epoch 00163: val_mDice did not improve from 0.81756
Epoch 164/300
 - 27s - loss: 0.4349 - acc: 0.9744 - mDice: 0.8515 - val_loss: 0.6705 - val_acc: 0.9821 - val_mDice: 0.8161

Epoch 00164: val_mDice did not improve from 0.81756
Epoch 165/300
 - 28s - loss: 0.4340 - acc: 0.9744 - mDice: 0.8519 - val_loss: 0.6813 - val_acc: 0.9814 - val_mDice: 0.8172

Epoch 00165: val_mDice did not improve from 0.81756
Epoch 166/300
 - 28s - loss: 0.4338 - acc: 0.9744 - mDice: 0.8518 - val_loss: 0.6810 - val_acc: 0.9811 - val_mDice: 0.8153

Epoch 00166: val_mDice did not improve from 0.81756
Epoch 167/300
 - 27s - loss: 0.4336 - acc: 0.9744 - mDice: 0.8518 - val_loss: 0.6819 - val_acc: 0.9818 - val_mDice: 0.8132

Epoch 00167: val_mDice did not improve from 0.81756
Epoch 168/300
 - 28s - loss: 0.4333 - acc: 0.9744 - mDice: 0.8519 - val_loss: 0.6863 - val_acc: 0.9823 - val_mDice: 0.8167

Epoch 00168: val_mDice did not improve from 0.81756
Epoch 169/300
 - 27s - loss: 0.4321 - acc: 0.9744 - mDice: 0.8524 - val_loss: 0.6861 - val_acc: 0.9821 - val_mDice: 0.8167

Epoch 00169: val_mDice did not improve from 0.81756
Epoch 170/300
 - 29s - loss: 0.4320 - acc: 0.9745 - mDice: 0.8525 - val_loss: 0.6741 - val_acc: 0.9816 - val_mDice: 0.8152

Epoch 00170: val_mDice did not improve from 0.81756
Epoch 171/300
 - 28s - loss: 0.4329 - acc: 0.9744 - mDice: 0.8523 - val_loss: 0.6748 - val_acc: 0.9817 - val_mDice: 0.8164

Epoch 00171: val_mDice did not improve from 0.81756
Epoch 172/300
 - 27s - loss: 0.4310 - acc: 0.9745 - mDice: 0.8528 - val_loss: 0.6851 - val_acc: 0.9811 - val_mDice: 0.8143

Epoch 00172: val_mDice did not improve from 0.81756
Epoch 173/300
 - 28s - loss: 0.4315 - acc: 0.9744 - mDice: 0.8526 - val_loss: 0.6860 - val_acc: 0.9818 - val_mDice: 0.8201

Epoch 00173: val_mDice improved from 0.81756 to 0.82007, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 174/300
 - 27s - loss: 0.4303 - acc: 0.9745 - mDice: 0.8532 - val_loss: 0.6727 - val_acc: 0.9813 - val_mDice: 0.8151

Epoch 00174: val_mDice did not improve from 0.82007
Epoch 175/300
 - 28s - loss: 0.4297 - acc: 0.9745 - mDice: 0.8533 - val_loss: 0.7036 - val_acc: 0.9817 - val_mDice: 0.8166

Epoch 00175: val_mDice did not improve from 0.82007
Epoch 176/300
 - 27s - loss: 0.4291 - acc: 0.9745 - mDice: 0.8535 - val_loss: 0.6757 - val_acc: 0.9820 - val_mDice: 0.8174

Epoch 00176: val_mDice did not improve from 0.82007
Epoch 177/300
 - 27s - loss: 0.4292 - acc: 0.9745 - mDice: 0.8534 - val_loss: 0.6809 - val_acc: 0.9820 - val_mDice: 0.8177

Epoch 00177: val_mDice did not improve from 0.82007
Epoch 178/300
 - 28s - loss: 0.4283 - acc: 0.9745 - mDice: 0.8537 - val_loss: 0.6919 - val_acc: 0.9817 - val_mDice: 0.8146

Epoch 00178: val_mDice did not improve from 0.82007
Epoch 179/300
 - 27s - loss: 0.4290 - acc: 0.9745 - mDice: 0.8535 - val_loss: 0.6864 - val_acc: 0.9814 - val_mDice: 0.8173

Epoch 00179: val_mDice did not improve from 0.82007
Epoch 180/300
 - 28s - loss: 0.4282 - acc: 0.9745 - mDice: 0.8538 - val_loss: 0.6938 - val_acc: 0.9814 - val_mDice: 0.8172

Epoch 00180: val_mDice did not improve from 0.82007
Epoch 181/300
 - 27s - loss: 0.4270 - acc: 0.9745 - mDice: 0.8542 - val_loss: 0.6867 - val_acc: 0.9816 - val_mDice: 0.8186

Epoch 00181: val_mDice did not improve from 0.82007
Epoch 182/300
 - 28s - loss: 0.4265 - acc: 0.9745 - mDice: 0.8543 - val_loss: 0.6845 - val_acc: 0.9817 - val_mDice: 0.8170

Epoch 00182: val_mDice did not improve from 0.82007
Epoch 183/300
 - 27s - loss: 0.4269 - acc: 0.9745 - mDice: 0.8542 - val_loss: 0.6920 - val_acc: 0.9820 - val_mDice: 0.8183

Epoch 00183: val_mDice did not improve from 0.82007
Epoch 184/300
 - 28s - loss: 0.4268 - acc: 0.9745 - mDice: 0.8543 - val_loss: 0.6766 - val_acc: 0.9821 - val_mDice: 0.8161

Epoch 00184: val_mDice did not improve from 0.82007
Epoch 185/300
 - 27s - loss: 0.4271 - acc: 0.9745 - mDice: 0.8541 - val_loss: 0.6883 - val_acc: 0.9811 - val_mDice: 0.8193

Epoch 00185: val_mDice did not improve from 0.82007
Epoch 186/300
 - 28s - loss: 0.4250 - acc: 0.9746 - mDice: 0.8548 - val_loss: 0.6822 - val_acc: 0.9817 - val_mDice: 0.8147

Epoch 00186: val_mDice did not improve from 0.82007
Epoch 187/300
 - 28s - loss: 0.4256 - acc: 0.9746 - mDice: 0.8547 - val_loss: 0.6792 - val_acc: 0.9816 - val_mDice: 0.8180

Epoch 00187: val_mDice did not improve from 0.82007
Epoch 188/300
 - 27s - loss: 0.4255 - acc: 0.9745 - mDice: 0.8547 - val_loss: 0.6815 - val_acc: 0.9825 - val_mDice: 0.8180

Epoch 00188: val_mDice did not improve from 0.82007
Epoch 189/300
 - 28s - loss: 0.4243 - acc: 0.9746 - mDice: 0.8551 - val_loss: 0.6980 - val_acc: 0.9819 - val_mDice: 0.8179

Epoch 00189: val_mDice did not improve from 0.82007
Epoch 190/300
 - 27s - loss: 0.4251 - acc: 0.9745 - mDice: 0.8547 - val_loss: 0.6845 - val_acc: 0.9816 - val_mDice: 0.8172

Epoch 00190: val_mDice did not improve from 0.82007
Epoch 191/300
 - 28s - loss: 0.4237 - acc: 0.9746 - mDice: 0.8554 - val_loss: 0.6765 - val_acc: 0.9818 - val_mDice: 0.8188

Epoch 00191: val_mDice did not improve from 0.82007
Epoch 192/300
 - 28s - loss: 0.4235 - acc: 0.9746 - mDice: 0.8554 - val_loss: 0.6865 - val_acc: 0.9819 - val_mDice: 0.8210

Epoch 00192: val_mDice improved from 0.82007 to 0.82101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 193/300
 - 29s - loss: 0.4226 - acc: 0.9746 - mDice: 0.8557 - val_loss: 0.6874 - val_acc: 0.9823 - val_mDice: 0.8199

Epoch 00193: val_mDice did not improve from 0.82101
Epoch 194/300
 - 28s - loss: 0.4223 - acc: 0.9746 - mDice: 0.8556 - val_loss: 0.6827 - val_acc: 0.9821 - val_mDice: 0.8188

Epoch 00194: val_mDice did not improve from 0.82101
Epoch 195/300
 - 28s - loss: 0.4228 - acc: 0.9746 - mDice: 0.8556 - val_loss: 0.6960 - val_acc: 0.9817 - val_mDice: 0.8209

Epoch 00195: val_mDice did not improve from 0.82101
Epoch 196/300
 - 28s - loss: 0.4225 - acc: 0.9746 - mDice: 0.8556 - val_loss: 0.6880 - val_acc: 0.9818 - val_mDice: 0.8181

Epoch 00196: val_mDice did not improve from 0.82101
Epoch 197/300
 - 28s - loss: 0.4207 - acc: 0.9746 - mDice: 0.8564 - val_loss: 0.6893 - val_acc: 0.9818 - val_mDice: 0.8170

Epoch 00197: val_mDice did not improve from 0.82101
Epoch 198/300
 - 27s - loss: 0.4207 - acc: 0.9746 - mDice: 0.8562 - val_loss: 0.6899 - val_acc: 0.9816 - val_mDice: 0.8196

Epoch 00198: val_mDice did not improve from 0.82101
Epoch 199/300
 - 28s - loss: 0.4215 - acc: 0.9746 - mDice: 0.8562 - val_loss: 0.6837 - val_acc: 0.9822 - val_mDice: 0.8166

Epoch 00199: val_mDice did not improve from 0.82101
Epoch 200/300
 - 28s - loss: 0.4201 - acc: 0.9746 - mDice: 0.8565 - val_loss: 0.6806 - val_acc: 0.9821 - val_mDice: 0.8191

Epoch 00200: val_mDice did not improve from 0.82101
Epoch 201/300
 - 28s - loss: 0.4207 - acc: 0.9746 - mDice: 0.8564 - val_loss: 0.7114 - val_acc: 0.9816 - val_mDice: 0.8152

Epoch 00201: val_mDice did not improve from 0.82101
Epoch 202/300
 - 27s - loss: 0.4207 - acc: 0.9746 - mDice: 0.8564 - val_loss: 0.6980 - val_acc: 0.9817 - val_mDice: 0.8202

Epoch 00202: val_mDice did not improve from 0.82101
Epoch 203/300
 - 29s - loss: 0.4201 - acc: 0.9746 - mDice: 0.8565 - val_loss: 0.6959 - val_acc: 0.9820 - val_mDice: 0.8183

Epoch 00203: val_mDice did not improve from 0.82101
Epoch 204/300
 - 28s - loss: 0.4207 - acc: 0.9746 - mDice: 0.8565 - val_loss: 0.6949 - val_acc: 0.9812 - val_mDice: 0.8196

Epoch 00204: val_mDice did not improve from 0.82101
Epoch 205/300
 - 28s - loss: 0.4188 - acc: 0.9746 - mDice: 0.8570 - val_loss: 0.6990 - val_acc: 0.9817 - val_mDice: 0.8153

Epoch 00205: val_mDice did not improve from 0.82101
Epoch 206/300
 - 28s - loss: 0.4172 - acc: 0.9747 - mDice: 0.8575 - val_loss: 0.6901 - val_acc: 0.9815 - val_mDice: 0.8207

Epoch 00206: val_mDice did not improve from 0.82101
Epoch 207/300
 - 27s - loss: 0.4178 - acc: 0.9746 - mDice: 0.8574 - val_loss: 0.7109 - val_acc: 0.9819 - val_mDice: 0.8173

Epoch 00207: val_mDice did not improve from 0.82101
Epoch 208/300
 - 28s - loss: 0.4182 - acc: 0.9747 - mDice: 0.8572 - val_loss: 0.6906 - val_acc: 0.9817 - val_mDice: 0.8192

Epoch 00208: val_mDice did not improve from 0.82101
Epoch 209/300
 - 27s - loss: 0.4183 - acc: 0.9747 - mDice: 0.8572 - val_loss: 0.6852 - val_acc: 0.9822 - val_mDice: 0.8163

Epoch 00209: val_mDice did not improve from 0.82101
Epoch 210/300
 - 28s - loss: 0.4186 - acc: 0.9747 - mDice: 0.8571 - val_loss: 0.6828 - val_acc: 0.9818 - val_mDice: 0.8179

Epoch 00210: val_mDice did not improve from 0.82101
Epoch 211/300
 - 27s - loss: 0.4171 - acc: 0.9747 - mDice: 0.8575 - val_loss: 0.6861 - val_acc: 0.9819 - val_mDice: 0.8184

Epoch 00211: val_mDice did not improve from 0.82101
Epoch 212/300
 - 28s - loss: 0.4150 - acc: 0.9747 - mDice: 0.8584 - val_loss: 0.7056 - val_acc: 0.9816 - val_mDice: 0.8185

Epoch 00212: val_mDice did not improve from 0.82101
Epoch 213/300
 - 28s - loss: 0.4154 - acc: 0.9747 - mDice: 0.8582 - val_loss: 0.7033 - val_acc: 0.9819 - val_mDice: 0.8148

Epoch 00213: val_mDice did not improve from 0.82101
Epoch 214/300
 - 28s - loss: 0.4151 - acc: 0.9747 - mDice: 0.8583 - val_loss: 0.6994 - val_acc: 0.9820 - val_mDice: 0.8177

Epoch 00214: val_mDice did not improve from 0.82101
Epoch 215/300
 - 27s - loss: 0.4167 - acc: 0.9747 - mDice: 0.8577 - val_loss: 0.7102 - val_acc: 0.9818 - val_mDice: 0.8175

Epoch 00215: val_mDice did not improve from 0.82101
Epoch 216/300
 - 28s - loss: 0.4152 - acc: 0.9747 - mDice: 0.8583 - val_loss: 0.7169 - val_acc: 0.9810 - val_mDice: 0.8176

Epoch 00216: val_mDice did not improve from 0.82101
Epoch 217/300
 - 27s - loss: 0.4154 - acc: 0.9747 - mDice: 0.8581 - val_loss: 0.6889 - val_acc: 0.9813 - val_mDice: 0.8203

Epoch 00217: val_mDice did not improve from 0.82101
Epoch 218/300
 - 27s - loss: 0.4154 - acc: 0.9747 - mDice: 0.8584 - val_loss: 0.7106 - val_acc: 0.9819 - val_mDice: 0.8188

Epoch 00218: val_mDice did not improve from 0.82101
Epoch 219/300
 - 28s - loss: 0.4152 - acc: 0.9747 - mDice: 0.8584 - val_loss: 0.6915 - val_acc: 0.9819 - val_mDice: 0.8171

Epoch 00219: val_mDice did not improve from 0.82101
Epoch 220/300
 - 28s - loss: 0.4147 - acc: 0.9747 - mDice: 0.8583 - val_loss: 0.6937 - val_acc: 0.9817 - val_mDice: 0.8170

Epoch 00220: val_mDice did not improve from 0.82101
Epoch 221/300
 - 28s - loss: 0.4159 - acc: 0.9747 - mDice: 0.8580 - val_loss: 0.6929 - val_acc: 0.9820 - val_mDice: 0.8205

Epoch 00221: val_mDice did not improve from 0.82101
Epoch 222/300
 - 28s - loss: 0.4121 - acc: 0.9748 - mDice: 0.8594 - val_loss: 0.7072 - val_acc: 0.9818 - val_mDice: 0.8176

Epoch 00222: val_mDice did not improve from 0.82101
Restoring model weights from the end of the best epoch
Epoch 00222: early stopping
{'val_loss': [4.545359353496604, 1.9435311237426653, 1.1778189425599086, 0.948928617451289, 0.889249223960589, 0.8478529832951011, 0.8003901867833856, 0.7821156692831484, 0.7568234425701507, 0.7645008515005243, 0.7380933381923257, 0.7389070179364453, 0.722063225834337, 0.7133252951380324, 0.7235858501636818, 0.72545997655555, 0.7178130149841309, 0.7048944083795156, 0.6947639564128771, 0.6882403419442373, 0.6956205747715415, 0.703752585061609, 0.6947352367721192, 0.6840307112425974, 0.6761919353922753, 0.6777650944582404, 0.6812867808015379, 0.6910583319729322, 0.6785810991917571, 0.716524460544325, 0.6722448296742897, 0.6755780185738655, 0.680783187485721, 0.6709972636340416, 0.6751457479310362, 0.6808882247503489, 0.6910263753100617, 0.6746440333862828, 0.6750988643871595, 0.6667849931814899, 0.6771973930809596, 0.6883489899847606, 0.6749857280352344, 0.6751051577803207, 0.6721459494469917, 0.6858743559824277, 0.682160643682088, 0.6756834616399792, 0.6951917248637709, 0.6740651669567579, 0.6766657774170785, 0.6823700401064468, 0.6743578637299472, 0.6704640672223209, 0.6792782773710277, 0.6773028753391684, 0.690985165842592, 0.6808040454371335, 0.6663668049524908, 0.6731556253890468, 0.6692048939940047, 0.6866432571247832, 0.661293260242841, 0.6607168267851007, 0.6660956503593758, 0.6747474615296273, 0.683165139531436, 0.665055344570173, 0.6624012681311124, 0.6778317453110054, 0.6612078098401631, 0.6779389873351136, 0.6624823947475381, 0.6639608655073871, 0.6622205574626792, 0.6667152304355413, 0.6587316261170661, 0.6654155458489509, 0.6551463083453375, 0.6570866416170172, 0.646003220791686, 0.6573869416566744, 0.6580330213047054, 0.648071385817985, 0.6454141466584924, 0.665025705548182, 0.6529595243604216, 0.6634509455259532, 0.648882972255145, 0.6561887594935012, 0.6431613542037468, 0.6580314405568658, 0.6628371019885965, 0.6620627631471582, 0.656761815082537, 0.6642162189499973, 0.6626230860001421, 0.6514043801859634, 0.6545528344912072, 0.6558609739558338, 0.6615897016982509, 0.6473987723050052, 0.6610854986595781, 0.648848587927753, 0.6621102282445724, 0.666028546756261, 0.6632169572866127, 0.660472130938752, 0.666600690108456, 0.6570248542583152, 0.6541867944067472, 0.6581510970853779, 0.6743047127168472, 0.6638854131714939, 0.6520965991363133, 0.6670860958425966, 0.6628476754851538, 0.6441352355153593, 0.6582310293635277, 0.6668382564636126, 0.6577648183662598, 0.6491794373891125, 0.6533015002943066, 0.6536416552246433, 0.6527074340679874, 0.6712952742021377, 0.6694513271524482, 0.6644581442009913, 0.6644503227243684, 0.6680213543650222, 0.660198858543618, 0.6840034317072123, 0.662506832243645, 0.6666987456687509, 0.6717598709749849, 0.6665776747546784, 0.6694621423335925, 0.6739395271017127, 0.6676343916213676, 0.6796716143415399, 0.6722335154063082, 0.6717757623489589, 0.6718140415949364, 0.6705667329569386, 0.6866784616284174, 0.6643923988489255, 0.6646296947378002, 0.6802511668368562, 0.6798350310897174, 0.6762831378472994, 0.657819980219619, 0.67211709855354, 0.6764140535299092, 0.6794165362642236, 0.6679484219583747, 0.6736870510120915, 0.689803835667976, 0.6707728668434979, 0.6773556219796611, 0.6681512277828504, 0.6850406656118289, 0.6666522962998037, 0.6756433689430968, 0.6704735074141254, 0.6812752146426946, 0.6810029314397132, 0.6819383397902528, 0.686286913409625, 0.6861028954999088, 0.6740666046126248, 0.674785945921728, 0.6851476871804015, 0.6860318977947104, 0.6727139813034502, 0.7036141489874826, 0.6756941572035828, 0.6809492858305369, 0.6919481201122885, 0.686389393185916, 0.6938490779840782, 0.6866591791175816, 0.6845020884520387, 0.6919579887635088, 0.6766044295405689, 0.6883439119548014, 0.6821636886221089, 0.6792478093953982, 0.6815099185460234, 0.6980288208347477, 0.6844957922011206, 0.6764553546497266, 0.6865224534109847, 0.6873568273978691, 0.6827090884724708, 0.6960436756888481, 0.6879968441104236, 0.6892560732691255, 0.68992651080432, 0.6836733713950196, 0.6806084391188948, 0.7114463256646509, 0.698031893944087, 0.6958529947146977, 0.6949342881163506, 0.6989811162834298, 0.6901336969578102, 0.710917305048198, 0.6906160585684319, 0.6851728480972655, 0.6828204381139311, 0.6861258536985476, 0.7056478727353762, 0.7032952398470004, 0.6993865093139753, 0.7101872434763059, 0.7169308550145528, 0.6889043082929638, 0.7106428846512756, 0.6915371373091659, 0.6937244614101437, 0.6929043127249365, 0.7072480533220996], 'val_acc': [0.9134586037021794, 0.9459648144571748, 0.9635067051404143, 0.968886136600416, 0.97105094096432, 0.9729354426468888, 0.9749331613109536, 0.9761497039500981, 0.9761056565258601, 0.9759043695991987, 0.9768325955900428, 0.9771361808254294, 0.9772890704135372, 0.9780906487817633, 0.9786697936384645, 0.9785238239046645, 0.9782329748754632, 0.9784422863836157, 0.9790705634306555, 0.9794207578652525, 0.9782722792396806, 0.978737133414778, 0.9784288100183827, 0.9799427365603512, 0.9799835045043737, 0.9792689697383201, 0.9786322868033631, 0.979425117577592, 0.9781263152213946, 0.9788688988718268, 0.979441130406236, 0.9791550195380433, 0.9804938594772391, 0.9800210039909572, 0.9799252785231969, 0.9793395779720725, 0.9797319839261982, 0.9799383756232588, 0.9802845569506083, 0.9807552270693322, 0.9798684916267656, 0.9800504935930853, 0.9803256976277861, 0.9799438412875345, 0.9801076336266243, 0.9798320740053098, 0.9800861743215012, 0.9794658838886104, 0.9796893886507374, 0.9800541364166835, 0.9806456655672152, 0.9804986033537616, 0.9805306183965239, 0.9807071763358705, 0.9804498083787422, 0.9791080567934741, 0.9805681219656174, 0.9800799909519823, 0.9802219483950366, 0.9803995910572679, 0.9803104074033973, 0.9806121636743415, 0.981297246805609, 0.9796198651398698, 0.9801560636252573, 0.9799008696046594, 0.979698851500472, 0.9803744758645149, 0.9809379818504804, 0.980282382197576, 0.9808112966687712, 0.9807767096447618, 0.980509877204895, 0.9805415550323382, 0.9808433292663261, 0.9811250791974264, 0.9809718250411831, 0.9806372947888832, 0.9813372925536273, 0.9806747742711681, 0.9810922990923059, 0.9812553822994232, 0.9810133135482056, 0.981486538093384, 0.981526209066992, 0.9811694691442463, 0.9811006727283949, 0.9810882961913331, 0.9809350530578665, 0.9812113512052248, 0.9811749450147969, 0.980860074497249, 0.9808600618414682, 0.981313263308512, 0.9806970014964065, 0.9809616261149106, 0.9801815495099107, 0.981494912545975, 0.9806434801996571, 0.9811039477178495, 0.9809674494070549, 0.9817762872944139, 0.9814727057332862, 0.9812408395826, 0.9817435304596, 0.9805968840644784, 0.9803176803948128, 0.9812222625294776, 0.9812724933232346, 0.9812251762168048, 0.9816416110077949, 0.9810846750050375, 0.9811490990527688, 0.9809729044568049, 0.9813190812933935, 0.98144760156331, 0.981523670154075, 0.9822680778699379, 0.9820402011479417, 0.9812775911533669, 0.981627051552681, 0.9822841094781275, 0.9814024457376297, 0.9817406228960377, 0.9818294399405179, 0.9813726144294216, 0.9812262515499167, 0.981918261067508, 0.9817792022064941, 0.981635424780519, 0.9819488345760189, 0.9818742034369952, 0.9817202307590066, 0.9810493523127413, 0.9815167441760024, 0.9811793071766423, 0.981395176420473, 0.9820678777074161, 0.981363497776528, 0.9808491594987373, 0.9817671939118268, 0.981846175781668, 0.9814916379647712, 0.9813205522217162, 0.9821632496298176, 0.9820081718163948, 0.9817351515162481, 0.9813893527200778, 0.9810027598518215, 0.9814989142221947, 0.9818312521666697, 0.9811996903321515, 0.9811126920458388, 0.981539686248727, 0.9817060468948051, 0.9811516342914268, 0.9814381158515199, 0.9819360897965628, 0.9816033811601874, 0.9817857615751763, 0.9813285408771202, 0.9819149868945553, 0.9821308496063703, 0.9820751437585648, 0.9813824312327659, 0.9810810195256586, 0.9818039540558645, 0.9822731687598032, 0.9821035318995175, 0.9815597097351126, 0.9816776363000478, 0.9810821054732963, 0.9817693800958869, 0.981348944036928, 0.981706393908148, 0.9819535645720077, 0.9819779575687565, 0.9817151370113844, 0.9814432104156442, 0.9814337626711963, 0.9816478021340828, 0.9816900079380976, 0.9819757775084613, 0.9821326716305459, 0.9810839332129857, 0.9817300557273708, 0.9816019318691672, 0.9825258128447075, 0.981935731352192, 0.9815833654305707, 0.981805778529546, 0.9819018902027443, 0.9822917372396548, 0.9820733303076601, 0.9817355291484153, 0.9818239755009952, 0.9817558984233908, 0.9816172143367872, 0.9821967233533728, 0.982118842128205, 0.9816365111364077, 0.9817369698661648, 0.981984507547666, 0.9812007770962912, 0.981707122636168, 0.9815415013326357, 0.9818723993758632, 0.9816827447447058, 0.9822003829152617, 0.9817988578587362, 0.9819379159032482, 0.9815986429991788, 0.9818727651687518, 0.981965572458424, 0.9817806723183149, 0.9810347989813922, 0.9813067194533674, 0.9819368266896026, 0.9819011504519476, 0.981739148293456, 0.9820016181632264, 0.9817730139379632], 'val_mDice': [0.031554026797107636, 0.3422253901419574, 0.6037007558019194, 0.6880995876168552, 0.7135526374594806, 0.7233079320763889, 0.7370295087768607, 0.7439316527484214, 0.752246961201707, 0.7528154396847503, 0.7591992285970139, 0.7614947626852009, 0.7630557238239132, 0.7694476908200407, 0.7637443897658831, 0.7654186327979989, 0.7685366726084931, 0.7702400035237613, 0.7691986144405522, 0.7714956255808268, 0.7717772783481911, 0.7710073933209458, 0.775601227806039, 0.7725184841515267, 0.7758165128427009, 0.7757100373098295, 0.7768062177586229, 0.7767331142948098, 0.7769112293034384, 0.7739392237304008, 0.7770367913866696, 0.7777013276537804, 0.7775186590135914, 0.7792413332690932, 0.782047696717798, 0.7775813439120985, 0.7776857777817608, 0.7797854228378975, 0.7807143465296863, 0.7820606047976507, 0.7827252733380827, 0.778435520113331, 0.7868783588278784, 0.7837079657267217, 0.7847082080089882, 0.7806827625999712, 0.78673065647687, 0.7842756236252719, 0.7806173648736249, 0.7847161721693326, 0.7856581847961635, 0.7846145082826483, 0.7887986404438542, 0.7858719180708063, 0.7861601424543825, 0.7900143967915888, 0.7846510479711506, 0.7919361770969547, 0.792618261624689, 0.7865109909070681, 0.7898352750360149, 0.7898820250818174, 0.7905054957899329, 0.7931357009770119, 0.7934052005206069, 0.7894423722404323, 0.7933167500855172, 0.7946147530862729, 0.7975112640694396, 0.7938822115937324, 0.7971075728331527, 0.7953940601381537, 0.7971131413766782, 0.7969946595903945, 0.797308723403983, 0.7957560281230979, 0.8026720322158238, 0.7962107388940576, 0.7979610382694088, 0.8006146803294143, 0.8018631449300949, 0.8064167156611404, 0.8000615382031219, 0.8066018974944337, 0.8026752222890723, 0.8082193319928156, 0.8040347789248375, 0.8037546583234447, 0.8075456174269114, 0.8035208431825246, 0.8072126760874709, 0.8061268239805143, 0.8075160796511663, 0.8048561660394277, 0.8103634898793207, 0.8097612408742513, 0.8095350616598782, 0.8101365517263543, 0.8111691527987179, 0.8083325722446181, 0.8103120584193975, 0.8091524361747585, 0.8051613463114385, 0.8111027585317011, 0.808179062931505, 0.810067260918552, 0.8093627009489764, 0.8075089242360364, 0.8127389656354304, 0.8137842445340875, 0.8086376137112918, 0.8093277893654288, 0.8074878482785943, 0.8029255862921885, 0.8118825093524097, 0.8051418674318758, 0.8125032174260649, 0.8142485745149116, 0.8123836656139322, 0.8121341385253488, 0.8121865293751024, 0.8131771981716156, 0.8163454761243847, 0.8111918021554816, 0.8143321353278749, 0.8145670486639623, 0.8136979353754488, 0.8144488073375127, 0.8131418771123233, 0.8153584452524577, 0.8144092780269988, 0.812926905204172, 0.8153033268778291, 0.8136112575661646, 0.8151629020089972, 0.8134726939136034, 0.8137129622779481, 0.8108652021786938, 0.8130789929873323, 0.813447016967486, 0.8130004299830084, 0.8123979098992805, 0.8139742608756235, 0.8156473922402891, 0.8136320771419838, 0.8145968799721705, 0.8118401745410815, 0.8156557470968325, 0.8173847418941863, 0.8173430284408674, 0.8127060463167217, 0.8151089945068098, 0.8169684802016167, 0.8146672073292406, 0.8127679089977317, 0.8175614933445029, 0.815169583852977, 0.8137810389473014, 0.8141912230073589, 0.8133101769506115, 0.813953454772087, 0.8168337806446911, 0.8168909827323809, 0.8160578461542521, 0.8172008595237993, 0.8152781541216864, 0.8131705322494246, 0.8166589185799638, 0.8166781378118959, 0.8152008693512172, 0.816405079952658, 0.814291143254058, 0.8200664042610012, 0.8151168578291592, 0.8165846311882751, 0.8173807908410895, 0.8176938577057564, 0.8145700423684838, 0.8173397677401973, 0.817228179680158, 0.8186486861477159, 0.8170294161528757, 0.8183156586673161, 0.8161099577603275, 0.8192960644421512, 0.8146852793758863, 0.8179867814664972, 0.8180188757099517, 0.817887545040209, 0.817180480859051, 0.8188162339876776, 0.8210061502783266, 0.8198542207071225, 0.818827212265093, 0.8208613060925105, 0.8180582564171046, 0.8169848069752732, 0.819637803590461, 0.816597633165856, 0.8191375695679286, 0.8151966504854699, 0.8201984669247718, 0.8182574314613865, 0.819643063087986, 0.8153404364847157, 0.8207137302993095, 0.817305472207396, 0.8192236929723661, 0.8162904438907153, 0.8178543256570215, 0.8183622527612399, 0.8185409646328181, 0.8147997994945474, 0.8176941079636143, 0.817478259132333, 0.817623887568304, 0.8202832422844352, 0.818807356978116, 0.8171397790516892, 0.8170498144136716, 0.8205383203617514, 0.8176415644279899], 'loss': [21.384259756297755, 3.0884528262936137, 1.9465879635171055, 1.533272831208666, 1.3258512939761717, 1.2147925182063124, 1.1345182871053268, 1.0696642356347714, 1.0142115713403945, 0.9780696745039245, 0.942468378393212, 0.9121844444919985, 0.8809577997500445, 0.8568541016660373, 0.8413523310445591, 0.8239714173076986, 0.8035239648592605, 0.7903380291284644, 0.7780147108180109, 0.7618695709083596, 0.7532249978909613, 0.740318025377631, 0.7312425692585085, 0.7208748827170646, 0.7111553561056061, 0.7051174905758156, 0.6970910517685304, 0.6901684254486881, 0.6838538958897756, 0.6774303086309917, 0.6729731982837522, 0.6647783660263583, 0.658776264676669, 0.6565637958024495, 0.6513873832881568, 0.6463246005211366, 0.6391237427113493, 0.6374542056663886, 0.6313734067535451, 0.6272854301632327, 0.62426088042182, 0.6187644215645667, 0.6161269146859821, 0.6104781362925835, 0.6066154073253242, 0.604814074801746, 0.6044020196134323, 0.5995106129245509, 0.597174168005065, 0.5954065688219787, 0.5899873374197775, 0.5881926553629353, 0.5853851855668719, 0.5828607751737819, 0.5804162144594879, 0.5790242994958863, 0.5746380971597702, 0.5719570536528205, 0.5702929707617752, 0.5660111114380589, 0.5649912504639888, 0.5614543050244617, 0.5615187093239933, 0.5564043511108661, 0.5531814660751491, 0.5522898445934197, 0.5490457662341508, 0.5479193256565666, 0.546726262451936, 0.5427224077857071, 0.5396468198924497, 0.5366143337533734, 0.5361422360075532, 0.5316686893685524, 0.5317329938922815, 0.5292124212660028, 0.5269719407510353, 0.5231971546057431, 0.5226863697799604, 0.5176608755712745, 0.517421614210102, 0.5146107283589225, 0.5125263430229514, 0.5093088851324811, 0.5061991102330983, 0.5077454338190532, 0.5039733775587356, 0.5031844879628445, 0.5012239991815655, 0.49855711116587503, 0.49891770700126264, 0.49695459545259046, 0.49579103474238045, 0.49415480481399915, 0.49342746093644424, 0.48980373598531424, 0.48850710652748863, 0.48857768925823497, 0.4873164135971163, 0.4841730777141737, 0.4834545799821674, 0.4833428037960022, 0.481738416832541, 0.4811113088557482, 0.47835918235944846, 0.47708325653146494, 0.47728033114113894, 0.47507306370578783, 0.47330256425287015, 0.47539106976098855, 0.47291753634676154, 0.4714639715139731, 0.4700603730180498, 0.46924780096695895, 0.46755649148771533, 0.4666048134155771, 0.46525048857551293, 0.4646040446095033, 0.4643200708868199, 0.4619301709683696, 0.4615327193453327, 0.4628594125346727, 0.4610616159950993, 0.46008121912807115, 0.4595809841326641, 0.4578752797194817, 0.45821978812579894, 0.45754155058005647, 0.45496456442330163, 0.4537481434420279, 0.4548268955337541, 0.4533867862355112, 0.45350667951414286, 0.4520539950061071, 0.45198697069333693, 0.45042354973378057, 0.4505625218612624, 0.4493378190086447, 0.44718927591845203, 0.44834614528696864, 0.4471553773308405, 0.4469708961600872, 0.447261535838125, 0.4440422947535808, 0.4455740508174165, 0.4449666469888495, 0.44402046995373473, 0.44255857319867814, 0.44300076207331424, 0.44199146386660565, 0.44100384895436945, 0.4402808167449342, 0.4403761649348373, 0.4392461778356446, 0.4400039142814336, 0.44034262387096074, 0.43758080573094954, 0.43729680383617353, 0.4358612242058229, 0.4367497406788255, 0.43632286258106134, 0.4372394756857956, 0.43383941158405975, 0.4349316355686451, 0.43398672173963043, 0.43378975900464867, 0.43361565878468367, 0.4333436645995083, 0.43208461333294423, 0.43204820747935424, 0.4328796114296596, 0.43095942872812376, 0.4314560395132186, 0.4302595299742895, 0.4297145995954599, 0.4291229334075604, 0.4292060256792876, 0.4283126479116105, 0.42902679232851476, 0.42822630684058854, 0.42698215738084666, 0.4264676333582598, 0.42692174348933326, 0.4267770610033354, 0.4270984021594619, 0.4250135875165052, 0.4256303016536408, 0.42551597011331205, 0.4242851623628931, 0.42507089240199725, 0.42374791484079255, 0.4235446051260817, 0.42255175189515976, 0.4223075150937895, 0.4228301401830385, 0.4225133737035071, 0.4206780203653075, 0.42068442708110515, 0.4215370039347195, 0.420122375321926, 0.42072155956110496, 0.42073401445420117, 0.42011648088318737, 0.42069367801268664, 0.4188110641382868, 0.41719789048095307, 0.41777109649360733, 0.4181752647067893, 0.4183333663193483, 0.41855921202340446, 0.41710056253580124, 0.4150327264758453, 0.41537245775186626, 0.415099351428303, 0.4167429020912815, 0.4152495059224633, 0.4153637760871577, 0.41541010154945957, 0.41520779362238597, 0.4146988589931174, 0.4158703623719891, 0.412075102584202], 'acc': [0.7787607098750088, 0.9170293990798937, 0.9400238971249661, 0.9520923455994792, 0.9573386039462248, 0.9598927991356561, 0.9615879179288166, 0.9629466386667315, 0.9640509676109412, 0.9648356315739052, 0.9654733380660522, 0.9661365127648507, 0.9667083790051896, 0.9671642955076069, 0.9674488874054902, 0.9677701255655718, 0.9681743019586287, 0.9683861397991376, 0.9686323897575113, 0.9688625561684848, 0.9690633217474542, 0.9692841923876299, 0.9694572819262288, 0.9696383514233776, 0.9698158039334964, 0.9698932498120372, 0.9700369970115605, 0.9701604238417794, 0.9703085851632541, 0.9703461645229666, 0.9704460500234512, 0.9706147420526419, 0.9707173521797124, 0.9707066239986174, 0.9708402979327395, 0.9708935761544795, 0.9710543077964945, 0.9710890631042818, 0.971224003555953, 0.9712361761349196, 0.9713036415433187, 0.9714345599583611, 0.9714835142737682, 0.9716182944458521, 0.9716656293269899, 0.9717012526207737, 0.9717336273450748, 0.9717925193108813, 0.9717815574108649, 0.9718272535736242, 0.9719274354915204, 0.9719567479923654, 0.9719932199736734, 0.9720715480980525, 0.9720981539235103, 0.9721087188230974, 0.9721894012519472, 0.97218965885122, 0.972240424384852, 0.9723122586019988, 0.9722835695756775, 0.9723624294709283, 0.9723412411931889, 0.9724525063030633, 0.9724814304041974, 0.9724943002704667, 0.972584123111379, 0.9725491552618568, 0.9725825865933209, 0.9727004816879267, 0.9727278054809047, 0.972789598336317, 0.9727702231871156, 0.9728440312519034, 0.9728509268103878, 0.972900147911807, 0.9729484340944511, 0.9730256309169845, 0.9729796720933406, 0.9730670790573713, 0.9730880184773164, 0.9731071571642143, 0.9731707923888838, 0.973237306995618, 0.9732941707603454, 0.9732408433329589, 0.9733099070903657, 0.9733268646620757, 0.9733727312937168, 0.97342248487268, 0.9733868133872956, 0.9734279347721988, 0.9734384700375136, 0.9734761323909988, 0.9734657015092195, 0.9735345320762729, 0.9735361600336991, 0.9735319475115531, 0.9735528613821364, 0.9736145924525615, 0.973615020127163, 0.9736072952140986, 0.9736422026689969, 0.9736421276669404, 0.9736865526584552, 0.9737152628763261, 0.9737034501429149, 0.9737508142504084, 0.9737830671714158, 0.9737448110380048, 0.973764105520281, 0.9737875857113746, 0.9738491937317753, 0.9738338452240629, 0.9738763010015234, 0.9738776540634425, 0.9739090259566159, 0.9739032894469174, 0.9739142514101321, 0.9739659026683833, 0.9739772221215794, 0.9739605090498517, 0.9739738926984493, 0.9739802239705203, 0.9740003669076359, 0.9740610403453616, 0.9740281405520845, 0.9740092792053201, 0.9740623516762732, 0.9741085886070484, 0.9740684064757514, 0.9740997047112692, 0.9740969091962633, 0.9741284749905365, 0.9741142750291963, 0.9741503934072204, 0.9741675012817992, 0.9741584641444286, 0.9741841608073041, 0.9741735141262031, 0.9741979513526152, 0.9741988950157414, 0.9742063317961208, 0.9742515869494306, 0.9742237202224573, 0.9742144055381224, 0.9742410652573938, 0.9742795260729655, 0.9742512490796585, 0.9742847829676518, 0.9742973325160493, 0.974310172989349, 0.9743058375665564, 0.9743071984765592, 0.9743335408856251, 0.9743038900467659, 0.974352319738458, 0.9743817660703884, 0.9744049644859747, 0.9743718289887741, 0.9743688395123429, 0.9743638468900987, 0.9744345719104526, 0.9744114368626845, 0.9744230976644335, 0.9743992039408016, 0.9744039291019606, 0.9744366118089682, 0.9744405778518751, 0.9744513129445695, 0.9744333127411459, 0.9744700851989221, 0.9744202808544626, 0.9744813926444351, 0.9744566521953, 0.9744953797825204, 0.9744939157213497, 0.9745059643770059, 0.9744669366494174, 0.9745184437666372, 0.974529693012232, 0.9745295495405284, 0.9745459324699165, 0.9745098471992004, 0.9745024297316567, 0.9745676000466765, 0.974565142746161, 0.9745330602337099, 0.9745614845854178, 0.9745392933817529, 0.9745941448081826, 0.9745798240259955, 0.9745833397635529, 0.9745867295037454, 0.9745800176398302, 0.9746210940734835, 0.9746194018117548, 0.9746237453900048, 0.9745977322442473, 0.9746335070330503, 0.974605634282127, 0.9746475769638228, 0.9746140629229784, 0.9746352733833323, 0.974649167249298, 0.9746599318642313, 0.9746362641636874, 0.9746797607142561, 0.9746795746813487, 0.9746651375969954, 0.9747019804002601, 0.9747163757564786, 0.9746960499119004, 0.9747248581388932, 0.974717574723629, 0.9747148390832973, 0.9747147267338994, 0.9747356609198735, 0.9747091134954197, 0.9747205986575141, 0.9746975668040039, 0.9747537927223324], 'mDice': [0.04217756238996141, 0.2106765464858162, 0.4318812437988579, 0.5371190634651226, 0.594566675055555, 0.6255251390628686, 0.6469951562751856, 0.6637336228966937, 0.6787630795076603, 0.6888875580821028, 0.697992222522493, 0.7061695549550002, 0.7144924565748259, 0.72112346651514, 0.7253680746321725, 0.7298375695887815, 0.7356356490496595, 0.7388422991391912, 0.7422185991061924, 0.7466613653338244, 0.7490649191929556, 0.7522700854890737, 0.7551055693489666, 0.7582409656721745, 0.7609893361530141, 0.7630486595176192, 0.7650584099039847, 0.7671386270880659, 0.7688372390713426, 0.7707073554727703, 0.771963162823547, 0.7745180084444631, 0.7763134150902871, 0.7771259829121169, 0.7786624128803918, 0.780058861327485, 0.7823318493866658, 0.7828905116828633, 0.7844799347306235, 0.785975469383046, 0.7866889720962986, 0.7887002877645811, 0.7892970589822939, 0.7910387812795349, 0.7922823713844876, 0.7929708434677197, 0.7928335961591618, 0.7944785768603456, 0.7956104301718023, 0.7958349769038117, 0.7976029993706516, 0.7981423747137923, 0.7990502582537915, 0.799947352637796, 0.8005155918339522, 0.8011562634711725, 0.8024668549909908, 0.8032485609100758, 0.803965099932596, 0.8052457572667074, 0.8058798564210605, 0.8070765425799364, 0.8068601151587986, 0.808644665471674, 0.8097784063535585, 0.8101214805685346, 0.8111541475857406, 0.8118510138150256, 0.812061975781237, 0.8136555967724372, 0.8146800558230027, 0.8158022880243953, 0.8159702439395266, 0.8175125642307489, 0.8175882247276919, 0.81840540123041, 0.8194261791562567, 0.8205668569800423, 0.8208147692671166, 0.8226040932774699, 0.822706560903793, 0.8237965803266094, 0.8244449987980973, 0.8255163583877754, 0.826836084864491, 0.8262524207509668, 0.8275069063775792, 0.8276721408949845, 0.8285969757759127, 0.8294852662847165, 0.8292920224505382, 0.8300150800563667, 0.830179680165937, 0.8311051175711419, 0.8311702320906529, 0.8324193474246632, 0.8327957997236363, 0.8329777689783066, 0.8333615782123123, 0.8344481451332474, 0.8346985534326882, 0.8346181548218008, 0.8351940462787937, 0.8354281683221852, 0.8365318186800533, 0.8368126038522787, 0.8367370686316652, 0.8374091399319804, 0.8381609669753893, 0.837488460209842, 0.8383672909998849, 0.8386577631549448, 0.8392975167846891, 0.8396870358874238, 0.8400064700367629, 0.8404472656230236, 0.8410649958895088, 0.8410210004570547, 0.8412932681373196, 0.8419960092634137, 0.8425095609638672, 0.841691124743166, 0.8423596566036768, 0.8425413838518264, 0.8427824203816028, 0.8433763812183386, 0.8432673842506274, 0.8436205512334355, 0.8445233697373928, 0.8449004922144567, 0.8445635418144098, 0.8448796187247349, 0.8448691630749549, 0.8454693562381554, 0.8454530021214612, 0.8460866483936965, 0.8461059388916052, 0.846284341246796, 0.8470269821254229, 0.8470075317070084, 0.8471376025663513, 0.8472621333795005, 0.8471689559481588, 0.8483203650928474, 0.8477723781927746, 0.8480011161490919, 0.8482526235278471, 0.8487442974481441, 0.8486756874863581, 0.8489864937592325, 0.8492439355274081, 0.849630358966151, 0.8494729381784473, 0.8499479300545246, 0.8497389848447489, 0.8495491774485849, 0.8504088469899254, 0.8505533730744155, 0.851167553823835, 0.8508279248630405, 0.8509399430528538, 0.8505183422190292, 0.8517879859597616, 0.8514986131741539, 0.8518663406389306, 0.8518246073447984, 0.8518012937243987, 0.8518840071465448, 0.8524065535093447, 0.8525273939823091, 0.8523352196477535, 0.8528369105697006, 0.8525866039563756, 0.8531938893455236, 0.8532828609250162, 0.8535255540381903, 0.8534148610570677, 0.8536760621320437, 0.8534950034963901, 0.8538367416871427, 0.8542109533237362, 0.854273830415473, 0.8541661128778791, 0.8542907501227643, 0.8540866046194525, 0.8548035868174013, 0.8547037359812018, 0.8547031046873208, 0.8551204961617888, 0.8547268800404774, 0.8554096291556899, 0.855394098852231, 0.8557313319958688, 0.8556315429561471, 0.8556170783087031, 0.8556398397078255, 0.8563842905981638, 0.8562362048475304, 0.8561544321813872, 0.8564606009405706, 0.8564030733779112, 0.856439872829993, 0.856545671531221, 0.8564661361090022, 0.8569741941048752, 0.8575164829341985, 0.8573687645066324, 0.8572448896433315, 0.85722197698762, 0.8571046882321722, 0.8575224696432195, 0.8583773139229889, 0.8581647371762035, 0.858265170293105, 0.8576999729389432, 0.8583471194290989, 0.8581498609496241, 0.8584293469020364, 0.8583689831907165, 0.8583381128251406, 0.8580429971784022, 0.8593917276849057]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:02<00:29,  2.07s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:25,  1.95s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:23,  1.95s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:21,  1.96s/it]predicting test subjects:  33%|███▎      | 5/15 [00:09<00:20,  2.06s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:19,  2.14s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:15,  1.96s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:16<00:14,  2.08s/it]predicting test subjects:  60%|██████    | 9/15 [00:18<00:12,  2.01s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:19<00:09,  1.85s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:21<00:07,  1.83s/it]predicting test subjects:  80%|████████  | 12/15 [00:23<00:05,  1.90s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:25<00:03,  1.96s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:27<00:01,  1.91s/it]predicting test subjects: 100%|██████████| 15/15 [00:29<00:00,  1.94s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:42,  2.34s/it]predicting train subjects:   0%|          | 2/532 [00:03<18:45,  2.12s/it]predicting train subjects:   1%|          | 3/532 [00:05<17:45,  2.01s/it]predicting train subjects:   1%|          | 4/532 [00:07<17:03,  1.94s/it]predicting train subjects:   1%|          | 5/532 [00:09<16:56,  1.93s/it]predicting train subjects:   1%|          | 6/532 [00:11<16:20,  1.86s/it]predicting train subjects:   1%|▏         | 7/532 [00:12<16:08,  1.85s/it]predicting train subjects:   2%|▏         | 8/532 [00:14<15:28,  1.77s/it]predicting train subjects:   2%|▏         | 9/532 [00:16<16:01,  1.84s/it]predicting train subjects:   2%|▏         | 10/532 [00:18<15:26,  1.78s/it]predicting train subjects:   2%|▏         | 11/532 [00:19<14:43,  1.70s/it]predicting train subjects:   2%|▏         | 12/532 [00:21<16:11,  1.87s/it]predicting train subjects:   2%|▏         | 13/532 [00:23<15:10,  1.75s/it]predicting train subjects:   3%|▎         | 14/532 [00:24<14:30,  1.68s/it]predicting train subjects:   3%|▎         | 15/532 [00:26<14:27,  1.68s/it]predicting train subjects:   3%|▎         | 16/532 [00:28<15:07,  1.76s/it]predicting train subjects:   3%|▎         | 17/532 [00:30<14:32,  1.69s/it]predicting train subjects:   3%|▎         | 18/532 [00:32<15:20,  1.79s/it]predicting train subjects:   4%|▎         | 19/532 [00:33<14:17,  1.67s/it]predicting train subjects:   4%|▍         | 20/532 [00:35<14:27,  1.69s/it]predicting train subjects:   4%|▍         | 21/532 [00:37<15:26,  1.81s/it]predicting train subjects:   4%|▍         | 22/532 [00:38<14:56,  1.76s/it]predicting train subjects:   4%|▍         | 23/532 [00:40<15:10,  1.79s/it]predicting train subjects:   5%|▍         | 24/532 [00:42<14:27,  1.71s/it]predicting train subjects:   5%|▍         | 25/532 [00:44<15:37,  1.85s/it]predicting train subjects:   5%|▍         | 26/532 [00:46<14:51,  1.76s/it]predicting train subjects:   5%|▌         | 27/532 [00:48<16:15,  1.93s/it]predicting train subjects:   5%|▌         | 28/532 [00:50<15:32,  1.85s/it]predicting train subjects:   5%|▌         | 29/532 [00:52<16:06,  1.92s/it]predicting train subjects:   6%|▌         | 30/532 [00:53<15:07,  1.81s/it]predicting train subjects:   6%|▌         | 31/532 [00:55<15:01,  1.80s/it]predicting train subjects:   6%|▌         | 32/532 [00:57<14:51,  1.78s/it]predicting train subjects:   6%|▌         | 33/532 [00:58<14:10,  1.70s/it]predicting train subjects:   6%|▋         | 34/532 [01:00<14:57,  1.80s/it]predicting train subjects:   7%|▋         | 35/532 [01:02<14:44,  1.78s/it]predicting train subjects:   7%|▋         | 36/532 [01:04<15:24,  1.86s/it]predicting train subjects:   7%|▋         | 37/532 [01:06<15:17,  1.85s/it]predicting train subjects:   7%|▋         | 38/532 [01:08<15:43,  1.91s/it]predicting train subjects:   7%|▋         | 39/532 [01:10<15:15,  1.86s/it]predicting train subjects:   8%|▊         | 40/532 [01:11<14:32,  1.77s/it]predicting train subjects:   8%|▊         | 41/532 [01:13<14:42,  1.80s/it]predicting train subjects:   8%|▊         | 42/532 [01:15<14:36,  1.79s/it]predicting train subjects:   8%|▊         | 43/532 [01:16<13:49,  1.70s/it]predicting train subjects:   8%|▊         | 44/532 [01:18<13:01,  1.60s/it]predicting train subjects:   8%|▊         | 45/532 [01:19<12:56,  1.60s/it]predicting train subjects:   9%|▊         | 46/532 [01:21<13:32,  1.67s/it]predicting train subjects:   9%|▉         | 47/532 [01:23<14:48,  1.83s/it]predicting train subjects:   9%|▉         | 48/532 [01:25<14:53,  1.85s/it]predicting train subjects:   9%|▉         | 49/532 [01:27<14:18,  1.78s/it]predicting train subjects:   9%|▉         | 50/532 [01:29<14:48,  1.84s/it]predicting train subjects:  10%|▉         | 51/532 [01:30<14:20,  1.79s/it]predicting train subjects:  10%|▉         | 52/532 [01:32<14:14,  1.78s/it]predicting train subjects:  10%|▉         | 53/532 [01:34<13:55,  1.75s/it]predicting train subjects:  10%|█         | 54/532 [01:36<14:33,  1.83s/it]predicting train subjects:  10%|█         | 55/532 [01:38<14:37,  1.84s/it]predicting train subjects:  11%|█         | 56/532 [01:40<14:33,  1.84s/it]predicting train subjects:  11%|█         | 57/532 [01:41<14:31,  1.83s/it]predicting train subjects:  11%|█         | 58/532 [01:43<14:38,  1.85s/it]predicting train subjects:  11%|█         | 59/532 [01:46<15:35,  1.98s/it]predicting train subjects:  11%|█▏        | 60/532 [01:47<14:23,  1.83s/it]predicting train subjects:  11%|█▏        | 61/532 [01:49<13:53,  1.77s/it]predicting train subjects:  12%|█▏        | 62/532 [01:51<14:57,  1.91s/it]predicting train subjects:  12%|█▏        | 63/532 [01:53<15:22,  1.97s/it]predicting train subjects:  12%|█▏        | 64/532 [01:55<14:25,  1.85s/it]predicting train subjects:  12%|█▏        | 65/532 [01:57<14:23,  1.85s/it]predicting train subjects:  12%|█▏        | 66/532 [01:59<15:42,  2.02s/it]predicting train subjects:  13%|█▎        | 67/532 [02:01<16:12,  2.09s/it]predicting train subjects:  13%|█▎        | 68/532 [02:03<15:35,  2.02s/it]predicting train subjects:  13%|█▎        | 69/532 [02:05<15:01,  1.95s/it]predicting train subjects:  13%|█▎        | 70/532 [02:06<14:23,  1.87s/it]predicting train subjects:  13%|█▎        | 71/532 [02:08<13:50,  1.80s/it]predicting train subjects:  14%|█▎        | 72/532 [02:10<13:08,  1.72s/it]predicting train subjects:  14%|█▎        | 73/532 [02:12<13:36,  1.78s/it]predicting train subjects:  14%|█▍        | 74/532 [02:14<15:00,  1.97s/it]predicting train subjects:  14%|█▍        | 75/532 [02:17<16:59,  2.23s/it]predicting train subjects:  14%|█▍        | 76/532 [02:19<15:42,  2.07s/it]predicting train subjects:  14%|█▍        | 77/532 [02:20<15:08,  2.00s/it]predicting train subjects:  15%|█▍        | 78/532 [02:22<14:49,  1.96s/it]predicting train subjects:  15%|█▍        | 79/532 [02:24<14:37,  1.94s/it]predicting train subjects:  15%|█▌        | 80/532 [02:26<14:21,  1.91s/it]predicting train subjects:  15%|█▌        | 81/532 [02:28<14:07,  1.88s/it]predicting train subjects:  15%|█▌        | 82/532 [02:30<14:06,  1.88s/it]predicting train subjects:  16%|█▌        | 83/532 [02:31<13:24,  1.79s/it]predicting train subjects:  16%|█▌        | 84/532 [02:33<13:06,  1.76s/it]predicting train subjects:  16%|█▌        | 85/532 [02:34<12:44,  1.71s/it]predicting train subjects:  16%|█▌        | 86/532 [02:36<12:24,  1.67s/it]predicting train subjects:  16%|█▋        | 87/532 [02:38<12:17,  1.66s/it]predicting train subjects:  17%|█▋        | 88/532 [02:39<12:07,  1.64s/it]predicting train subjects:  17%|█▋        | 89/532 [02:41<12:25,  1.68s/it]predicting train subjects:  17%|█▋        | 90/532 [02:43<12:31,  1.70s/it]predicting train subjects:  17%|█▋        | 91/532 [02:45<12:37,  1.72s/it]predicting train subjects:  17%|█▋        | 92/532 [02:46<12:42,  1.73s/it]predicting train subjects:  17%|█▋        | 93/532 [02:48<12:44,  1.74s/it]predicting train subjects:  18%|█▊        | 94/532 [02:50<12:55,  1.77s/it]predicting train subjects:  18%|█▊        | 95/532 [02:52<13:42,  1.88s/it]predicting train subjects:  18%|█▊        | 96/532 [02:54<14:06,  1.94s/it]predicting train subjects:  18%|█▊        | 97/532 [02:56<14:07,  1.95s/it]predicting train subjects:  18%|█▊        | 98/532 [02:58<14:11,  1.96s/it]predicting train subjects:  19%|█▊        | 99/532 [03:00<14:04,  1.95s/it]predicting train subjects:  19%|█▉        | 100/532 [03:02<14:06,  1.96s/it]predicting train subjects:  19%|█▉        | 101/532 [03:04<13:11,  1.84s/it]predicting train subjects:  19%|█▉        | 102/532 [03:05<12:20,  1.72s/it]predicting train subjects:  19%|█▉        | 103/532 [03:06<11:43,  1.64s/it]predicting train subjects:  20%|█▉        | 104/532 [03:08<11:21,  1.59s/it]predicting train subjects:  20%|█▉        | 105/532 [03:09<11:06,  1.56s/it]predicting train subjects:  20%|█▉        | 106/532 [03:11<10:48,  1.52s/it]predicting train subjects:  20%|██        | 107/532 [03:12<10:34,  1.49s/it]predicting train subjects:  20%|██        | 108/532 [03:14<10:31,  1.49s/it]predicting train subjects:  20%|██        | 109/532 [03:15<10:28,  1.48s/it]predicting train subjects:  21%|██        | 110/532 [03:17<10:10,  1.45s/it]predicting train subjects:  21%|██        | 111/532 [03:18<10:08,  1.44s/it]predicting train subjects:  21%|██        | 112/532 [03:19<10:04,  1.44s/it]predicting train subjects:  21%|██        | 113/532 [03:21<10:47,  1.55s/it]predicting train subjects:  21%|██▏       | 114/532 [03:23<11:24,  1.64s/it]predicting train subjects:  22%|██▏       | 115/532 [03:25<11:41,  1.68s/it]predicting train subjects:  22%|██▏       | 116/532 [03:27<11:56,  1.72s/it]predicting train subjects:  22%|██▏       | 117/532 [03:29<12:04,  1.75s/it]predicting train subjects:  22%|██▏       | 118/532 [03:30<12:12,  1.77s/it]predicting train subjects:  22%|██▏       | 119/532 [03:32<12:14,  1.78s/it]predicting train subjects:  23%|██▎       | 120/532 [03:34<12:12,  1.78s/it]predicting train subjects:  23%|██▎       | 121/532 [03:36<12:11,  1.78s/it]predicting train subjects:  23%|██▎       | 122/532 [03:38<12:13,  1.79s/it]predicting train subjects:  23%|██▎       | 123/532 [03:39<12:06,  1.78s/it]predicting train subjects:  23%|██▎       | 124/532 [03:41<11:56,  1.76s/it]predicting train subjects:  23%|██▎       | 125/532 [03:43<11:50,  1.75s/it]predicting train subjects:  24%|██▎       | 126/532 [03:44<11:48,  1.75s/it]predicting train subjects:  24%|██▍       | 127/532 [03:46<11:53,  1.76s/it]predicting train subjects:  24%|██▍       | 128/532 [03:48<11:47,  1.75s/it]predicting train subjects:  24%|██▍       | 129/532 [03:50<11:47,  1.76s/it]predicting train subjects:  24%|██▍       | 130/532 [03:52<11:48,  1.76s/it]predicting train subjects:  25%|██▍       | 131/532 [03:54<12:33,  1.88s/it]predicting train subjects:  25%|██▍       | 132/532 [03:56<12:53,  1.93s/it]predicting train subjects:  25%|██▌       | 133/532 [03:58<13:18,  2.00s/it]predicting train subjects:  25%|██▌       | 134/532 [04:00<13:30,  2.04s/it]predicting train subjects:  25%|██▌       | 135/532 [04:02<13:46,  2.08s/it]predicting train subjects:  26%|██▌       | 136/532 [04:04<13:39,  2.07s/it]predicting train subjects:  26%|██▌       | 137/532 [04:06<13:43,  2.08s/it]predicting train subjects:  26%|██▌       | 138/532 [04:09<14:01,  2.14s/it]predicting train subjects:  26%|██▌       | 139/532 [04:11<13:58,  2.13s/it]predicting train subjects:  26%|██▋       | 140/532 [04:13<13:58,  2.14s/it]predicting train subjects:  27%|██▋       | 141/532 [04:15<13:53,  2.13s/it]predicting train subjects:  27%|██▋       | 142/532 [04:17<14:10,  2.18s/it]predicting train subjects:  27%|██▋       | 143/532 [04:19<12:53,  1.99s/it]predicting train subjects:  27%|██▋       | 144/532 [04:20<11:54,  1.84s/it]predicting train subjects:  27%|██▋       | 145/532 [04:22<11:20,  1.76s/it]predicting train subjects:  27%|██▋       | 146/532 [04:23<10:56,  1.70s/it]predicting train subjects:  28%|██▊       | 147/532 [04:25<10:36,  1.65s/it]predicting train subjects:  28%|██▊       | 148/532 [04:27<10:31,  1.65s/it]predicting train subjects:  28%|██▊       | 149/532 [04:28<10:28,  1.64s/it]predicting train subjects:  28%|██▊       | 150/532 [04:30<10:25,  1.64s/it]predicting train subjects:  28%|██▊       | 151/532 [04:32<10:23,  1.64s/it]predicting train subjects:  29%|██▊       | 152/532 [04:33<10:22,  1.64s/it]predicting train subjects:  29%|██▉       | 153/532 [04:35<10:18,  1.63s/it]predicting train subjects:  29%|██▉       | 154/532 [04:36<10:13,  1.62s/it]predicting train subjects:  29%|██▉       | 155/532 [04:39<11:21,  1.81s/it]predicting train subjects:  29%|██▉       | 156/532 [04:41<12:05,  1.93s/it]predicting train subjects:  30%|██▉       | 157/532 [04:43<12:33,  2.01s/it]predicting train subjects:  30%|██▉       | 158/532 [04:45<13:07,  2.11s/it]predicting train subjects:  30%|██▉       | 159/532 [04:48<13:24,  2.16s/it]predicting train subjects:  30%|███       | 160/532 [04:50<13:37,  2.20s/it]predicting train subjects:  30%|███       | 161/532 [04:52<12:36,  2.04s/it]predicting train subjects:  30%|███       | 162/532 [04:53<11:46,  1.91s/it]predicting train subjects:  31%|███       | 163/532 [04:55<11:03,  1.80s/it]predicting train subjects:  31%|███       | 164/532 [04:56<10:36,  1.73s/it]predicting train subjects:  31%|███       | 165/532 [04:58<10:18,  1.69s/it]predicting train subjects:  31%|███       | 166/532 [04:59<10:05,  1.65s/it]predicting train subjects:  31%|███▏      | 167/532 [05:01<10:08,  1.67s/it]predicting train subjects:  32%|███▏      | 168/532 [05:03<10:07,  1.67s/it]predicting train subjects:  32%|███▏      | 169/532 [05:05<10:06,  1.67s/it]predicting train subjects:  32%|███▏      | 170/532 [05:06<10:07,  1.68s/it]predicting train subjects:  32%|███▏      | 171/532 [05:08<10:10,  1.69s/it]predicting train subjects:  32%|███▏      | 172/532 [05:10<10:13,  1.71s/it]predicting train subjects:  33%|███▎      | 173/532 [05:11<09:51,  1.65s/it]predicting train subjects:  33%|███▎      | 174/532 [05:13<09:36,  1.61s/it]predicting train subjects:  33%|███▎      | 175/532 [05:14<09:22,  1.57s/it]predicting train subjects:  33%|███▎      | 176/532 [05:16<09:16,  1.56s/it]predicting train subjects:  33%|███▎      | 177/532 [05:17<09:08,  1.55s/it]predicting train subjects:  33%|███▎      | 178/532 [05:19<09:02,  1.53s/it]predicting train subjects:  34%|███▎      | 179/532 [05:20<09:00,  1.53s/it]predicting train subjects:  34%|███▍      | 180/532 [05:22<09:06,  1.55s/it]predicting train subjects:  34%|███▍      | 181/532 [05:24<09:21,  1.60s/it]predicting train subjects:  34%|███▍      | 182/532 [05:25<09:11,  1.58s/it]predicting train subjects:  34%|███▍      | 183/532 [05:27<09:10,  1.58s/it]predicting train subjects:  35%|███▍      | 184/532 [05:28<09:07,  1.57s/it]predicting train subjects:  35%|███▍      | 185/532 [05:30<08:55,  1.54s/it]predicting train subjects:  35%|███▍      | 186/532 [05:31<08:42,  1.51s/it]predicting train subjects:  35%|███▌      | 187/532 [05:33<08:36,  1.50s/it]predicting train subjects:  35%|███▌      | 188/532 [05:34<08:32,  1.49s/it]predicting train subjects:  36%|███▌      | 189/532 [05:36<08:34,  1.50s/it]predicting train subjects:  36%|███▌      | 190/532 [05:37<08:29,  1.49s/it]predicting train subjects:  36%|███▌      | 191/532 [05:39<09:32,  1.68s/it]predicting train subjects:  36%|███▌      | 192/532 [05:41<10:22,  1.83s/it]predicting train subjects:  36%|███▋      | 193/532 [05:44<10:54,  1.93s/it]predicting train subjects:  36%|███▋      | 194/532 [05:46<11:09,  1.98s/it]predicting train subjects:  37%|███▋      | 195/532 [05:48<11:27,  2.04s/it]predicting train subjects:  37%|███▋      | 196/532 [05:50<11:39,  2.08s/it]predicting train subjects:  37%|███▋      | 197/532 [05:52<11:11,  2.01s/it]predicting train subjects:  37%|███▋      | 198/532 [05:54<11:01,  1.98s/it]predicting train subjects:  37%|███▋      | 199/532 [05:56<10:51,  1.96s/it]predicting train subjects:  38%|███▊      | 200/532 [05:58<10:42,  1.93s/it]predicting train subjects:  38%|███▊      | 201/532 [06:00<10:41,  1.94s/it]predicting train subjects:  38%|███▊      | 202/532 [06:01<10:40,  1.94s/it]predicting train subjects:  38%|███▊      | 203/532 [06:03<10:08,  1.85s/it]predicting train subjects:  38%|███▊      | 204/532 [06:05<09:33,  1.75s/it]predicting train subjects:  39%|███▊      | 205/532 [06:06<09:10,  1.68s/it]predicting train subjects:  39%|███▊      | 206/532 [06:08<08:57,  1.65s/it]predicting train subjects:  39%|███▉      | 207/532 [06:09<08:45,  1.62s/it]predicting train subjects:  39%|███▉      | 208/532 [06:11<08:41,  1.61s/it]predicting train subjects:  39%|███▉      | 209/532 [06:12<08:17,  1.54s/it]predicting train subjects:  39%|███▉      | 210/532 [06:14<08:07,  1.51s/it]predicting train subjects:  40%|███▉      | 211/532 [06:15<07:51,  1.47s/it]predicting train subjects:  40%|███▉      | 212/532 [06:16<07:43,  1.45s/it]predicting train subjects:  40%|████      | 213/532 [06:18<07:33,  1.42s/it]predicting train subjects:  40%|████      | 214/532 [06:19<07:40,  1.45s/it]predicting train subjects:  40%|████      | 215/532 [06:21<08:37,  1.63s/it]predicting train subjects:  41%|████      | 216/532 [06:24<09:26,  1.79s/it]predicting train subjects:  41%|████      | 217/532 [06:26<09:51,  1.88s/it]predicting train subjects:  41%|████      | 218/532 [06:28<10:15,  1.96s/it]predicting train subjects:  41%|████      | 219/532 [06:30<10:21,  1.99s/it]predicting train subjects:  41%|████▏     | 220/532 [06:32<10:24,  2.00s/it]predicting train subjects:  42%|████▏     | 221/532 [06:33<09:27,  1.83s/it]predicting train subjects:  42%|████▏     | 222/532 [06:35<08:38,  1.67s/it]predicting train subjects:  42%|████▏     | 223/532 [06:36<08:09,  1.58s/it]predicting train subjects:  42%|████▏     | 224/532 [06:37<07:54,  1.54s/it]predicting train subjects:  42%|████▏     | 225/532 [06:39<07:36,  1.49s/it]predicting train subjects:  42%|████▏     | 226/532 [06:40<07:16,  1.43s/it]predicting train subjects:  43%|████▎     | 227/532 [06:41<07:04,  1.39s/it]predicting train subjects:  43%|████▎     | 228/532 [06:43<06:54,  1.36s/it]predicting train subjects:  43%|████▎     | 229/532 [06:44<06:51,  1.36s/it]predicting train subjects:  43%|████▎     | 230/532 [06:45<06:46,  1.35s/it]predicting train subjects:  43%|████▎     | 231/532 [06:47<06:49,  1.36s/it]predicting train subjects:  44%|████▎     | 232/532 [06:48<06:55,  1.38s/it]predicting train subjects:  44%|████▍     | 233/532 [06:50<07:12,  1.45s/it]predicting train subjects:  44%|████▍     | 234/532 [06:51<07:13,  1.46s/it]predicting train subjects:  44%|████▍     | 235/532 [06:53<07:26,  1.50s/it]predicting train subjects:  44%|████▍     | 236/532 [06:54<07:28,  1.52s/it]predicting train subjects:  45%|████▍     | 237/532 [06:56<07:20,  1.49s/it]predicting train subjects:  45%|████▍     | 238/532 [06:58<07:35,  1.55s/it]predicting train subjects:  45%|████▍     | 239/532 [06:59<07:46,  1.59s/it]predicting train subjects:  45%|████▌     | 240/532 [07:01<07:58,  1.64s/it]predicting train subjects:  45%|████▌     | 241/532 [07:03<08:09,  1.68s/it]predicting train subjects:  45%|████▌     | 242/532 [07:04<08:10,  1.69s/it]predicting train subjects:  46%|████▌     | 243/532 [07:06<08:12,  1.70s/it]predicting train subjects:  46%|████▌     | 244/532 [07:08<08:11,  1.71s/it]predicting train subjects:  46%|████▌     | 245/532 [07:09<07:41,  1.61s/it]predicting train subjects:  46%|████▌     | 246/532 [07:11<07:22,  1.55s/it]predicting train subjects:  46%|████▋     | 247/532 [07:12<07:07,  1.50s/it]predicting train subjects:  47%|████▋     | 248/532 [07:13<06:51,  1.45s/it]predicting train subjects:  47%|████▋     | 249/532 [07:15<06:34,  1.39s/it]predicting train subjects:  47%|████▋     | 250/532 [07:16<06:34,  1.40s/it]predicting train subjects:  47%|████▋     | 251/532 [07:18<06:42,  1.43s/it]predicting train subjects:  47%|████▋     | 252/532 [07:19<06:47,  1.46s/it]predicting train subjects:  48%|████▊     | 253/532 [07:21<06:50,  1.47s/it]predicting train subjects:  48%|████▊     | 254/532 [07:22<06:46,  1.46s/it]predicting train subjects:  48%|████▊     | 255/532 [07:24<06:45,  1.46s/it]predicting train subjects:  48%|████▊     | 256/532 [07:25<06:46,  1.47s/it]predicting train subjects:  48%|████▊     | 257/532 [07:27<07:20,  1.60s/it]predicting train subjects:  48%|████▊     | 258/532 [07:29<07:48,  1.71s/it]predicting train subjects:  49%|████▊     | 259/532 [07:31<07:58,  1.75s/it]predicting train subjects:  49%|████▉     | 260/532 [07:33<08:13,  1.81s/it]predicting train subjects:  49%|████▉     | 261/532 [07:35<08:23,  1.86s/it]predicting train subjects:  49%|████▉     | 262/532 [07:37<08:34,  1.91s/it]predicting train subjects:  49%|████▉     | 263/532 [07:38<07:55,  1.77s/it]predicting train subjects:  50%|████▉     | 264/532 [07:39<07:24,  1.66s/it]predicting train subjects:  50%|████▉     | 265/532 [07:41<06:59,  1.57s/it]predicting train subjects:  50%|█████     | 266/532 [07:42<06:45,  1.53s/it]predicting train subjects:  50%|█████     | 267/532 [07:44<06:30,  1.47s/it]predicting train subjects:  50%|█████     | 268/532 [07:45<06:17,  1.43s/it]predicting train subjects:  51%|█████     | 269/532 [07:47<06:33,  1.50s/it]predicting train subjects:  51%|█████     | 270/532 [07:48<06:37,  1.52s/it]predicting train subjects:  51%|█████     | 271/532 [07:50<06:50,  1.57s/it]predicting train subjects:  51%|█████     | 272/532 [07:52<07:19,  1.69s/it]predicting train subjects:  51%|█████▏    | 273/532 [07:54<07:20,  1.70s/it]predicting train subjects:  52%|█████▏    | 274/532 [07:55<07:15,  1.69s/it]predicting train subjects:  52%|█████▏    | 275/532 [07:57<07:49,  1.82s/it]predicting train subjects:  52%|█████▏    | 276/532 [07:59<08:05,  1.90s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:02<08:18,  1.96s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:04<08:23,  1.98s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:06<08:20,  1.98s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:08<08:28,  2.02s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:10<08:22,  2.00s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:12<08:15,  1.98s/it]predicting train subjects:  53%|█████▎    | 283/532 [08:14<08:13,  1.98s/it]predicting train subjects:  53%|█████▎    | 284/532 [08:15<08:05,  1.96s/it]predicting train subjects:  54%|█████▎    | 285/532 [08:17<08:03,  1.96s/it]predicting train subjects:  54%|█████▍    | 286/532 [08:19<08:03,  1.97s/it]predicting train subjects:  54%|█████▍    | 287/532 [08:21<07:22,  1.81s/it]predicting train subjects:  54%|█████▍    | 288/532 [08:22<06:56,  1.71s/it]predicting train subjects:  54%|█████▍    | 289/532 [08:24<06:35,  1.63s/it]predicting train subjects:  55%|█████▍    | 290/532 [08:25<06:23,  1.58s/it]predicting train subjects:  55%|█████▍    | 291/532 [08:27<06:12,  1.54s/it]predicting train subjects:  55%|█████▍    | 292/532 [08:28<06:04,  1.52s/it]predicting train subjects:  55%|█████▌    | 293/532 [08:30<06:17,  1.58s/it]predicting train subjects:  55%|█████▌    | 294/532 [08:32<06:21,  1.60s/it]predicting train subjects:  55%|█████▌    | 295/532 [08:33<06:26,  1.63s/it]predicting train subjects:  56%|█████▌    | 296/532 [08:35<06:31,  1.66s/it]predicting train subjects:  56%|█████▌    | 297/532 [08:37<06:35,  1.68s/it]predicting train subjects:  56%|█████▌    | 298/532 [08:38<06:32,  1.68s/it]predicting train subjects:  56%|█████▌    | 299/532 [08:40<06:09,  1.59s/it]predicting train subjects:  56%|█████▋    | 300/532 [08:41<05:51,  1.51s/it]predicting train subjects:  57%|█████▋    | 301/532 [08:42<05:37,  1.46s/it]predicting train subjects:  57%|█████▋    | 302/532 [08:44<05:33,  1.45s/it]predicting train subjects:  57%|█████▋    | 303/532 [08:45<05:23,  1.41s/it]predicting train subjects:  57%|█████▋    | 304/532 [08:47<05:21,  1.41s/it]predicting train subjects:  57%|█████▋    | 305/532 [08:49<06:03,  1.60s/it]predicting train subjects:  58%|█████▊    | 306/532 [08:51<06:31,  1.73s/it]predicting train subjects:  58%|█████▊    | 307/532 [08:53<06:45,  1.80s/it]predicting train subjects:  58%|█████▊    | 308/532 [08:55<07:00,  1.88s/it]predicting train subjects:  58%|█████▊    | 309/532 [08:57<07:06,  1.91s/it]predicting train subjects:  58%|█████▊    | 310/532 [08:59<07:07,  1.93s/it]predicting train subjects:  58%|█████▊    | 311/532 [09:01<08:03,  2.19s/it]predicting train subjects:  59%|█████▊    | 312/532 [09:04<08:37,  2.35s/it]predicting train subjects:  59%|█████▉    | 313/532 [09:07<09:00,  2.47s/it]predicting train subjects:  59%|█████▉    | 314/532 [09:10<09:14,  2.55s/it]predicting train subjects:  59%|█████▉    | 315/532 [09:12<09:19,  2.58s/it]predicting train subjects:  59%|█████▉    | 316/532 [09:15<09:30,  2.64s/it]predicting train subjects:  60%|█████▉    | 317/532 [09:17<08:15,  2.30s/it]predicting train subjects:  60%|█████▉    | 318/532 [09:18<07:18,  2.05s/it]predicting train subjects:  60%|█████▉    | 319/532 [09:20<06:42,  1.89s/it]predicting train subjects:  60%|██████    | 320/532 [09:21<06:12,  1.76s/it]predicting train subjects:  60%|██████    | 321/532 [09:22<05:51,  1.67s/it]predicting train subjects:  61%|██████    | 322/532 [09:24<05:39,  1.62s/it]predicting train subjects:  61%|██████    | 323/532 [09:26<06:14,  1.79s/it]predicting train subjects:  61%|██████    | 324/532 [09:28<06:43,  1.94s/it]predicting train subjects:  61%|██████    | 325/532 [09:31<06:56,  2.01s/it]predicting train subjects:  61%|██████▏   | 326/532 [09:33<07:07,  2.08s/it]predicting train subjects:  61%|██████▏   | 327/532 [09:35<07:13,  2.11s/it]predicting train subjects:  62%|██████▏   | 328/532 [09:37<07:16,  2.14s/it]predicting train subjects:  62%|██████▏   | 329/532 [09:39<06:39,  1.97s/it]predicting train subjects:  62%|██████▏   | 330/532 [09:40<06:15,  1.86s/it]predicting train subjects:  62%|██████▏   | 331/532 [09:42<05:59,  1.79s/it]predicting train subjects:  62%|██████▏   | 332/532 [09:44<05:48,  1.74s/it]predicting train subjects:  63%|██████▎   | 333/532 [09:45<05:38,  1.70s/it]predicting train subjects:  63%|██████▎   | 334/532 [09:47<05:28,  1.66s/it]predicting train subjects:  63%|██████▎   | 335/532 [09:49<05:41,  1.73s/it]predicting train subjects:  63%|██████▎   | 336/532 [09:51<05:48,  1.78s/it]predicting train subjects:  63%|██████▎   | 337/532 [09:53<05:54,  1.82s/it]predicting train subjects:  64%|██████▎   | 338/532 [09:54<05:58,  1.85s/it]predicting train subjects:  64%|██████▎   | 339/532 [09:56<05:58,  1.86s/it]predicting train subjects:  64%|██████▍   | 340/532 [09:58<05:58,  1.87s/it]predicting train subjects:  64%|██████▍   | 341/532 [10:00<05:30,  1.73s/it]predicting train subjects:  64%|██████▍   | 342/532 [10:01<05:16,  1.67s/it]predicting train subjects:  64%|██████▍   | 343/532 [10:03<05:01,  1.60s/it]predicting train subjects:  65%|██████▍   | 344/532 [10:04<04:48,  1.53s/it]predicting train subjects:  65%|██████▍   | 345/532 [10:05<04:38,  1.49s/it]predicting train subjects:  65%|██████▌   | 346/532 [10:07<04:32,  1.46s/it]predicting train subjects:  65%|██████▌   | 347/532 [10:08<04:38,  1.51s/it]predicting train subjects:  65%|██████▌   | 348/532 [10:10<04:46,  1.56s/it]predicting train subjects:  66%|██████▌   | 349/532 [10:12<04:49,  1.58s/it]predicting train subjects:  66%|██████▌   | 350/532 [10:13<04:51,  1.60s/it]predicting train subjects:  66%|██████▌   | 351/532 [10:15<04:53,  1.62s/it]predicting train subjects:  66%|██████▌   | 352/532 [10:17<04:59,  1.66s/it]predicting train subjects:  66%|██████▋   | 353/532 [10:18<04:55,  1.65s/it]predicting train subjects:  67%|██████▋   | 354/532 [10:20<04:51,  1.64s/it]predicting train subjects:  67%|██████▋   | 355/532 [10:22<04:45,  1.61s/it]predicting train subjects:  67%|██████▋   | 356/532 [10:23<04:43,  1.61s/it]predicting train subjects:  67%|██████▋   | 357/532 [10:25<04:44,  1.63s/it]predicting train subjects:  67%|██████▋   | 358/532 [10:27<04:47,  1.65s/it]predicting train subjects:  67%|██████▋   | 359/532 [10:28<04:35,  1.59s/it]predicting train subjects:  68%|██████▊   | 360/532 [10:29<04:29,  1.57s/it]predicting train subjects:  68%|██████▊   | 361/532 [10:31<04:22,  1.54s/it]predicting train subjects:  68%|██████▊   | 362/532 [10:32<04:16,  1.51s/it]predicting train subjects:  68%|██████▊   | 363/532 [10:34<04:10,  1.48s/it]predicting train subjects:  68%|██████▊   | 364/532 [10:35<04:08,  1.48s/it]predicting train subjects:  69%|██████▊   | 365/532 [10:37<04:01,  1.45s/it]predicting train subjects:  69%|██████▉   | 366/532 [10:38<03:58,  1.44s/it]predicting train subjects:  69%|██████▉   | 367/532 [10:39<03:55,  1.43s/it]predicting train subjects:  69%|██████▉   | 368/532 [10:41<03:53,  1.42s/it]predicting train subjects:  69%|██████▉   | 369/532 [10:42<03:49,  1.41s/it]predicting train subjects:  70%|██████▉   | 370/532 [10:44<03:46,  1.40s/it]predicting train subjects:  70%|██████▉   | 371/532 [10:46<04:15,  1.59s/it]predicting train subjects:  70%|██████▉   | 372/532 [10:48<04:37,  1.73s/it]predicting train subjects:  70%|███████   | 373/532 [10:50<04:51,  1.83s/it]predicting train subjects:  70%|███████   | 374/532 [10:52<05:05,  1.93s/it]predicting train subjects:  70%|███████   | 375/532 [10:54<05:10,  1.98s/it]predicting train subjects:  71%|███████   | 376/532 [10:56<05:12,  2.00s/it]predicting train subjects:  71%|███████   | 377/532 [10:58<04:54,  1.90s/it]predicting train subjects:  71%|███████   | 378/532 [10:59<04:40,  1.82s/it]predicting train subjects:  71%|███████   | 379/532 [11:01<04:29,  1.76s/it]predicting train subjects:  71%|███████▏  | 380/532 [11:03<04:21,  1.72s/it]predicting train subjects:  72%|███████▏  | 381/532 [11:04<04:11,  1.66s/it]predicting train subjects:  72%|███████▏  | 382/532 [11:06<04:08,  1.66s/it]predicting train subjects:  72%|███████▏  | 383/532 [11:07<04:07,  1.66s/it]predicting train subjects:  72%|███████▏  | 384/532 [11:09<04:05,  1.66s/it]predicting train subjects:  72%|███████▏  | 385/532 [11:11<04:03,  1.65s/it]predicting train subjects:  73%|███████▎  | 386/532 [11:12<04:02,  1.66s/it]predicting train subjects:  73%|███████▎  | 387/532 [11:14<04:02,  1.67s/it]predicting train subjects:  73%|███████▎  | 388/532 [11:16<03:57,  1.65s/it]predicting train subjects:  73%|███████▎  | 389/532 [11:17<03:57,  1.66s/it]predicting train subjects:  73%|███████▎  | 390/532 [11:19<04:02,  1.71s/it]predicting train subjects:  73%|███████▎  | 391/532 [11:21<04:01,  1.72s/it]predicting train subjects:  74%|███████▎  | 392/532 [11:23<04:01,  1.72s/it]predicting train subjects:  74%|███████▍  | 393/532 [11:25<04:02,  1.75s/it]predicting train subjects:  74%|███████▍  | 394/532 [11:26<03:59,  1.74s/it]predicting train subjects:  74%|███████▍  | 395/532 [11:28<03:55,  1.72s/it]predicting train subjects:  74%|███████▍  | 396/532 [11:30<03:55,  1.73s/it]predicting train subjects:  75%|███████▍  | 397/532 [11:31<03:55,  1.75s/it]predicting train subjects:  75%|███████▍  | 398/532 [11:33<03:55,  1.76s/it]predicting train subjects:  75%|███████▌  | 399/532 [11:35<03:51,  1.74s/it]predicting train subjects:  75%|███████▌  | 400/532 [11:37<03:47,  1.73s/it]predicting train subjects:  75%|███████▌  | 401/532 [11:39<03:51,  1.77s/it]predicting train subjects:  76%|███████▌  | 402/532 [11:40<03:56,  1.82s/it]predicting train subjects:  76%|███████▌  | 403/532 [11:42<03:56,  1.83s/it]predicting train subjects:  76%|███████▌  | 404/532 [11:44<03:56,  1.85s/it]predicting train subjects:  76%|███████▌  | 405/532 [11:46<03:55,  1.85s/it]predicting train subjects:  76%|███████▋  | 406/532 [11:48<03:50,  1.83s/it]predicting train subjects:  77%|███████▋  | 407/532 [11:49<03:41,  1.77s/it]predicting train subjects:  77%|███████▋  | 408/532 [11:51<03:32,  1.71s/it]predicting train subjects:  77%|███████▋  | 409/532 [11:53<03:29,  1.70s/it]predicting train subjects:  77%|███████▋  | 410/532 [11:54<03:27,  1.70s/it]predicting train subjects:  77%|███████▋  | 411/532 [11:56<03:22,  1.67s/it]predicting train subjects:  77%|███████▋  | 412/532 [11:58<03:19,  1.66s/it]predicting train subjects:  78%|███████▊  | 413/532 [11:59<03:13,  1.63s/it]predicting train subjects:  78%|███████▊  | 414/532 [12:01<03:07,  1.59s/it]predicting train subjects:  78%|███████▊  | 415/532 [12:02<03:03,  1.57s/it]predicting train subjects:  78%|███████▊  | 416/532 [12:04<03:00,  1.56s/it]predicting train subjects:  78%|███████▊  | 417/532 [12:05<02:58,  1.55s/it]predicting train subjects:  79%|███████▊  | 418/532 [12:07<02:55,  1.54s/it]predicting train subjects:  79%|███████▉  | 419/532 [12:09<03:01,  1.60s/it]predicting train subjects:  79%|███████▉  | 420/532 [12:10<03:02,  1.63s/it]predicting train subjects:  79%|███████▉  | 421/532 [12:12<03:10,  1.71s/it]predicting train subjects:  79%|███████▉  | 422/532 [12:14<03:19,  1.81s/it]predicting train subjects:  80%|███████▉  | 423/532 [12:16<03:13,  1.77s/it]predicting train subjects:  80%|███████▉  | 424/532 [12:18<03:06,  1.73s/it]predicting train subjects:  80%|███████▉  | 425/532 [12:19<03:05,  1.73s/it]predicting train subjects:  80%|████████  | 426/532 [12:21<03:01,  1.72s/it]predicting train subjects:  80%|████████  | 427/532 [12:23<03:02,  1.74s/it]predicting train subjects:  80%|████████  | 428/532 [12:24<03:00,  1.74s/it]predicting train subjects:  81%|████████  | 429/532 [12:26<03:01,  1.76s/it]predicting train subjects:  81%|████████  | 430/532 [12:28<03:01,  1.78s/it]predicting train subjects:  81%|████████  | 431/532 [12:30<03:05,  1.84s/it]predicting train subjects:  81%|████████  | 432/532 [12:32<03:08,  1.89s/it]predicting train subjects:  81%|████████▏ | 433/532 [12:34<03:08,  1.90s/it]predicting train subjects:  82%|████████▏ | 434/532 [12:36<03:08,  1.92s/it]predicting train subjects:  82%|████████▏ | 435/532 [12:38<03:05,  1.91s/it]predicting train subjects:  82%|████████▏ | 436/532 [12:40<03:02,  1.90s/it]predicting train subjects:  82%|████████▏ | 437/532 [12:41<02:45,  1.75s/it]predicting train subjects:  82%|████████▏ | 438/532 [12:42<02:33,  1.63s/it]predicting train subjects:  83%|████████▎ | 439/532 [12:44<02:25,  1.56s/it]predicting train subjects:  83%|████████▎ | 440/532 [12:45<02:19,  1.52s/it]predicting train subjects:  83%|████████▎ | 441/532 [12:47<02:16,  1.49s/it]predicting train subjects:  83%|████████▎ | 442/532 [12:48<02:15,  1.50s/it]predicting train subjects:  83%|████████▎ | 443/532 [12:50<02:09,  1.45s/it]predicting train subjects:  83%|████████▎ | 444/532 [12:51<02:05,  1.43s/it]predicting train subjects:  84%|████████▎ | 445/532 [12:52<02:03,  1.42s/it]predicting train subjects:  84%|████████▍ | 446/532 [12:54<02:00,  1.40s/it]predicting train subjects:  84%|████████▍ | 447/532 [12:55<01:56,  1.38s/it]predicting train subjects:  84%|████████▍ | 448/532 [12:56<01:55,  1.37s/it]predicting train subjects:  84%|████████▍ | 449/532 [12:58<02:00,  1.45s/it]predicting train subjects:  85%|████████▍ | 450/532 [13:00<02:00,  1.47s/it]predicting train subjects:  85%|████████▍ | 451/532 [13:01<02:00,  1.49s/it]predicting train subjects:  85%|████████▍ | 452/532 [13:03<02:00,  1.50s/it]predicting train subjects:  85%|████████▌ | 453/532 [13:04<01:58,  1.51s/it]predicting train subjects:  85%|████████▌ | 454/532 [13:06<01:57,  1.51s/it]predicting train subjects:  86%|████████▌ | 455/532 [13:07<02:02,  1.59s/it]predicting train subjects:  86%|████████▌ | 456/532 [13:09<02:03,  1.63s/it]predicting train subjects:  86%|████████▌ | 457/532 [13:11<02:05,  1.67s/it]predicting train subjects:  86%|████████▌ | 458/532 [13:13<02:03,  1.67s/it]predicting train subjects:  86%|████████▋ | 459/532 [13:14<02:05,  1.72s/it]predicting train subjects:  86%|████████▋ | 460/532 [13:16<02:02,  1.70s/it]predicting train subjects:  87%|████████▋ | 461/532 [13:18<02:09,  1.82s/it]predicting train subjects:  87%|████████▋ | 462/532 [13:20<02:12,  1.89s/it]predicting train subjects:  87%|████████▋ | 463/532 [13:22<02:12,  1.93s/it]predicting train subjects:  87%|████████▋ | 464/532 [13:24<02:13,  1.96s/it]predicting train subjects:  87%|████████▋ | 465/532 [13:26<02:15,  2.03s/it]predicting train subjects:  88%|████████▊ | 466/532 [13:28<02:14,  2.03s/it]predicting train subjects:  88%|████████▊ | 467/532 [13:30<02:03,  1.90s/it]predicting train subjects:  88%|████████▊ | 468/532 [13:32<01:55,  1.80s/it]predicting train subjects:  88%|████████▊ | 469/532 [13:33<01:49,  1.73s/it]predicting train subjects:  88%|████████▊ | 470/532 [13:35<01:43,  1.68s/it]predicting train subjects:  89%|████████▊ | 471/532 [13:36<01:41,  1.67s/it]predicting train subjects:  89%|████████▊ | 472/532 [13:38<01:39,  1.66s/it]predicting train subjects:  89%|████████▉ | 473/532 [13:40<01:41,  1.72s/it]predicting train subjects:  89%|████████▉ | 474/532 [13:42<01:41,  1.75s/it]predicting train subjects:  89%|████████▉ | 475/532 [13:44<01:41,  1.78s/it]predicting train subjects:  89%|████████▉ | 476/532 [13:45<01:41,  1.81s/it]predicting train subjects:  90%|████████▉ | 477/532 [13:47<01:39,  1.80s/it]predicting train subjects:  90%|████████▉ | 478/532 [13:49<01:37,  1.81s/it]predicting train subjects:  90%|█████████ | 479/532 [13:51<01:32,  1.75s/it]predicting train subjects:  90%|█████████ | 480/532 [13:52<01:27,  1.69s/it]predicting train subjects:  90%|█████████ | 481/532 [13:54<01:24,  1.66s/it]predicting train subjects:  91%|█████████ | 482/532 [13:55<01:21,  1.63s/it]predicting train subjects:  91%|█████████ | 483/532 [13:57<01:19,  1.63s/it]predicting train subjects:  91%|█████████ | 484/532 [13:59<01:17,  1.61s/it]predicting train subjects:  91%|█████████ | 485/532 [14:01<01:21,  1.74s/it]predicting train subjects:  91%|█████████▏| 486/532 [14:03<01:24,  1.83s/it]predicting train subjects:  92%|█████████▏| 487/532 [14:05<01:25,  1.89s/it]predicting train subjects:  92%|█████████▏| 488/532 [14:07<01:24,  1.91s/it]predicting train subjects:  92%|█████████▏| 489/532 [14:09<01:22,  1.92s/it]predicting train subjects:  92%|█████████▏| 490/532 [14:10<01:19,  1.90s/it]predicting train subjects:  92%|█████████▏| 491/532 [14:12<01:13,  1.79s/it]predicting train subjects:  92%|█████████▏| 492/532 [14:14<01:08,  1.72s/it]predicting train subjects:  93%|█████████▎| 493/532 [14:15<01:04,  1.66s/it]predicting train subjects:  93%|█████████▎| 494/532 [14:17<01:01,  1.63s/it]predicting train subjects:  93%|█████████▎| 495/532 [14:18<01:00,  1.63s/it]predicting train subjects:  93%|█████████▎| 496/532 [14:20<00:58,  1.62s/it]predicting train subjects:  93%|█████████▎| 497/532 [14:21<00:56,  1.62s/it]predicting train subjects:  94%|█████████▎| 498/532 [14:23<00:56,  1.67s/it]predicting train subjects:  94%|█████████▍| 499/532 [14:25<00:56,  1.72s/it]predicting train subjects:  94%|█████████▍| 500/532 [14:27<00:55,  1.73s/it]predicting train subjects:  94%|█████████▍| 501/532 [14:29<00:53,  1.72s/it]predicting train subjects:  94%|█████████▍| 502/532 [14:30<00:51,  1.71s/it]predicting train subjects:  95%|█████████▍| 503/532 [14:32<00:47,  1.65s/it]predicting train subjects:  95%|█████████▍| 504/532 [14:33<00:44,  1.60s/it]predicting train subjects:  95%|█████████▍| 505/532 [14:35<00:42,  1.56s/it]predicting train subjects:  95%|█████████▌| 506/532 [14:36<00:40,  1.54s/it]predicting train subjects:  95%|█████████▌| 507/532 [14:38<00:38,  1.54s/it]predicting train subjects:  95%|█████████▌| 508/532 [14:39<00:36,  1.52s/it]predicting train subjects:  96%|█████████▌| 509/532 [14:41<00:38,  1.67s/it]predicting train subjects:  96%|█████████▌| 510/532 [14:43<00:38,  1.76s/it]predicting train subjects:  96%|█████████▌| 511/532 [14:45<00:38,  1.83s/it]predicting train subjects:  96%|█████████▌| 512/532 [14:47<00:37,  1.89s/it]predicting train subjects:  96%|█████████▋| 513/532 [14:49<00:36,  1.90s/it]predicting train subjects:  97%|█████████▋| 514/532 [14:51<00:34,  1.89s/it]predicting train subjects:  97%|█████████▋| 515/532 [14:53<00:30,  1.80s/it]predicting train subjects:  97%|█████████▋| 516/532 [14:54<00:27,  1.73s/it]predicting train subjects:  97%|█████████▋| 517/532 [14:56<00:25,  1.71s/it]predicting train subjects:  97%|█████████▋| 518/532 [14:57<00:23,  1.70s/it]predicting train subjects:  98%|█████████▊| 519/532 [14:59<00:21,  1.68s/it]predicting train subjects:  98%|█████████▊| 520/532 [15:01<00:20,  1.69s/it]predicting train subjects:  98%|█████████▊| 521/532 [15:03<00:19,  1.76s/it]predicting train subjects:  98%|█████████▊| 522/532 [15:05<00:17,  1.78s/it]predicting train subjects:  98%|█████████▊| 523/532 [15:06<00:15,  1.77s/it]predicting train subjects:  98%|█████████▊| 524/532 [15:08<00:14,  1.75s/it]predicting train subjects:  99%|█████████▊| 525/532 [15:10<00:12,  1.74s/it]predicting train subjects:  99%|█████████▉| 526/532 [15:12<00:10,  1.75s/it]predicting train subjects:  99%|█████████▉| 527/532 [15:13<00:08,  1.68s/it]predicting train subjects:  99%|█████████▉| 528/532 [15:15<00:06,  1.63s/it]predicting train subjects:  99%|█████████▉| 529/532 [15:16<00:04,  1.61s/it]predicting train subjects: 100%|█████████▉| 530/532 [15:18<00:03,  1.61s/it]predicting train subjects: 100%|█████████▉| 531/532 [15:19<00:01,  1.60s/it]predicting train subjects: 100%|██████████| 532/532 [15:21<00:00,  1.59s/it]
Loading train:   0%|          | 0/532 [00:00<?, ?it/s]Loading train:   0%|          | 1/532 [00:01<13:14,  1.50s/it]Loading train:   0%|          | 2/532 [00:02<12:28,  1.41s/it]Loading train:   1%|          | 3/532 [00:03<12:01,  1.36s/it]Loading train:   1%|          | 4/532 [00:05<11:32,  1.31s/it]Loading train:   1%|          | 5/532 [00:06<11:04,  1.26s/it]Loading train:   1%|          | 6/532 [00:07<10:57,  1.25s/it]Loading train:   1%|▏         | 7/532 [00:08<10:20,  1.18s/it]Loading train:   2%|▏         | 8/532 [00:09<10:16,  1.18s/it]Loading train:   2%|▏         | 9/532 [00:10<10:15,  1.18s/it]Loading train:   2%|▏         | 10/532 [00:11<10:02,  1.15s/it]Loading train:   2%|▏         | 11/532 [00:12<09:32,  1.10s/it]Loading train:   2%|▏         | 12/532 [00:14<10:04,  1.16s/it]Loading train:   2%|▏         | 13/532 [00:15<09:48,  1.13s/it]Loading train:   3%|▎         | 14/532 [00:16<09:03,  1.05s/it]Loading train:   3%|▎         | 15/532 [00:17<08:43,  1.01s/it]Loading train:   3%|▎         | 16/532 [00:18<08:50,  1.03s/it]Loading train:   3%|▎         | 17/532 [00:19<08:25,  1.02it/s]Loading train:   3%|▎         | 18/532 [00:20<08:44,  1.02s/it]Loading train:   4%|▎         | 19/532 [00:21<08:29,  1.01it/s]Loading train:   4%|▍         | 20/532 [00:22<08:44,  1.02s/it]Loading train:   4%|▍         | 21/532 [00:23<09:10,  1.08s/it]Loading train:   4%|▍         | 22/532 [00:24<08:46,  1.03s/it]Loading train:   4%|▍         | 23/532 [00:25<08:47,  1.04s/it]Loading train:   5%|▍         | 24/532 [00:26<08:38,  1.02s/it]Loading train:   5%|▍         | 25/532 [00:27<09:44,  1.15s/it]Loading train:   5%|▍         | 26/532 [00:28<09:31,  1.13s/it]Loading train:   5%|▌         | 27/532 [00:30<09:54,  1.18s/it]Loading train:   5%|▌         | 28/532 [00:31<09:36,  1.14s/it]Loading train:   5%|▌         | 29/532 [00:32<09:36,  1.15s/it]Loading train:   6%|▌         | 30/532 [00:33<09:05,  1.09s/it]Loading train:   6%|▌         | 31/532 [00:34<08:44,  1.05s/it]Loading train:   6%|▌         | 32/532 [00:35<08:36,  1.03s/it]Loading train:   6%|▌         | 33/532 [00:36<08:43,  1.05s/it]Loading train:   6%|▋         | 34/532 [00:37<09:05,  1.09s/it]Loading train:   7%|▋         | 35/532 [00:38<08:52,  1.07s/it]Loading train:   7%|▋         | 36/532 [00:39<09:01,  1.09s/it]Loading train:   7%|▋         | 37/532 [00:40<09:09,  1.11s/it]Loading train:   7%|▋         | 38/532 [00:42<09:21,  1.14s/it]Loading train:   7%|▋         | 39/532 [00:43<09:00,  1.10s/it]Loading train:   8%|▊         | 40/532 [00:44<08:37,  1.05s/it]Loading train:   8%|▊         | 41/532 [00:45<08:50,  1.08s/it]Loading train:   8%|▊         | 42/532 [00:46<09:07,  1.12s/it]Loading train:   8%|▊         | 43/532 [00:47<08:31,  1.05s/it]Loading train:   8%|▊         | 44/532 [00:48<08:11,  1.01s/it]Loading train:   8%|▊         | 45/532 [00:49<07:53,  1.03it/s]Loading train:   9%|▊         | 46/532 [00:50<08:29,  1.05s/it]Loading train:   9%|▉         | 47/532 [00:51<08:42,  1.08s/it]Loading train:   9%|▉         | 48/532 [00:52<08:37,  1.07s/it]Loading train:   9%|▉         | 49/532 [00:53<08:14,  1.02s/it]Loading train:   9%|▉         | 50/532 [00:54<08:15,  1.03s/it]Loading train:  10%|▉         | 51/532 [00:55<07:52,  1.02it/s]Loading train:  10%|▉         | 52/532 [00:56<07:47,  1.03it/s]Loading train:  10%|▉         | 53/532 [00:57<07:24,  1.08it/s]Loading train:  10%|█         | 54/532 [00:58<07:50,  1.02it/s]Loading train:  10%|█         | 55/532 [00:59<08:09,  1.03s/it]Loading train:  11%|█         | 56/532 [01:00<08:06,  1.02s/it]Loading train:  11%|█         | 57/532 [01:01<07:55,  1.00s/it]Loading train:  11%|█         | 58/532 [01:02<08:11,  1.04s/it]Loading train:  11%|█         | 59/532 [01:03<08:43,  1.11s/it]Loading train:  11%|█▏        | 60/532 [01:04<08:16,  1.05s/it]Loading train:  11%|█▏        | 61/532 [01:05<07:40,  1.02it/s]Loading train:  12%|█▏        | 62/532 [01:06<08:09,  1.04s/it]Loading train:  12%|█▏        | 63/532 [01:07<08:40,  1.11s/it]Loading train:  12%|█▏        | 64/532 [01:08<08:14,  1.06s/it]Loading train:  12%|█▏        | 65/532 [01:09<08:06,  1.04s/it]Loading train:  12%|█▏        | 66/532 [01:11<08:47,  1.13s/it]Loading train:  13%|█▎        | 67/532 [01:12<09:00,  1.16s/it]Loading train:  13%|█▎        | 68/532 [01:13<08:49,  1.14s/it]Loading train:  13%|█▎        | 69/532 [01:14<08:44,  1.13s/it]Loading train:  13%|█▎        | 70/532 [01:15<08:36,  1.12s/it]Loading train:  13%|█▎        | 71/532 [01:16<08:18,  1.08s/it]Loading train:  14%|█▎        | 72/532 [01:17<08:00,  1.04s/it]Loading train:  14%|█▎        | 73/532 [01:18<08:02,  1.05s/it]Loading train:  14%|█▍        | 74/532 [01:20<08:33,  1.12s/it]Loading train:  14%|█▍        | 75/532 [01:21<09:30,  1.25s/it]Loading train:  14%|█▍        | 76/532 [01:22<08:53,  1.17s/it]Loading train:  14%|█▍        | 77/532 [01:23<08:31,  1.12s/it]Loading train:  15%|█▍        | 78/532 [01:24<08:06,  1.07s/it]Loading train:  15%|█▍        | 79/532 [01:25<08:11,  1.08s/it]Loading train:  15%|█▌        | 80/532 [01:26<08:21,  1.11s/it]Loading train:  15%|█▌        | 81/532 [01:27<08:10,  1.09s/it]Loading train:  15%|█▌        | 82/532 [01:28<08:02,  1.07s/it]Loading train:  16%|█▌        | 83/532 [01:29<07:52,  1.05s/it]Loading train:  16%|█▌        | 84/532 [01:30<07:27,  1.00it/s]Loading train:  16%|█▌        | 85/532 [01:31<07:06,  1.05it/s]Loading train:  16%|█▌        | 86/532 [01:32<06:57,  1.07it/s]Loading train:  16%|█▋        | 87/532 [01:33<06:45,  1.10it/s]Loading train:  17%|█▋        | 88/532 [01:34<06:26,  1.15it/s]Loading train:  17%|█▋        | 89/532 [01:35<06:38,  1.11it/s]Loading train:  17%|█▋        | 90/532 [01:35<06:35,  1.12it/s]Loading train:  17%|█▋        | 91/532 [01:36<06:35,  1.11it/s]Loading train:  17%|█▋        | 92/532 [01:37<06:42,  1.09it/s]Loading train:  17%|█▋        | 93/532 [01:38<06:42,  1.09it/s]Loading train:  18%|█▊        | 94/532 [01:39<06:41,  1.09it/s]Loading train:  18%|█▊        | 95/532 [01:40<07:09,  1.02it/s]Loading train:  18%|█▊        | 96/532 [01:41<07:18,  1.01s/it]Loading train:  18%|█▊        | 97/532 [01:43<07:45,  1.07s/it]Loading train:  18%|█▊        | 98/532 [01:44<07:56,  1.10s/it]Loading train:  19%|█▊        | 99/532 [01:45<07:59,  1.11s/it]Loading train:  19%|█▉        | 100/532 [01:46<08:07,  1.13s/it]Loading train:  19%|█▉        | 101/532 [01:47<07:56,  1.10s/it]Loading train:  19%|█▉        | 102/532 [01:48<07:29,  1.05s/it]Loading train:  19%|█▉        | 103/532 [01:49<07:24,  1.04s/it]Loading train:  20%|█▉        | 104/532 [01:50<07:08,  1.00s/it]Loading train:  20%|█▉        | 105/532 [01:51<06:55,  1.03it/s]Loading train:  20%|█▉        | 106/532 [01:52<06:57,  1.02it/s]Loading train:  20%|██        | 107/532 [01:53<07:08,  1.01s/it]Loading train:  20%|██        | 108/532 [01:54<07:11,  1.02s/it]Loading train:  20%|██        | 109/532 [01:55<06:51,  1.03it/s]Loading train:  21%|██        | 110/532 [01:56<06:44,  1.04it/s]Loading train:  21%|██        | 111/532 [01:57<06:39,  1.05it/s]Loading train:  21%|██        | 112/532 [01:58<06:37,  1.06it/s]Loading train:  21%|██        | 113/532 [01:59<06:55,  1.01it/s]Loading train:  21%|██▏       | 114/532 [02:00<06:59,  1.00s/it]Loading train:  22%|██▏       | 115/532 [02:01<07:06,  1.02s/it]Loading train:  22%|██▏       | 116/532 [02:02<07:09,  1.03s/it]Loading train:  22%|██▏       | 117/532 [02:03<07:11,  1.04s/it]Loading train:  22%|██▏       | 118/532 [02:04<07:09,  1.04s/it]Loading train:  22%|██▏       | 119/532 [02:05<07:15,  1.06s/it]Loading train:  23%|██▎       | 120/532 [02:06<07:06,  1.04s/it]Loading train:  23%|██▎       | 121/532 [02:07<06:49,  1.00it/s]Loading train:  23%|██▎       | 122/532 [02:08<06:55,  1.01s/it]Loading train:  23%|██▎       | 123/532 [02:09<06:53,  1.01s/it]Loading train:  23%|██▎       | 124/532 [02:10<06:44,  1.01it/s]Loading train:  23%|██▎       | 125/532 [02:11<07:01,  1.03s/it]Loading train:  24%|██▎       | 126/532 [02:12<07:04,  1.05s/it]Loading train:  24%|██▍       | 127/532 [02:13<07:24,  1.10s/it]Loading train:  24%|██▍       | 128/532 [02:14<07:21,  1.09s/it]Loading train:  24%|██▍       | 129/532 [02:16<07:23,  1.10s/it]Loading train:  24%|██▍       | 130/532 [02:17<07:18,  1.09s/it]Loading train:  25%|██▍       | 131/532 [02:18<07:49,  1.17s/it]Loading train:  25%|██▍       | 132/532 [02:19<08:07,  1.22s/it]Loading train:  25%|██▌       | 133/532 [02:21<08:09,  1.23s/it]Loading train:  25%|██▌       | 134/532 [02:22<08:10,  1.23s/it]Loading train:  25%|██▌       | 135/532 [02:23<08:05,  1.22s/it]Loading train:  26%|██▌       | 136/532 [02:24<08:06,  1.23s/it]Loading train:  26%|██▌       | 137/532 [02:26<08:14,  1.25s/it]Loading train:  26%|██▌       | 138/532 [02:27<08:12,  1.25s/it]Loading train:  26%|██▌       | 139/532 [02:28<08:07,  1.24s/it]Loading train:  26%|██▋       | 140/532 [02:29<08:03,  1.23s/it]Loading train:  27%|██▋       | 141/532 [02:31<08:07,  1.25s/it]Loading train:  27%|██▋       | 142/532 [02:32<07:57,  1.22s/it]Loading train:  27%|██▋       | 143/532 [02:33<07:22,  1.14s/it]Loading train:  27%|██▋       | 144/532 [02:33<06:47,  1.05s/it]Loading train:  27%|██▋       | 145/532 [02:34<06:23,  1.01it/s]Loading train:  27%|██▋       | 146/532 [02:35<06:05,  1.06it/s]Loading train:  28%|██▊       | 147/532 [02:36<05:53,  1.09it/s]Loading train:  28%|██▊       | 148/532 [02:37<05:44,  1.11it/s]Loading train:  28%|██▊       | 149/532 [02:38<05:57,  1.07it/s]Loading train:  28%|██▊       | 150/532 [02:39<05:57,  1.07it/s]Loading train:  28%|██▊       | 151/532 [02:40<06:00,  1.06it/s]Loading train:  29%|██▊       | 152/532 [02:41<05:59,  1.06it/s]Loading train:  29%|██▉       | 153/532 [02:42<06:04,  1.04it/s]Loading train:  29%|██▉       | 154/532 [02:43<05:59,  1.05it/s]Loading train:  29%|██▉       | 155/532 [02:44<06:24,  1.02s/it]Loading train:  29%|██▉       | 156/532 [02:45<06:38,  1.06s/it]Loading train:  30%|██▉       | 157/532 [02:46<06:50,  1.10s/it]Loading train:  30%|██▉       | 158/532 [02:47<06:56,  1.11s/it]Loading train:  30%|██▉       | 159/532 [02:49<06:59,  1.13s/it]Loading train:  30%|███       | 160/532 [02:50<07:11,  1.16s/it]Loading train:  30%|███       | 161/532 [02:51<07:01,  1.14s/it]Loading train:  30%|███       | 162/532 [02:52<06:49,  1.11s/it]Loading train:  31%|███       | 163/532 [02:53<06:31,  1.06s/it]Loading train:  31%|███       | 164/532 [02:54<06:17,  1.03s/it]Loading train:  31%|███       | 165/532 [02:55<06:05,  1.00it/s]Loading train:  31%|███       | 166/532 [02:56<05:58,  1.02it/s]Loading train:  31%|███▏      | 167/532 [02:57<06:04,  1.00it/s]Loading train:  32%|███▏      | 168/532 [02:58<06:05,  1.00s/it]Loading train:  32%|███▏      | 169/532 [02:59<06:08,  1.02s/it]Loading train:  32%|███▏      | 170/532 [03:00<06:00,  1.00it/s]Loading train:  32%|███▏      | 171/532 [03:01<05:49,  1.03it/s]Loading train:  32%|███▏      | 172/532 [03:01<05:40,  1.06it/s]Loading train:  33%|███▎      | 173/532 [03:02<05:38,  1.06it/s]Loading train:  33%|███▎      | 174/532 [03:03<05:29,  1.09it/s]Loading train:  33%|███▎      | 175/532 [03:04<05:23,  1.10it/s]Loading train:  33%|███▎      | 176/532 [03:05<05:23,  1.10it/s]Loading train:  33%|███▎      | 177/532 [03:06<05:20,  1.11it/s]Loading train:  33%|███▎      | 178/532 [03:07<05:14,  1.12it/s]Loading train:  34%|███▎      | 179/532 [03:08<05:22,  1.09it/s]Loading train:  34%|███▍      | 180/532 [03:09<05:20,  1.10it/s]Loading train:  34%|███▍      | 181/532 [03:10<05:14,  1.12it/s]Loading train:  34%|███▍      | 182/532 [03:10<05:05,  1.15it/s]Loading train:  34%|███▍      | 183/532 [03:11<05:02,  1.15it/s]Loading train:  35%|███▍      | 184/532 [03:12<04:57,  1.17it/s]Loading train:  35%|███▍      | 185/532 [03:13<05:08,  1.12it/s]Loading train:  35%|███▍      | 186/532 [03:14<05:09,  1.12it/s]Loading train:  35%|███▌      | 187/532 [03:15<05:02,  1.14it/s]Loading train:  35%|███▌      | 188/532 [03:16<04:57,  1.16it/s]Loading train:  36%|███▌      | 189/532 [03:16<04:51,  1.18it/s]Loading train:  36%|███▌      | 190/532 [03:17<04:51,  1.17it/s]Loading train:  36%|███▌      | 191/532 [03:19<05:35,  1.02it/s]Loading train:  36%|███▌      | 192/532 [03:20<06:13,  1.10s/it]Loading train:  36%|███▋      | 193/532 [03:21<06:26,  1.14s/it]Loading train:  36%|███▋      | 194/532 [03:22<06:28,  1.15s/it]Loading train:  37%|███▋      | 195/532 [03:24<06:31,  1.16s/it]Loading train:  37%|███▋      | 196/532 [03:25<06:40,  1.19s/it]Loading train:  37%|███▋      | 197/532 [03:26<06:31,  1.17s/it]Loading train:  37%|███▋      | 198/532 [03:27<06:24,  1.15s/it]Loading train:  37%|███▋      | 199/532 [03:28<06:19,  1.14s/it]Loading train:  38%|███▊      | 200/532 [03:29<06:10,  1.12s/it]Loading train:  38%|███▊      | 201/532 [03:30<06:08,  1.11s/it]Loading train:  38%|███▊      | 202/532 [03:31<06:03,  1.10s/it]Loading train:  38%|███▊      | 203/532 [03:33<06:10,  1.13s/it]Loading train:  38%|███▊      | 204/532 [03:33<05:47,  1.06s/it]Loading train:  39%|███▊      | 205/532 [03:34<05:31,  1.01s/it]Loading train:  39%|███▊      | 206/532 [03:35<05:30,  1.01s/it]Loading train:  39%|███▉      | 207/532 [03:36<05:24,  1.00it/s]Loading train:  39%|███▉      | 208/532 [03:37<05:21,  1.01it/s]Loading train:  39%|███▉      | 209/532 [03:38<05:13,  1.03it/s]Loading train:  39%|███▉      | 210/532 [03:39<05:05,  1.05it/s]Loading train:  40%|███▉      | 211/532 [03:40<05:02,  1.06it/s]Loading train:  40%|███▉      | 212/532 [03:41<04:50,  1.10it/s]Loading train:  40%|████      | 213/532 [03:42<04:47,  1.11it/s]Loading train:  40%|████      | 214/532 [03:43<04:44,  1.12it/s]Loading train:  40%|████      | 215/532 [03:44<05:04,  1.04it/s]Loading train:  41%|████      | 216/532 [03:45<05:16,  1.00s/it]Loading train:  41%|████      | 217/532 [03:46<05:29,  1.05s/it]Loading train:  41%|████      | 218/532 [03:47<05:36,  1.07s/it]Loading train:  41%|████      | 219/532 [03:48<05:41,  1.09s/it]Loading train:  41%|████▏     | 220/532 [03:49<05:41,  1.09s/it]Loading train:  42%|████▏     | 221/532 [03:50<05:23,  1.04s/it]Loading train:  42%|████▏     | 222/532 [03:51<05:18,  1.03s/it]Loading train:  42%|████▏     | 223/532 [03:52<05:04,  1.02it/s]Loading train:  42%|████▏     | 224/532 [03:53<04:57,  1.03it/s]Loading train:  42%|████▏     | 225/532 [03:54<04:49,  1.06it/s]Loading train:  42%|████▏     | 226/532 [03:55<04:42,  1.08it/s]Loading train:  43%|████▎     | 227/532 [03:56<04:32,  1.12it/s]Loading train:  43%|████▎     | 228/532 [03:57<04:29,  1.13it/s]Loading train:  43%|████▎     | 229/532 [03:57<04:27,  1.13it/s]Loading train:  43%|████▎     | 230/532 [03:58<04:24,  1.14it/s]Loading train:  43%|████▎     | 231/532 [03:59<04:22,  1.14it/s]Loading train:  44%|████▎     | 232/532 [04:00<04:25,  1.13it/s]Loading train:  44%|████▍     | 233/532 [04:01<04:32,  1.10it/s]Loading train:  44%|████▍     | 234/532 [04:02<04:38,  1.07it/s]Loading train:  44%|████▍     | 235/532 [04:03<04:36,  1.07it/s]Loading train:  44%|████▍     | 236/532 [04:04<04:32,  1.09it/s]Loading train:  45%|████▍     | 237/532 [04:05<04:32,  1.08it/s]Loading train:  45%|████▍     | 238/532 [04:06<04:26,  1.10it/s]Loading train:  45%|████▍     | 239/532 [04:07<04:40,  1.04it/s]Loading train:  45%|████▌     | 240/532 [04:08<04:42,  1.03it/s]Loading train:  45%|████▌     | 241/532 [04:09<04:39,  1.04it/s]Loading train:  45%|████▌     | 242/532 [04:10<04:48,  1.01it/s]Loading train:  46%|████▌     | 243/532 [04:11<04:50,  1.00s/it]Loading train:  46%|████▌     | 244/532 [04:12<04:48,  1.00s/it]Loading train:  46%|████▌     | 245/532 [04:13<04:37,  1.03it/s]Loading train:  46%|████▌     | 246/532 [04:14<04:31,  1.05it/s]Loading train:  46%|████▋     | 247/532 [04:14<04:20,  1.10it/s]Loading train:  47%|████▋     | 248/532 [04:15<04:11,  1.13it/s]Loading train:  47%|████▋     | 249/532 [04:16<04:04,  1.16it/s]Loading train:  47%|████▋     | 250/532 [04:17<04:00,  1.17it/s]Loading train:  47%|████▋     | 251/532 [04:18<04:00,  1.17it/s]Loading train:  47%|████▋     | 252/532 [04:19<03:58,  1.17it/s]Loading train:  48%|████▊     | 253/532 [04:20<04:05,  1.14it/s]Loading train:  48%|████▊     | 254/532 [04:20<04:06,  1.13it/s]Loading train:  48%|████▊     | 255/532 [04:21<04:08,  1.11it/s]Loading train:  48%|████▊     | 256/532 [04:22<04:06,  1.12it/s]Loading train:  48%|████▊     | 257/532 [04:23<04:28,  1.02it/s]Loading train:  48%|████▊     | 258/532 [04:25<04:38,  1.02s/it]Loading train:  49%|████▊     | 259/532 [04:26<04:43,  1.04s/it]Loading train:  49%|████▉     | 260/532 [04:27<04:44,  1.05s/it]Loading train:  49%|████▉     | 261/532 [04:28<04:52,  1.08s/it]Loading train:  49%|████▉     | 262/532 [04:29<04:50,  1.08s/it]Loading train:  49%|████▉     | 263/532 [04:30<04:33,  1.02s/it]Loading train:  50%|████▉     | 264/532 [04:31<04:12,  1.06it/s]Loading train:  50%|████▉     | 265/532 [04:31<04:07,  1.08it/s]Loading train:  50%|█████     | 266/532 [04:32<04:00,  1.11it/s]Loading train:  50%|█████     | 267/532 [04:33<03:48,  1.16it/s]Loading train:  50%|█████     | 268/532 [04:34<03:49,  1.15it/s]Loading train:  51%|█████     | 269/532 [04:35<03:58,  1.10it/s]Loading train:  51%|█████     | 270/532 [04:36<03:58,  1.10it/s]Loading train:  51%|█████     | 271/532 [04:37<03:58,  1.09it/s]Loading train:  51%|█████     | 272/532 [04:38<04:02,  1.07it/s]Loading train:  51%|█████▏    | 273/532 [04:39<04:01,  1.07it/s]Loading train:  52%|█████▏    | 274/532 [04:40<04:01,  1.07it/s]Loading train:  52%|█████▏    | 275/532 [04:41<04:19,  1.01s/it]Loading train:  52%|█████▏    | 276/532 [04:42<04:32,  1.07s/it]Loading train:  52%|█████▏    | 277/532 [04:43<04:45,  1.12s/it]Loading train:  52%|█████▏    | 278/532 [04:44<04:51,  1.15s/it]Loading train:  52%|█████▏    | 279/532 [04:46<04:56,  1.17s/it]Loading train:  53%|█████▎    | 280/532 [04:47<04:53,  1.16s/it]Loading train:  53%|█████▎    | 281/532 [04:48<04:52,  1.17s/it]Loading train:  53%|█████▎    | 282/532 [04:49<04:46,  1.15s/it]Loading train:  53%|█████▎    | 283/532 [04:50<04:40,  1.13s/it]Loading train:  53%|█████▎    | 284/532 [04:51<04:35,  1.11s/it]Loading train:  54%|█████▎    | 285/532 [04:52<04:38,  1.13s/it]Loading train:  54%|█████▍    | 286/532 [04:54<04:36,  1.12s/it]Loading train:  54%|█████▍    | 287/532 [04:55<04:26,  1.09s/it]Loading train:  54%|█████▍    | 288/532 [04:55<04:13,  1.04s/it]Loading train:  54%|█████▍    | 289/532 [04:56<04:03,  1.00s/it]Loading train:  55%|█████▍    | 290/532 [04:57<03:59,  1.01it/s]Loading train:  55%|█████▍    | 291/532 [04:58<03:56,  1.02it/s]Loading train:  55%|█████▍    | 292/532 [04:59<03:48,  1.05it/s]Loading train:  55%|█████▌    | 293/532 [05:00<03:56,  1.01it/s]Loading train:  55%|█████▌    | 294/532 [05:01<03:52,  1.03it/s]Loading train:  55%|█████▌    | 295/532 [05:02<03:51,  1.03it/s]Loading train:  56%|█████▌    | 296/532 [05:03<03:49,  1.03it/s]Loading train:  56%|█████▌    | 297/532 [05:04<03:53,  1.00it/s]Loading train:  56%|█████▌    | 298/532 [05:05<03:51,  1.01it/s]Loading train:  56%|█████▌    | 299/532 [05:06<03:43,  1.04it/s]Loading train:  56%|█████▋    | 300/532 [05:07<03:34,  1.08it/s]Loading train:  57%|█████▋    | 301/532 [05:08<03:32,  1.09it/s]Loading train:  57%|█████▋    | 302/532 [05:09<03:24,  1.12it/s]Loading train:  57%|█████▋    | 303/532 [05:09<03:20,  1.14it/s]Loading train:  57%|█████▋    | 304/532 [05:10<03:21,  1.13it/s]Loading train:  57%|█████▋    | 305/532 [05:12<03:38,  1.04it/s]Loading train:  58%|█████▊    | 306/532 [05:13<03:56,  1.04s/it]Loading train:  58%|█████▊    | 307/532 [05:14<04:06,  1.10s/it]Loading train:  58%|█████▊    | 308/532 [05:15<04:12,  1.13s/it]Loading train:  58%|█████▊    | 309/532 [05:16<04:21,  1.17s/it]Loading train:  58%|█████▊    | 310/532 [05:18<04:22,  1.18s/it]Loading train:  58%|█████▊    | 311/532 [05:19<04:49,  1.31s/it]Loading train:  59%|█████▊    | 312/532 [05:21<04:58,  1.36s/it]Loading train:  59%|█████▉    | 313/532 [05:22<05:13,  1.43s/it]Loading train:  59%|█████▉    | 314/532 [05:24<05:17,  1.46s/it]Loading train:  59%|█████▉    | 315/532 [05:25<05:13,  1.45s/it]Loading train:  59%|█████▉    | 316/532 [05:27<05:12,  1.45s/it]Loading train:  60%|█████▉    | 317/532 [05:28<04:44,  1.32s/it]Loading train:  60%|█████▉    | 318/532 [05:29<04:17,  1.20s/it]Loading train:  60%|█████▉    | 319/532 [05:30<03:57,  1.11s/it]Loading train:  60%|██████    | 320/532 [05:31<03:43,  1.06s/it]Loading train:  60%|██████    | 321/532 [05:31<03:35,  1.02s/it]Loading train:  61%|██████    | 322/532 [05:32<03:23,  1.03it/s]Loading train:  61%|██████    | 323/532 [05:34<03:39,  1.05s/it]Loading train:  61%|██████    | 324/532 [05:35<03:46,  1.09s/it]Loading train:  61%|██████    | 325/532 [05:36<03:59,  1.16s/it]Loading train:  61%|██████▏   | 326/532 [05:37<03:58,  1.16s/it]Loading train:  61%|██████▏   | 327/532 [05:38<04:02,  1.18s/it]Loading train:  62%|██████▏   | 328/532 [05:40<04:04,  1.20s/it]Loading train:  62%|██████▏   | 329/532 [05:41<04:03,  1.20s/it]Loading train:  62%|██████▏   | 330/532 [05:42<03:50,  1.14s/it]Loading train:  62%|██████▏   | 331/532 [05:43<03:37,  1.08s/it]Loading train:  62%|██████▏   | 332/532 [05:44<03:26,  1.03s/it]Loading train:  63%|██████▎   | 333/532 [05:45<03:20,  1.01s/it]Loading train:  63%|██████▎   | 334/532 [05:46<03:17,  1.00it/s]Loading train:  63%|██████▎   | 335/532 [05:47<03:25,  1.04s/it]Loading train:  63%|██████▎   | 336/532 [05:48<03:30,  1.08s/it]Loading train:  63%|██████▎   | 337/532 [05:49<03:26,  1.06s/it]Loading train:  64%|██████▎   | 338/532 [05:50<03:24,  1.05s/it]Loading train:  64%|██████▎   | 339/532 [05:51<03:24,  1.06s/it]Loading train:  64%|██████▍   | 340/532 [05:52<03:18,  1.03s/it]Loading train:  64%|██████▍   | 341/532 [05:53<03:09,  1.01it/s]Loading train:  64%|██████▍   | 342/532 [05:54<03:01,  1.05it/s]Loading train:  64%|██████▍   | 343/532 [05:55<02:53,  1.09it/s]Loading train:  65%|██████▍   | 344/532 [05:56<02:50,  1.10it/s]Loading train:  65%|██████▍   | 345/532 [05:57<02:51,  1.09it/s]Loading train:  65%|██████▌   | 346/532 [05:57<02:50,  1.09it/s]Loading train:  65%|██████▌   | 347/532 [05:58<02:58,  1.04it/s]Loading train:  65%|██████▌   | 348/532 [05:59<02:53,  1.06it/s]Loading train:  66%|██████▌   | 349/532 [06:00<02:52,  1.06it/s]Loading train:  66%|██████▌   | 350/532 [06:01<02:49,  1.07it/s]Loading train:  66%|██████▌   | 351/532 [06:02<02:51,  1.05it/s]Loading train:  66%|██████▌   | 352/532 [06:03<02:50,  1.05it/s]Loading train:  66%|██████▋   | 353/532 [06:04<02:56,  1.01it/s]Loading train:  67%|██████▋   | 354/532 [06:05<02:55,  1.01it/s]Loading train:  67%|██████▋   | 355/532 [06:06<02:57,  1.00s/it]Loading train:  67%|██████▋   | 356/532 [06:07<02:55,  1.00it/s]Loading train:  67%|██████▋   | 357/532 [06:08<02:53,  1.01it/s]Loading train:  67%|██████▋   | 358/532 [06:09<02:48,  1.03it/s]Loading train:  67%|██████▋   | 359/532 [06:10<02:45,  1.04it/s]Loading train:  68%|██████▊   | 360/532 [06:11<02:38,  1.08it/s]Loading train:  68%|██████▊   | 361/532 [06:12<02:35,  1.10it/s]Loading train:  68%|██████▊   | 362/532 [06:13<02:28,  1.14it/s]Loading train:  68%|██████▊   | 363/532 [06:13<02:23,  1.18it/s]Loading train:  68%|██████▊   | 364/532 [06:14<02:25,  1.16it/s]Loading train:  69%|██████▊   | 365/532 [06:15<02:30,  1.11it/s]Loading train:  69%|██████▉   | 366/532 [06:16<02:31,  1.10it/s]Loading train:  69%|██████▉   | 367/532 [06:17<02:30,  1.10it/s]Loading train:  69%|██████▉   | 368/532 [06:18<02:29,  1.09it/s]Loading train:  69%|██████▉   | 369/532 [06:19<02:29,  1.09it/s]Loading train:  70%|██████▉   | 370/532 [06:20<02:24,  1.12it/s]Loading train:  70%|██████▉   | 371/532 [06:21<02:38,  1.02it/s]Loading train:  70%|██████▉   | 372/532 [06:22<02:46,  1.04s/it]Loading train:  70%|███████   | 373/532 [06:23<02:53,  1.09s/it]Loading train:  70%|███████   | 374/532 [06:25<02:57,  1.12s/it]Loading train:  70%|███████   | 375/532 [06:26<02:58,  1.14s/it]Loading train:  71%|███████   | 376/532 [06:27<03:01,  1.16s/it]Loading train:  71%|███████   | 377/532 [06:28<02:52,  1.11s/it]Loading train:  71%|███████   | 378/532 [06:29<02:44,  1.07s/it]Loading train:  71%|███████   | 379/532 [06:30<02:36,  1.02s/it]Loading train:  71%|███████▏  | 380/532 [06:31<02:31,  1.00it/s]Loading train:  72%|███████▏  | 381/532 [06:32<02:27,  1.02it/s]Loading train:  72%|███████▏  | 382/532 [06:33<02:25,  1.03it/s]Loading train:  72%|███████▏  | 383/532 [06:34<02:25,  1.02it/s]Loading train:  72%|███████▏  | 384/532 [06:35<02:22,  1.04it/s]Loading train:  72%|███████▏  | 385/532 [06:35<02:18,  1.06it/s]Loading train:  73%|███████▎  | 386/532 [06:36<02:19,  1.05it/s]Loading train:  73%|███████▎  | 387/532 [06:37<02:19,  1.04it/s]Loading train:  73%|███████▎  | 388/532 [06:38<02:19,  1.03it/s]Loading train:  73%|███████▎  | 389/532 [06:40<02:25,  1.02s/it]Loading train:  73%|███████▎  | 390/532 [06:41<02:25,  1.02s/it]Loading train:  73%|███████▎  | 391/532 [06:42<02:29,  1.06s/it]Loading train:  74%|███████▎  | 392/532 [06:43<02:28,  1.06s/it]Loading train:  74%|███████▍  | 393/532 [06:44<02:26,  1.05s/it]Loading train:  74%|███████▍  | 394/532 [06:45<02:26,  1.06s/it]Loading train:  74%|███████▍  | 395/532 [06:46<02:25,  1.06s/it]Loading train:  74%|███████▍  | 396/532 [06:47<02:24,  1.06s/it]Loading train:  75%|███████▍  | 397/532 [06:48<02:23,  1.07s/it]Loading train:  75%|███████▍  | 398/532 [06:49<02:21,  1.06s/it]Loading train:  75%|███████▌  | 399/532 [06:50<02:18,  1.04s/it]Loading train:  75%|███████▌  | 400/532 [06:51<02:16,  1.04s/it]Loading train:  75%|███████▌  | 401/532 [06:52<02:21,  1.08s/it]Loading train:  76%|███████▌  | 402/532 [06:53<02:20,  1.08s/it]Loading train:  76%|███████▌  | 403/532 [06:54<02:17,  1.07s/it]Loading train:  76%|███████▌  | 404/532 [06:56<02:17,  1.07s/it]Loading train:  76%|███████▌  | 405/532 [06:57<02:18,  1.09s/it]Loading train:  76%|███████▋  | 406/532 [06:58<02:17,  1.09s/it]Loading train:  77%|███████▋  | 407/532 [06:59<02:15,  1.09s/it]Loading train:  77%|███████▋  | 408/532 [07:00<02:13,  1.07s/it]Loading train:  77%|███████▋  | 409/532 [07:01<02:09,  1.05s/it]Loading train:  77%|███████▋  | 410/532 [07:02<02:06,  1.04s/it]Loading train:  77%|███████▋  | 411/532 [07:03<02:04,  1.03s/it]Loading train:  77%|███████▋  | 412/532 [07:04<02:00,  1.01s/it]Loading train:  78%|███████▊  | 413/532 [07:05<01:59,  1.01s/it]Loading train:  78%|███████▊  | 414/532 [07:06<01:58,  1.00s/it]Loading train:  78%|███████▊  | 415/532 [07:07<01:58,  1.01s/it]Loading train:  78%|███████▊  | 416/532 [07:08<01:55,  1.01it/s]Loading train:  78%|███████▊  | 417/532 [07:09<01:53,  1.02it/s]Loading train:  79%|███████▊  | 418/532 [07:10<01:51,  1.02it/s]Loading train:  79%|███████▉  | 419/532 [07:11<01:53,  1.00s/it]Loading train:  79%|███████▉  | 420/532 [07:12<01:52,  1.00s/it]Loading train:  79%|███████▉  | 421/532 [07:13<01:52,  1.01s/it]Loading train:  79%|███████▉  | 422/532 [07:14<01:51,  1.02s/it]Loading train:  80%|███████▉  | 423/532 [07:15<01:53,  1.04s/it]Loading train:  80%|███████▉  | 424/532 [07:16<01:55,  1.07s/it]Loading train:  80%|███████▉  | 425/532 [07:17<01:56,  1.09s/it]Loading train:  80%|████████  | 426/532 [07:18<01:54,  1.08s/it]Loading train:  80%|████████  | 427/532 [07:19<01:53,  1.08s/it]Loading train:  80%|████████  | 428/532 [07:20<01:51,  1.07s/it]Loading train:  81%|████████  | 429/532 [07:21<01:48,  1.05s/it]Loading train:  81%|████████  | 430/532 [07:23<01:47,  1.06s/it]Loading train:  81%|████████  | 431/532 [07:24<01:49,  1.08s/it]Loading train:  81%|████████  | 432/532 [07:25<01:48,  1.09s/it]Loading train:  81%|████████▏ | 433/532 [07:26<01:49,  1.11s/it]Loading train:  82%|████████▏ | 434/532 [07:27<01:48,  1.11s/it]Loading train:  82%|████████▏ | 435/532 [07:28<01:46,  1.09s/it]Loading train:  82%|████████▏ | 436/532 [07:29<01:44,  1.08s/it]Loading train:  82%|████████▏ | 437/532 [07:30<01:36,  1.02s/it]Loading train:  82%|████████▏ | 438/532 [07:31<01:31,  1.03it/s]Loading train:  83%|████████▎ | 439/532 [07:32<01:26,  1.07it/s]Loading train:  83%|████████▎ | 440/532 [07:33<01:25,  1.08it/s]Loading train:  83%|████████▎ | 441/532 [07:33<01:21,  1.12it/s]Loading train:  83%|████████▎ | 442/532 [07:34<01:21,  1.11it/s]Loading train:  83%|████████▎ | 443/532 [07:35<01:20,  1.11it/s]Loading train:  83%|████████▎ | 444/532 [07:36<01:20,  1.10it/s]Loading train:  84%|████████▎ | 445/532 [07:37<01:21,  1.06it/s]Loading train:  84%|████████▍ | 446/532 [07:38<01:19,  1.09it/s]Loading train:  84%|████████▍ | 447/532 [07:39<01:16,  1.11it/s]Loading train:  84%|████████▍ | 448/532 [07:40<01:14,  1.13it/s]Loading train:  84%|████████▍ | 449/532 [07:41<01:17,  1.07it/s]Loading train:  85%|████████▍ | 450/532 [07:42<01:16,  1.07it/s]Loading train:  85%|████████▍ | 451/532 [07:43<01:16,  1.06it/s]Loading train:  85%|████████▍ | 452/532 [07:44<01:13,  1.09it/s]Loading train:  85%|████████▌ | 453/532 [07:44<01:11,  1.11it/s]Loading train:  85%|████████▌ | 454/532 [07:45<01:11,  1.10it/s]Loading train:  86%|████████▌ | 455/532 [07:46<01:13,  1.04it/s]Loading train:  86%|████████▌ | 456/532 [07:47<01:14,  1.03it/s]Loading train:  86%|████████▌ | 457/532 [07:48<01:13,  1.03it/s]Loading train:  86%|████████▌ | 458/532 [07:50<01:13,  1.01it/s]Loading train:  86%|████████▋ | 459/532 [07:51<01:13,  1.00s/it]Loading train:  86%|████████▋ | 460/532 [07:52<01:12,  1.01s/it]Loading train:  87%|████████▋ | 461/532 [07:53<01:15,  1.06s/it]Loading train:  87%|████████▋ | 462/532 [07:54<01:18,  1.12s/it]Loading train:  87%|████████▋ | 463/532 [07:55<01:18,  1.14s/it]Loading train:  87%|████████▋ | 464/532 [07:56<01:17,  1.14s/it]Loading train:  87%|████████▋ | 465/532 [07:57<01:16,  1.15s/it]Loading train:  88%|████████▊ | 466/532 [07:59<01:15,  1.15s/it]Loading train:  88%|████████▊ | 467/532 [08:00<01:11,  1.11s/it]Loading train:  88%|████████▊ | 468/532 [08:01<01:07,  1.05s/it]Loading train:  88%|████████▊ | 469/532 [08:01<01:03,  1.01s/it]Loading train:  88%|████████▊ | 470/532 [08:02<00:59,  1.04it/s]Loading train:  89%|████████▊ | 471/532 [08:03<00:57,  1.06it/s]Loading train:  89%|████████▊ | 472/532 [08:04<00:57,  1.05it/s]Loading train:  89%|████████▉ | 473/532 [08:05<00:58,  1.00it/s]Loading train:  89%|████████▉ | 474/532 [08:06<00:59,  1.03s/it]Loading train:  89%|████████▉ | 475/532 [08:07<00:59,  1.04s/it]Loading train:  89%|████████▉ | 476/532 [08:09<00:58,  1.05s/it]Loading train:  90%|████████▉ | 477/532 [08:10<00:57,  1.04s/it]Loading train:  90%|████████▉ | 478/532 [08:11<00:56,  1.04s/it]Loading train:  90%|█████████ | 479/532 [08:12<00:53,  1.01s/it]Loading train:  90%|█████████ | 480/532 [08:12<00:50,  1.03it/s]Loading train:  90%|█████████ | 481/532 [08:13<00:49,  1.04it/s]Loading train:  91%|█████████ | 482/532 [08:14<00:46,  1.07it/s]Loading train:  91%|█████████ | 483/532 [08:15<00:45,  1.09it/s]Loading train:  91%|█████████ | 484/532 [08:16<00:44,  1.08it/s]Loading train:  91%|█████████ | 485/532 [08:17<00:46,  1.02it/s]Loading train:  91%|█████████▏| 486/532 [08:18<00:46,  1.02s/it]Loading train:  92%|█████████▏| 487/532 [08:19<00:46,  1.04s/it]Loading train:  92%|█████████▏| 488/532 [08:21<00:46,  1.07s/it]Loading train:  92%|█████████▏| 489/532 [08:22<00:45,  1.06s/it]Loading train:  92%|█████████▏| 490/532 [08:23<00:45,  1.09s/it]Loading train:  92%|█████████▏| 491/532 [08:24<00:43,  1.05s/it]Loading train:  92%|█████████▏| 492/532 [08:25<00:40,  1.01s/it]Loading train:  93%|█████████▎| 493/532 [08:26<00:38,  1.01it/s]Loading train:  93%|█████████▎| 494/532 [08:26<00:37,  1.02it/s]Loading train:  93%|█████████▎| 495/532 [08:27<00:35,  1.03it/s]Loading train:  93%|█████████▎| 496/532 [08:28<00:34,  1.03it/s]Loading train:  93%|█████████▎| 497/532 [08:29<00:34,  1.01it/s]Loading train:  94%|█████████▎| 498/532 [08:30<00:34,  1.00s/it]Loading train:  94%|█████████▍| 499/532 [08:31<00:33,  1.01s/it]Loading train:  94%|█████████▍| 500/532 [08:32<00:32,  1.00s/it]Loading train:  94%|█████████▍| 501/532 [08:34<00:31,  1.02s/it]Loading train:  94%|█████████▍| 502/532 [08:35<00:30,  1.02s/it]Loading train:  95%|█████████▍| 503/532 [08:36<00:29,  1.03s/it]Loading train:  95%|█████████▍| 504/532 [08:37<00:28,  1.00s/it]Loading train:  95%|█████████▍| 505/532 [08:38<00:26,  1.01it/s]Loading train:  95%|█████████▌| 506/532 [08:38<00:25,  1.01it/s]Loading train:  95%|█████████▌| 507/532 [08:39<00:24,  1.04it/s]Loading train:  95%|█████████▌| 508/532 [08:40<00:23,  1.02it/s]Loading train:  96%|█████████▌| 509/532 [08:42<00:23,  1.02s/it]Loading train:  96%|█████████▌| 510/532 [08:43<00:23,  1.05s/it]Loading train:  96%|█████████▌| 511/532 [08:44<00:22,  1.06s/it]Loading train:  96%|█████████▌| 512/532 [08:45<00:21,  1.07s/it]Loading train:  96%|█████████▋| 513/532 [08:46<00:21,  1.12s/it]Loading train:  97%|█████████▋| 514/532 [08:47<00:20,  1.12s/it]Loading train:  97%|█████████▋| 515/532 [08:48<00:18,  1.07s/it]Loading train:  97%|█████████▋| 516/532 [08:49<00:16,  1.02s/it]Loading train:  97%|█████████▋| 517/532 [08:50<00:14,  1.02it/s]Loading train:  97%|█████████▋| 518/532 [08:51<00:13,  1.03it/s]Loading train:  98%|█████████▊| 519/532 [08:52<00:12,  1.06it/s]Loading train:  98%|█████████▊| 520/532 [08:53<00:11,  1.07it/s]Loading train:  98%|█████████▊| 521/532 [08:54<00:10,  1.01it/s]Loading train:  98%|█████████▊| 522/532 [08:55<00:10,  1.01s/it]Loading train:  98%|█████████▊| 523/532 [08:56<00:09,  1.02s/it]Loading train:  98%|█████████▊| 524/532 [08:57<00:08,  1.02s/it]Loading train:  99%|█████████▊| 525/532 [08:58<00:07,  1.02s/it]Loading train:  99%|█████████▉| 526/532 [08:59<00:06,  1.03s/it]Loading train:  99%|█████████▉| 527/532 [09:00<00:04,  1.01it/s]Loading train:  99%|█████████▉| 528/532 [09:01<00:03,  1.03it/s]Loading train:  99%|█████████▉| 529/532 [09:02<00:02,  1.06it/s]Loading train: 100%|█████████▉| 530/532 [09:03<00:01,  1.09it/s]Loading train: 100%|█████████▉| 531/532 [09:03<00:00,  1.10it/s]Loading train: 100%|██████████| 532/532 [09:04<00:00,  1.12it/s]
concatenating: train:   0%|          | 0/532 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 13/532 [00:00<00:04, 123.39it/s]concatenating: train:   6%|▌         | 31/532 [00:00<00:03, 135.85it/s]concatenating: train:  10%|█         | 54/532 [00:00<00:03, 154.16it/s]concatenating: train:  13%|█▎        | 71/532 [00:00<00:02, 157.16it/s]concatenating: train:  17%|█▋        | 88/532 [00:00<00:02, 159.81it/s]concatenating: train:  21%|██        | 112/532 [00:00<00:02, 176.84it/s]concatenating: train:  26%|██▌       | 136/532 [00:00<00:02, 190.40it/s]concatenating: train:  29%|██▉       | 155/532 [00:00<00:02, 185.94it/s]concatenating: train:  34%|███▍      | 180/532 [00:00<00:01, 200.92it/s]concatenating: train:  39%|███▊      | 205/532 [00:01<00:01, 212.81it/s]concatenating: train:  43%|████▎     | 227/532 [00:01<00:01, 203.33it/s]concatenating: train:  47%|████▋     | 251/532 [00:01<00:01, 211.03it/s]concatenating: train:  51%|█████▏    | 273/532 [00:01<00:01, 209.95it/s]concatenating: train:  55%|█████▌    | 295/532 [00:01<00:01, 199.97it/s]concatenating: train:  60%|█████▉    | 317/532 [00:01<00:01, 205.00it/s]concatenating: train:  64%|██████▍   | 340/532 [00:01<00:00, 208.29it/s]concatenating: train:  68%|██████▊   | 361/532 [00:01<00:00, 204.80it/s]concatenating: train:  72%|███████▏  | 382/532 [00:01<00:00, 198.30it/s]concatenating: train:  76%|███████▌  | 402/532 [00:02<00:00, 193.22it/s]concatenating: train:  80%|███████▉  | 424/532 [00:02<00:00, 198.55it/s]concatenating: train:  84%|████████▍ | 446/532 [00:02<00:00, 202.89it/s]concatenating: train:  88%|████████▊ | 467/532 [00:02<00:00, 195.86it/s]concatenating: train:  92%|█████████▏| 491/532 [00:02<00:00, 206.96it/s]concatenating: train:  96%|█████████▌| 512/532 [00:02<00:00, 207.55it/s]concatenating: train: 100%|██████████| 532/532 [00:02<00:00, 203.98it/s]
Loading test:   0%|          | 0/15 [00:00<?, ?it/s]Loading test:   7%|▋         | 1/15 [00:00<00:13,  1.04it/s]Loading test:  13%|█▎        | 2/15 [00:01<00:12,  1.03it/s]Loading test:  20%|██        | 3/15 [00:03<00:12,  1.03s/it]Loading test:  27%|██▋       | 4/15 [00:04<00:11,  1.04s/it]Loading test:  33%|███▎      | 5/15 [00:05<00:10,  1.09s/it]Loading test:  40%|████      | 6/15 [00:06<00:10,  1.14s/it]Loading test:  47%|████▋     | 7/15 [00:07<00:08,  1.05s/it]Loading test:  53%|█████▎    | 8/15 [00:08<00:07,  1.11s/it]Loading test:  60%|██████    | 9/15 [00:09<00:06,  1.08s/it]Loading test:  67%|██████▋   | 10/15 [00:10<00:05,  1.02s/it]Loading test:  73%|███████▎  | 11/15 [00:11<00:03,  1.03it/s]Loading test:  80%|████████  | 12/15 [00:12<00:03,  1.02s/it]Loading test:  87%|████████▋ | 13/15 [00:13<00:02,  1.03s/it]Loading test:  93%|█████████▎| 14/15 [00:14<00:01,  1.03s/it]Loading test: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it]
concatenating: validation:   0%|          | 0/15 [00:00<?, ?it/s]concatenating: validation:  40%|████      | 6/15 [00:00<00:00, 54.50it/s]concatenating: validation: 100%|██████████| 15/15 [00:00<00:00, 84.17it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 56, 56, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 56, 56, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 56, 56, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 56, 56, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 56, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 56, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 56, 56, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 31)   0           input_1[0][0]                    
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 31)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 28, 28, 31)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 60)   16800       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 91)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 91)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 14, 14, 91)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 14, 120)  98400       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 14, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 14, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 14, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 14, 14, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 14, 14, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 14, 14, 211)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 211)  0           concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 60)   50700       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 151)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 60)   81600       concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 211)  0           concatenate_4[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-06 15:30:52.397581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 15:30:52.397706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 15:30:52.397724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 15:30:52.397733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 15:30:52.398081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14197 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 28, 28, 211)  0           concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 56, 30)   25350       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 56, 56, 61)   0           conv2d_transpose_2[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 56, 56, 30)   16500       concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 56, 56, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 56, 56, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 56, 56, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 56, 56, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 56, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 56, 56, 91)   0           concatenate_6[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 56, 56, 91)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 56, 56, 13)   1196        dropout_5[0][0]                  
==================================================================================================
Total params: 504,146
Trainable params: 502,946
Non-trainable params: 1,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.53974061e-02 2.89048015e-02 1.16758472e-01 1.00223856e-02
 3.03440156e-02 5.80063118e-03 6.84518755e-02 1.28261328e-01
 7.55818021e-02 1.22545826e-02 2.73712232e-01 1.84335085e-01
 1.75382711e-04]
Train on 33496 samples, validate on 969 samples
Epoch 1/300
 - 38s - loss: 16.2200 - acc: 0.8535 - mDice: 0.0683 - val_loss: 2.8734 - val_acc: 0.9064 - val_mDice: 0.1253

Epoch 00001: val_mDice improved from -inf to 0.12528, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 32s - loss: 2.2413 - acc: 0.9362 - mDice: 0.3271 - val_loss: 1.1759 - val_acc: 0.9654 - val_mDice: 0.5497

Epoch 00002: val_mDice improved from 0.12528 to 0.54969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 31s - loss: 1.4992 - acc: 0.9546 - mDice: 0.5140 - val_loss: 0.8856 - val_acc: 0.9729 - val_mDice: 0.6623

Epoch 00003: val_mDice improved from 0.54969 to 0.66226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 32s - loss: 1.2442 - acc: 0.9603 - mDice: 0.5878 - val_loss: 0.7930 - val_acc: 0.9770 - val_mDice: 0.6981

Epoch 00004: val_mDice improved from 0.66226 to 0.69808, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300
 - 32s - loss: 1.1019 - acc: 0.9632 - mDice: 0.6281 - val_loss: 0.7655 - val_acc: 0.9774 - val_mDice: 0.7191

Epoch 00005: val_mDice improved from 0.69808 to 0.71910, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 31s - loss: 1.0151 - acc: 0.9648 - mDice: 0.6532 - val_loss: 0.7424 - val_acc: 0.9781 - val_mDice: 0.7274

Epoch 00006: val_mDice improved from 0.71910 to 0.72739, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 32s - loss: 0.9488 - acc: 0.9662 - mDice: 0.6724 - val_loss: 0.6908 - val_acc: 0.9799 - val_mDice: 0.7405

Epoch 00007: val_mDice improved from 0.72739 to 0.74046, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300
 - 32s - loss: 0.8978 - acc: 0.9672 - mDice: 0.6873 - val_loss: 0.6930 - val_acc: 0.9777 - val_mDice: 0.7430

Epoch 00008: val_mDice improved from 0.74046 to 0.74297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 32s - loss: 0.8570 - acc: 0.9680 - mDice: 0.6989 - val_loss: 0.6742 - val_acc: 0.9801 - val_mDice: 0.7484

Epoch 00009: val_mDice improved from 0.74297 to 0.74841, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 30s - loss: 0.8228 - acc: 0.9687 - mDice: 0.7097 - val_loss: 0.6829 - val_acc: 0.9800 - val_mDice: 0.7473

Epoch 00010: val_mDice did not improve from 0.74841
Epoch 11/300
 - 32s - loss: 0.7936 - acc: 0.9693 - mDice: 0.7182 - val_loss: 0.6686 - val_acc: 0.9805 - val_mDice: 0.7527

Epoch 00011: val_mDice improved from 0.74841 to 0.75270, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300
 - 31s - loss: 0.7743 - acc: 0.9697 - mDice: 0.7236 - val_loss: 0.6754 - val_acc: 0.9809 - val_mDice: 0.7491

Epoch 00012: val_mDice did not improve from 0.75270
Epoch 13/300
 - 32s - loss: 0.7497 - acc: 0.9702 - mDice: 0.7316 - val_loss: 0.6491 - val_acc: 0.9805 - val_mDice: 0.7585

Epoch 00013: val_mDice improved from 0.75270 to 0.75846, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 14/300
 - 32s - loss: 0.7315 - acc: 0.9705 - mDice: 0.7372 - val_loss: 0.6350 - val_acc: 0.9799 - val_mDice: 0.7580

Epoch 00014: val_mDice did not improve from 0.75846
Epoch 15/300
 - 30s - loss: 0.7154 - acc: 0.9709 - mDice: 0.7421 - val_loss: 0.6478 - val_acc: 0.9809 - val_mDice: 0.7566

Epoch 00015: val_mDice did not improve from 0.75846
Epoch 16/300
 - 32s - loss: 0.7033 - acc: 0.9711 - mDice: 0.7459 - val_loss: 0.6676 - val_acc: 0.9804 - val_mDice: 0.7531

Epoch 00016: val_mDice did not improve from 0.75846
Epoch 17/300
 - 30s - loss: 0.6916 - acc: 0.9714 - mDice: 0.7494 - val_loss: 0.6449 - val_acc: 0.9809 - val_mDice: 0.7590

Epoch 00017: val_mDice improved from 0.75846 to 0.75900, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300
 - 32s - loss: 0.6768 - acc: 0.9717 - mDice: 0.7541 - val_loss: 0.6256 - val_acc: 0.9810 - val_mDice: 0.7649

Epoch 00018: val_mDice improved from 0.75900 to 0.76487, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 19/300
 - 32s - loss: 0.6642 - acc: 0.9719 - mDice: 0.7583 - val_loss: 0.6323 - val_acc: 0.9813 - val_mDice: 0.7645

Epoch 00019: val_mDice did not improve from 0.76487
Epoch 20/300
 - 32s - loss: 0.6551 - acc: 0.9721 - mDice: 0.7611 - val_loss: 0.6363 - val_acc: 0.9815 - val_mDice: 0.7616

Epoch 00020: val_mDice did not improve from 0.76487
Epoch 21/300
 - 31s - loss: 0.6460 - acc: 0.9722 - mDice: 0.7641 - val_loss: 0.6637 - val_acc: 0.9817 - val_mDice: 0.7589

Epoch 00021: val_mDice did not improve from 0.76487
Epoch 22/300
 - 32s - loss: 0.6367 - acc: 0.9725 - mDice: 0.7671 - val_loss: 0.6457 - val_acc: 0.9816 - val_mDice: 0.7588

Epoch 00022: val_mDice did not improve from 0.76487
Epoch 23/300
 - 32s - loss: 0.6290 - acc: 0.9726 - mDice: 0.7696 - val_loss: 0.6260 - val_acc: 0.9798 - val_mDice: 0.7621

Epoch 00023: val_mDice did not improve from 0.76487
Epoch 24/300
 - 32s - loss: 0.6247 - acc: 0.9726 - mDice: 0.7712 - val_loss: 0.6308 - val_acc: 0.9817 - val_mDice: 0.7657

Epoch 00024: val_mDice improved from 0.76487 to 0.76567, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 25/300
 - 31s - loss: 0.6159 - acc: 0.9728 - mDice: 0.7740 - val_loss: 0.6323 - val_acc: 0.9816 - val_mDice: 0.7690

Epoch 00025: val_mDice improved from 0.76567 to 0.76902, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 26/300
 - 32s - loss: 0.6092 - acc: 0.9729 - mDice: 0.7764 - val_loss: 0.6385 - val_acc: 0.9813 - val_mDice: 0.7650

Epoch 00026: val_mDice did not improve from 0.76902
Epoch 27/300
 - 30s - loss: 0.6050 - acc: 0.9731 - mDice: 0.7773 - val_loss: 0.6389 - val_acc: 0.9815 - val_mDice: 0.7629

Epoch 00027: val_mDice did not improve from 0.76902
Epoch 28/300
 - 31s - loss: 0.5974 - acc: 0.9732 - mDice: 0.7800 - val_loss: 0.6294 - val_acc: 0.9803 - val_mDice: 0.7718

Epoch 00028: val_mDice improved from 0.76902 to 0.77180, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 29/300
 - 31s - loss: 0.5915 - acc: 0.9733 - mDice: 0.7819 - val_loss: 0.6443 - val_acc: 0.9821 - val_mDice: 0.7626

Epoch 00029: val_mDice did not improve from 0.77180
Epoch 30/300
 - 32s - loss: 0.5860 - acc: 0.9734 - mDice: 0.7840 - val_loss: 0.6233 - val_acc: 0.9819 - val_mDice: 0.7724

Epoch 00030: val_mDice improved from 0.77180 to 0.77243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 31/300
 - 30s - loss: 0.5826 - acc: 0.9734 - mDice: 0.7849 - val_loss: 0.6376 - val_acc: 0.9825 - val_mDice: 0.7689

Epoch 00031: val_mDice did not improve from 0.77243
Epoch 32/300
 - 32s - loss: 0.5781 - acc: 0.9735 - mDice: 0.7866 - val_loss: 0.6186 - val_acc: 0.9823 - val_mDice: 0.7713

Epoch 00032: val_mDice did not improve from 0.77243
Epoch 33/300
 - 32s - loss: 0.5734 - acc: 0.9736 - mDice: 0.7882 - val_loss: 0.6246 - val_acc: 0.9823 - val_mDice: 0.7744

Epoch 00033: val_mDice improved from 0.77243 to 0.77438, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 34/300
 - 32s - loss: 0.5699 - acc: 0.9736 - mDice: 0.7893 - val_loss: 0.6465 - val_acc: 0.9820 - val_mDice: 0.7665

Epoch 00034: val_mDice did not improve from 0.77438
Epoch 35/300
 - 32s - loss: 0.5659 - acc: 0.9737 - mDice: 0.7907 - val_loss: 0.6230 - val_acc: 0.9818 - val_mDice: 0.7736

Epoch 00035: val_mDice did not improve from 0.77438
Epoch 36/300
 - 31s - loss: 0.5610 - acc: 0.9738 - mDice: 0.7925 - val_loss: 0.6212 - val_acc: 0.9820 - val_mDice: 0.7721

Epoch 00036: val_mDice did not improve from 0.77438
Epoch 37/300
 - 31s - loss: 0.5586 - acc: 0.9738 - mDice: 0.7931 - val_loss: 0.6255 - val_acc: 0.9822 - val_mDice: 0.7712

Epoch 00037: val_mDice did not improve from 0.77438
Epoch 38/300
 - 31s - loss: 0.5533 - acc: 0.9739 - mDice: 0.7948 - val_loss: 0.6201 - val_acc: 0.9818 - val_mDice: 0.7779

Epoch 00038: val_mDice improved from 0.77438 to 0.77789, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 39/300
 - 32s - loss: 0.5493 - acc: 0.9740 - mDice: 0.7966 - val_loss: 0.6525 - val_acc: 0.9816 - val_mDice: 0.7744

Epoch 00039: val_mDice did not improve from 0.77789
Epoch 40/300
 - 31s - loss: 0.5462 - acc: 0.9740 - mDice: 0.7975 - val_loss: 0.6166 - val_acc: 0.9821 - val_mDice: 0.7773

Epoch 00040: val_mDice did not improve from 0.77789
Epoch 41/300
 - 30s - loss: 0.5434 - acc: 0.9741 - mDice: 0.7985 - val_loss: 0.6253 - val_acc: 0.9830 - val_mDice: 0.7770

Epoch 00041: val_mDice did not improve from 0.77789
Epoch 42/300
 - 31s - loss: 0.5416 - acc: 0.9741 - mDice: 0.7993 - val_loss: 0.6251 - val_acc: 0.9821 - val_mDice: 0.7733

Epoch 00042: val_mDice did not improve from 0.77789
Epoch 43/300
 - 30s - loss: 0.5380 - acc: 0.9742 - mDice: 0.8005 - val_loss: 0.6338 - val_acc: 0.9824 - val_mDice: 0.7713

Epoch 00043: val_mDice did not improve from 0.77789
Epoch 44/300
 - 30s - loss: 0.5357 - acc: 0.9742 - mDice: 0.8012 - val_loss: 0.6276 - val_acc: 0.9820 - val_mDice: 0.7748

Epoch 00044: val_mDice did not improve from 0.77789
Epoch 45/300
 - 32s - loss: 0.5313 - acc: 0.9743 - mDice: 0.8027 - val_loss: 0.6259 - val_acc: 0.9818 - val_mDice: 0.7788

Epoch 00045: val_mDice improved from 0.77789 to 0.77884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 46/300
 - 30s - loss: 0.5309 - acc: 0.9743 - mDice: 0.8028 - val_loss: 0.6186 - val_acc: 0.9821 - val_mDice: 0.7776

Epoch 00046: val_mDice did not improve from 0.77884
Epoch 47/300
 - 30s - loss: 0.5275 - acc: 0.9743 - mDice: 0.8039 - val_loss: 0.6360 - val_acc: 0.9823 - val_mDice: 0.7800

Epoch 00047: val_mDice improved from 0.77884 to 0.78004, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 48/300
 - 31s - loss: 0.5271 - acc: 0.9743 - mDice: 0.8041 - val_loss: 0.6360 - val_acc: 0.9812 - val_mDice: 0.7769

Epoch 00048: val_mDice did not improve from 0.78004
Epoch 49/300
 - 30s - loss: 0.5235 - acc: 0.9744 - mDice: 0.8053 - val_loss: 0.6315 - val_acc: 0.9824 - val_mDice: 0.7807

Epoch 00049: val_mDice improved from 0.78004 to 0.78072, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 50/300
 - 31s - loss: 0.5213 - acc: 0.9744 - mDice: 0.8063 - val_loss: 0.6344 - val_acc: 0.9822 - val_mDice: 0.7706

Epoch 00050: val_mDice did not improve from 0.78072
Epoch 51/300
 - 31s - loss: 0.5184 - acc: 0.9745 - mDice: 0.8071 - val_loss: 0.6387 - val_acc: 0.9817 - val_mDice: 0.7784

Epoch 00051: val_mDice did not improve from 0.78072
Epoch 52/300
 - 30s - loss: 0.5167 - acc: 0.9745 - mDice: 0.8079 - val_loss: 0.6316 - val_acc: 0.9820 - val_mDice: 0.7838

Epoch 00052: val_mDice improved from 0.78072 to 0.78380, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 53/300
 - 31s - loss: 0.5160 - acc: 0.9745 - mDice: 0.8080 - val_loss: 0.6468 - val_acc: 0.9814 - val_mDice: 0.7780

Epoch 00053: val_mDice did not improve from 0.78380
Epoch 54/300
 - 30s - loss: 0.5124 - acc: 0.9746 - mDice: 0.8094 - val_loss: 0.6389 - val_acc: 0.9821 - val_mDice: 0.7796

Epoch 00054: val_mDice did not improve from 0.78380
Epoch 55/300
 - 31s - loss: 0.5106 - acc: 0.9746 - mDice: 0.8100 - val_loss: 0.6400 - val_acc: 0.9823 - val_mDice: 0.7822

Epoch 00055: val_mDice did not improve from 0.78380
Epoch 56/300
 - 30s - loss: 0.5097 - acc: 0.9746 - mDice: 0.8103 - val_loss: 0.6356 - val_acc: 0.9818 - val_mDice: 0.7814

Epoch 00056: val_mDice did not improve from 0.78380
Epoch 57/300
 - 30s - loss: 0.5081 - acc: 0.9746 - mDice: 0.8110 - val_loss: 0.6564 - val_acc: 0.9819 - val_mDice: 0.7815

Epoch 00057: val_mDice did not improve from 0.78380
Epoch 58/300
 - 32s - loss: 0.5075 - acc: 0.9747 - mDice: 0.8111 - val_loss: 0.6586 - val_acc: 0.9820 - val_mDice: 0.7808

Epoch 00058: val_mDice did not improve from 0.78380
Epoch 59/300
 - 30s - loss: 0.5037 - acc: 0.9747 - mDice: 0.8125 - val_loss: 0.6485 - val_acc: 0.9824 - val_mDice: 0.7770

Epoch 00059: val_mDice did not improve from 0.78380
Epoch 60/300
 - 30s - loss: 0.5018 - acc: 0.9747 - mDice: 0.8131 - val_loss: 0.6535 - val_acc: 0.9817 - val_mDice: 0.7799

Epoch 00060: val_mDice did not improve from 0.78380
Epoch 61/300
 - 31s - loss: 0.5008 - acc: 0.9748 - mDice: 0.8135 - val_loss: 0.6413 - val_acc: 0.9822 - val_mDice: 0.7809

Epoch 00061: val_mDice did not improve from 0.78380
Epoch 62/300
 - 30s - loss: 0.4985 - acc: 0.9748 - mDice: 0.8143 - val_loss: 0.6340 - val_acc: 0.9823 - val_mDice: 0.7825

Epoch 00062: val_mDice did not improve from 0.78380
Epoch 63/300
 - 30s - loss: 0.4977 - acc: 0.9748 - mDice: 0.8146 - val_loss: 0.6659 - val_acc: 0.9817 - val_mDice: 0.7829

Epoch 00063: val_mDice did not improve from 0.78380
Epoch 64/300
 - 31s - loss: 0.4966 - acc: 0.9748 - mDice: 0.8150 - val_loss: 0.6394 - val_acc: 0.9829 - val_mDice: 0.7811

Epoch 00064: val_mDice did not improve from 0.78380
Epoch 65/300
 - 31s - loss: 0.4957 - acc: 0.9748 - mDice: 0.8152 - val_loss: 0.6511 - val_acc: 0.9824 - val_mDice: 0.7806

Epoch 00065: val_mDice did not improve from 0.78380
Epoch 66/300
 - 30s - loss: 0.4931 - acc: 0.9749 - mDice: 0.8161 - val_loss: 0.6468 - val_acc: 0.9824 - val_mDice: 0.7815

Epoch 00066: val_mDice did not improve from 0.78380
Epoch 67/300
 - 30s - loss: 0.4913 - acc: 0.9749 - mDice: 0.8169 - val_loss: 0.6368 - val_acc: 0.9825 - val_mDice: 0.7822

Epoch 00067: val_mDice did not improve from 0.78380
Epoch 68/300
 - 31s - loss: 0.4910 - acc: 0.9749 - mDice: 0.8172 - val_loss: 0.6545 - val_acc: 0.9820 - val_mDice: 0.7793

Epoch 00068: val_mDice did not improve from 0.78380
Epoch 69/300
 - 30s - loss: 0.4890 - acc: 0.9749 - mDice: 0.8177 - val_loss: 0.6867 - val_acc: 0.9820 - val_mDice: 0.7824

Epoch 00069: val_mDice did not improve from 0.78380
Epoch 70/300
 - 30s - loss: 0.4899 - acc: 0.9749 - mDice: 0.8175 - val_loss: 0.6588 - val_acc: 0.9812 - val_mDice: 0.7846

Epoch 00070: val_mDice improved from 0.78380 to 0.78457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 71/300
 - 31s - loss: 0.4874 - acc: 0.9750 - mDice: 0.8185 - val_loss: 0.6518 - val_acc: 0.9821 - val_mDice: 0.7814

Epoch 00071: val_mDice did not improve from 0.78457
Epoch 72/300
 - 30s - loss: 0.4855 - acc: 0.9751 - mDice: 0.8192 - val_loss: 0.6566 - val_acc: 0.9825 - val_mDice: 0.7841

Epoch 00072: val_mDice did not improve from 0.78457
Epoch 73/300
 - 30s - loss: 0.4850 - acc: 0.9750 - mDice: 0.8192 - val_loss: 0.6594 - val_acc: 0.9818 - val_mDice: 0.7814

Epoch 00073: val_mDice did not improve from 0.78457
Epoch 74/300
 - 31s - loss: 0.4839 - acc: 0.9750 - mDice: 0.8199 - val_loss: 0.6691 - val_acc: 0.9817 - val_mDice: 0.7808

Epoch 00074: val_mDice did not improve from 0.78457
Epoch 75/300
 - 30s - loss: 0.4829 - acc: 0.9751 - mDice: 0.8200 - val_loss: 0.6728 - val_acc: 0.9817 - val_mDice: 0.7827

Epoch 00075: val_mDice did not improve from 0.78457
Epoch 76/300
 - 30s - loss: 0.4813 - acc: 0.9751 - mDice: 0.8206 - val_loss: 0.6564 - val_acc: 0.9828 - val_mDice: 0.7871

Epoch 00076: val_mDice improved from 0.78457 to 0.78714, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 77/300
 - 31s - loss: 0.4818 - acc: 0.9751 - mDice: 0.8204 - val_loss: 0.6644 - val_acc: 0.9820 - val_mDice: 0.7811

Epoch 00077: val_mDice did not improve from 0.78714
Epoch 78/300
 - 31s - loss: 0.4782 - acc: 0.9751 - mDice: 0.8217 - val_loss: 0.6628 - val_acc: 0.9816 - val_mDice: 0.7854

Epoch 00078: val_mDice did not improve from 0.78714
Epoch 79/300
 - 30s - loss: 0.4768 - acc: 0.9752 - mDice: 0.8222 - val_loss: 0.6645 - val_acc: 0.9824 - val_mDice: 0.7821

Epoch 00079: val_mDice did not improve from 0.78714
Epoch 80/300
 - 30s - loss: 0.4783 - acc: 0.9751 - mDice: 0.8218 - val_loss: 0.6765 - val_acc: 0.9823 - val_mDice: 0.7853

Epoch 00080: val_mDice did not improve from 0.78714
Epoch 81/300
 - 31s - loss: 0.4769 - acc: 0.9752 - mDice: 0.8224 - val_loss: 0.6966 - val_acc: 0.9822 - val_mDice: 0.7786

Epoch 00081: val_mDice did not improve from 0.78714
Epoch 82/300
 - 30s - loss: 0.4748 - acc: 0.9752 - mDice: 0.8231 - val_loss: 0.6877 - val_acc: 0.9823 - val_mDice: 0.7869

Epoch 00082: val_mDice did not improve from 0.78714
Epoch 83/300
 - 30s - loss: 0.4749 - acc: 0.9752 - mDice: 0.8230 - val_loss: 0.6465 - val_acc: 0.9825 - val_mDice: 0.7868

Epoch 00083: val_mDice did not improve from 0.78714
Epoch 84/300
 - 31s - loss: 0.4748 - acc: 0.9752 - mDice: 0.8231 - val_loss: 0.6693 - val_acc: 0.9823 - val_mDice: 0.7824

Epoch 00084: val_mDice did not improve from 0.78714
Epoch 85/300
 - 30s - loss: 0.4728 - acc: 0.9752 - mDice: 0.8240 - val_loss: 0.6576 - val_acc: 0.9824 - val_mDice: 0.7839

Epoch 00085: val_mDice did not improve from 0.78714
Epoch 86/300
 - 30s - loss: 0.4730 - acc: 0.9752 - mDice: 0.8237 - val_loss: 0.6731 - val_acc: 0.9816 - val_mDice: 0.7825

Epoch 00086: val_mDice did not improve from 0.78714
Epoch 87/300
 - 31s - loss: 0.4702 - acc: 0.9753 - mDice: 0.8249 - val_loss: 0.6724 - val_acc: 0.9820 - val_mDice: 0.7805

Epoch 00087: val_mDice did not improve from 0.78714
Epoch 88/300
 - 30s - loss: 0.4702 - acc: 0.9753 - mDice: 0.8247 - val_loss: 0.6563 - val_acc: 0.9817 - val_mDice: 0.7861

Epoch 00088: val_mDice did not improve from 0.78714
Epoch 89/300
 - 30s - loss: 0.4705 - acc: 0.9752 - mDice: 0.8248 - val_loss: 0.6634 - val_acc: 0.9825 - val_mDice: 0.7840

Epoch 00089: val_mDice did not improve from 0.78714
Epoch 90/300
 - 31s - loss: 0.4666 - acc: 0.9753 - mDice: 0.8262 - val_loss: 0.6662 - val_acc: 0.9824 - val_mDice: 0.7872

Epoch 00090: val_mDice improved from 0.78714 to 0.78720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 91/300
 - 30s - loss: 0.4683 - acc: 0.9753 - mDice: 0.8257 - val_loss: 0.6654 - val_acc: 0.9819 - val_mDice: 0.7886

Epoch 00091: val_mDice improved from 0.78720 to 0.78858, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 92/300
 - 30s - loss: 0.4667 - acc: 0.9753 - mDice: 0.8261 - val_loss: 0.6695 - val_acc: 0.9821 - val_mDice: 0.7871

Epoch 00092: val_mDice did not improve from 0.78858
Epoch 93/300
 - 31s - loss: 0.4654 - acc: 0.9753 - mDice: 0.8266 - val_loss: 0.6664 - val_acc: 0.9825 - val_mDice: 0.7908

Epoch 00093: val_mDice improved from 0.78858 to 0.79080, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 94/300
 - 31s - loss: 0.4644 - acc: 0.9754 - mDice: 0.8271 - val_loss: 0.6712 - val_acc: 0.9829 - val_mDice: 0.7831

Epoch 00094: val_mDice did not improve from 0.79080
Epoch 95/300
 - 30s - loss: 0.4641 - acc: 0.9754 - mDice: 0.8271 - val_loss: 0.6844 - val_acc: 0.9824 - val_mDice: 0.7805

Epoch 00095: val_mDice did not improve from 0.79080
Epoch 96/300
 - 31s - loss: 0.4646 - acc: 0.9754 - mDice: 0.8268 - val_loss: 0.6587 - val_acc: 0.9822 - val_mDice: 0.7861

Epoch 00096: val_mDice did not improve from 0.79080
Epoch 97/300
 - 30s - loss: 0.4617 - acc: 0.9754 - mDice: 0.8280 - val_loss: 0.6621 - val_acc: 0.9825 - val_mDice: 0.7881

Epoch 00097: val_mDice did not improve from 0.79080
Epoch 98/300
 - 30s - loss: 0.4625 - acc: 0.9754 - mDice: 0.8279 - val_loss: 0.6641 - val_acc: 0.9826 - val_mDice: 0.7883

Epoch 00098: val_mDice did not improve from 0.79080
Epoch 99/300
 - 31s - loss: 0.4608 - acc: 0.9754 - mDice: 0.8283 - val_loss: 0.6807 - val_acc: 0.9825 - val_mDice: 0.7866

Epoch 00099: val_mDice did not improve from 0.79080
Epoch 100/300
 - 30s - loss: 0.4607 - acc: 0.9754 - mDice: 0.8284 - val_loss: 0.6888 - val_acc: 0.9823 - val_mDice: 0.7830

Epoch 00100: val_mDice did not improve from 0.79080
Epoch 101/300
 - 30s - loss: 0.4606 - acc: 0.9754 - mDice: 0.8286 - val_loss: 0.6672 - val_acc: 0.9823 - val_mDice: 0.7827

Epoch 00101: val_mDice did not improve from 0.79080
Epoch 102/300
 - 32s - loss: 0.4593 - acc: 0.9754 - mDice: 0.8288 - val_loss: 0.6785 - val_acc: 0.9826 - val_mDice: 0.7842

Epoch 00102: val_mDice did not improve from 0.79080
Epoch 103/300
 - 30s - loss: 0.4596 - acc: 0.9754 - mDice: 0.8288 - val_loss: 0.6795 - val_acc: 0.9815 - val_mDice: 0.7846

Epoch 00103: val_mDice did not improve from 0.79080
Epoch 104/300
 - 30s - loss: 0.4580 - acc: 0.9755 - mDice: 0.8292 - val_loss: 0.6752 - val_acc: 0.9818 - val_mDice: 0.7852

Epoch 00104: val_mDice did not improve from 0.79080
Epoch 105/300
 - 31s - loss: 0.4600 - acc: 0.9754 - mDice: 0.8285 - val_loss: 0.6834 - val_acc: 0.9827 - val_mDice: 0.7845

Epoch 00105: val_mDice did not improve from 0.79080
Epoch 106/300
 - 30s - loss: 0.4564 - acc: 0.9755 - mDice: 0.8299 - val_loss: 0.6588 - val_acc: 0.9824 - val_mDice: 0.7899

Epoch 00106: val_mDice did not improve from 0.79080
Epoch 107/300
 - 30s - loss: 0.4558 - acc: 0.9755 - mDice: 0.8302 - val_loss: 0.6822 - val_acc: 0.9822 - val_mDice: 0.7847

Epoch 00107: val_mDice did not improve from 0.79080
Epoch 108/300
 - 31s - loss: 0.4559 - acc: 0.9755 - mDice: 0.8301 - val_loss: 0.6905 - val_acc: 0.9826 - val_mDice: 0.7824

Epoch 00108: val_mDice did not improve from 0.79080
Epoch 109/300
 - 30s - loss: 0.4553 - acc: 0.9755 - mDice: 0.8305 - val_loss: 0.6834 - val_acc: 0.9817 - val_mDice: 0.7833

Epoch 00109: val_mDice did not improve from 0.79080
Epoch 110/300
 - 30s - loss: 0.4559 - acc: 0.9755 - mDice: 0.8302 - val_loss: 0.6997 - val_acc: 0.9820 - val_mDice: 0.7833

Epoch 00110: val_mDice did not improve from 0.79080
Epoch 111/300
 - 31s - loss: 0.4539 - acc: 0.9755 - mDice: 0.8309 - val_loss: 0.6796 - val_acc: 0.9822 - val_mDice: 0.7857

Epoch 00111: val_mDice did not improve from 0.79080
Epoch 112/300
 - 30s - loss: 0.4536 - acc: 0.9755 - mDice: 0.8312 - val_loss: 0.6658 - val_acc: 0.9828 - val_mDice: 0.7863

Epoch 00112: val_mDice did not improve from 0.79080
Epoch 113/300
 - 30s - loss: 0.4534 - acc: 0.9755 - mDice: 0.8311 - val_loss: 0.7119 - val_acc: 0.9824 - val_mDice: 0.7827

Epoch 00113: val_mDice did not improve from 0.79080
Epoch 114/300
 - 31s - loss: 0.4528 - acc: 0.9756 - mDice: 0.8312 - val_loss: 0.6808 - val_acc: 0.9823 - val_mDice: 0.7874

Epoch 00114: val_mDice did not improve from 0.79080
Epoch 115/300
 - 30s - loss: 0.4521 - acc: 0.9756 - mDice: 0.8317 - val_loss: 0.6751 - val_acc: 0.9826 - val_mDice: 0.7844

Epoch 00115: val_mDice did not improve from 0.79080
Epoch 116/300
 - 30s - loss: 0.4508 - acc: 0.9756 - mDice: 0.8321 - val_loss: 0.6787 - val_acc: 0.9821 - val_mDice: 0.7883

Epoch 00116: val_mDice did not improve from 0.79080
Epoch 117/300
 - 30s - loss: 0.4519 - acc: 0.9756 - mDice: 0.8317 - val_loss: 0.6933 - val_acc: 0.9818 - val_mDice: 0.7826

Epoch 00117: val_mDice did not improve from 0.79080
Epoch 118/300
 - 31s - loss: 0.4502 - acc: 0.9756 - mDice: 0.8323 - val_loss: 0.6928 - val_acc: 0.9820 - val_mDice: 0.7852

Epoch 00118: val_mDice did not improve from 0.79080
Epoch 119/300
 - 30s - loss: 0.4487 - acc: 0.9756 - mDice: 0.8329 - val_loss: 0.6942 - val_acc: 0.9818 - val_mDice: 0.7836

Epoch 00119: val_mDice did not improve from 0.79080
Epoch 120/300
 - 30s - loss: 0.4486 - acc: 0.9756 - mDice: 0.8330 - val_loss: 0.7060 - val_acc: 0.9825 - val_mDice: 0.7861

Epoch 00120: val_mDice did not improve from 0.79080
Epoch 121/300
 - 32s - loss: 0.4489 - acc: 0.9756 - mDice: 0.8329 - val_loss: 0.6962 - val_acc: 0.9822 - val_mDice: 0.7841

Epoch 00121: val_mDice did not improve from 0.79080
Epoch 122/300
 - 30s - loss: 0.4480 - acc: 0.9756 - mDice: 0.8332 - val_loss: 0.6723 - val_acc: 0.9825 - val_mDice: 0.7928

Epoch 00122: val_mDice improved from 0.79080 to 0.79279, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_Main_BestNetwork_CVa_ResUnet_JointLoss/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 123/300
 - 30s - loss: 0.4479 - acc: 0.9756 - mDice: 0.8334 - val_loss: 0.6863 - val_acc: 0.9828 - val_mDice: 0.7817

Epoch 00123: val_mDice did not improve from 0.79279
Epoch 124/300
 - 31s - loss: 0.4472 - acc: 0.9756 - mDice: 0.8334 - val_loss: 0.6918 - val_acc: 0.9825 - val_mDice: 0.7884

Epoch 00124: val_mDice did not improve from 0.79279
Epoch 125/300
 - 31s - loss: 0.4469 - acc: 0.9757 - mDice: 0.8335 - val_loss: 0.6884 - val_acc: 0.9825 - val_mDice: 0.7872

Epoch 00125: val_mDice did not improve from 0.79279
Epoch 126/300
 - 30s - loss: 0.4458 - acc: 0.9757 - mDice: 0.8340 - val_loss: 0.6856 - val_acc: 0.9822 - val_mDice: 0.7893

Epoch 00126: val_mDice did not improve from 0.79279
Epoch 127/300
 - 31s - loss: 0.4458 - acc: 0.9757 - mDice: 0.8340 - val_loss: 0.6776 - val_acc: 0.9824 - val_mDice: 0.7852

Epoch 00127: val_mDice did not improve from 0.79279
Epoch 128/300
 - 31s - loss: 0.4450 - acc: 0.9757 - mDice: 0.8341 - val_loss: 0.6924 - val_acc: 0.9821 - val_mDice: 0.7879

Epoch 00128: val_mDice did not improve from 0.79279
Epoch 129/300
 - 30s - loss: 0.4453 - acc: 0.9757 - mDice: 0.8343 - val_loss: 0.6647 - val_acc: 0.9825 - val_mDice: 0.7889

Epoch 00129: val_mDice did not improve from 0.79279
Epoch 130/300
 - 31s - loss: 0.4439 - acc: 0.9757 - mDice: 0.8348 - val_loss: 0.6804 - val_acc: 0.9817 - val_mDice: 0.7911

Epoch 00130: val_mDice did not improve from 0.79279
Epoch 131/300
 - 32s - loss: 0.4432 - acc: 0.9757 - mDice: 0.8350 - val_loss: 0.6801 - val_acc: 0.9822 - val_mDice: 0.7869

Epoch 00131: val_mDice did not improve from 0.79279
Epoch 132/300
 - 34s - loss: 0.4439 - acc: 0.9757 - mDice: 0.8347 - val_loss: 0.7021 - val_acc: 0.9826 - val_mDice: 0.7916

Epoch 00132: val_mDice did not improve from 0.79279
Epoch 133/300
 - 36s - loss: 0.4426 - acc: 0.9757 - mDice: 0.8354 - val_loss: 0.6863 - val_acc: 0.9822 - val_mDice: 0.7870

Epoch 00133: val_mDice did not improve from 0.79279
Epoch 134/300
 - 38s - loss: 0.4418 - acc: 0.9757 - mDice: 0.8354 - val_loss: 0.6851 - val_acc: 0.9827 - val_mDice: 0.7924

Epoch 00134: val_mDice did not improve from 0.79279
Epoch 135/300
 - 38s - loss: 0.4419 - acc: 0.9757 - mDice: 0.8354 - val_loss: 0.7046 - val_acc: 0.9824 - val_mDice: 0.7862

Epoch 00135: val_mDice did not improve from 0.79279
Epoch 136/300
 - 37s - loss: 0.4414 - acc: 0.9757 - mDice: 0.8356 - val_loss: 0.6874 - val_acc: 0.9826 - val_mDice: 0.7872

Epoch 00136: val_mDice did not improve from 0.79279
Epoch 137/300
 - 37s - loss: 0.4401 - acc: 0.9758 - mDice: 0.8362 - val_loss: 0.7000 - val_acc: 0.9821 - val_mDice: 0.7868

Epoch 00137: val_mDice did not improve from 0.79279
Epoch 138/300
 - 37s - loss: 0.4401 - acc: 0.9758 - mDice: 0.8361 - val_loss: 0.6954 - val_acc: 0.9825 - val_mDice: 0.7881

Epoch 00138: val_mDice did not improve from 0.79279
Epoch 139/300
 - 37s - loss: 0.4402 - acc: 0.9758 - mDice: 0.8361 - val_loss: 0.6859 - val_acc: 0.9828 - val_mDice: 0.7892

Epoch 00139: val_mDice did not improve from 0.79279
Epoch 140/300
 - 38s - loss: 0.4393 - acc: 0.9758 - mDice: 0.8366 - val_loss: 0.6874 - val_acc: 0.9826 - val_mDice: 0.7877

Epoch 00140: val_mDice did not improve from 0.79279
Epoch 141/300
 - 39s - loss: 0.4383 - acc: 0.9758 - mDice: 0.8366 - val_loss: 0.6908 - val_acc: 0.9824 - val_mDice: 0.7886

Epoch 00141: val_mDice did not improve from 0.79279
Epoch 142/300
 - 36s - loss: 0.4377 - acc: 0.9758 - mDice: 0.8369 - val_loss: 0.6935 - val_acc: 0.9820 - val_mDice: 0.7901

Epoch 00142: val_mDice did not improve from 0.79279
Epoch 143/300
 - 34s - loss: 0.4387 - acc: 0.9758 - mDice: 0.8367 - val_loss: 0.6795 - val_acc: 0.9824 - val_mDice: 0.7881

Epoch 00143: val_mDice did not improve from 0.79279
Epoch 144/300
 - 33s - loss: 0.4389 - acc: 0.9758 - mDice: 0.8365 - val_loss: 0.6896 - val_acc: 0.9820 - val_mDice: 0.7882

Epoch 00144: val_mDice did not improve from 0.79279
Epoch 145/300
 - 32s - loss: 0.4383 - acc: 0.9758 - mDice: 0.8366 - val_loss: 0.6832 - val_acc: 0.9824 - val_mDice: 0.7893

Epoch 00145: val_mDice did not improve from 0.79279
Epoch 146/300
 - 30s - loss: 0.4376 - acc: 0.9759 - mDice: 0.8372 - val_loss: 0.6899 - val_acc: 0.9823 - val_mDice: 0.7861

Epoch 00146: val_mDice did not improve from 0.79279
Epoch 147/300
 - 32s - loss: 0.4358 - acc: 0.9758 - mDice: 0.8377 - val_loss: 0.7060 - val_acc: 0.9824 - val_mDice: 0.7846

Epoch 00147: val_mDice did not improve from 0.79279
Epoch 148/300
 - 30s - loss: 0.4359 - acc: 0.9758 - mDice: 0.8378 - val_loss: 0.6852 - val_acc: 0.9821 - val_mDice: 0.7898

Epoch 00148: val_mDice did not improve from 0.79279
Epoch 149/300
 - 31s - loss: 0.4365 - acc: 0.9759 - mDice: 0.8374 - val_loss: 0.6924 - val_acc: 0.9826 - val_mDice: 0.7887

Epoch 00149: val_mDice did not improve from 0.79279
Epoch 150/300
 - 31s - loss: 0.4345 - acc: 0.9759 - mDice: 0.8384 - val_loss: 0.7017 - val_acc: 0.9824 - val_mDice: 0.7890

Epoch 00150: val_mDice did not improve from 0.79279
Epoch 151/300
 - 30s - loss: 0.4345 - acc: 0.9759 - mDice: 0.8382 - val_loss: 0.6831 - val_acc: 0.9826 - val_mDice: 0.7906

Epoch 00151: val_mDice did not improve from 0.79279
Epoch 152/300
 - 31s - loss: 0.4348 - acc: 0.9759 - mDice: 0.8382 - val_loss: 0.6985 - val_acc: 0.9820 - val_mDice: 0.7917

Epoch 00152: val_mDice did not improve from 0.79279
Restoring model weights from the end of the best epoch
Epoch 00152: early stopping
{'val_loss': [2.8733515650864354, 1.1759333926088669, 0.8856284011874282, 0.79303469315894, 0.7654516778125113, 0.7423779352715141, 0.6907800943182226, 0.6929873565156147, 0.6741689205231189, 0.6828570263489112, 0.6685774235594999, 0.6754254041195408, 0.6491198163898614, 0.6349648409768155, 0.6478044749905574, 0.6675655042177876, 0.6448763832219245, 0.6255867853068715, 0.6322508041580641, 0.6363111168176889, 0.6637488789664211, 0.6457432504777938, 0.6260176617171619, 0.6307983035212085, 0.6323416775286628, 0.6385054071313703, 0.6389340164122567, 0.6294312294730215, 0.6442980096436137, 0.6233336716367487, 0.6375912950749982, 0.6186269082828695, 0.6245618485321817, 0.6464634799120719, 0.6230040207981941, 0.6212220862738488, 0.6255277108604816, 0.6201079137800156, 0.6524801084808275, 0.6165556466923902, 0.6252686360733674, 0.6250853309690398, 0.6338155675906268, 0.6275821518541244, 0.6259347019549862, 0.6186457147785261, 0.6360258628893932, 0.6360164859531096, 0.6314834838250596, 0.6344468421672766, 0.6386726642848292, 0.6315876138468645, 0.6468388749472989, 0.6389424477014748, 0.6400339045514756, 0.6355896432517851, 0.6564148761355102, 0.6586223591284364, 0.6484643987949911, 0.6535085632213006, 0.641292882285497, 0.6340258576368031, 0.6658565467848251, 0.6393881018501317, 0.6510722647325912, 0.6467553062827718, 0.6367997561568939, 0.6545329715506826, 0.6866925412594842, 0.6588106625650936, 0.6518248863079968, 0.6566121208224872, 0.659389293107701, 0.6690688846773162, 0.6727537277497744, 0.6563852753555566, 0.6644068366787382, 0.66283836262637, 0.6645117171716148, 0.6764841065871826, 0.6965577282526668, 0.6876825742861804, 0.6465049203950431, 0.6692621733751091, 0.6576451102215931, 0.6731264582912988, 0.6723832990916521, 0.6562631949859261, 0.6634044069623799, 0.6662495263589794, 0.6654474438774574, 0.6695023976243317, 0.666435896519906, 0.6711675279895833, 0.6843888954912054, 0.6587018082446974, 0.662145021991464, 0.6640639339868745, 0.6806694201517647, 0.6887629151159764, 0.667176037626985, 0.6784644536251131, 0.679504733613401, 0.675243870331161, 0.6834457595712506, 0.6587833633609846, 0.6821990029423106, 0.6905108423609483, 0.6833916766294139, 0.6996659275424985, 0.6795770196538222, 0.6657941880240899, 0.7118626932610669, 0.6808206438403135, 0.6750669077392456, 0.678719268156402, 0.6933292657352207, 0.6927701452875777, 0.6942122553708753, 0.7059791878834597, 0.6962223326704696, 0.6723488656605976, 0.6863039210976954, 0.6918029212484172, 0.6884193946025935, 0.6856233557249862, 0.6775986775217656, 0.6924375908601149, 0.6647291235448898, 0.6803534070589224, 0.6800698633656537, 0.7020709404512333, 0.6863106006931349, 0.685098354876718, 0.7045600844856632, 0.6874328191988978, 0.6999749986318365, 0.6953700841457359, 0.6858859100331955, 0.6873861245572629, 0.6907778400447223, 0.693515688980573, 0.6794863964504994, 0.6895799586093831, 0.6831993006085217, 0.6898582380868579, 0.7060234187126652, 0.6851901230858821, 0.6924067718141219, 0.7016614574027873, 0.683091072475209, 0.6984712362166405], 'val_acc': [0.9064191459871298, 0.9653716677236607, 0.972892777346482, 0.9769657893323553, 0.977364299897685, 0.9780520818919959, 0.9798903043917212, 0.9777095105010781, 0.9800871025051987, 0.9800127253812903, 0.9805326712758917, 0.9809173702332503, 0.9804852754954091, 0.9798784666878274, 0.980944013017373, 0.9804112412981204, 0.9808805118650352, 0.9809907495175845, 0.9812941643475748, 0.9815416296077094, 0.9817064918723761, 0.9816074498666698, 0.9797748073212749, 0.9816864216536806, 0.9815804627411628, 0.9812513767627248, 0.9814530973956066, 0.9803444460203528, 0.9820553278406338, 0.9818723610188078, 0.9824617311927433, 0.9822632967503078, 0.9822514626755926, 0.9820099111311945, 0.9817986417604059, 0.9820128614073321, 0.982200449957321, 0.9818259494223461, 0.9815863909121022, 0.982146139976533, 0.9829918763708896, 0.982059259901844, 0.9824209303063628, 0.9819872079741967, 0.9818358290921301, 0.9821339762001707, 0.9822962094632711, 0.9811872165761619, 0.9823735416612143, 0.9821948535179084, 0.9817295263918314, 0.9820316349635798, 0.9814442246690992, 0.9820507134938511, 0.9822823893052014, 0.9817861246004685, 0.981930600231277, 0.9820003635004947, 0.9823847354627123, 0.9816568099554355, 0.9822254573966697, 0.9822646196781666, 0.9817183445850762, 0.9828618976476884, 0.9824400170791752, 0.982445942851166, 0.9824824750854012, 0.9819802881148332, 0.9819974035055399, 0.9812079359872422, 0.9820981076135232, 0.9824926687098878, 0.9818206841608565, 0.9816831346020733, 0.981709130654271, 0.9827852278671029, 0.9820122163977293, 0.9815900050818736, 0.9823797861858049, 0.9823041108616611, 0.9822264535754335, 0.9822679097438375, 0.9825403897631895, 0.982271530802897, 0.9824021710207834, 0.9816344258585951, 0.9819559473986473, 0.981676885587143, 0.9824785128220439, 0.9823909728519687, 0.9819388211204049, 0.9820645407872549, 0.9825423551171679, 0.9828799913788235, 0.9823837461732366, 0.9821856445080233, 0.9824702951315141, 0.9825716484318823, 0.9824686467463017, 0.9823192481655085, 0.982271531171966, 0.9826358190138889, 0.9814820652529674, 0.9818154221594764, 0.9827088791150427, 0.9823597221182596, 0.982159646671992, 0.9825749375748807, 0.981716047991663, 0.9819612046021303, 0.9821629361840594, 0.9827552704127088, 0.9823600559411773, 0.9823376696914342, 0.9825808683908146, 0.982141209768191, 0.9818341885188785, 0.9820187873023459, 0.9817719731414527, 0.9825453253845435, 0.9822136114502347, 0.982485106485916, 0.9828260297991789, 0.9824656916107556, 0.982453183616032, 0.9822020929295212, 0.9824341003493751, 0.9821438340948832, 0.9825091191612653, 0.98166437679277, 0.9821737917953232, 0.9825699975246984, 0.9822254577657387, 0.9826591853263823, 0.9823626932467961, 0.9826239722062928, 0.9820540079883501, 0.9824936573842484, 0.9828286572014461, 0.982613112781316, 0.9823850597513473, 0.9819769985412543, 0.9823969146784615, 0.9820204439300994, 0.9823712465440771, 0.9822517846267905, 0.9824327832651089, 0.9820862462277014, 0.9826384483845241, 0.9823913140562666, 0.9826164032160559, 0.9820003713124553], 'val_mDice': [0.12528009841870227, 0.5496917047736815, 0.6622641880076736, 0.6980823291344539, 0.7190990850283265, 0.727394041508221, 0.7404622377133837, 0.7429705759812189, 0.7484093047024911, 0.7473339028156701, 0.7527032257110587, 0.7491373793136471, 0.7584606818739474, 0.7580315457778081, 0.756632268859383, 0.7531116956035665, 0.7590020725847644, 0.764872765270426, 0.7644894658842569, 0.7615899297844145, 0.758896291009905, 0.7587540658388836, 0.7620665859757808, 0.7656735330666304, 0.7690213390917232, 0.7649799493075156, 0.7628638064652159, 0.7717981598694628, 0.7626484112104764, 0.7724308519171488, 0.7688876495026705, 0.7713316639633494, 0.7743817733906371, 0.7665453824465489, 0.7735580112419876, 0.7720794173340064, 0.7712036122724614, 0.7778907928796499, 0.7744104280314332, 0.7773330543801511, 0.7769541673369944, 0.7733299716830376, 0.7713091377749408, 0.774828247918187, 0.7788445547269225, 0.7775599119956034, 0.7800430512157633, 0.7769168723601428, 0.7807180676051829, 0.7705663706496035, 0.7784144816260835, 0.7838016445791758, 0.7779730377551571, 0.7796202259536129, 0.7822195977987519, 0.7814415957782537, 0.7814570725025654, 0.7808230053037559, 0.7769600940316577, 0.7799407750583408, 0.7808976293963421, 0.7824626811640674, 0.7828872629486493, 0.7811130709077305, 0.7806046493159237, 0.7815032713922554, 0.782204176619326, 0.7792508570156353, 0.7824350312521338, 0.7845746129289869, 0.78137431597439, 0.7841044286087201, 0.7814479518847077, 0.7808264865471729, 0.7827231961507177, 0.7871400089578855, 0.7810951377831253, 0.7854071424964535, 0.7821070694578937, 0.7853391273225916, 0.7785654031086263, 0.7869321751274684, 0.7867615550172096, 0.7824323159499311, 0.7839092700227988, 0.7825283246389729, 0.7805194333118797, 0.7861154241827619, 0.7840449253348989, 0.7872014133061664, 0.7885792431324505, 0.7871044084998722, 0.7907961695555932, 0.7831108749712461, 0.7805172390120932, 0.7860639471756784, 0.7880624407831237, 0.7883395525570133, 0.7866300314941642, 0.7829590660622737, 0.7826728115878976, 0.7841968914672687, 0.7846208179698271, 0.7851596564454314, 0.7845154770633631, 0.7898893073370337, 0.7847233386354673, 0.7823866525793716, 0.7833493141567006, 0.783311315611297, 0.7856855575760329, 0.7862838298174619, 0.7827324451430786, 0.7874047295473924, 0.7844301054352208, 0.788296245323, 0.7825859564868781, 0.7852480294657689, 0.7835838808855897, 0.7860770581676495, 0.7840528021902, 0.792786528820593, 0.7816615446064126, 0.7884284977573359, 0.7871815823056996, 0.7892910683733267, 0.78523053664049, 0.7879452406437404, 0.7889060926757238, 0.7911095566301769, 0.7868801197892495, 0.7916118869840545, 0.786973937746172, 0.7923863137100503, 0.78617739997289, 0.7872288888945053, 0.7868158856043506, 0.7880851046342722, 0.7891917228083616, 0.787733748847363, 0.7886241524950269, 0.7900950154529882, 0.7881280578080838, 0.7881832668409751, 0.789277594586521, 0.7860623217342562, 0.7845668959420778, 0.7898460568781361, 0.7886740993420037, 0.7890446753201716, 0.7906383682576742, 0.7917003396617862], 'loss': [16.220043751286354, 2.241280952671817, 1.4991726528951939, 1.2441571844501031, 1.101910497120408, 1.0151192788466195, 0.9488303768754376, 0.897805524478814, 0.8569889634826288, 0.8228304727082609, 0.7936415303100798, 0.7743311698564288, 0.7496566810470109, 0.7314810299036296, 0.7153774204005311, 0.7032686091562658, 0.6915965946286682, 0.6767802895510541, 0.6641686950630751, 0.6551413558884398, 0.6459952659579317, 0.6367020019987816, 0.6290110177866508, 0.6246576850158604, 0.6159011398461886, 0.6092191543745943, 0.6049883329045497, 0.5973611335168455, 0.5915061243506585, 0.5860180352528226, 0.5825750931389212, 0.578146406857985, 0.573360532863857, 0.5698720996045884, 0.5659114005836405, 0.5610207993353014, 0.5586441872382978, 0.5532557009184363, 0.5493175381296769, 0.5461826912644144, 0.5434368855455823, 0.5416481369807836, 0.5380133075035926, 0.5356812372266292, 0.5312602774415576, 0.5309262240782882, 0.5275237776333715, 0.5271314407465479, 0.523518752747115, 0.5212558853596356, 0.5183734076388103, 0.5166797971267076, 0.5160029712883818, 0.5123788011950982, 0.5105917905605235, 0.5096585194891439, 0.50814996848378, 0.5075448601477067, 0.5036739205151035, 0.5017987349087736, 0.5007513698492069, 0.4984631788974051, 0.497722772131793, 0.4965610152458786, 0.49566395745190284, 0.493149817897421, 0.4913241421091605, 0.4909676190511065, 0.4890119597717945, 0.48990313815905423, 0.48741460784981466, 0.48552268079422184, 0.4849595783760134, 0.4838869985231272, 0.4828937065104525, 0.48134511411332387, 0.48180435934925464, 0.47820529944933143, 0.47684046429995, 0.47831750227125497, 0.47689738032967033, 0.4747658949892533, 0.4748587307289104, 0.47476120640846037, 0.4727671510754151, 0.4730039806042248, 0.4702196563979963, 0.4702343141794888, 0.4704695342262975, 0.4665758032302797, 0.4682896859269453, 0.46668137109931623, 0.46537212613931783, 0.4644157177925907, 0.4641179258303224, 0.4645984174814035, 0.4616540474192123, 0.46250063067750086, 0.4608091906101003, 0.46070324168488264, 0.46059220720724015, 0.45925112635943555, 0.4595824760285431, 0.4580296816676677, 0.45999246790510445, 0.45636719852821167, 0.4557985459274855, 0.4558867435908585, 0.4553133405768162, 0.4559415486951076, 0.4539343034107872, 0.4535652497716129, 0.45342583556447985, 0.4528481759182559, 0.45208318789178387, 0.45077158548765317, 0.45188494606407553, 0.4502071493532369, 0.44873269749322653, 0.4485634784973804, 0.4489115025907762, 0.4479537280278642, 0.44790187965636324, 0.44719352497173603, 0.4468989299793986, 0.44582751806151877, 0.44578853938555074, 0.44500875749905755, 0.44532160355249284, 0.44390772262079664, 0.44323537958701814, 0.4438779951743417, 0.4426135353251121, 0.44177008769180204, 0.4419096524230842, 0.44144416826990474, 0.4400590678438071, 0.44012823680974944, 0.44016331699032873, 0.439312951680667, 0.4383044239299976, 0.43767901053925756, 0.4386975056221982, 0.4389311638265912, 0.43834946848539286, 0.4375770856429035, 0.4358178079626513, 0.43585525629612665, 0.4365032587083849, 0.43447334549792604, 0.4345108560269335, 0.4347640523940133], 'acc': [0.8534659313185686, 0.936217825752128, 0.9545882682983079, 0.9603040289460317, 0.9632039816504251, 0.9648245106257031, 0.9661691151301331, 0.9672123796905114, 0.9680170645527604, 0.9686791255788983, 0.9692777986881811, 0.969683205046459, 0.9702249038666451, 0.970533586776937, 0.9708807648139323, 0.9711242940251059, 0.9713609773289311, 0.9716669723644216, 0.9719374138841431, 0.9721061741505712, 0.9722449752889842, 0.9724573613793291, 0.9725882671942482, 0.9726484998373761, 0.9728476282290123, 0.9729405322534986, 0.9730573325089403, 0.9731714173863149, 0.973258951319867, 0.9733538583494012, 0.9734039028760838, 0.9734758628861044, 0.9735844734747426, 0.9736452411289912, 0.9736975620562802, 0.9737801560716126, 0.9738263768115702, 0.9738985631469897, 0.9739940966445448, 0.9740164969797092, 0.9741060497056393, 0.9741185889527084, 0.9741846277103351, 0.9741827136709492, 0.9742680115761836, 0.9742616721374879, 0.9743216275129621, 0.9743448090006634, 0.9743755777097984, 0.9743882746949297, 0.9744660718673904, 0.9744859204062819, 0.9745125211321541, 0.9745848601946889, 0.974607689933843, 0.9746426381784662, 0.9746497302984093, 0.974651975864609, 0.9747001676932991, 0.9747345991413092, 0.9747578280123543, 0.9747742967102219, 0.9747875384080658, 0.9748374417100341, 0.9748282069120141, 0.9748964563158038, 0.9749136873324503, 0.9749362117953194, 0.9749485022531422, 0.9749190094421438, 0.9749945508485652, 0.9750515365475312, 0.9750397477751847, 0.9750471550011254, 0.9750834186122103, 0.9750752576017596, 0.9750994758430005, 0.9751449635967253, 0.9751940564990242, 0.9751459909333857, 0.9751765797609129, 0.9752034815706727, 0.9751980735132738, 0.9752117912012551, 0.975234629802098, 0.9752324885831746, 0.975281286295143, 0.975277231549317, 0.9752386092200096, 0.9753133242401326, 0.9752896937886656, 0.9753166338994277, 0.9753398147678786, 0.975365119608769, 0.9753943933189584, 0.9753624726472107, 0.9753917179999956, 0.9753932031976997, 0.9754061586096089, 0.9754197251008068, 0.9754287207827012, 0.9754267698374061, 0.9754241424213881, 0.9754763212911776, 0.9754307315319399, 0.9754797105846696, 0.9755081177284531, 0.9755105363787402, 0.9755104682255907, 0.9755222922667474, 0.9755493665840508, 0.9755317358233805, 0.975542378576496, 0.9755769171334976, 0.9755509092439352, 0.9755960986114272, 0.9755843996161745, 0.9755938528032214, 0.9756148556607248, 0.9756276395544676, 0.9756155111623431, 0.9756349884712072, 0.9756486964433609, 0.9756419652824274, 0.9756748768974096, 0.975686784508919, 0.9756778387870201, 0.9757091199580143, 0.9756755810991687, 0.9757219630578039, 0.9757316650127437, 0.9757110628245809, 0.9757360048700637, 0.9757475697370711, 0.9757299128184035, 0.97574129729132, 0.975778490297829, 0.9757627455394422, 0.9757990998671736, 0.9758057460341899, 0.9758345634350208, 0.9758177509320958, 0.9758046703390441, 0.9758069170369776, 0.9758087266863802, 0.9758684055913966, 0.97584580672593, 0.9758444933097516, 0.9758646758951661, 0.9758973082352687, 0.9758751102028527, 0.9758741290964487], 'mDice': [0.06832995777682772, 0.327132386253519, 0.5140328332408106, 0.5877928346972037, 0.6280913089108632, 0.6531729465286004, 0.6724157964938405, 0.6872662925295993, 0.6989064686603582, 0.7096642736164915, 0.7182173110048213, 0.723606993239849, 0.7316420638307854, 0.7371884052867903, 0.7421209965553379, 0.7458875975276851, 0.7494270586870666, 0.75408082130646, 0.7582795341657744, 0.7611477913778495, 0.7641332698716792, 0.7670926812414284, 0.7696371294659478, 0.7712358303789822, 0.7740351471463621, 0.7763576585935584, 0.7773475210650144, 0.7799762697458438, 0.7819228194652208, 0.7839834352851569, 0.7848548183356887, 0.7866082696334428, 0.7882154607419783, 0.7892948431689101, 0.7906680678037233, 0.792484935019846, 0.7930857296174484, 0.7947586303555029, 0.7966232765239094, 0.797544007059894, 0.7985168930907394, 0.7992614778721848, 0.8005075424674704, 0.801200746415138, 0.8027140336729104, 0.8027616671440224, 0.8038587348470262, 0.8040725239882869, 0.8053283928500914, 0.8062508005514503, 0.8071178266547704, 0.8078665531404781, 0.8080359708161009, 0.809420917322848, 0.8100206507100733, 0.8102662930247719, 0.8109584040450457, 0.8110838912041284, 0.8124638663745878, 0.8131364135973375, 0.8134981500581423, 0.8142997858374563, 0.8146001355358203, 0.815033764508336, 0.8152387280053617, 0.8161437480339208, 0.8168723181633896, 0.8171994928275027, 0.8177457134650208, 0.8174607205752515, 0.818520115231568, 0.819173371767639, 0.8192363944120842, 0.8199259596292916, 0.820002322432418, 0.8205730747587219, 0.8203987075771726, 0.8216819760336465, 0.8221733445038737, 0.8217806816599322, 0.8223568203841014, 0.8230786130725555, 0.8230483405752913, 0.8231198604761173, 0.823955610789517, 0.8236715437149074, 0.8249160460122366, 0.8246826130762458, 0.8247772243104347, 0.8261966827037028, 0.8257252880033343, 0.8261227313262738, 0.8266198019267325, 0.8270873090646874, 0.8271484951970683, 0.8268190586527066, 0.8279590316282299, 0.8278594969820515, 0.8282530904100937, 0.8284126292742434, 0.8285989231644196, 0.8288432312290738, 0.8288270750534343, 0.8292102100514899, 0.8285207247318389, 0.8298701377808407, 0.8301773234785443, 0.8301168121513159, 0.8304926437083993, 0.8302005588623969, 0.8308859251551677, 0.8311596510659148, 0.8310579290203812, 0.831159887996869, 0.8317161390839484, 0.8320787625677807, 0.8316536118684135, 0.8323107252748478, 0.8329489357806857, 0.8329757711028666, 0.8328509621085487, 0.833205174112878, 0.8333907253294649, 0.8334060980819818, 0.8334801298552149, 0.8339596441545434, 0.833993491613697, 0.8340518741354841, 0.8343150321156648, 0.834791057536048, 0.8350432044727466, 0.8346667269376915, 0.8353543981023583, 0.835440408494497, 0.8354153630412904, 0.8356342490274011, 0.8361999400694956, 0.8361067722585112, 0.8360525792158102, 0.8365761156166998, 0.8366231397025649, 0.8369233031061631, 0.8367246717449472, 0.8364523768368045, 0.8365756698915433, 0.8371848852574071, 0.8377337002628937, 0.8378051106773756, 0.8373622230906062, 0.8383997724026064, 0.8381852833587513, 0.8381961655482946]}
predicting test subjects:   0%|          | 0/15 [00:00<?, ?it/s]predicting test subjects:   7%|▋         | 1/15 [00:02<00:29,  2.14s/it]predicting test subjects:  13%|█▎        | 2/15 [00:03<00:26,  2.04s/it]predicting test subjects:  20%|██        | 3/15 [00:05<00:24,  2.02s/it]predicting test subjects:  27%|██▋       | 4/15 [00:07<00:21,  1.98s/it]predicting test subjects:  33%|███▎      | 5/15 [00:10<00:21,  2.10s/it]predicting test subjects:  40%|████      | 6/15 [00:12<00:19,  2.15s/it]predicting test subjects:  47%|████▋     | 7/15 [00:13<00:15,  1.96s/it]predicting test subjects:  53%|█████▎    | 8/15 [00:16<00:14,  2.07s/it]predicting test subjects:  60%|██████    | 9/15 [00:18<00:12,  2.05s/it]predicting test subjects:  67%|██████▋   | 10/15 [00:19<00:09,  1.92s/it]predicting test subjects:  73%|███████▎  | 11/15 [00:21<00:07,  1.89s/it]predicting test subjects:  80%|████████  | 12/15 [00:23<00:05,  1.94s/it]predicting test subjects:  87%|████████▋ | 13/15 [00:25<00:03,  2.00s/it]predicting test subjects:  93%|█████████▎| 14/15 [00:27<00:01,  1.97s/it]predicting test subjects: 100%|██████████| 15/15 [00:29<00:00,  1.96s/it]
predicting train subjects:   0%|          | 0/532 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/532 [00:02<20:56,  2.37s/it]predicting train subjects:   0%|          | 2/532 [00:04<19:31,  2.21s/it]predicting train subjects:   1%|          | 3/532 [00:06<18:31,  2.10s/it]predicting train subjects:   1%|          | 4/532 [00:07<17:48,  2.02s/it]predicting train subjects:   1%|          | 5/532 [00:09<17:25,  1.98s/it]predicting train subjects:   1%|          | 6/532 [00:11<16:46,  1.91s/it]predicting train subjects:   1%|▏         | 7/532 [00:13<16:20,  1.87s/it]predicting train subjects:   2%|▏         | 8/532 [00:15<16:03,  1.84s/it]predicting train subjects:   2%|▏         | 9/532 [00:17<16:38,  1.91s/it]predicting train subjects:   2%|▏         | 10/532 [00:18<16:16,  1.87s/it]predicting train subjects:   2%|▏         | 11/532 [00:20<15:24,  1.77s/it]predicting train subjects:   2%|▏         | 12/532 [00:22<16:37,  1.92s/it]predicting train subjects:   2%|▏         | 13/532 [00:24<15:35,  1.80s/it]predicting train subjects:   3%|▎         | 14/532 [00:25<14:54,  1.73s/it]predicting train subjects:   3%|▎         | 15/532 [00:27<14:42,  1.71s/it]predicting train subjects:   3%|▎         | 16/532 [00:29<14:53,  1.73s/it]predicting train subjects:   3%|▎         | 17/532 [00:30<14:37,  1.70s/it]predicting train subjects:   3%|▎         | 18/532 [00:32<15:27,  1.80s/it]predicting train subjects:   4%|▎         | 19/532 [00:34<14:45,  1.73s/it]predicting train subjects:   4%|▍         | 20/532 [00:36<15:02,  1.76s/it]predicting train subjects:   4%|▍         | 21/532 [00:38<16:25,  1.93s/it]predicting train subjects:   4%|▍         | 22/532 [00:40<15:51,  1.87s/it]predicting train subjects:   4%|▍         | 23/532 [00:42<16:07,  1.90s/it]predicting train subjects:   5%|▍         | 24/532 [00:43<15:19,  1.81s/it]predicting train subjects:   5%|▍         | 25/532 [00:46<16:16,  1.93s/it]predicting train subjects:   5%|▍         | 26/532 [00:47<15:44,  1.87s/it]predicting train subjects:   5%|▌         | 27/532 [00:50<17:29,  2.08s/it]predicting train subjects:   5%|▌         | 28/532 [00:52<16:43,  1.99s/it]predicting train subjects:   5%|▌         | 29/532 [00:54<16:34,  1.98s/it]predicting train subjects:   6%|▌         | 30/532 [00:55<15:25,  1.84s/it]predicting train subjects:   6%|▌         | 31/532 [00:57<15:05,  1.81s/it]predicting train subjects:   6%|▌         | 32/532 [00:59<14:46,  1.77s/it]predicting train subjects:   6%|▌         | 33/532 [01:00<14:19,  1.72s/it]predicting train subjects:   6%|▋         | 34/532 [01:02<15:30,  1.87s/it]predicting train subjects:   7%|▋         | 35/532 [01:04<15:19,  1.85s/it]predicting train subjects:   7%|▋         | 36/532 [01:06<15:37,  1.89s/it]predicting train subjects:   7%|▋         | 37/532 [01:08<15:39,  1.90s/it]predicting train subjects:   7%|▋         | 38/532 [01:10<16:00,  1.94s/it]predicting train subjects:   7%|▋         | 39/532 [01:12<15:28,  1.88s/it]predicting train subjects:   8%|▊         | 40/532 [01:14<14:56,  1.82s/it]predicting train subjects:   8%|▊         | 41/532 [01:16<15:18,  1.87s/it]predicting train subjects:   8%|▊         | 42/532 [01:18<15:59,  1.96s/it]predicting train subjects:   8%|▊         | 43/532 [01:19<15:16,  1.87s/it]predicting train subjects:   8%|▊         | 44/532 [01:21<14:29,  1.78s/it]predicting train subjects:   8%|▊         | 45/532 [01:23<14:12,  1.75s/it]predicting train subjects:   9%|▊         | 46/532 [01:25<14:28,  1.79s/it]predicting train subjects:   9%|▉         | 47/532 [01:27<15:30,  1.92s/it]predicting train subjects:   9%|▉         | 48/532 [01:29<15:27,  1.92s/it]predicting train subjects:   9%|▉         | 49/532 [01:30<14:50,  1.84s/it]predicting train subjects:   9%|▉         | 50/532 [01:33<15:41,  1.95s/it]predicting train subjects:  10%|▉         | 51/532 [01:34<15:17,  1.91s/it]predicting train subjects:  10%|▉         | 52/532 [01:36<15:05,  1.89s/it]predicting train subjects:  10%|▉         | 53/532 [01:38<14:46,  1.85s/it]predicting train subjects:  10%|█         | 54/532 [01:40<15:26,  1.94s/it]predicting train subjects:  10%|█         | 55/532 [01:42<15:27,  1.94s/it]predicting train subjects:  11%|█         | 56/532 [01:44<15:29,  1.95s/it]predicting train subjects:  11%|█         | 57/532 [01:46<15:00,  1.90s/it]predicting train subjects:  11%|█         | 58/532 [01:48<15:03,  1.91s/it]predicting train subjects:  11%|█         | 59/532 [01:50<16:00,  2.03s/it]predicting train subjects:  11%|█▏        | 60/532 [01:52<14:51,  1.89s/it]predicting train subjects:  11%|█▏        | 61/532 [01:53<14:01,  1.79s/it]predicting train subjects:  12%|█▏        | 62/532 [01:55<14:47,  1.89s/it]predicting train subjects:  12%|█▏        | 63/532 [01:58<15:35,  2.00s/it]predicting train subjects:  12%|█▏        | 64/532 [01:59<14:54,  1.91s/it]predicting train subjects:  12%|█▏        | 65/532 [02:01<14:39,  1.88s/it]predicting train subjects:  12%|█▏        | 66/532 [02:03<15:38,  2.01s/it]predicting train subjects:  13%|█▎        | 67/532 [02:06<15:56,  2.06s/it]predicting train subjects:  13%|█▎        | 68/532 [02:07<15:35,  2.02s/it]predicting train subjects:  13%|█▎        | 69/532 [02:09<15:02,  1.95s/it]predicting train subjects:  13%|█▎        | 70/532 [02:11<14:25,  1.87s/it]predicting train subjects:  13%|█▎        | 71/532 [02:13<13:55,  1.81s/it]predicting train subjects:  14%|█▎        | 72/532 [02:14<13:44,  1.79s/it]predicting train subjects:  14%|█▎        | 73/532 [02:16<14:14,  1.86s/it]predicting train subjects:  14%|█▍        | 74/532 [02:19<15:23,  2.02s/it]predicting train subjects:  14%|█▍        | 75/532 [02:22<17:18,  2.27s/it]predicting train subjects:  14%|█▍        | 76/532 [02:23<16:00,  2.11s/it]predicting train subjects:  14%|█▍        | 77/532 [02:25<15:27,  2.04s/it]predicting train subjects:  15%|█▍        | 78/532 [02:27<15:04,  1.99s/it]predicting train subjects:  15%|█▍        | 79/532 [02:29<14:41,  1.95s/it]predicting train subjects:  15%|█▌        | 80/532 [02:31<14:31,  1.93s/it]predicting train subjects:  15%|█▌        | 81/532 [02:33<14:05,  1.88s/it]predicting train subjects:  15%|█▌        | 82/532 [02:34<14:00,  1.87s/it]predicting train subjects:  16%|█▌        | 83/532 [02:36<13:22,  1.79s/it]predicting train subjects:  16%|█▌        | 84/532 [02:38<12:46,  1.71s/it]predicting train subjects:  16%|█▌        | 85/532 [02:39<12:21,  1.66s/it]predicting train subjects:  16%|█▌        | 86/532 [02:41<12:09,  1.64s/it]predicting train subjects:  16%|█▋        | 87/532 [02:42<12:04,  1.63s/it]predicting train subjects:  17%|█▋        | 88/532 [02:44<11:53,  1.61s/it]predicting train subjects:  17%|█▋        | 89/532 [02:46<12:16,  1.66s/it]predicting train subjects:  17%|█▋        | 90/532 [02:47<12:31,  1.70s/it]predicting train subjects:  17%|█▋        | 91/532 [02:49<12:36,  1.72s/it]predicting train subjects:  17%|█▋        | 92/532 [02:51<12:57,  1.77s/it]predicting train subjects:  17%|█▋        | 93/532 [02:53<13:19,  1.82s/it]predicting train subjects:  18%|█▊        | 94/532 [02:55<13:14,  1.81s/it]predicting train subjects:  18%|█▊        | 95/532 [02:57<13:50,  1.90s/it]predicting train subjects:  18%|█▊        | 96/532 [02:59<14:15,  1.96s/it]predicting train subjects:  18%|█▊        | 97/532 [03:01<14:39,  2.02s/it]predicting train subjects:  18%|█▊        | 98/532 [03:03<14:54,  2.06s/it]predicting train subjects:  19%|█▊        | 99/532 [03:06<15:08,  2.10s/it]predicting train subjects:  19%|█▉        | 100/532 [03:08<15:16,  2.12s/it]predicting train subjects:  19%|█▉        | 101/532 [03:09<14:16,  1.99s/it]predicting train subjects:  19%|█▉        | 102/532 [03:11<13:33,  1.89s/it]predicting train subjects:  19%|█▉        | 103/532 [03:13<13:03,  1.83s/it]predicting train subjects:  20%|█▉        | 104/532 [03:14<12:35,  1.77s/it]predicting train subjects:  20%|█▉        | 105/532 [03:16<12:21,  1.74s/it]predicting train subjects:  20%|█▉        | 106/532 [03:18<12:12,  1.72s/it]predicting train subjects:  20%|██        | 107/532 [03:19<12:04,  1.70s/it]predicting train subjects:  20%|██        | 108/532 [03:21<12:01,  1.70s/it]predicting train subjects:  20%|██        | 109/532 [03:23<11:58,  1.70s/it]predicting train subjects:  21%|██        | 110/532 [03:24<11:45,  1.67s/it]predicting train subjects:  21%|██        | 111/532 [03:26<11:51,  1.69s/it]predicting train subjects:  21%|██        | 112/532 [03:28<11:52,  1.70s/it]predicting train subjects:  21%|██        | 113/532 [03:30<12:29,  1.79s/it]predicting train subjects:  21%|██▏       | 114/532 [03:32<12:44,  1.83s/it]predicting train subjects:  22%|██▏       | 115/532 [03:34<12:56,  1.86s/it]predicting train subjects:  22%|██▏       | 116/532 [03:36<12:57,  1.87s/it]predicting train subjects:  22%|██▏       | 117/532 [03:37<12:58,  1.88s/it]predicting train subjects:  22%|██▏       | 118/532 [03:39<13:08,  1.91s/it]predicting train subjects:  22%|██▏       | 119/532 [03:41<12:52,  1.87s/it]predicting train subjects:  23%|██▎       | 120/532 [03:43<12:44,  1.85s/it]predicting train subjects:  23%|██▎       | 121/532 [03:45<12:59,  1.90s/it]predicting train subjects:  23%|██▎       | 122/532 [03:47<13:11,  1.93s/it]predicting train subjects:  23%|██▎       | 123/532 [03:49<12:57,  1.90s/it]predicting train subjects:  23%|██▎       | 124/532 [03:51<12:47,  1.88s/it]predicting train subjects:  23%|██▎       | 125/532 [03:53<13:04,  1.93s/it]predicting train subjects:  24%|██▎       | 126/532 [03:55<13:14,  1.96s/it]predicting train subjects:  24%|██▍       | 127/532 [03:57<13:25,  1.99s/it]predicting train subjects:  24%|██▍       | 128/532 [03:59<13:37,  2.02s/it]predicting train subjects:  24%|██▍       | 129/532 [04:01<13:34,  2.02s/it]predicting train subjects:  24%|██▍       | 130/532 [04:03<13:50,  2.07s/it]predicting train subjects:  25%|██▍       | 131/532 [04:06<14:30,  2.17s/it]predicting train subjects:  25%|██▍       | 132/532 [04:08<14:43,  2.21s/it]predicting train subjects:  25%|██▌       | 133/532 [04:10<14:41,  2.21s/it]predicting train subjects:  25%|██▌       | 134/532 [04:12<14:47,  2.23s/it]predicting train subjects:  25%|██▌       | 135/532 [04:15<14:55,  2.26s/it]predicting train subjects:  26%|██▌       | 136/532 [04:17<15:11,  2.30s/it]predicting train subjects:  26%|██▌       | 137/532 [04:19<15:23,  2.34s/it]predicting train subjects:  26%|██▌       | 138/532 [04:22<15:40,  2.39s/it]predicting train subjects:  26%|██▌       | 139/532 [04:24<15:39,  2.39s/it]predicting train subjects:  26%|██▋       | 140/532 [04:27<15:52,  2.43s/it]predicting train subjects:  27%|██▋       | 141/532 [04:29<15:45,  2.42s/it]predicting train subjects:  27%|██▋       | 142/532 [04:32<15:40,  2.41s/it]predicting train subjects:  27%|██▋       | 143/532 [04:34<14:34,  2.25s/it]predicting train subjects:  27%|██▋       | 144/532 [04:35<13:46,  2.13s/it]predicting train subjects:  27%|██▋       | 145/532 [04:37<13:19,  2.07s/it]predicting train subjects:  27%|██▋       | 146/532 [04:39<12:54,  2.01s/it]predicting train subjects:  28%|██▊       | 147/532 [04:41<12:34,  1.96s/it]predicting train subjects:  28%|██▊       | 148/532 [04:43<12:11,  1.90s/it]predicting train subjects:  28%|██▊       | 149/532 [04:45<12:10,  1.91s/it]predicting train subjects:  28%|██▊       | 150/532 [04:47<12:10,  1.91s/it]predicting train subjects:  28%|██▊       | 151/532 [04:49<12:02,  1.90s/it]predicting train subjects:  29%|██▊       | 152/532 [04:50<12:00,  1.90s/it]predicting train subjects:  29%|██▉       | 153/532 [04:52<11:50,  1.87s/it]predicting train subjects:  29%|██▉       | 154/532 [04:54<11:45,  1.87s/it]predicting train subjects:  29%|██▉       | 155/532 [04:57<12:44,  2.03s/it]predicting train subjects:  29%|██▉       | 156/532 [04:59<13:17,  2.12s/it]predicting train subjects:  30%|██▉       | 157/532 [05:01<13:45,  2.20s/it]predicting train subjects:  30%|██▉       | 158/532 [05:04<14:14,  2.28s/it]predicting train subjects:  30%|██▉       | 159/532 [05:06<14:20,  2.31s/it]predicting train subjects:  30%|███       | 160/532 [05:08<14:24,  2.32s/it]predicting train subjects:  30%|███       | 161/532 [05:10<13:25,  2.17s/it]predicting train subjects:  30%|███       | 162/532 [05:12<12:50,  2.08s/it]predicting train subjects:  31%|███       | 163/532 [05:14<12:24,  2.02s/it]predicting train subjects:  31%|███       | 164/532 [05:16<12:07,  1.98s/it]predicting train subjects:  31%|███       | 165/532 [05:18<11:51,  1.94s/it]predicting train subjects:  31%|███       | 166/532 [05:20<11:42,  1.92s/it]predicting train subjects:  31%|███▏      | 167/532 [05:22<11:42,  1.92s/it]predicting train subjects:  32%|███▏      | 168/532 [05:23<11:34,  1.91s/it]predicting train subjects:  32%|███▏      | 169/532 [05:25<11:23,  1.88s/it]predicting train subjects:  32%|███▏      | 170/532 [05:27<11:08,  1.85s/it]predicting train subjects:  32%|███▏      | 171/532 [05:29<11:07,  1.85s/it]predicting train subjects:  32%|███▏      | 172/532 [05:31<10:58,  1.83s/it]predicting train subjects:  33%|███▎      | 173/532 [05:32<10:36,  1.77s/it]predicting train subjects:  33%|███▎      | 174/532 [05:34<10:28,  1.76s/it]predicting train subjects:  33%|███▎      | 175/532 [05:36<10:21,  1.74s/it]predicting train subjects:  33%|███▎      | 176/532 [05:37<10:18,  1.74s/it]predicting train subjects:  33%|███▎      | 177/532 [05:39<10:05,  1.71s/it]predicting train subjects:  33%|███▎      | 178/532 [05:41<10:14,  1.74s/it]predicting train subjects:  34%|███▎      | 179/532 [05:43<10:16,  1.75s/it]predicting train subjects:  34%|███▍      | 180/532 [05:44<10:19,  1.76s/it]predicting train subjects:  34%|███▍      | 181/532 [05:46<10:19,  1.76s/it]predicting train subjects:  34%|███▍      | 182/532 [05:48<10:14,  1.76s/it]predicting train subjects:  34%|███▍      | 183/532 [05:50<10:14,  1.76s/it]predicting train subjects:  35%|███▍      | 184/532 [05:51<10:11,  1.76s/it]predicting train subjects:  35%|███▍      | 185/532 [05:53<10:00,  1.73s/it]predicting train subjects:  35%|███▍      | 186/532 [05:55<09:48,  1.70s/it]predicting train subjects:  35%|███▌      | 187/532 [05:56<09:35,  1.67s/it]predicting train subjects:  35%|███▌      | 188/532 [05:58<09:33,  1.67s/it]predicting train subjects:  36%|███▌      | 189/532 [06:00<09:26,  1.65s/it]predicting train subjects:  36%|███▌      | 190/532 [06:01<09:18,  1.63s/it]predicting train subjects:  36%|███▌      | 191/532 [06:04<10:28,  1.84s/it]predicting train subjects:  36%|███▌      | 192/532 [06:06<11:19,  2.00s/it]predicting train subjects:  36%|███▋      | 193/532 [06:08<11:55,  2.11s/it]predicting train subjects:  36%|███▋      | 194/532 [06:11<12:16,  2.18s/it]predicting train subjects:  37%|███▋      | 195/532 [06:13<12:32,  2.23s/it]predicting train subjects:  37%|███▋      | 196/532 [06:15<12:44,  2.28s/it]predicting train subjects:  37%|███▋      | 197/532 [06:17<12:27,  2.23s/it]predicting train subjects:  37%|███▋      | 198/532 [06:20<12:20,  2.22s/it]predicting train subjects:  37%|███▋      | 199/532 [06:22<11:59,  2.16s/it]predicting train subjects:  38%|███▊      | 200/532 [06:24<11:44,  2.12s/it]predicting train subjects:  38%|███▊      | 201/532 [06:26<11:33,  2.10s/it]predicting train subjects:  38%|███▊      | 202/532 [06:28<11:30,  2.09s/it]predicting train subjects:  38%|███▊      | 203/532 [06:30<11:01,  2.01s/it]predicting train subjects:  38%|███▊      | 204/532 [06:32<10:44,  1.96s/it]predicting train subjects:  39%|███▊      | 205/532 [06:33<10:20,  1.90s/it]predicting train subjects:  39%|███▊      | 206/532 [06:35<10:00,  1.84s/it]predicting train subjects:  39%|███▉      | 207/532 [06:37<09:55,  1.83s/it]predicting train subjects:  39%|███▉      | 208/532 [06:39<09:48,  1.82s/it]predicting train subjects:  39%|███▉      | 209/532 [06:40<09:27,  1.76s/it]predicting train subjects:  39%|███▉      | 210/532 [06:42<09:07,  1.70s/it]predicting train subjects:  40%|███▉      | 211/532 [06:43<08:55,  1.67s/it]predicting train subjects:  40%|███▉      | 212/532 [06:45<08:46,  1.65s/it]predicting train subjects:  40%|████      | 213/532 [06:46<08:35,  1.62s/it]predicting train subjects:  40%|████      | 214/532 [06:48<08:21,  1.58s/it]predicting train subjects:  40%|████      | 215/532 [06:50<09:18,  1.76s/it]predicting train subjects:  41%|████      | 216/532 [06:52<09:49,  1.86s/it]predicting train subjects:  41%|████      | 217/532 [06:55<10:26,  1.99s/it]predicting train subjects:  41%|████      | 218/532 [06:57<10:42,  2.04s/it]predicting train subjects:  41%|████      | 219/532 [06:59<11:03,  2.12s/it]predicting train subjects:  41%|████▏     | 220/532 [07:01<11:14,  2.16s/it]predicting train subjects:  42%|████▏     | 221/532 [07:03<10:28,  2.02s/it]predicting train subjects:  42%|████▏     | 222/532 [07:05<09:47,  1.89s/it]predicting train subjects:  42%|████▏     | 223/532 [07:06<09:21,  1.82s/it]predicting train subjects:  42%|████▏     | 224/532 [07:08<08:56,  1.74s/it]predicting train subjects:  42%|████▏     | 225/532 [07:09<08:42,  1.70s/it]predicting train subjects:  42%|████▏     | 226/532 [07:11<08:31,  1.67s/it]predicting train subjects:  43%|████▎     | 227/532 [07:13<08:18,  1.63s/it]predicting train subjects:  43%|████▎     | 228/532 [07:14<08:23,  1.66s/it]predicting train subjects:  43%|████▎     | 229/532 [07:16<08:27,  1.68s/it]predicting train subjects:  43%|████▎     | 230/532 [07:18<08:22,  1.66s/it]predicting train subjects:  43%|████▎     | 231/532 [07:19<08:17,  1.65s/it]predicting train subjects:  44%|████▎     | 232/532 [07:21<08:21,  1.67s/it]predicting train subjects:  44%|████▍     | 233/532 [07:23<08:35,  1.72s/it]predicting train subjects:  44%|████▍     | 234/532 [07:25<08:55,  1.80s/it]predicting train subjects:  44%|████▍     | 235/532 [07:27<09:05,  1.84s/it]predicting train subjects:  44%|████▍     | 236/532 [07:29<09:16,  1.88s/it]predicting train subjects:  45%|████▍     | 237/532 [07:30<09:09,  1.86s/it]predicting train subjects:  45%|████▍     | 238/532 [07:32<09:11,  1.88s/it]predicting train subjects:  45%|████▍     | 239/532 [07:34<09:29,  1.94s/it]predicting train subjects:  45%|████▌     | 240/532 [07:37<09:39,  1.99s/it]predicting train subjects:  45%|████▌     | 241/532 [07:39<09:42,  2.00s/it]predicting train subjects:  45%|████▌     | 242/532 [07:41<09:47,  2.03s/it]predicting train subjects:  46%|████▌     | 243/532 [07:43<09:46,  2.03s/it]predicting train subjects:  46%|████▌     | 244/532 [07:45<09:44,  2.03s/it]predicting train subjects:  46%|████▌     | 245/532 [07:46<09:09,  1.92s/it]predicting train subjects:  46%|████▌     | 246/532 [07:48<08:54,  1.87s/it]predicting train subjects:  46%|████▋     | 247/532 [07:50<08:34,  1.80s/it]predicting train subjects:  47%|████▋     | 248/532 [07:51<08:17,  1.75s/it]predicting train subjects:  47%|████▋     | 249/532 [07:53<08:01,  1.70s/it]predicting train subjects:  47%|████▋     | 250/532 [07:55<07:52,  1.67s/it]predicting train subjects:  47%|████▋     | 251/532 [07:56<07:55,  1.69s/it]predicting train subjects:  47%|████▋     | 252/532 [07:58<07:52,  1.69s/it]predicting train subjects:  48%|████▊     | 253/532 [08:00<07:54,  1.70s/it]predicting train subjects:  48%|████▊     | 254/532 [08:02<07:54,  1.71s/it]predicting train subjects:  48%|████▊     | 255/532 [08:03<07:52,  1.70s/it]predicting train subjects:  48%|████▊     | 256/532 [08:05<08:00,  1.74s/it]predicting train subjects:  48%|████▊     | 257/532 [08:07<08:32,  1.86s/it]predicting train subjects:  48%|████▊     | 258/532 [08:09<08:53,  1.95s/it]predicting train subjects:  49%|████▊     | 259/532 [08:12<09:17,  2.04s/it]predicting train subjects:  49%|████▉     | 260/532 [08:14<09:28,  2.09s/it]predicting train subjects:  49%|████▉     | 261/532 [08:16<09:31,  2.11s/it]predicting train subjects:  49%|████▉     | 262/532 [08:18<09:34,  2.13s/it]predicting train subjects:  49%|████▉     | 263/532 [08:20<08:56,  1.99s/it]predicting train subjects:  50%|████▉     | 264/532 [08:21<08:27,  1.89s/it]predicting train subjects:  50%|████▉     | 265/532 [08:23<08:03,  1.81s/it]predicting train subjects:  50%|█████     | 266/532 [08:25<07:49,  1.76s/it]predicting train subjects:  50%|█████     | 267/532 [08:26<07:33,  1.71s/it]predicting train subjects:  50%|█████     | 268/532 [08:28<07:21,  1.67s/it]predicting train subjects:  51%|█████     | 269/532 [08:30<07:46,  1.77s/it]predicting train subjects:  51%|█████     | 270/532 [08:32<07:55,  1.82s/it]predicting train subjects:  51%|█████     | 271/532 [08:34<07:59,  1.84s/it]predicting train subjects:  51%|█████     | 272/532 [08:36<08:08,  1.88s/it]predicting train subjects:  51%|█████▏    | 273/532 [08:38<08:14,  1.91s/it]predicting train subjects:  52%|█████▏    | 274/532 [08:40<08:12,  1.91s/it]predicting train subjects:  52%|█████▏    | 275/532 [08:42<08:50,  2.06s/it]predicting train subjects:  52%|█████▏    | 276/532 [08:44<09:08,  2.14s/it]predicting train subjects:  52%|█████▏    | 277/532 [08:47<09:18,  2.19s/it]predicting train subjects:  52%|█████▏    | 278/532 [08:49<09:19,  2.20s/it]predicting train subjects:  52%|█████▏    | 279/532 [08:51<09:25,  2.24s/it]predicting train subjects:  53%|█████▎    | 280/532 [08:54<09:31,  2.27s/it]predicting train subjects:  53%|█████▎    | 281/532 [08:56<09:22,  2.24s/it]predicting train subjects:  53%|█████▎    | 282/532 [08:58<09:19,  2.24s/it]predicting train subjects:  53%|█████▎    | 283/532 [09:00<09:15,  2.23s/it]predicting train subjects:  53%|█████▎    | 284/532 [09:02<09:07,  2.21s/it]predicting train subjects:  54%|█████▎    | 285/532 [09:04<09:03,  2.20s/it]predicting train subjects:  54%|█████▍    | 286/532 [09:07<09:11,  2.24s/it]predicting train subjects:  54%|█████▍    | 287/532 [09:09<08:44,  2.14s/it]predicting train subjects:  54%|█████▍    | 288/532 [09:11<08:29,  2.09s/it]predicting train subjects:  54%|█████▍    | 289/532 [09:13<08:39,  2.14s/it]predicting train subjects:  55%|█████▍    | 290/532 [09:15<08:28,  2.10s/it]predicting train subjects:  55%|█████▍    | 291/532 [09:17<08:18,  2.07s/it]predicting train subjects:  55%|█████▍    | 292/532 [09:19<08:23,  2.10s/it]predicting train subjects:  55%|█████▌    | 293/532 [09:21<08:16,  2.08s/it]predicting train subjects:  55%|█████▌    | 294/532 [09:23<08:24,  2.12s/it]predicting train subjects:  55%|█████▌    | 295/532 [09:26<08:26,  2.14s/it]predicting train subjects:  56%|█████▌    | 296/532 [09:28<08:41,  2.21s/it]predicting train subjects:  56%|█████▌    | 297/532 [09:30<08:54,  2.27s/it]predicting train subjects:  56%|█████▌    | 298/532 [09:32<08:41,  2.23s/it]predicting train subjects:  56%|█████▌    | 299/532 [09:34<08:11,  2.11s/it]predicting train subjects:  56%|█████▋    | 300/532 [09:36<07:43,  2.00s/it]predicting train subjects:  57%|█████▋    | 301/532 [09:38<07:24,  1.93s/it]predicting train subjects:  57%|█████▋    | 302/532 [09:40<07:23,  1.93s/it]predicting train subjects:  57%|█████▋    | 303/532 [09:42<07:15,  1.90s/it]predicting train subjects:  57%|█████▋    | 304/532 [09:43<07:13,  1.90s/it]predicting train subjects:  57%|█████▋    | 305/532 [09:46<08:20,  2.20s/it]predicting train subjects:  58%|█████▊    | 306/532 [09:49<09:00,  2.39s/it]predicting train subjects:  58%|█████▊    | 307/532 [09:52<09:40,  2.58s/it]predicting train subjects:  58%|█████▊    | 308/532 [09:55<09:46,  2.62s/it]predicting train subjects:  58%|█████▊    | 309/532 [09:58<10:04,  2.71s/it]predicting train subjects:  58%|█████▊    | 310/532 [10:01<10:12,  2.76s/it]predicting train subjects:  58%|█████▊    | 311/532 [10:05<11:18,  3.07s/it]predicting train subjects:  59%|█████▊    | 312/532 [10:08<11:51,  3.23s/it]predicting train subjects:  59%|█████▉    | 313/532 [10:12<12:09,  3.33s/it]predicting train subjects:  59%|█████▉    | 314/532 [10:15<12:04,  3.32s/it]predicting train subjects:  59%|█████▉    | 315/532 [10:19<12:40,  3.50s/it]predicting train subjects:  59%|█████▉    | 316/532 [10:22<12:14,  3.40s/it]predicting train subjects:  60%|█████▉    | 317/532 [10:24<10:38,  2.97s/it]predicting train subjects:  60%|█████▉    | 318/532 [10:26<09:32,  2.67s/it]predicting train subjects:  60%|█████▉    | 319/532 [10:28<08:55,  2.51s/it]predicting train subjects:  60%|██████    | 320/532 [10:30<08:20,  2.36s/it]predicting train subjects:  60%|██████    | 321/532 [10:32<07:56,  2.26s/it]predicting train subjects:  61%|██████    | 322/532 [10:34<07:38,  2.18s/it]predicting train subjects:  61%|██████    | 323/532 [10:37<08:30,  2.44s/it]predicting train subjects:  61%|██████    | 324/532 [10:40<09:02,  2.61s/it]predicting train subjects:  61%|██████    | 325/532 [10:43<09:11,  2.67s/it]predicting train subjects:  61%|██████▏   | 326/532 [10:46<09:05,  2.65s/it]predicting train subjects:  61%|██████▏   | 327/532 [10:49<09:20,  2.73s/it]predicting train subjects:  62%|██████▏   | 328/532 [10:52<09:37,  2.83s/it]predicting train subjects:  62%|██████▏   | 329/532 [10:54<09:07,  2.70s/it]predicting train subjects:  62%|██████▏   | 330/532 [10:56<08:41,  2.58s/it]predicting train subjects:  62%|██████▏   | 331/532 [10:59<08:25,  2.52s/it]predicting train subjects:  62%|██████▏   | 332/532 [11:01<08:05,  2.43s/it]predicting train subjects:  63%|██████▎   | 333/532 [11:03<07:55,  2.39s/it]predicting train subjects:  63%|██████▎   | 334/532 [11:05<07:39,  2.32s/it]predicting train subjects:  63%|██████▎   | 335/532 [11:08<07:48,  2.38s/it]predicting train subjects:  63%|██████▎   | 336/532 [11:11<08:03,  2.46s/it]predicting train subjects:  63%|██████▎   | 337/532 [11:13<08:02,  2.47s/it]predicting train subjects:  64%|██████▎   | 338/532 [11:16<07:59,  2.47s/it]predicting train subjects:  64%|██████▎   | 339/532 [11:18<08:25,  2.62s/it]predicting train subjects:  64%|██████▍   | 340/532 [11:21<08:10,  2.55s/it]predicting train subjects:  64%|██████▍   | 341/532 [11:23<07:38,  2.40s/it]predicting train subjects:  64%|██████▍   | 342/532 [11:25<07:22,  2.33s/it]predicting train subjects:  64%|██████▍   | 343/532 [11:27<07:20,  2.33s/it]predicting train subjects:  65%|██████▍   | 344/532 [11:30<07:42,  2.46s/it]predicting train subjects:  65%|██████▍   | 345/532 [11:33<07:51,  2.52s/it]predicting train subjects:  65%|██████▌   | 346/532 [11:35<07:38,  2.46s/it]predicting train subjects:  65%|██████▌   | 347/532 [11:38<07:59,  2.59s/it]predicting train subjects:  65%|██████▌   | 348/532 [11:41<07:56,  2.59s/it]predicting train subjects:  66%|██████▌   | 349/532 [11:43<07:46,  2.55s/it]predicting train subjects:  66%|██████▌   | 350/532 [11:46<07:42,  2.54s/it]predicting train subjects:  66%|██████▌   | 351/532 [11:48<07:30,  2.49s/it]predicting train subjects:  66%|██████▌   | 352/532 [11:51<07:34,  2.52s/it]predicting train subjects:  66%|██████▋   | 353/532 [11:53<07:23,  2.48s/it]predicting train subjects:  67%|██████▋   | 354/532 [11:56<07:31,  2.54s/it]predicting train subjects:  67%|██████▋   | 355/532 [11:59<07:57,  2.70s/it]predicting train subjects:  67%|██████▋   | 356/532 [12:02<08:12,  2.80s/it]predicting train subjects:  67%|██████▋   | 357/532 [12:05<08:10,  2.80s/it]predicting train subjects:  67%|██████▋   | 358/532 [12:07<08:03,  2.78s/it]predicting train subjects:  67%|██████▋   | 359/532 [12:09<07:29,  2.60s/it]predicting train subjects:  68%|██████▊   | 360/532 [12:12<07:01,  2.45s/it]predicting train subjects:  68%|██████▊   | 361/532 [12:13<06:32,  2.29s/it]predicting train subjects:  68%|██████▊   | 362/532 [12:16<06:27,  2.28s/it]predicting train subjects:  68%|██████▊   | 363/532 [12:18<06:14,  2.21s/it]predicting train subjects:  68%|██████▊   | 364/532 [12:20<06:09,  2.20s/it]predicting train subjects:  69%|██████▊   | 365/532 [12:23<06:24,  2.30s/it]predicting train subjects:  69%|██████▉   | 366/532 [12:25<06:10,  2.23s/it]predicting train subjects:  69%|██████▉   | 367/532 [12:27<06:18,  2.29s/it]predicting train subjects:  69%|██████▉   | 368/532 [12:29<06:01,  2.21s/it]predicting train subjects:  69%|██████▉   | 369/532 [12:32<06:21,  2.34s/it]predicting train subjects:  70%|██████▉   | 370/532 [12:34<06:22,  2.36s/it]predicting train subjects:  70%|██████▉   | 371/532 [12:37<07:00,  2.61s/it]predicting train subjects:  70%|██████▉   | 372/532 [12:40<07:20,  2.75s/it]predicting train subjects:  70%|███████   | 373/532 [12:44<08:05,  3.05s/it]predicting train subjects:  70%|███████   | 374/532 [12:48<08:20,  3.17s/it]predicting train subjects:  70%|███████   | 375/532 [12:50<07:53,  3.02s/it]predicting train subjects:  71%|███████   | 376/532 [12:53<07:53,  3.04s/it]predicting train subjects:  71%|███████   | 377/532 [12:56<07:19,  2.83s/it]predicting train subjects:  71%|███████   | 378/532 [12:58<07:03,  2.75s/it]predicting train subjects:  71%|███████   | 379/532 [13:01<06:52,  2.69s/it]predicting train subjects:  71%|███████▏  | 380/532 [13:04<06:55,  2.74s/it]predicting train subjects:  72%|███████▏  | 381/532 [13:06<06:51,  2.73s/it]predicting train subjects:  72%|███████▏  | 382/532 [13:09<07:02,  2.82s/it]predicting train subjects:  72%|███████▏  | 383/532 [13:12<06:49,  2.75s/it]predicting train subjects:  72%|███████▏  | 384/532 [13:15<06:57,  2.82s/it]predicting train subjects:  72%|███████▏  | 385/532 [13:18<07:11,  2.93s/it]predicting train subjects:  73%|███████▎  | 386/532 [13:21<06:56,  2.85s/it]predicting train subjects:  73%|███████▎  | 387/532 [13:23<06:46,  2.80s/it]predicting train subjects:  73%|███████▎  | 388/532 [13:26<06:30,  2.71s/it]predicting train subjects:  73%|███████▎  | 389/532 [13:29<06:21,  2.67s/it]predicting train subjects:  73%|███████▎  | 390/532 [13:31<06:14,  2.64s/it]predicting train subjects:  73%|███████▎  | 391/532 [13:34<06:41,  2.85s/it]predicting train subjects:  74%|███████▎  | 392/532 [13:37<06:33,  2.81s/it]predicting train subjects:  74%|███████▍  | 393/532 [13:40<06:16,  2.71s/it]predicting train subjects:  74%|███████▍  | 394/532 [13:42<06:13,  2.71s/it]predicting train subjects:  74%|███████▍  | 395/532 [13:45<06:16,  2.74s/it]predicting train subjects:  74%|███████▍  | 396/532 [13:48<06:07,  2.70s/it]predicting train subjects:  75%|███████▍  | 397/532 [13:50<05:59,  2.67s/it]predicting train subjects:  75%|███████▍  | 398/532 [13:53<05:53,  2.64s/it]predicting train subjects:  75%|███████▌  | 399/532 [13:55<05:42,  2.57s/it]predicting train subjects:  75%|███████▌  | 400/532 [13:58<05:27,  2.48s/it]predicting train subjects:  75%|███████▌  | 401/532 [14:00<05:31,  2.53s/it]predicting train subjects:  76%|███████▌  | 402/532 [14:03<05:40,  2.62s/it]predicting train subjects:  76%|███████▌  | 403/532 [14:06<05:41,  2.65s/it]predicting train subjects:  76%|███████▌  | 404/532 [14:09<05:45,  2.70s/it]predicting train subjects:  76%|███████▌  | 405/532 [14:11<05:40,  2.68s/it]predicting train subjects:  76%|███████▋  | 406/532 [14:14<05:30,  2.62s/it]predicting train subjects:  77%|███████▋  | 407/532 [14:16<05:29,  2.64s/it]predicting train subjects:  77%|███████▋  | 408/532 [14:19<05:34,  2.69s/it]predicting train subjects:  77%|███████▋  | 409/532 [14:22<05:24,  2.64s/it]predicting train subjects:  77%|███████▋  | 410/532 [14:24<05:14,  2.58s/it]predicting train subjects:  77%|███████▋  | 411/532 [14:27<05:03,  2.50s/it]predicting train subjects:  77%|███████▋  | 412/532 [14:29<04:56,  2.47s/it]predicting train subjects:  78%|███████▊  | 413/532 [14:31<04:45,  2.40s/it]predicting train subjects:  78%|███████▊  | 414/532 [14:33<04:36,  2.34s/it]predicting train subjects:  78%|███████▊  | 415/532 [14:35<04:21,  2.24s/it]predicting train subjects:  78%|███████▊  | 416/532 [14:38<04:19,  2.24s/it]predicting train subjects:  78%|███████▊  | 417/532 [14:40<04:14,  2.21s/it]predicting train subjects:  79%|███████▊  | 418/532 [14:42<04:09,  2.19s/it]predicting train subjects:  79%|███████▉  | 419/532 [14:44<04:18,  2.29s/it]predicting train subjects:  79%|███████▉  | 420/532 [14:47<04:26,  2.38s/it]predicting train subjects:  79%|███████▉  | 421/532 [14:50<04:28,  2.42s/it]predicting train subjects:  79%|███████▉  | 422/532 [14:52<04:38,  2.53s/it]