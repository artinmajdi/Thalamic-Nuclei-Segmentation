2020-01-22 14:57:04.781465: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-22 14:57:05.684370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-22 14:57:05.684433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 14:57:06.115864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 14:57:06.115931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 14:57:06.115943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 14:57:06.116420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/26 [00:00<?, ?it/s]Loading train:   4%|▍         | 1/26 [00:01<00:29,  1.18s/it]Loading train:   8%|▊         | 2/26 [00:02<00:30,  1.28s/it]Loading train:  12%|█▏        | 3/26 [00:03<00:25,  1.11s/it]Loading train:  15%|█▌        | 4/26 [00:04<00:22,  1.03s/it]Loading train:  19%|█▉        | 5/26 [00:05<00:20,  1.04it/s]Loading train:  23%|██▎       | 6/26 [00:05<00:18,  1.06it/s]Loading train:  27%|██▋       | 7/26 [00:06<00:17,  1.10it/s]Loading train:  31%|███       | 8/26 [00:07<00:17,  1.02it/s]Loading train:  35%|███▍      | 9/26 [00:09<00:17,  1.01s/it]Loading train:  38%|███▊      | 10/26 [00:09<00:15,  1.01it/s]Loading train:  42%|████▏     | 11/26 [00:10<00:13,  1.12it/s]Loading train:  46%|████▌     | 12/26 [00:11<00:11,  1.22it/s]Loading train:  50%|█████     | 13/26 [00:12<00:11,  1.14it/s]Loading train:  54%|█████▍    | 14/26 [00:13<00:10,  1.19it/s]Loading train:  58%|█████▊    | 15/26 [00:13<00:09,  1.20it/s]Loading train:  62%|██████▏   | 16/26 [00:14<00:07,  1.29it/s]Loading train:  65%|██████▌   | 17/26 [00:15<00:06,  1.30it/s]Loading train:  69%|██████▉   | 18/26 [00:15<00:05,  1.34it/s]Loading train:  73%|███████▎  | 19/26 [00:16<00:05,  1.36it/s]Loading train:  77%|███████▋  | 20/26 [00:17<00:04,  1.44it/s]Loading train:  81%|████████  | 21/26 [00:18<00:04,  1.22it/s]Loading train:  85%|████████▍ | 22/26 [00:19<00:03,  1.25it/s]Loading train:  88%|████████▊ | 23/26 [00:19<00:02,  1.29it/s]Loading train:  92%|█████████▏| 24/26 [00:20<00:01,  1.29it/s]Loading train:  96%|█████████▌| 25/26 [00:21<00:00,  1.33it/s]Loading train: 100%|██████████| 26/26 [00:21<00:00,  1.42it/s]Loading train: 100%|██████████| 26/26 [00:21<00:00,  1.19it/s]
concatenating: train:   0%|          | 0/26 [00:00<?, ?it/s]concatenating: train:  19%|█▉        | 5/26 [00:00<00:00, 43.79it/s]concatenating: train:  38%|███▊      | 10/26 [00:00<00:00, 43.46it/s]concatenating: train:  58%|█████▊    | 15/26 [00:00<00:00, 44.30it/s]concatenating: train:  77%|███████▋  | 20/26 [00:00<00:00, 45.50it/s]concatenating: train:  96%|█████████▌| 25/26 [00:00<00:00, 44.81it/s]concatenating: train: 100%|██████████| 26/26 [00:00<00:00, 44.88it/s]
Loading test:   0%|          | 0/8 [00:00<?, ?it/s]Loading test:  12%|█▎        | 1/8 [00:00<00:05,  1.23it/s]Loading test:  25%|██▌       | 2/8 [00:01<00:04,  1.31it/s]Loading test:  38%|███▊      | 3/8 [00:02<00:04,  1.22it/s]Loading test:  50%|█████     | 4/8 [00:02<00:02,  1.37it/s]Loading test:  62%|██████▎   | 5/8 [00:03<00:02,  1.31it/s]Loading test:  75%|███████▌  | 6/8 [00:04<00:01,  1.30it/s]Loading test:  88%|████████▊ | 7/8 [00:05<00:00,  1.38it/s]Loading test: 100%|██████████| 8/8 [00:06<00:00,  1.26it/s]Loading test: 100%|██████████| 8/8 [00:06<00:00,  1.30it/s]
concatenating: validation:   0%|          | 0/8 [00:00<?, ?it/s]concatenating: validation:  62%|██████▎   | 5/8 [00:00<00:00, 44.44it/s]concatenating: validation: 100%|██████████| 8/8 [00:00<00:00, 50.53it/s]
Loading trainS:   0%|          | 0/26 [00:00<?, ?it/s]Loading trainS:   4%|▍         | 1/26 [00:00<00:19,  1.28it/s]Loading trainS:   8%|▊         | 2/26 [00:01<00:20,  1.15it/s]Loading trainS:  12%|█▏        | 3/26 [00:02<00:18,  1.22it/s]Loading trainS:  15%|█▌        | 4/26 [00:03<00:17,  1.26it/s]Loading trainS:  19%|█▉        | 5/26 [00:04<00:16,  1.29it/s]Loading trainS:  23%|██▎       | 6/26 [00:04<00:15,  1.29it/s]Loading trainS:  27%|██▋       | 7/26 [00:05<00:15,  1.22it/s]Loading trainS:  31%|███       | 8/26 [00:06<00:15,  1.15it/s]Loading trainS:  35%|███▍      | 9/26 [00:07<00:14,  1.20it/s]Loading trainS:  38%|███▊      | 10/26 [00:08<00:13,  1.23it/s]Loading trainS:  42%|████▏     | 11/26 [00:09<00:12,  1.21it/s]Loading trainS:  46%|████▌     | 12/26 [00:09<00:11,  1.22it/s]Loading trainS:  50%|█████     | 13/26 [00:10<00:10,  1.20it/s]Loading trainS:  54%|█████▍    | 14/26 [00:11<00:09,  1.27it/s]Loading trainS:  58%|█████▊    | 15/26 [00:12<00:08,  1.23it/s]Loading trainS:  62%|██████▏   | 16/26 [00:13<00:07,  1.28it/s]Loading trainS:  65%|██████▌   | 17/26 [00:13<00:06,  1.30it/s]Loading trainS:  69%|██████▉   | 18/26 [00:14<00:05,  1.38it/s]Loading trainS:  73%|███████▎  | 19/26 [00:15<00:05,  1.39it/s]Loading trainS:  77%|███████▋  | 20/26 [00:15<00:04,  1.34it/s]Loading trainS:  81%|████████  | 21/26 [00:16<00:04,  1.24it/s]Loading trainS:  85%|████████▍ | 22/26 [00:17<00:03,  1.26it/s]Loading trainS:  88%|████████▊ | 23/26 [00:18<00:02,  1.31it/s]Loading trainS:  92%|█████████▏| 24/26 [00:18<00:01,  1.35it/s]Loading trainS:  96%|█████████▌| 25/26 [00:19<00:00,  1.29it/s]Loading trainS: 100%|██████████| 26/26 [00:20<00:00,  1.31it/s]Loading trainS: 100%|██████████| 26/26 [00:20<00:00,  1.26it/s]
Loading testS:   0%|          | 0/8 [00:00<?, ?it/s]Loading testS:  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Loading testS:  25%|██▌       | 2/8 [00:01<00:04,  1.36it/s]Loading testS:  38%|███▊      | 3/8 [00:02<00:03,  1.28it/s]Loading testS:  50%|█████     | 4/8 [00:02<00:02,  1.51it/s]Loading testS:  62%|██████▎   | 5/8 [00:03<00:02,  1.39it/s]Loading testS:  75%|███████▌  | 6/8 [00:04<00:01,  1.39it/s]Loading testS:  88%|████████▊ | 7/8 [00:04<00:00,  1.46it/s]Loading testS: 100%|██████████| 8/8 [00:05<00:00,  1.44it/s]Loading testS: 100%|██████████| 8/8 [00:05<00:00,  1.43it/s]----------+++ 
CrossVal ['d']
CrossVal ['d']
(0/8) test vimp2_L_ET_CSFn2
(1/8) test vimp2_ANON972_MS_CSFn2
(2/8) test vimp2_M_ET_CSFn2
(3/8) test vimp2_ctrl_921_07122013_MP
(4/8) test vimp2_ANON967_MS_CSFn2
(5/8) test vimp2_N_ET_CSFn2
(6/8) test vimp2_K_ET_CSFn2
(7/8) test vimp2_ANON988_MS_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 112, 120, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 112, 120, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 112, 120, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 112, 120, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 120, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 120, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 120, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 60, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 60, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 56, 60, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 56, 60, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 56, 60, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 60, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 60, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 60, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 60, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 30, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 30, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 30, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 30, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 30, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 30, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 30, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 30, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 30, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 28, 30, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 56, 60, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 56, 60, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 56, 60, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 56, 60, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 56, 60, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 56, 60, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 56, 60, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 56, 60, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 56, 60, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 56, 60, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 112, 120, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 112, 120, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 112, 120, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 112, 120, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 112, 120, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 112, 120, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 112, 120, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 112, 120, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 112, 120, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 112, 120, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 112, 120, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97530947 0.02469053]
Train on 1677 samples, validate on 496 samples
Epoch 1/300
 - 14s - loss: 0.2077 - acc: 0.9772 - mDice: 0.5968 - val_loss: 0.2688 - val_acc: 0.9894 - val_mDice: 0.4264


Epoch 00001: val_mDice improved from -inf to 0.42640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
Traceback (most recent call last):
  File "main.py", line 1956, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1933, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1883, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 104, in Loop_Over_Nuclei
    Run_Main(UserI)
  File "main.py", line 224, in Run_Main
    Loop_slicing_orientations(UserInfoB, InitValues)
  File "main.py", line 222, in Loop_slicing_orientations
    subRun(UserInfoB)
  File "main.py", line 216, in subRun
    else: normal_run(params)
  File "main.py", line 203, in normal_run
    choosingModel.check_Run(params, Data)              
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 53, in check_Run
    model      = trainingExperiment(Data, params) if not params.preprocess.TestOnly else loadModel(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 484, in trainingExperiment
    model, hist = modelTrain_Unet(Data, params, model)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 456, in modelTrain_Unet
    model, hist = modelFit(model)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 452, in modelFit
    else: hist = func_without_Generator()
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 445, in func_without_Generator
    hist = model.fit(x=Data.Train.Image, y=Data.Train.Mask, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(Data.Validation.Image, Data.Validation.Mask), verbose=verbose, callbacks=callbacks) # , callbacks=[TQDMCallback()])        
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/keras/engine/training.py", line 1039, in fit
    validation_steps=validation_steps)
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/keras/engine/training_arrays.py", line 199, in fit_loop
    outs = f(ins_batch)
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2715, in __call__
    return self._call(inputs)
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1439, in __call__
    run_metadata_ptr)
KeyboardInterrupt
