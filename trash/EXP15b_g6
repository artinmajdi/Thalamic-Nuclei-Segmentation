*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/285) train vimp2_ANON967_CSFn2
(1/285) train vimp2_E_CSFn2
(2/285) train vimp2_G_CSFn2
(3/285) train vimp2_J_CSFn2
(4/285) train vimp2_K_CSFn2
(5/285) train vimp2_L_CSFn2
(6/285) train vimp2_ANON911_CSFn2
(7/285) train vimp2_D_CSFn2
(8/285) train vimp2_F_CSFn2
(9/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd0
(10/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd1
(11/285) train vimp2_ANON911_CSFn_Aug0_Rot_7_sd2
(12/285) train vimp2_ANON911_CSFn_Aug1_Rot_1_sd2
(13/285) train vimp2_ANON911_CSFn_Aug1_Rot_3_sd1
(14/285) train vimp2_ANON911_CSFn_Aug1_Rot_4_sd0
(15/285) train vimp2_ANON911_CSFn_Aug2_Rot_1_sd1
(16/285) train vimp2_ANON911_CSFn_Aug2_Rot_2_sd2
(17/285) train vimp2_ANON911_CSFn_Aug2_Rot_5_sd0
(18/285) train vimp2_ANON911_CSFn_Aug3_Rot_-4_sd0
(19/285) train vimp2_ANON911_CSFn_Aug3_Rot_6_sd1
(20/285) train vimp2_ANON911_CSFn_Aug3_Rot_7_sd2
(21/285) train vimp2_ANON911_CSFn_Aug4_Rot_-2_sd1
(22/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd0
(23/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd2
(24/285) train vimp2_ANON911_CSFn_Aug5_Rot_1_sd0
(25/285) train vimp2_ANON911_CSFn_Aug5_Rot_-6_sd1
(26/285) train vimp2_ANON911_CSFn_Aug5_Rot_7_sd2
(27/285) train vimp2_D_CSFn_Aug0_Rot_4_sd0
(28/285) train vimp2_D_CSFn_Aug0_Rot_-5_sd2
(29/285) train vimp2_D_CSFn_Aug0_Rot_7_sd1
(30/285) train vimp2_D_CSFn_Aug1_Rot_-2_sd1
(31/285) train vimp2_D_CSFn_Aug1_Rot_-4_sd0
(32/285) train vimp2_D_CSFn_Aug1_Rot_5_sd2
(33/285) train vimp2_D_CSFn_Aug2_Rot_1_sd1
(34/285) train vimp2_D_CSFn_Aug2_Rot_-1_sd2
(35/285) train vimp2_D_CSFn_Aug2_Rot_-5_sd0
(36/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd0
(37/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd1
(38/285) train vimp2_D_CSFn_Aug3_Rot_3_sd2
(39/285) train vimp2_D_CSFn_Aug4_Rot_-4_sd0
(40/285) train vimp2_D_CSFn_Aug4_Rot_-5_sd2
(41/285) train vimp2_D_CSFn_Aug4_Rot_6_sd1
(42/285) train vimp2_D_CSFn_Aug5_Rot_1_sd2
(43/285) train vimp2_D_CSFn_Aug5_Rot_2_sd0
(44/285) train vimp2_D_CSFn_Aug5_Rot_7_sd1
(45/285) train vimp2_F_CSFn_Aug0_Rot_-1_sd2
(46/285) train vimp2_F_CSFn_Aug0_Rot_5_sd0
(47/285) train vimp2_F_CSFn_Aug0_Rot_7_sd1
(48/285) train vimp2_F_CSFn_Aug1_Rot_-5_sd2
(49/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd0
(50/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd1
(51/285) train vimp2_F_CSFn_Aug2_Rot_-3_sd1
(52/285) train vimp2_F_CSFn_Aug2_Rot_4_sd0
(53/285) train vimp2_F_CSFn_Aug2_Rot_-7_sd2
(54/285) train vimp2_F_CSFn_Aug3_Rot_2_sd0
(55/285) train vimp2_F_CSFn_Aug3_Rot_-3_sd2
(56/285) train vimp2_F_CSFn_Aug3_Rot_-7_sd1
(57/285) train vimp2_F_CSFn_Aug4_Rot_-1_sd0
(58/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd1
(59/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd2
(60/285) train vimp2_F_CSFn_Aug5_Rot_-1_sd1
(61/285) train vimp2_F_CSFn_Aug5_Rot_4_sd0
(62/285) train vimp2_F_CSFn_Aug5_Rot_-5_sd2
(63/285) train vimp2_ANON972_CSFn2
(64/285) train vimp2_H_CSFn2
(65/285) train vimp2_I_CSFn2
(66/285) train vimp2_ANON972_CSFn_Aug0_Rot_-1_sd1
(67/285) train vimp2_ANON972_CSFn_Aug0_Rot_3_sd0
(68/285) train vimp2_ANON972_CSFn_Aug0_Rot_5_sd2
(69/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd1
(70/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd2
(71/285) train vimp2_ANON972_CSFn_Aug1_Rot_-5_sd0
(72/285) train vimp2_ANON972_CSFn_Aug2_Rot_1_sd1
(73/285) train vimp2_ANON972_CSFn_Aug2_Rot_-5_sd2
(74/285) train vimp2_ANON972_CSFn_Aug2_Rot_7_sd0
(75/285) train vimp2_ANON972_CSFn_Aug3_Rot_-3_sd0
(76/285) train vimp2_ANON972_CSFn_Aug3_Rot_-6_sd1
(77/285) train vimp2_ANON972_CSFn_Aug3_Rot_7_sd2
(78/285) train vimp2_ANON972_CSFn_Aug4_Rot_-4_sd0
(79/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd1
(80/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd2
(81/285) train vimp2_ANON972_CSFn_Aug5_Rot_-1_sd0
(82/285) train vimp2_ANON972_CSFn_Aug5_Rot_2_sd2
(83/285) train vimp2_ANON972_CSFn_Aug5_Rot_-4_sd1
(84/285) train vimp2_H_CSFn_Aug0_Rot_-3_sd2
(85/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd0
(86/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd1
(87/285) train vimp2_H_CSFn_Aug1_Rot_-1_sd2
(88/285) train vimp2_H_CSFn_Aug1_Rot_-2_sd0
(89/285) train vimp2_H_CSFn_Aug1_Rot_-4_sd1
(90/285) train vimp2_H_CSFn_Aug2_Rot_-1_sd1
(91/285) train vimp2_H_CSFn_Aug2_Rot_2_sd2
(92/285) train vimp2_H_CSFn_Aug2_Rot_6_sd0
(93/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd0
(94/285) train vimp2_H_CSFn_Aug3_Rot_1_sd1
(95/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd2
(96/285) train vimp2_H_CSFn_Aug4_Rot_-3_sd0
(97/285) train vimp2_H_CSFn_Aug4_Rot_4_sd1
(98/285) train vimp2_H_CSFn_Aug4_Rot_5_sd2
(99/285) train vimp2_H_CSFn_Aug5_Rot_-1_sd2
(100/285) train vimp2_H_CSFn_Aug5_Rot_-2_sd1
(101/285) train vimp2_H_CSFn_Aug5_Rot_-3_sd0
(102/285) train vimp2_I_CSFn_Aug0_Rot_1_sd1
(103/285) train vimp2_I_CSFn_Aug0_Rot_6_sd2
(104/285) train vimp2_I_CSFn_Aug0_Rot_-7_sd0
(105/285) train vimp2_I_CSFn_Aug1_Rot_2_sd0
(106/285) train vimp2_I_CSFn_Aug1_Rot_-5_sd2
(107/285) train vimp2_I_CSFn_Aug1_Rot_-6_sd1
(108/285) train vimp2_I_CSFn_Aug2_Rot_1_sd1
(109/285) train vimp2_I_CSFn_Aug2_Rot_-6_sd0
(110/285) train vimp2_I_CSFn_Aug2_Rot_-7_sd2
(111/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd0
(112/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd2
(113/285) train vimp2_I_CSFn_Aug3_Rot_-5_sd1
(114/285) train vimp2_I_CSFn_Aug4_Rot_2_sd1
(115/285) train vimp2_I_CSFn_Aug4_Rot_-6_sd0
(116/285) train vimp2_I_CSFn_Aug4_Rot_6_sd2
(117/285) train vimp2_I_CSFn_Aug5_Rot_-2_sd1
(118/285) train vimp2_I_CSFn_Aug5_Rot_3_sd2
(119/285) train vimp2_I_CSFn_Aug5_Rot_5_sd0
(120/285) train vimp2_ANON988_CSFn2
(121/285) train vimp2_M_CSFn2
(122/285) train vimp2_N_CSFn2
(123/285) train vimp2_ANON988_CSFn_Aug0_Rot_2_sd0
(124/285) train vimp2_ANON988_CSFn_Aug0_Rot_-2_sd2
(125/285) train vimp2_ANON988_CSFn_Aug0_Rot_4_sd1
(126/285) train vimp2_ANON988_CSFn_Aug1_Rot_-2_sd0
(127/285) train vimp2_ANON988_CSFn_Aug1_Rot_3_sd1
(128/285) train vimp2_ANON988_CSFn_Aug1_Rot_5_sd2
(129/285) train vimp2_ANON988_CSFn_Aug2_Rot_-6_sd2
(130/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd0
(131/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd1
(132/285) train vimp2_ANON988_CSFn_Aug3_Rot_-5_sd0
(133/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd1
(134/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd2
(135/285) train vimp2_ANON988_CSFn_Aug4_Rot_-1_sd0
(136/285) train vimp2_ANON988_CSFn_Aug4_Rot_2_sd1
(137/285) train vimp2_ANON988_CSFn_Aug4_Rot_-4_sd2
(138/285) train vimp2_ANON988_CSFn_Aug5_Rot_2_sd0
(139/285) train vimp2_ANON988_CSFn_Aug5_Rot_4_sd2
(140/285) train vimp2_ANON988_CSFn_Aug5_Rot_-5_sd1
(141/285) train vimp2_M_CSFn_Aug0_Rot_-1_sd1
(142/285) train vimp2_M_CSFn_Aug0_Rot_1_sd2
(143/285) train vimp2_M_CSFn_Aug0_Rot_-2_sd0
(144/285) train vimp2_M_CSFn_Aug1_Rot_-3_sd2
(145/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd0
(146/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd1
(147/285) train vimp2_M_CSFn_Aug2_Rot_-2_sd1
(148/285) train vimp2_M_CSFn_Aug2_Rot_4_sd2
(149/285) train vimp2_M_CSFn_Aug2_Rot_7_sd0
(150/285) train vimp2_M_CSFn_Aug3_Rot_2_sd1
(151/285) train vimp2_M_CSFn_Aug3_Rot_-3_sd0
(152/285) train vimp2_M_CSFn_Aug3_Rot_4_sd2
(153/285) train vimp2_M_CSFn_Aug4_Rot_4_sd0
(154/285) train vimp2_M_CSFn_Aug4_Rot_-5_sd1
(155/285) train vimp2_M_CSFn_Aug4_Rot_7_sd2
(156/285) train vimp2_M_CSFn_Aug5_Rot_-3_sd2
(157/285) train vimp2_M_CSFn_Aug5_Rot_4_sd1
(158/285) train vimp2_M_CSFn_Aug5_Rot_7_sd0
(159/285) train vimp2_N_CSFn_Aug0_Rot_2_sd0
(160/285) train vimp2_N_CSFn_Aug0_Rot_3_sd1
(161/285) train vimp2_N_CSFn_Aug0_Rot_6_sd2
(162/285) train vimp2_N_CSFn_Aug1_Rot_2_sd2
(163/285) train vimp2_N_CSFn_Aug1_Rot_3_sd1
(164/285) train vimp2_N_CSFn_Aug1_Rot_-4_sd0
(165/285) train vimp2_N_CSFn_Aug2_Rot_-1_sd1
(166/285) train vimp2_N_CSFn_Aug2_Rot_2_sd2
(167/285) train vimp2_N_CSFn_Aug2_Rot_5_sd0
(168/285) train vimp2_N_CSFn_Aug3_Rot_4_sd0
(169/285) train vimp2_N_CSFn_Aug3_Rot_6_sd1
(170/285) train vimp2_N_CSFn_Aug3_Rot_-6_sd2
(171/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd0
(172/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd2
(173/285) train vimp2_N_CSFn_Aug4_Rot_-2_sd1
(174/285) train vimp2_N_CSFn_Aug5_Rot_-1_sd0
(175/285) train vimp2_N_CSFn_Aug5_Rot_5_sd1
(176/285) train vimp2_N_CSFn_Aug5_Rot_-5_sd2
(177/285) train vimp2_ANON967_CSFn_Aug0_Rot_-3_sd0
(178/285) train vimp2_ANON967_CSFn_Aug0_Rot_-5_sd2
(179/285) train vimp2_ANON967_CSFn_Aug0_Rot_7_sd1
(180/285) train vimp2_ANON967_CSFn_Aug1_Rot_1_sd2
(181/285) train 2019-07-05 17:40:17.760133: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-05 17:40:18.367014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:88:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-05 17:40:18.367078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 17:40:18.760538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 17:40:18.760636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 17:40:18.760654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 17:40:18.761275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<08:47,  1.86s/it]Loading train:   1%|          | 2/285 [00:04<09:16,  1.97s/it]Loading train:   1%|          | 3/285 [00:05<08:45,  1.86s/it]Loading train:   1%|▏         | 4/285 [00:07<08:56,  1.91s/it]Loading train:   2%|▏         | 5/285 [00:10<09:34,  2.05s/it]Loading train:   2%|▏         | 6/285 [00:12<10:10,  2.19s/it]Loading train:   2%|▏         | 7/285 [00:15<10:34,  2.28s/it]Loading train:   3%|▎         | 8/285 [00:17<10:19,  2.24s/it]Loading train:   3%|▎         | 9/285 [00:19<10:00,  2.18s/it]Loading train:   4%|▎         | 10/285 [00:21<09:49,  2.14s/it]Loading train:   4%|▍         | 11/285 [00:23<09:16,  2.03s/it]Loading train:   4%|▍         | 12/285 [00:25<09:35,  2.11s/it]Loading train:   5%|▍         | 13/285 [00:27<09:18,  2.05s/it]Loading train:   5%|▍         | 14/285 [00:28<08:23,  1.86s/it]Loading train:   5%|▌         | 15/285 [00:30<08:00,  1.78s/it]vimp2_ANON967_CSFn_Aug1_Rot_3_sd0
(182/285) train vimp2_ANON967_CSFn_Aug1_Rot_-6_sd1
(183/285) train vimp2_ANON967_CSFn_Aug2_Rot_4_sd2
(184/285) train vimp2_ANON967_CSFn_Aug2_Rot_-5_sd0
(185/285) train vimp2_ANON967_CSFn_Aug2_Rot_-7_sd1
(186/285) train vimp2_ANON967_CSFn_Aug3_Rot_4_sd2
(187/285) train vimp2_ANON967_CSFn_Aug3_Rot_5_sd1
(188/285) train vimp2_ANON967_CSFn_Aug3_Rot_-7_sd0
(189/285) train vimp2_ANON967_CSFn_Aug4_Rot_-4_sd0
(190/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd1
(191/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd2
(192/285) train vimp2_ANON967_CSFn_Aug5_Rot_-5_sd1
(193/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd0
(194/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd2
(195/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd0
(196/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd2
(197/285) train vimp2_E_CSFn_Aug0_Rot_6_sd1
(198/285) train vimp2_E_CSFn_Aug1_Rot_0_sd1
(199/285) train vimp2_E_CSFn_Aug1_Rot_1_sd0
(200/285) train vimp2_E_CSFn_Aug1_Rot_4_sd2
(201/285) train vimp2_E_CSFn_Aug2_Rot_2_sd1
(202/285) train vimp2_E_CSFn_Aug2_Rot_-3_sd0
(203/285) train vimp2_E_CSFn_Aug2_Rot_-7_sd2
(204/285) train vimp2_E_CSFn_Aug3_Rot_-1_sd2
(205/285) train vimp2_E_CSFn_Aug3_Rot_5_sd1
(206/285) train vimp2_E_CSFn_Aug3_Rot_7_sd0
(207/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd1
(208/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd2
(209/285) train vimp2_E_CSFn_Aug4_Rot_7_sd0
(210/285) train vimp2_E_CSFn_Aug5_Rot_-1_sd2
(211/285) train vimp2_E_CSFn_Aug5_Rot_6_sd0
(212/285) train vimp2_E_CSFn_Aug5_Rot_-6_sd1
(213/285) train vimp2_G_CSFn_Aug0_Rot_-6_sd1
(214/285) train vimp2_G_CSFn_Aug0_Rot_-7_sd0
(215/285) train vimp2_G_CSFn_Aug0_Rot_7_sd2
(216/285) train vimp2_G_CSFn_Aug1_Rot_2_sd0
(217/285) train vimp2_G_CSFn_Aug1_Rot_-2_sd2
(218/285) train vimp2_G_CSFn_Aug1_Rot_-5_sd1
(219/285) train vimp2_G_CSFn_Aug2_Rot_1_sd0
(220/285) train vimp2_G_CSFn_Aug2_Rot_-1_sd2
(221/285) train vimp2_G_CSFn_Aug2_Rot_4_sd1
(222/285) train vimp2_G_CSFn_Aug3_Rot_-5_sd2
(223/285) train vimp2_G_CSFn_Aug3_Rot_-6_sd0
(224/285) train vimp2_G_CSFn_Aug3_Rot_7_sd1
(225/285) train vimp2_G_CSFn_Aug4_Rot_-2_sd0
(226/285) train vimp2_G_CSFn_Aug4_Rot_2_sd1
(227/285) train vimp2_G_CSFn_Aug4_Rot_-6_sd2
(228/285) train vimp2_G_CSFn_Aug5_Rot_-1_sd2
(229/285) train vimp2_G_CSFn_Aug5_Rot_4_sd0
(230/285) train vimp2_G_CSFn_Aug5_Rot_4_sd1
(231/285) train vimp2_J_CSFn_Aug0_Rot_1_sd1
(232/285) train vimp2_J_CSFn_Aug0_Rot_-3_sd2
(233/285) train vimp2_J_CSFn_Aug0_Rot_-4_sd0
(234/285) train vimp2_J_CSFn_Aug1_Rot_-2_sd2
(235/285) train vimp2_J_CSFn_Aug1_Rot_4_sd0
(236/285) train vimp2_J_CSFn_Aug1_Rot_-6_sd1
(237/285) train vimp2_J_CSFn_Aug2_Rot_4_sd1
(238/285) train vimp2_J_CSFn_Aug2_Rot_-4_sd2
(239/285) train vimp2_J_CSFn_Aug2_Rot_7_sd0
(240/285) train vimp2_J_CSFn_Aug3_Rot_-1_sd1
(241/285) train vimp2_J_CSFn_Aug3_Rot_-2_sd0
(242/285) train vimp2_J_CSFn_Aug3_Rot_-3_sd2
(243/285) train vimp2_J_CSFn_Aug4_Rot_4_sd2
(244/285) train vimp2_J_CSFn_Aug4_Rot_5_sd0
(245/285) train vimp2_J_CSFn_Aug4_Rot_7_sd1
(246/285) train vimp2_J_CSFn_Aug5_Rot_4_sd2
(247/285) train vimp2_J_CSFn_Aug5_Rot_-6_sd1
(248/285) train vimp2_J_CSFn_Aug5_Rot_-7_sd0
(249/285) train vimp2_K_CSFn_Aug0_Rot_-2_sd2
(250/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd0
(251/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd1
(252/285) train vimp2_K_CSFn_Aug1_Rot_1_sd1
(253/285) train vimp2_K_CSFn_Aug1_Rot_1_sd2
(254/285) train vimp2_K_CSFn_Aug1_Rot_-5_sd0
(255/285) train vimp2_K_CSFn_Aug2_Rot_0_sd0
(256/285) train vimp2_K_CSFn_Aug2_Rot_-2_sd1
(257/285) train vimp2_K_CSFn_Aug2_Rot_-5_sd2
(258/285) train vimp2_K_CSFn_Aug3_Rot_5_sd2
(259/285) train vimp2_K_CSFn_Aug3_Rot_6_sd0
(260/285) train vimp2_K_CSFn_Aug3_Rot_-6_sd1
(261/285) train vimp2_K_CSFn_Aug4_Rot_-1_sd0
(262/285) train vimp2_K_CSFn_Aug4_Rot_2_sd2
(263/285) train vimp2_K_CSFn_Aug4_Rot_-6_sd1
(264/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd1
(265/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd2
(266/285) train vimp2_K_CSFn_Aug5_Rot_-3_sd0
(267/285) train vimp2_L_CSFn_Aug0_Rot_4_sd0
(268/285) train vimp2_L_CSFn_Aug0_Rot_5_sd2
(269/285) train vimp2_L_CSFn_Aug0_Rot_-7_sd1
(270/285) train vimp2_L_CSFn_Aug1_Rot_3_sd0
(271/285) train vimp2_L_CSFn_Aug1_Rot_5_sd1
(272/285) train vimp2_L_CSFn_Aug1_Rot_-5_sd2
(273/285) train vimp2_L_CSFn_Aug2_Rot_-4_sd1
(274/285) train vimp2_L_CSFn_Aug2_Rot_5_sd0
(275/285) train vimp2_L_CSFn_Aug2_Rot_-7_sd2
(276/285) train vimp2_L_CSFn_Aug3_Rot_5_sd2
(277/285) train vimp2_L_CSFn_Aug3_Rot_-7_sd0
(278/285) train vimp2_L_CSFn_Aug3_Rot_7_sd1
(279/285) train vimp2_L_CSFn_Aug4_Rot_-1_sd0
(280/285) train vimp2_L_CSFn_Aug4_Rot_3_sd2
(281/285) train vimp2_L_CSFn_Aug4_Rot_-7_sd1
(282/285) train vimp2_L_CSFn_Aug5_Rot_-1_sd0
(283/285) train vimp2_L_CSFn_Aug5_Rot_-5_sd1
(284/285) train vimp2_L_CSFn_Aug5_Rot_6_sd2
(0/3) test vimp2_A_CSFn2
(1/3) test vimp2_ANON765_CSFn2
(2/3) test vimp2_B_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Traceback (most recent call last):
  File "main.py", line 584, in <module>
    print(e)
  File "main.py", line 581, in EXP15b_TL_CSFn2
    try:
  File "main.py", line 219, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 101, in Loop_Over_Nuclei
    if not check_if_num_Layers_fit(UserInfoB): Run_Main(UserInfoB)
  File "main.py", line 213, in Run_Main
    Loop_slicing_orientations(UserInfoB, InitValues)
  File "main.py", line 211, in Loop_slicing_orientations
    subRun(UserInfoB)
  File "main.py", line 205, in subRun
    else: normal_run(params)
  File "main.py", line 191, in normal_run
    Data, params = datasets.loadDataset(params)                             
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 384, in loadDataset
    Data = main_ReadingDataset(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 364, in main_ReadingDataset
    DataAll.Train_ForTest = readingAllSubjects(params.directories.Train.Input.Subjects, 'train')
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 329, in readingAllSubjects
    origMsk , msk = readingNuclei(params, subject, imF.shape)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 193, in readingNuclei
    origMsk = origMsk1N if cnt == 0 else np.concatenate((origMsk, origMsk1N) ,axis=3).astype('float32')
KeyboardInterrupt
Exception ignored in: <bound method tqdm.__del__ of Loading train:   5%|▌         | 15/285 [00:31<08:00,  1.78s/it]>
Traceback (most recent call last):
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_tqdm.py", line 931, in __del__
    self.close()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_tqdm.py", line 1133, in close
    self._decr_instances(self)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_tqdm.py", line 496, in _decr_instances
    cls.monitor.exit()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_monitor.py", line 52, in exit
    self.join()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/threading.py", line 1053, in join
    raise RuntimeError("cannot join current thread")
RuntimeError: cannot join current thread
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/285) train vimp2_ANON967_CSFn2
(1/285) train vimp2_E_CSFn2
(2/285) train vimp2_G_CSFn2
(3/285) train vimp2_J_CSFn2
(4/285) train vimp2_K_CSFn2
(5/285) train vimp2_L_CSFn2
(6/285) train vimp2_ANON911_CSFn2
(7/285) train vimp2_D_CSFn2
(8/285) train vimp2_F_CSFn2
(9/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd0
(10/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd1
(11/285) train vimp2_ANON911_CSFn_Aug0_Rot_7_sd2
(12/285) train vimp2_ANON911_CSFn_Aug1_Rot_1_sd2
(13/285) train vimp2_ANON911_CSFn_Aug1_Rot_3_sd1
(14/285) train vimp2_ANON911_CSFn_Aug1_Rot_4_sd0
(15/285) train vimp2_ANON911_CSFn_Aug2_Rot_1_sd1
(16/285) train vimp2_ANON911_CSFn_Aug2_Rot_2_sd2
(17/285) train vimp2_ANON911_CSFn_Aug2_Rot_5_sd0
(18/285) train vimp2_ANON911_CSFn_Aug3_Rot_-4_sd0
(19/285) train vimp2_ANON911_CSFn_Aug3_Rot_6_sd1
(20/285) train vimp2_ANON911_CSFn_Aug3_Rot_7_sd2
(21/285) train vimp2_ANON911_CSFn_Aug4_Rot_-2_sd1
(22/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd0
(23/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd2
(24/285) train vimp2_ANON911_CSFn_Aug5_Rot_1_sd0
(25/285) train vimp2_ANON911_CSFn_Aug5_Rot_-6_sd1
(26/285) train vimp2_ANON911_CSFn_Aug5_Rot_7_sd2
(27/285) train vimp2_D_CSFn_Aug0_Rot_4_sd0
(28/285) train vimp2_D_CSFn_Aug0_Rot_-5_sd2
(29/285) train vimp2_D_CSFn_Aug0_Rot_7_sd1
(30/285) train vimp2_D_CSFn_Aug1_Rot_-2_sd1
(31/285) train vimp2_D_CSFn_Aug1_Rot_-4_sd0
(32/285) train vimp2_D_CSFn_Aug1_Rot_5_sd2
(33/285) train vimp2_D_CSFn_Aug2_Rot_1_sd1
(34/285) train vimp2_D_CSFn_Aug2_Rot_-1_sd2
(35/285) train vimp2_D_CSFn_Aug2_Rot_-5_sd0
(36/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd0
(37/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd1
(38/285) train vimp2_D_CSFn_Aug3_Rot_3_sd2
(39/285) train vimp2_D_CSFn_Aug4_Rot_-4_sd0
(40/285) train vimp2_D_CSFn_Aug4_Rot_-5_sd2
(41/285) train vimp2_D_CSFn_Aug4_Rot_6_sd1
(42/285) train vimp2_D_CSFn_Aug5_Rot_1_sd2
(43/285) train vimp2_D_CSFn_Aug5_Rot_2_sd0
(44/285) train vimp2_D_CSFn_Aug5_Rot_7_sd1
(45/285) train vimp2_F_CSFn_Aug0_Rot_-1_sd2
(46/285) train vimp2_F_CSFn_Aug0_Rot_5_sd0
(47/285) train vimp2_F_CSFn_Aug0_Rot_7_sd1
(48/285) train vimp2_F_CSFn_Aug1_Rot_-5_sd2
(49/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd0
(50/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd1
(51/285) train vimp2_F_CSFn_Aug2_Rot_-3_sd1
(52/285) train vimp2_F_CSFn_Aug2_Rot_4_sd0
(53/285) train vimp2_F_CSFn_Aug2_Rot_-7_sd2
(54/285) train vimp2_F_CSFn_Aug3_Rot_2_sd0
(55/285) train vimp2_F_CSFn_Aug3_Rot_-3_sd2
(56/285) train vimp2_F_CSFn_Aug3_Rot_-7_sd1
(57/285) train vimp2_F_CSFn_Aug4_Rot_-1_sd0
(58/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd1
(59/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd2
(60/285) train vimp2_F_CSFn_Aug5_Rot_-1_sd1
(61/285) train vimp2_F_CSFn_Aug5_Rot_4_sd0
(62/285) train vimp2_F_CSFn_Aug5_Rot_-5_sd2
(63/285) train vimp2_ANON972_CSFn2
(64/285) train vimp2_H_CSFn2
(65/285) train vimp2_I_CSFn2
(66/285) train vimp2_ANON972_CSFn_Aug0_Rot_-1_sd1
(67/285) train vimp2_ANON972_CSFn_Aug0_Rot_3_sd0
(68/285) train vimp2_ANON972_CSFn_Aug0_Rot_5_sd2
(69/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd1
(70/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd2
(71/285) train vimp2_ANON972_CSFn_Aug1_Rot_-5_sd0
(72/285) train vimp2_ANON972_CSFn_Aug2_Rot_1_sd1
(73/285) train vimp2_ANON972_CSFn_Aug2_Rot_-5_sd2
(74/285) train vimp2_ANON972_CSFn_Aug2_Rot_7_sd0
(75/285) train vimp2_ANON972_CSFn_Aug3_Rot_-3_sd0
(76/285) train vimp2_ANON972_CSFn_Aug3_Rot_-6_sd1
(77/285) train vimp2_ANON972_CSFn_Aug3_Rot_7_sd2
(78/285) train vimp2_ANON972_CSFn_Aug4_Rot_-4_sd0
(79/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd1
(80/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd2
(81/285) train vimp2_ANON972_CSFn_Aug5_Rot_-1_sd0
(82/285) train vimp2_ANON972_CSFn_Aug5_Rot_2_sd2
(83/285) train vimp2_ANON972_CSFn_Aug5_Rot_-4_sd1
(84/285) train vimp2_H_CSFn_Aug0_Rot_-3_sd2
(85/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd0
(86/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd1
(87/285) train vimp2_H_CSFn_Aug1_Rot_-1_sd2
(88/285) train vimp2_H_CSFn_Aug1_Rot_-2_sd0
(89/285) train vimp2_H_CSFn_Aug1_Rot_-4_sd1
(90/285) train vimp2_H_CSFn_Aug2_Rot_-1_sd1
(91/285) train vimp2_H_CSFn_Aug2_Rot_2_sd2
(92/285) train vimp2_H_CSFn_Aug2_Rot_6_sd0
(93/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd0
(94/285) train vimp2_H_CSFn_Aug3_Rot_1_sd1
(95/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd2
(96/285) train vimp2_H_CSFn_Aug4_Rot_-3_sd0
(97/285) train vimp2_H_CSFn_Aug4_Rot_4_sd1
(98/285) train vimp2_H_CSFn_Aug4_Rot_5_sd2
(99/285) train vimp2_H_CSFn_Aug5_Rot_-1_sd2
(100/285) train vimp2_H_CSFn_Aug5_Rot_-2_sd1
(101/285) train vimp2_H_CSFn_Aug5_Rot_-3_sd0
(102/285) train vimp2_I_CSFn_Aug0_Rot_1_sd1
(103/285) train vimp2_I_CSFn_Aug0_Rot_6_sd2
(104/285) train vimp2_I_CSFn_Aug0_Rot_-7_sd0
(105/285) train vimp2_I_CSFn_Aug1_Rot_2_sd0
(106/285) train vimp2_I_CSFn_Aug1_Rot_-5_sd2
(107/285) train vimp2_I_CSFn_Aug1_Rot_-6_sd1
(108/285) train vimp2_I_CSFn_Aug2_Rot_1_sd1
(109/285) train vimp2_I_CSFn_Aug2_Rot_-6_sd0
(110/285) train vimp2_I_CSFn_Aug2_Rot_-7_sd2
(111/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd0
(112/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd2
(113/285) train vimp2_I_CSFn_Aug3_Rot_-5_sd1
(114/285) train vimp2_I_CSFn_Aug4_Rot_2_sd1
(115/285) train vimp2_I_CSFn_Aug4_Rot_-6_sd0
(116/285) train vimp2_I_CSFn_Aug4_Rot_6_sd2
(117/285) train vimp2_I_CSFn_Aug5_Rot_-2_sd1
(118/285) train vimp2_I_CSFn_Aug5_Rot_3_sd2
(119/285) train vimp2_I_CSFn_Aug5_Rot_5_sd0
(120/285) train vimp2_ANON988_CSFn2
(121/285) train vimp2_M_CSFn2
(122/285) train vimp2_N_CSFn2
(123/285) train vimp2_ANON988_CSFn_Aug0_Rot_2_sd0
(124/285) train vimp2_ANON988_CSFn_Aug0_Rot_-2_sd2
(125/285) train vimp2_ANON988_CSFn_Aug0_Rot_4_sd1
(126/285) train vimp2_ANON988_CSFn_Aug1_Rot_-2_sd0
(127/285) train vimp2_ANON988_CSFn_Aug1_Rot_3_sd1
(128/285) train vimp2_ANON988_CSFn_Aug1_Rot_5_sd2
(129/285) train vimp2_ANON988_CSFn_Aug2_Rot_-6_sd2
(130/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd0
(131/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd1
(132/285) train vimp2_ANON988_CSFn_Aug3_Rot_-5_sd0
(133/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd1
(134/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd2
(135/285) train vimp2_ANON988_CSFn_Aug4_Rot_-1_sd0
(136/285) train vimp2_ANON988_CSFn_Aug4_Rot_2_sd1
(137/285) train vimp2_ANON988_CSFn_Aug4_Rot_-4_sd2
(138/285) train vimp2_ANON988_CSFn_Aug5_Rot_2_sd0
(139/285) train vimp2_ANON988_CSFn_Aug5_Rot_4_sd2
(140/285) train vimp2_ANON988_CSFn_Aug5_Rot_-5_sd1
(141/285) train vimp2_M_CSFn_Aug0_Rot_-1_sd1
(142/285) train vimp2_M_CSFn_Aug0_Rot_1_sd2
(143/285) train vimp2_M_CSFn_Aug0_Rot_-2_sd0
(144/285) train vimp2_M_CSFn_Aug1_Rot_-3_sd2
(145/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd0
(146/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd1
(147/285) train vimp2_M_CSFn_Aug2_Rot_-2_sd1
(148/285) train vimp2_M_CSFn_Aug2_Rot_4_sd2
(149/285) train vimp2_M_CSFn_Aug2_Rot_7_sd0
(150/285) train vimp2_M_CSFn_Aug3_Rot_2_sd1
(151/285) train vimp2_M_CSFn_Aug3_Rot_-3_sd0
(152/285) train vimp2_M_CSFn_Aug3_Rot_4_sd2
(153/285) train vimp2_M_CSFn_Aug4_Rot_4_sd0
(154/285) train vimp2_M_CSFn_Aug4_Rot_-5_sd1
(155/285) train vimp2_M_CSFn_Aug4_Rot_7_sd2
(156/285) train vimp2_M_CSFn_Aug5_Rot_-3_sd2
(157/285) train vimp2_M_CSFn_Aug5_Rot_4_sd1
(158/285) train vimp2_M_CSFn_Aug5_Rot_7_sd0
(159/285) train vimp2_N_CSFn_Aug0_Rot_2_sd0
(160/285) train vimp2_N_CSFn_Aug0_Rot_3_sd1
(161/285) train vimp2_N_CSFn_Aug0_Rot_6_sd2
(162/285) train vimp2_N_CSFn_Aug1_Rot_2_sd2
(163/285) train vimp2_N_CSFn_Aug1_Rot_3_sd1
(164/285) train vimp2_N_CSFn_Aug1_Rot_-4_sd0
(165/285) train vimp2_N_CSFn_Aug2_Rot_-1_sd1
(166/285) train vimp2_N_CSFn_Aug2_Rot_2_sd2
(167/285) train vimp2_N_CSFn_Aug2_Rot_5_sd0
(168/285) train vimp2_N_CSFn_Aug3_Rot_4_sd0
(169/285) train vimp2_N_CSFn_Aug3_Rot_6_sd1
(170/285) train vimp2_N_CSFn_Aug3_Rot_-6_sd2
(171/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd0
(172/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd2
(173/285) train vimp2_N_CSFn_Aug4_Rot_-2_sd1
(174/285) train vimp2_N_CSFn_Aug5_Rot_-1_sd0
(175/285) train vimp2_N_CSFn_Aug5_Rot_5_sd1
(176/285) train vimp2_N_CSFn_Aug5_Rot_-5_sd2
(177/285) train vimp2_ANON967_CSFn_Aug0_Rot_-3_sd0
(178/285) train vimp2_ANON967_CSFn_Aug0_Rot_-5_sd2
(179/285) train vimp2_ANON967_CSFn_Aug0_Rot_7_sd1
(180/285) train vimp2_ANON967_CSFn_Aug1_Rot_1_sd2
(181/285) train 2019-07-05 17:42:03.174329: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-05 17:42:04.189843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:88:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-05 17:42:04.189910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 17:42:04.577974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 17:42:04.578040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 17:42:04.578053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 17:42:04.578583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:37,  1.61s/it]Loading train:   1%|          | 2/285 [00:03<07:40,  1.63s/it]Loading train:   1%|          | 3/285 [00:04<07:35,  1.61s/it]Loading train:   1%|▏         | 4/285 [00:07<08:32,  1.83s/it]Loading train:   2%|▏         | 5/285 [00:08<08:17,  1.78s/it]Loading train:   2%|▏         | 6/285 [00:11<08:51,  1.90s/it]Loading train:   2%|▏         | 7/285 [00:13<09:01,  1.95s/it]Loading train:   3%|▎         | 8/285 [00:15<09:15,  2.01s/it]Loading train:   3%|▎         | 9/285 [00:16<08:42,  1.89s/it]Loading train:   4%|▎         | 10/285 [00:18<08:22,  1.83s/it]Loading train:   4%|▍         | 11/285 [00:20<08:38,  1.89s/it]Loading train:   4%|▍         | 12/285 [00:22<08:54,  1.96s/it]Loading train:   5%|▍         | 13/285 [00:24<08:35,  1.90s/it]Loading train:   5%|▍         | 14/285 [00:26<08:50,  1.96s/it]Loading train:   5%|▌         | 15/285 [00:28<08:24,  1.87s/it]Loading train:   6%|▌         | 16/285 [00:30<08:16,  1.85s/it]Loading train:   6%|▌         | 17/285 [00:31<07:48,  1.75s/it]Loading train:   6%|▋         | 18/285 [00:33<08:00,  1.80s/it]Loading train:   7%|▋         | 19/285 [00:35<08:00,  1.80s/it]Loading train:   7%|▋         | 20/285 [00:37<08:29,  1.92s/it]Loading train:   7%|▋         | 21/285 [00:39<08:43,  1.98s/it]Loading train:   8%|▊         | 22/285 [00:41<09:03,  2.07s/it]Loading train:   8%|▊         | 23/285 [00:43<08:49,  2.02s/it]Loading train:   8%|▊         | 24/285 [00:45<08:35,  1.97s/it]Loading train:   9%|▉         | 25/285 [00:47<08:11,  1.89s/it]Loading train:   9%|▉         | 26/285 [00:48<07:36,  1.76s/it]Loading train:   9%|▉         | 27/285 [00:50<07:34,  1.76s/it]Loading train:  10%|▉         | 28/285 [00:52<07:38,  1.79s/it]Loading train:  10%|█         | 29/285 [00:54<07:38,  1.79s/it]Loading train:  11%|█         | 30/285 [00:56<07:56,  1.87s/it]Loading train:  11%|█         | 31/285 [00:57<07:42,  1.82s/it]Loading train:  11%|█         | 32/285 [01:00<08:00,  1.90s/it]Loading train:  12%|█▏        | 33/285 [01:02<08:43,  2.08s/it]Loading train:  12%|█▏        | 34/285 [01:04<08:19,  1.99s/it]Loading train:  12%|█▏        | 35/285 [01:06<08:28,  2.04s/it]Loading train:  13%|█▎        | 36/285 [01:07<07:44,  1.86s/it]Loading train:  13%|█▎        | 37/285 [01:09<07:16,  1.76s/it]Loading train:  13%|█▎        | 38/285 [01:11<07:01,  1.71s/it]Loading train:  14%|█▎        | 39/285 [01:12<07:05,  1.73s/it]Loading train:  14%|█▍        | 40/285 [01:14<06:34,  1.61s/it]Loading train:  14%|█▍        | 41/285 [01:16<06:56,  1.71s/it]Loading train:  15%|█▍        | 42/285 [01:17<07:09,  1.77s/it]Loading train:  15%|█▌        | 43/285 [01:20<07:35,  1.88s/it]Loading train:  15%|█▌        | 44/285 [01:22<07:55,  1.97s/it]Loading train:  16%|█▌        | 45/285 [01:24<08:11,  2.05s/it]Loading train:  16%|█▌        | 46/285 [01:26<08:08,  2.04s/it]Loading train:  16%|█▋        | 47/285 [01:28<07:48,  1.97s/it]Loading train:  17%|█▋        | 48/285 [01:29<07:04,  1.79s/it]Loading train:  17%|█▋        | 49/285 [01:31<07:01,  1.79s/it]Loading train:  18%|█▊        | 50/285 [01:33<07:08,  1.82s/it]Loading train:  18%|█▊        | 51/285 [01:35<07:06,  1.82s/it]Loading train:  18%|█▊        | 52/285 [01:36<06:52,  1.77s/it]Loading train:  19%|█▊        | 53/285 [01:38<06:10,  1.60s/it]Loading train:  19%|█▉        | 54/285 [01:39<06:00,  1.56s/it]Loading train:  19%|█▉        | 55/285 [01:41<06:08,  1.60s/it]Loading train:  20%|█▉        | 56/285 [01:42<05:56,  1.56s/it]Loading train:  20%|██        | 57/285 [01:44<06:20,  1.67s/it]Loading train:  20%|██        | 58/285 [01:46<06:16,  1.66s/it]Loading train:  21%|██        | 59/285 [01:48<06:45,  1.79s/it]Loading train:  21%|██        | 60/285 [01:49<06:29,  1.73s/it]Loading train:  21%|██▏       | 61/285 [01:51<06:02,  1.62s/it]Loading train:  22%|██▏       | 62/285 [01:53<06:09,  1.66s/it]Loading train:  22%|██▏       | 63/285 [01:54<06:08,  1.66s/it]Loading train:  22%|██▏       | 64/285 [01:56<06:24,  1.74s/it]Loading train:  23%|██▎       | 65/285 [01:58<06:58,  1.90s/it]Loading train:  23%|██▎       | 66/285 [02:02<08:19,  2.28s/it]Loading train:  24%|██▎       | 67/285 [02:04<08:09,  2.25s/it]Loading train:  24%|██▍       | 68/285 [02:06<07:50,  2.17s/it]Loading train:  24%|██▍       | 69/285 [02:08<07:28,  2.08s/it]Loading train:  25%|██▍       | 70/285 [02:10<07:31,  2.10s/it]Loading train:  25%|██▍       | 71/285 [02:11<07:01,  1.97s/it]Loading train:  25%|██▌       | 72/285 [02:13<06:45,  1.90s/it]Loading train:  26%|██▌       | 73/285 [02:15<06:24,  1.82s/it]Loading train:  26%|██▌       | 74/285 [02:17<06:38,  1.89s/it]Loading train:  26%|██▋       | 75/285 [02:19<06:33,  1.87s/it]Loading train:  27%|██▋       | 76/285 [02:21<06:47,  1.95s/it]Loading train:  27%|██▋       | 77/285 [02:23<06:46,  1.95s/it]Loading train:  27%|██▋       | 78/285 [02:24<06:21,  1.84s/it]Loading train:  28%|██▊       | 79/285 [02:26<06:20,  1.85s/it]Loading train:  28%|██▊       | 80/285 [02:28<06:23,  1.87s/it]Loading train:  28%|██▊       | 81/285 [02:30<06:07,  1.80s/it]Loading train:  29%|██▉       | 82/285 [02:32<06:06,  1.80s/it]Loading train:  29%|██▉       | 83/285 [02:33<05:38,  1.68s/it]Loading train:  29%|██▉       | 84/285 [02:34<05:14,  1.57s/it]Loading train:  30%|██▉       | 85/285 [02:36<05:25,  1.63s/it]Loading train:  30%|███       | 86/285 [02:38<05:21,  1.62s/it]Loading train:  31%|███       | 87/285 [02:39<05:22,  1.63s/it]Loading train:  31%|███       | 88/285 [02:41<05:21,  1.63s/it]Loading train:  31%|███       | 89/285 [02:43<05:19,  1.63s/it]Loading train:  32%|███▏      | 90/285 [02:45<05:35,  1.72s/it]Loading train:  32%|███▏      | 91/285 [02:46<05:22,  1.66s/it]Loading train:  32%|███▏      | 92/285 [02:48<05:26,  1.69s/it]Loading train:  33%|███▎      | 93/285 [02:50<05:30,  1.72s/it]Loading train:  33%|███▎      | 94/285 [02:51<05:25,  1.70s/it]Loading train:  33%|███▎      | 95/285 [02:54<06:06,  1.93s/it]Loading train:  34%|███▎      | 96/285 [02:55<05:36,  1.78s/it]Loading train:  34%|███▍      | 97/285 [02:57<05:29,  1.75s/it]Loading train:  34%|███▍      | 98/285 [02:59<05:38,  1.81s/it]Loading train:  35%|███▍      | 99/285 [03:00<05:10,  1.67s/it]Loading train:  35%|███▌      | 100/285 [03:02<05:03,  1.64s/it]Loading train:  35%|███▌      | 101/285 [03:03<04:51,  1.58s/it]Loading train:  36%|███▌      | 102/285 [03:05<05:00,  1.64s/it]Loading train:  36%|███▌      | 103/285 [03:06<04:55,  1.62s/it]Loading train:  36%|███▋      | 104/285 [03:08<04:55,  1.63s/it]Loading train:  37%|███▋      | 105/285 [03:10<04:43,  1.57s/it]Loading train:  37%|███▋      | 106/285 [03:12<05:31,  1.85s/it]Loading train:  38%|███▊      | 107/285 [03:15<06:03,  2.04s/it]Loading train:  38%|███▊      | 108/285 [03:16<05:54,  2.00s/it]Loading train:  38%|███▊      | 109/285 [03:18<05:33,  1.90s/it]Loading train:  39%|███▊      | 110/285 [03:20<05:14,  1.80s/it]Loading train:  39%|███▉      | 111/285 [03:21<04:43,  1.63s/it]Loading train:  39%|███▉      | 112/285 [03:23<04:48,  1.67s/it]Loading train:  40%|███▉      | 113/285 [03:24<04:34,  1.59s/it]Loading train:  40%|████      | 114/285 [03:26<05:09,  1.81s/it]Loading train:  40%|████      | 115/285 [03:28<05:11,  1.83s/it]Loading train:  41%|████      | 116/285 [03:30<04:51,  1.72s/it]Loading train:  41%|████      | 117/285 [03:31<04:49,  1.72s/it]Loading train:  41%|████▏     | 118/285 [03:33<04:49,  1.73s/it]Loading train:  42%|████▏     | 119/285 [03:35<04:57,  1.79s/it]Loading train:  42%|████▏     | 120/285 [03:37<05:14,  1.91s/it]Loading train:  42%|████▏     | 121/285 [03:39<05:11,  1.90s/it]Loading train:  43%|████▎     | 122/285 [03:41<04:58,  1.83s/it]Loading train:  43%|████▎     | 123/285 [03:43<04:46,  1.77s/it]Loading train:  44%|████▎     | 124/285 [03:44<04:34,  1.70s/it]Loading train:  44%|████▍     | 125/285 [03:45<04:13,  1.58s/it]Loading train:  44%|████▍     | 126/285 [03:47<04:25,  1.67s/it]Loading train:  45%|████▍     | 127/285 [03:49<04:14,  1.61s/it]Loading train:  45%|████▍     | 128/285 [03:50<04:11,  1.60s/it]Loading train:  45%|████▌     | 129/285 [03:52<04:00,  1.54s/it]Loading train:  46%|████▌     | 130/285 [03:53<03:46,  1.46s/it]Loading train:  46%|████▌     | 131/285 [03:54<03:39,  1.43s/it]Loading train:  46%|████▋     | 132/285 [03:56<03:34,  1.40s/it]Loading train:  47%|████▋     | 133/285 [03:57<03:44,  1.47s/it]Loading train:  47%|████▋     | 134/285 [03:59<03:41,  1.47s/it]Loading train:  47%|████▋     | 135/285 [04:00<03:45,  1.50s/it]Loading train:  48%|████▊     | 136/285 [04:02<03:47,  1.53s/it]Loading train:  48%|████▊     | 137/285 [04:04<04:01,  1.63s/it]Loading train:  48%|████▊     | 138/285 [04:05<03:42,  1.52s/it]Loading train:  49%|████▉     | 139/285 [04:06<03:31,  1.45s/it]Loading train:  49%|████▉     | 140/285 [04:08<03:33,  1.47s/it]Loading train:  49%|████▉     | 141/285 [04:09<03:23,  1.41s/it]Loading train:  50%|████▉     | 142/285 [04:11<03:29,  1.47s/it]Loading train:  50%|█████     | 143/285 [04:12<03:21,  1.42s/it]Loading train:  51%|█████     | 144/285 [04:14<03:38,  1.55s/it]Loading train:  51%|█████     | 145/285 [04:15<03:27,  1.48s/it]Loading train:  51%|█████     | 146/285 [04:17<03:35,  1.55s/it]Loading train:  52%|█████▏    | 147/285 [04:18<03:33,  1.55s/it]Loading train:  52%|█████▏    | 148/285 [04:20<03:23,  1.48s/it]Loading train:  52%|█████▏    | 149/285 [04:21<03:19,  1.47s/it]Loading train:  53%|█████▎    | 150/285 [04:23<03:35,  1.60s/it]Loading train:  53%|█████▎    | 151/285 [04:25<03:40,  1.64s/it]Loading train:  53%|█████▎    | 152/285 [04:26<03:28,  1.57s/it]Loading train:  54%|█████▎    | 153/285 [04:28<03:30,  1.60s/it]Loading train:  54%|█████▍    | 154/285 [04:29<03:23,  1.55s/it]Loading train:  54%|█████▍    | 155/285 [04:31<03:19,  1.53s/it]Loading train:  55%|█████▍    | 156/285 [04:32<03:14,  1.51s/it]Loading train:  55%|█████▌    | 157/285 [04:34<03:07,  1.47s/it]Loading train:  55%|█████▌    | 158/285 [04:35<03:01,  1.43s/it]Loading train:  56%|█████▌    | 159/285 [04:37<03:02,  1.45s/it]Loading train:  56%|█████▌    | 160/285 [04:39<03:39,  1.75s/it]Loading train:  56%|█████▋    | 161/285 [04:41<03:34,  1.73s/it]Loading train:  57%|█████▋    | 162/285 [04:42<03:27,  1.69s/it]Loading train:  57%|█████▋    | 163/285 [04:44<03:36,  1.77s/it]Loading train:  58%|█████▊    | 164/285 [04:46<03:36,  1.79s/it]Loading train:  58%|█████▊    | 165/285 [04:48<03:41,  1.84s/it]Loading train:  58%|█████▊    | 166/285 [04:50<03:28,  1.75s/it]Loading train:  59%|█████▊    | 167/285 [04:51<03:19,  1.69s/it]Loading train:  59%|█████▉    | 168/285 [04:52<03:03,  1.57s/it]Loading train:  59%|█████▉    | 169/285 [04:54<02:59,  1.55s/it]Loading train:  60%|█████▉    | 170/285 [04:56<03:02,  1.59s/it]Loading train:  60%|██████    | 171/285 [04:57<02:56,  1.55s/it]Loading train:  60%|██████    | 172/285 [04:58<02:49,  1.50s/it]Loading train:  61%|██████    | 173/285 [05:00<02:49,  1.51s/it]Loading train:  61%|██████    | 174/285 [05:01<02:48,  1.51s/it]Loading train:  61%|██████▏   | 175/285 [05:03<03:02,  1.66s/it]Loading train:  62%|██████▏   | 176/285 [05:05<02:54,  1.60s/it]Loading train:  62%|██████▏   | 177/285 [05:07<02:55,  1.62s/it]Loading train:  62%|██████▏   | 178/285 [05:08<02:44,  1.54s/it]Loading train:  63%|██████▎   | 179/285 [05:09<02:36,  1.48s/it]Loading train:  63%|██████▎   | 180/285 [05:11<02:38,  1.51s/it]Loading train:  64%|██████▎   | 181/285 [05:13<02:41,  1.55s/it]Loading train:  64%|██████▍   | 182/285 [05:14<02:31,  1.47s/it]Loading train:  64%|██████▍   | 183/285 [05:16<02:37,  1.54s/it]Loading train:  65%|██████▍   | 184/285 [05:17<02:28,  1.47s/it]Loading train:  65%|██████▍   | 185/285 [05:18<02:32,  1.52s/it]Loading train:  65%|██████▌   | 186/285 [05:20<02:28,  1.50s/it]Loading train:  66%|██████▌   | 187/285 [05:21<02:18,  1.42s/it]Loading train:  66%|██████▌   | 188/285 [05:23<02:16,  1.41s/it]Loading train:  66%|██████▋   | 189/285 [05:24<02:11,  1.37s/it]Loading train:  67%|██████▋   | 190/285 [05:25<02:16,  1.44s/it]Loading train:  67%|██████▋   | 191/285 [05:27<02:13,  1.42s/it]Loading train:  67%|██████▋   | 192/285 [05:29<02:21,  1.52s/it]Loading train:  68%|██████▊   | 193/285 [05:30<02:24,  1.57s/it]Loading train:  68%|██████▊   | 194/285 [05:32<02:27,  1.62s/it]Loading train:  68%|██████▊   | 195/285 [05:33<02:20,  1.57s/it]Loading train:  69%|██████▉   | 196/285 [05:35<02:17,  1.54s/it]Loading train:  69%|██████▉   | 197/285 [05:37<02:41,  1.83s/it]Loading train:  69%|██████▉   | 198/285 [05:39<02:39,  1.83s/it]Loading train:  70%|██████▉   | 199/285 [05:41<02:32,  1.77s/it]Loading train:  70%|███████   | 200/285 [05:42<02:24,  1.70s/it]Loading train:  71%|███████   | 201/285 [05:45<02:35,  1.86s/it]Loading train:  71%|███████   | 202/285 [05:46<02:28,  1.79s/it]Loading train:  71%|███████   | 203/285 [05:48<02:16,  1.67s/it]Loading train:  72%|███████▏  | 204/285 [05:49<02:08,  1.59s/it]Loading train:  72%|███████▏  | 205/285 [05:50<02:03,  1.54s/it]Loading train:  72%|███████▏  | 206/285 [05:52<02:01,  1.53s/it]Loading train:  73%|███████▎  | 207/285 [05:54<02:01,  1.56s/it]Loading train:  73%|███████▎  | 208/285 [05:55<02:01,  1.58s/it]Loading train:  73%|███████▎  | 209/285 [05:57<01:55,  1.52s/it]Loading train:  74%|███████▎  | 210/285 [05:58<01:53,  1.52s/it]Loading train:  74%|███████▍  | 211/285 [06:00<01:50,  1.50s/it]Loading train:  74%|███████▍  | 212/285 [06:01<01:58,  1.62s/it]Loading train:  75%|███████▍  | 213/285 [06:04<02:11,  1.82s/it]Loading train:  75%|███████▌  | 214/285 [06:06<02:11,  1.86s/it]Loading train:  75%|███████▌  | 215/285 [06:08<02:09,  1.85s/it]Loading train:  76%|███████▌  | 216/285 [06:09<01:59,  1.73s/it]Loading train:  76%|███████▌  | 217/285 [06:11<01:52,  1.66s/it]Loading train:  76%|███████▋  | 218/285 [06:13<01:58,  1.77s/it]Loading train:  77%|███████▋  | 219/285 [06:14<01:45,  1.59s/it]Loading train:  77%|███████▋  | 220/285 [06:15<01:37,  1.49s/it]Loading train:  78%|███████▊  | 221/285 [06:16<01:33,  1.46s/it]Loading train:  78%|███████▊  | 222/285 [06:18<01:36,  1.54s/it]Loading train:  78%|███████▊  | 223/285 [06:20<01:33,  1.50s/it]Loading train:  79%|███████▊  | 224/285 [06:21<01:29,  1.47s/it]Loading train:  79%|███████▉  | 225/285 [06:22<01:24,  1.41s/it]Loading train:  79%|███████▉  | 226/285 [06:24<01:25,  1.45s/it]Loading train:  80%|███████▉  | 227/285 [06:25<01:26,  1.50s/it]Loading train:  80%|████████  | 228/285 [06:27<01:23,  1.47s/it]Loading train:  80%|████████  | 229/285 [06:28<01:16,  1.37s/it]Loading train:  81%|████████  | 230/285 [06:30<01:19,  1.45s/it]Loading train:  81%|████████  | 231/285 [06:31<01:26,  1.61s/it]Loading train:  81%|████████▏ | 232/285 [06:33<01:27,  1.65s/it]Loading train:  82%|████████▏ | 233/285 [06:35<01:32,  1.77s/it]Loading train:  82%|████████▏ | 234/285 [06:37<01:30,  1.78s/it]Loading train:  82%|████████▏ | 235/285 [06:39<01:24,  1.69s/it]Loading train:  83%|████████▎ | 236/285 [06:40<01:21,  1.66s/it]Loading train:  83%|████████▎ | 237/285 [06:43<01:34,  1.96s/it]Loading train:  84%|████████▎ | 238/285 [06:44<01:24,  1.80s/it]Loading train:  84%|████████▍ | 239/285 [06:46<01:24,  1.83s/it]Loading train:  84%|████████▍ | 240/285 [06:48<01:23,  1.85s/it]Loading train:  85%|████████▍ | 241/285 [06:49<01:15,  1.73s/it]Loading train:  85%|████████▍ | 242/285 [06:51<01:07,  1.58s/it]Loading train:  85%|████████▌ | 243/285 [06:52<01:05,  1.55s/it]Loading train:  86%|████████▌ | 244/285 [06:54<01:03,  1.54s/it]Loading train:  86%|████████▌ | 245/285 [06:55<01:03,  1.60s/it]Loading train:  86%|████████▋ | 246/285 [06:57<00:56,  1.44s/it]Loading train:  87%|████████▋ | 247/285 [06:58<00:56,  1.50s/it]Loading train:  87%|████████▋ | 248/285 [07:00<00:56,  1.54s/it]Loading train:  87%|████████▋ | 249/285 [07:01<00:52,  1.47s/it]Loading train:  88%|████████▊ | 250/285 [07:03<00:51,  1.48s/it]Loading train:  88%|████████▊ | 251/285 [07:04<00:50,  1.50s/it]Loading train:  88%|████████▊ | 252/285 [07:06<00:48,  1.48s/it]Loading train:  89%|████████▉ | 253/285 [07:07<00:47,  1.50s/it]Loading train:  89%|████████▉ | 254/285 [07:09<00:45,  1.48s/it]Loading train:  89%|████████▉ | 255/285 [07:10<00:43,  1.45s/it]Loading train:  90%|████████▉ | 256/285 [07:11<00:40,  1.39s/it]Loading train:  90%|█████████ | 257/285 [07:13<00:38,  1.39s/it]Loading train:  91%|█████████ | 258/285 [07:13<00:33,  1.24s/it]Loading train:  91%|█████████ | 259/285 [07:15<00:33,  1.29s/it]Loading train:  91%|█████████ | 260/285 [07:16<00:32,  1.29s/it]Loading train:  92%|█████████▏| 261/285 [07:17<00:29,  1.22s/it]Loading train:  92%|█████████▏| 262/285 [07:18<00:27,  1.20s/it]Loading train:  92%|█████████▏| 263/285 [07:20<00:27,  1.23s/it]Loading train:  93%|█████████▎| 264/285 [07:21<00:25,  1.20s/it]Loading train:  93%|█████████▎| 265/285 [07:22<00:26,  1.31s/it]Loading train:  93%|█████████▎| 266/285 [07:24<00:25,  1.34s/it]Loading train:  94%|█████████▎| 267/285 [07:25<00:22,  1.26s/it]Loading train:  94%|█████████▍| 268/285 [07:27<00:24,  1.44s/it]Loading train:  94%|█████████▍| 269/285 [07:28<00:23,  1.50s/it]Loading train:  95%|█████████▍| 270/285 [07:30<00:22,  1.51s/it]Loading train:  95%|█████████▌| 271/285 [07:32<00:22,  1.58s/it]Loading train:  95%|█████████▌| 272/285 [07:33<00:21,  1.64s/it]Loading train:  96%|█████████▌| 273/285 [07:35<00:19,  1.66s/it]Loading train:  96%|█████████▌| 274/285 [07:37<00:17,  1.61s/it]Loading train:  96%|█████████▋| 275/285 [07:38<00:16,  1.64s/it]Loading train:  97%|█████████▋| 276/285 [07:40<00:14,  1.65s/it]Loading train:  97%|█████████▋| 277/285 [07:42<00:12,  1.62s/it]Loading train:  98%|█████████▊| 278/285 [07:43<00:11,  1.71s/it]Loading train:  98%|█████████▊| 279/285 [07:46<00:10,  1.83s/it]Loading train:  98%|█████████▊| 280/285 [07:47<00:07,  1.59s/it]Loading train:  99%|█████████▊| 281/285 [07:48<00:05,  1.43s/it]Loading train:  99%|█████████▉| 282/285 [07:49<00:04,  1.47s/it]Loading train:  99%|█████████▉| 283/285 [07:51<00:02,  1.48s/it]Loading train: 100%|█████████▉| 284/285 [07:53<00:01,  1.57s/it]Loading train: 100%|██████████| 285/285 [07:54<00:00,  1.53s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:10, 26.16it/s]concatenating: train:  11%|█         | 30/285 [00:00<00:07, 35.84it/s]concatenating: train:  20%|██        | 58/285 [00:00<00:04, 48.49it/s]concatenating: train:  27%|██▋       | 76/285 [00:00<00:03, 61.95it/s]concatenating: train:  33%|███▎      | 94/285 [00:00<00:02, 76.31it/s]concatenating: train:  44%|████▎     | 124/285 [00:00<00:01, 98.11it/s]concatenating: train:  53%|█████▎    | 150/285 [00:00<00:01, 120.20it/s]concatenating: train:  64%|██████▎   | 181/285 [00:00<00:00, 146.70it/s]concatenating: train:  76%|███████▌  | 216/285 [00:00<00:00, 177.49it/s]concatenating: train:  87%|████████▋ | 247/285 [00:01<00:00, 202.59it/s]concatenating: train:  97%|█████████▋| 276/285 [00:01<00:00, 214.73it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 240.76it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.34s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 164.07it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.75it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  5.18it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  5.03it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:04,  6.50it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.84it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  6.71it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:04,  6.01it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  7.53it/s]loading the weights for Unet:  45%|████▌     | 18/40 [00:01<00:02,  7.88it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  6.56it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  7.50it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:02<00:01,  8.09it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  6.50it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  7.11it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  5.79it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  6.95it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.41it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.71it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.35it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.46it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.39it/s]vimp2_ANON967_CSFn_Aug1_Rot_3_sd0
(182/285) train vimp2_ANON967_CSFn_Aug1_Rot_-6_sd1
(183/285) train vimp2_ANON967_CSFn_Aug2_Rot_4_sd2
(184/285) train vimp2_ANON967_CSFn_Aug2_Rot_-5_sd0
(185/285) train vimp2_ANON967_CSFn_Aug2_Rot_-7_sd1
(186/285) train vimp2_ANON967_CSFn_Aug3_Rot_4_sd2
(187/285) train vimp2_ANON967_CSFn_Aug3_Rot_5_sd1
(188/285) train vimp2_ANON967_CSFn_Aug3_Rot_-7_sd0
(189/285) train vimp2_ANON967_CSFn_Aug4_Rot_-4_sd0
(190/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd1
(191/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd2
(192/285) train vimp2_ANON967_CSFn_Aug5_Rot_-5_sd1
(193/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd0
(194/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd2
(195/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd0
(196/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd2
(197/285) train vimp2_E_CSFn_Aug0_Rot_6_sd1
(198/285) train vimp2_E_CSFn_Aug1_Rot_0_sd1
(199/285) train vimp2_E_CSFn_Aug1_Rot_1_sd0
(200/285) train vimp2_E_CSFn_Aug1_Rot_4_sd2
(201/285) train vimp2_E_CSFn_Aug2_Rot_2_sd1
(202/285) train vimp2_E_CSFn_Aug2_Rot_-3_sd0
(203/285) train vimp2_E_CSFn_Aug2_Rot_-7_sd2
(204/285) train vimp2_E_CSFn_Aug3_Rot_-1_sd2
(205/285) train vimp2_E_CSFn_Aug3_Rot_5_sd1
(206/285) train vimp2_E_CSFn_Aug3_Rot_7_sd0
(207/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd1
(208/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd2
(209/285) train vimp2_E_CSFn_Aug4_Rot_7_sd0
(210/285) train vimp2_E_CSFn_Aug5_Rot_-1_sd2
(211/285) train vimp2_E_CSFn_Aug5_Rot_6_sd0
(212/285) train vimp2_E_CSFn_Aug5_Rot_-6_sd1
(213/285) train vimp2_G_CSFn_Aug0_Rot_-6_sd1
(214/285) train vimp2_G_CSFn_Aug0_Rot_-7_sd0
(215/285) train vimp2_G_CSFn_Aug0_Rot_7_sd2
(216/285) train vimp2_G_CSFn_Aug1_Rot_2_sd0
(217/285) train vimp2_G_CSFn_Aug1_Rot_-2_sd2
(218/285) train vimp2_G_CSFn_Aug1_Rot_-5_sd1
(219/285) train vimp2_G_CSFn_Aug2_Rot_1_sd0
(220/285) train vimp2_G_CSFn_Aug2_Rot_-1_sd2
(221/285) train vimp2_G_CSFn_Aug2_Rot_4_sd1
(222/285) train vimp2_G_CSFn_Aug3_Rot_-5_sd2
(223/285) train vimp2_G_CSFn_Aug3_Rot_-6_sd0
(224/285) train vimp2_G_CSFn_Aug3_Rot_7_sd1
(225/285) train vimp2_G_CSFn_Aug4_Rot_-2_sd0
(226/285) train vimp2_G_CSFn_Aug4_Rot_2_sd1
(227/285) train vimp2_G_CSFn_Aug4_Rot_-6_sd2
(228/285) train vimp2_G_CSFn_Aug5_Rot_-1_sd2
(229/285) train vimp2_G_CSFn_Aug5_Rot_4_sd0
(230/285) train vimp2_G_CSFn_Aug5_Rot_4_sd1
(231/285) train vimp2_J_CSFn_Aug0_Rot_1_sd1
(232/285) train vimp2_J_CSFn_Aug0_Rot_-3_sd2
(233/285) train vimp2_J_CSFn_Aug0_Rot_-4_sd0
(234/285) train vimp2_J_CSFn_Aug1_Rot_-2_sd2
(235/285) train vimp2_J_CSFn_Aug1_Rot_4_sd0
(236/285) train vimp2_J_CSFn_Aug1_Rot_-6_sd1
(237/285) train vimp2_J_CSFn_Aug2_Rot_4_sd1
(238/285) train vimp2_J_CSFn_Aug2_Rot_-4_sd2
(239/285) train vimp2_J_CSFn_Aug2_Rot_7_sd0
(240/285) train vimp2_J_CSFn_Aug3_Rot_-1_sd1
(241/285) train vimp2_J_CSFn_Aug3_Rot_-2_sd0
(242/285) train vimp2_J_CSFn_Aug3_Rot_-3_sd2
(243/285) train vimp2_J_CSFn_Aug4_Rot_4_sd2
(244/285) train vimp2_J_CSFn_Aug4_Rot_5_sd0
(245/285) train vimp2_J_CSFn_Aug4_Rot_7_sd1
(246/285) train vimp2_J_CSFn_Aug5_Rot_4_sd2
(247/285) train vimp2_J_CSFn_Aug5_Rot_-6_sd1
(248/285) train vimp2_J_CSFn_Aug5_Rot_-7_sd0
(249/285) train vimp2_K_CSFn_Aug0_Rot_-2_sd2
(250/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd0
(251/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd1
(252/285) train vimp2_K_CSFn_Aug1_Rot_1_sd1
(253/285) train vimp2_K_CSFn_Aug1_Rot_1_sd2
(254/285) train vimp2_K_CSFn_Aug1_Rot_-5_sd0
(255/285) train vimp2_K_CSFn_Aug2_Rot_0_sd0
(256/285) train vimp2_K_CSFn_Aug2_Rot_-2_sd1
(257/285) train vimp2_K_CSFn_Aug2_Rot_-5_sd2
(258/285) train vimp2_K_CSFn_Aug3_Rot_5_sd2
(259/285) train vimp2_K_CSFn_Aug3_Rot_6_sd0
(260/285) train vimp2_K_CSFn_Aug3_Rot_-6_sd1
(261/285) train vimp2_K_CSFn_Aug4_Rot_-1_sd0
(262/285) train vimp2_K_CSFn_Aug4_Rot_2_sd2
(263/285) train vimp2_K_CSFn_Aug4_Rot_-6_sd1
(264/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd1
(265/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd2
(266/285) train vimp2_K_CSFn_Aug5_Rot_-3_sd0
(267/285) train vimp2_L_CSFn_Aug0_Rot_4_sd0
(268/285) train vimp2_L_CSFn_Aug0_Rot_5_sd2
(269/285) train vimp2_L_CSFn_Aug0_Rot_-7_sd1
(270/285) train vimp2_L_CSFn_Aug1_Rot_3_sd0
(271/285) train vimp2_L_CSFn_Aug1_Rot_5_sd1
(272/285) train vimp2_L_CSFn_Aug1_Rot_-5_sd2
(273/285) train vimp2_L_CSFn_Aug2_Rot_-4_sd1
(274/285) train vimp2_L_CSFn_Aug2_Rot_5_sd0
(275/285) train vimp2_L_CSFn_Aug2_Rot_-7_sd2
(276/285) train vimp2_L_CSFn_Aug3_Rot_5_sd2
(277/285) train vimp2_L_CSFn_Aug3_Rot_-7_sd0
(278/285) train vimp2_L_CSFn_Aug3_Rot_7_sd1
(279/285) train vimp2_L_CSFn_Aug4_Rot_-1_sd0
(280/285) train vimp2_L_CSFn_Aug4_Rot_3_sd2
(281/285) train vimp2_L_CSFn_Aug4_Rot_-7_sd1
(282/285) train vimp2_L_CSFn_Aug5_Rot_-1_sd0
(283/285) train vimp2_L_CSFn_Aug5_Rot_-5_sd1
(284/285) train vimp2_L_CSFn_Aug5_Rot_6_sd2
(0/3) test vimp2_A_CSFn2
(1/3) test vimp2_ANON765_CSFn2
(2/3) test vimp2_B_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 10)   1810        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 10)   40          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 10)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 10)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 13)   143         dropout_9[0][0]                  
==================================================================================================
Total params: 189,493
Trainable params: 45,933
Non-trainable params: 143,560
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 20s - loss: 207.1345 - acc: 0.4913 - mDice: 0.0159 - val_loss: 137.2330 - val_acc: 0.6175 - val_mDice: 0.0126

Epoch 00001: val_mDice improved from -inf to 0.01265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 126.0308 - acc: 0.7634 - mDice: 0.0157 - val_loss: 78.5571 - val_acc: 0.9047 - val_mDice: 0.0135

Epoch 00002: val_mDice improved from 0.01265 to 0.01346, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 10s - loss: 82.1386 - acc: 0.8692 - mDice: 0.0165 - val_loss: 47.7706 - val_acc: 0.9047 - val_mDice: 0.0138

Epoch 00003: val_mDice improved from 0.01346 to 0.01378, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 10s - loss: 57.0410 - acc: 0.8692 - mDice: 0.0172 - val_loss: 32.0760 - val_acc: 0.9047 - val_mDice: 0.0144

Epoch 00004: val_mDice improved from 0.01378 to 0.01435, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 10s - loss: 41.3069 - acc: 0.8692 - mDice: 0.0178 - val_loss: 24.1988 - val_acc: 0.9047 - val_mDice: 0.0174

Epoch 00005: val_mDice improved from 0.01435 to 0.01743, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 10s - loss: 31.3353 - acc: 0.8692 - mDice: 0.0178 - val_loss: 19.1992 - val_acc: 0.9047 - val_mDice: 0.0189

Epoch 00006: val_mDice improved from 0.01743 to 0.01892, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 10s - loss: 25.0217 - acc: 0.8692 - mDice: 0.0177 - val_loss: 16.7987 - val_acc: 0.9047 - val_mDice: 0.0179

Epoch 00007: val_mDice did not improve from 0.01892
Epoch 8/300
 - 10s - loss: 20.8612 - acc: 0.8692 - mDice: 0.0179 - val_loss: 12.4766 - val_acc: 0.9047 - val_mDice: 0.0177

Epoch 00008: val_mDice did not improve from 0.01892
Epoch 9/300
 - 10s - loss: 17.9670 - acc: 0.8692 - mDice: 0.0182 - val_loss: 9.6999 - val_acc: 0.9047 - val_mDice: 0.0180

Epoch 00009: val_mDice did not improve from 0.01892
Epoch 10/300
 - 10s - loss: 15.8817 - acc: 0.8692 - mDice: 0.0186 - val_loss: 8.2825 - val_acc: 0.9047 - val_mDice: 0.0185

Epoch 00010: val_mDice did not improve from 0.01892
Epoch 11/300
 - 10s - loss: 14.3139 - acc: 0.8692 - mDice: 0.0191 - val_loss: 7.3843 - val_acc: 0.9047 - val_mDice: 0.0182

Epoch 00011: val_mDice did not improve from 0.01892
Epoch 12/300
 - 10s - loss: 13.1176 - acc: 0.8692 - mDice: 0.0199 - val_loss: 6.7139 - val_acc: 0.9047 - val_mDice: 0.0170

Epoch 00012: val_mDice did not improve from 0.01892
Epoch 13/300
 - 10s - loss: 12.1770 - acc: 0.8692 - mDice: 0.0207 - val_loss: 6.4577 - val_acc: 0.9047 - val_mDice: 0.0175

Epoch 00013: val_mDice did not improve from 0.01892
Epoch 14/300
 - 10s - loss: 11.3996 - acc: 0.8692 - mDice: 0.0214 - val_loss: 6.3604 - val_acc: 0.9047 - val_mDice: 0.0215

Epoch 00014: val_mDice improved from 0.01892 to 0.02151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 10s - loss: 10.7376 - acc: 0.8692 - mDice: 0.0223 - val_loss: 6.2186 - val_acc: 0.9047 - val_mDice: 0.0229

Epoch 00015: val_mDice improved from 0.02151 to 0.02291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 10s - loss: 10.1594 - acc: 0.8692 - mDice: 0.0232 - val_loss: 6.0291 - val_acc: 0.9047 - val_mDice: 0.0261

Epoch 00016: val_mDice improved from 0.02291 to 0.02614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 10s - loss: 9.6920 - acc: 0.8692 - mDice: 0.0243 - val_loss: 5.7865 - val_acc: 0.9047 - val_mDice: 0.0276

Epoch 00017: val_mDice improved from 0.02614 to 0.02758, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 10s - loss: 9.2777 - acc: 0.8692 - mDice: 0.0256 - val_loss: 5.6256 - val_acc: 0.9047 - val_mDice: 0.0307

Epoch 00018: val_mDice improved from 0.02758 to 0.03074, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 10s - loss: 8.9102 - acc: 0.8692 - mDice: 0.0272 - val_loss: 5.5809 - val_acc: 0.9047 - val_mDice: 0.0318

Epoch 00019: val_mDice improved from 0.03074 to 0.03181, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 10s - loss: 8.5766 - acc: 0.8692 - mDice: 0.0290 - val_loss: 5.3853 - val_acc: 0.9047 - val_mDice: 0.0357

Epoch 00020: val_mDice improved from 0.03181 to 0.03566, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 10s - loss: 8.2832 - acc: 0.8692 - mDice: 0.0308 - val_loss: 5.5801 - val_acc: 0.9047 - val_mDice: 0.0366

Epoch 00021: val_mDice improved from 0.03566 to 0.03657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 10s - loss: 8.0297 - acc: 0.8692 - mDice: 0.0324 - val_loss: 5.2511 - val_acc: 0.9047 - val_mDice: 0.0415

Epoch 00022: val_mDice improved from 0.03657 to 0.04149, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 10s - loss: 7.7714 - acc: 0.8692 - mDice: 0.0343 - val_loss: 5.2050 - val_acc: 0.9047 - val_mDice: 0.0425

Epoch 00023: val_mDice improved from 0.04149 to 0.04250, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 10s - loss: 7.5524 - acc: 0.8692 - mDice: 0.0363 - val_loss: 5.2329 - val_acc: 0.9047 - val_mDice: 0.0447

Epoch 00024: val_mDice improved from 0.04250 to 0.04474, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 10s - loss: 7.3534 - acc: 0.8692 - mDice: 0.0383 - val_loss: 5.1789 - val_acc: 0.9047 - val_mDice: 0.0491

Epoch 00025: val_mDice improved from 0.04474 to 0.04912, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 10s - loss: 7.1560 - acc: 0.8692 - mDice: 0.0405 - val_loss: 4.9917 - val_acc: 0.9047 - val_mDice: 0.0542

Epoch 00026: val_mDice improved from 0.04912 to 0.05421, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 6.9640 - acc: 0.8692 - mDice: 0.0428 - val_loss: 5.2015 - val_acc: 0.9047 - val_mDice: 0.0535

Epoch 00027: val_mDice did not improve from 0.05421
Epoch 28/300
 - 10s - loss: 6.7841 - acc: 0.8692 - mDice: 0.0452 - val_loss: 5.0580 - val_acc: 0.9047 - val_mDice: 0.0581

Epoch 00028: val_mDice improved from 0.05421 to 0.05809, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 10s - loss: 6.6269 - acc: 0.8692 - mDice: 0.0473 - val_loss: 5.0983 - val_acc: 0.9047 - val_mDice: 0.0574

Epoch 00029: val_mDice did not improve from 0.05809
Epoch 30/300
 - 10s - loss: 6.4772 - acc: 0.8692 - mDice: 0.0497 - val_loss: 5.0476 - val_acc: 0.9047 - val_mDice: 0.0630

Epoch 00030: val_mDice improved from 0.05809 to 0.06299, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 10s - loss: 6.3265 - acc: 0.8692 - mDice: 0.0521 - val_loss: 5.6311 - val_acc: 0.9047 - val_mDice: 0.0497

Epoch 00031: val_mDice did not improve from 0.06299
Epoch 32/300
 - 10s - loss: 6.1971 - acc: 0.8692 - mDice: 0.0542 - val_loss: 5.1903 - val_acc: 0.9047 - val_mDice: 0.0614

Epoch 00032: val_mDice did not improve from 0.06299
Epoch 33/300
 - 10s - loss: 6.0660 - acc: 0.8692 - mDice: 0.0567 - val_loss: 5.0189 - val_acc: 0.9047 - val_mDice: 0.0676

Epoch 00033: val_mDice improved from 0.06299 to 0.06757, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 5.9453 - acc: 0.8692 - mDice: 0.0589 - val_loss: 4.6782 - val_acc: 0.9047 - val_mDice: 0.0708

Epoch 00034: val_mDice improved from 0.06757 to 0.07081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 10s - loss: 5.8412 - acc: 0.8692 - mDice: 0.0613 - val_loss: 5.0553 - val_acc: 0.9047 - val_mDice: 0.0698

Epoch 00035: val_mDice did not improve from 0.07081
Epoch 36/300
 - 10s - loss: 5.7339 - acc: 0.8692 - mDice: 0.0637 - val_loss: 4.8573 - val_acc: 0.9047 - val_mDice: 0.0745

Epoch 00036: val_mDice improved from 0.07081 to 0.07448, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 5.6203 - acc: 0.8692 - mDice: 0.0663 - val_loss: 4.9654 - val_acc: 0.9047 - val_mDice: 0.0749

Epoch 00037: val_mDice improved from 0.07448 to 0.07493, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 5.5236 - acc: 0.8689 - mDice: 0.0688 - val_loss: 4.6468 - val_acc: 0.9047 - val_mDice: 0.0816

Epoch 00038: val_mDice improved from 0.07493 to 0.08156, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 5.4254 - acc: 0.8691 - mDice: 0.0708 - val_loss: 4.9872 - val_acc: 0.9047 - val_mDice: 0.0788

Epoch 00039: val_mDice did not improve from 0.08156
Epoch 40/300
 - 10s - loss: 5.3298 - acc: 0.8690 - mDice: 0.0739 - val_loss: 4.5126 - val_acc: 0.9047 - val_mDice: 0.0881

Epoch 00040: val_mDice improved from 0.08156 to 0.08814, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 10s - loss: 5.2194 - acc: 0.8689 - mDice: 0.0774 - val_loss: 4.4331 - val_acc: 0.9047 - val_mDice: 0.0902

Epoch 00041: val_mDice improved from 0.08814 to 0.09022, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 10s - loss: 5.1312 - acc: 0.8689 - mDice: 0.0815 - val_loss: 4.7924 - val_acc: 0.9047 - val_mDice: 0.0869

Epoch 00042: val_mDice did not improve from 0.09022
Epoch 43/300
 - 10s - loss: 5.0345 - acc: 0.8687 - mDice: 0.0857 - val_loss: 4.8072 - val_acc: 0.9047 - val_mDice: 0.0878

Epoch 00043: val_mDice did not improve from 0.09022
Epoch 44/300
 - 10s - loss: 4.9460 - acc: 0.8686 - mDice: 0.0892 - val_loss: 4.6899 - val_acc: 0.9047 - val_mDice: 0.0934

Epoch 00044: val_mDice improved from 0.09022 to 0.09339, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 10s - loss: 4.8563 - acc: 0.8685 - mDice: 0.0943 - val_loss: 4.2864 - val_acc: 0.9044 - val_mDice: 0.1071

Epoch 00045: val_mDice improved from 0.09339 to 0.10707, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 10s - loss: 4.7589 - acc: 0.8684 - mDice: 0.1001 - val_loss: 4.1772 - val_acc: 0.9047 - val_mDice: 0.1132

Epoch 00046: val_mDice improved from 0.10707 to 0.11321, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 10s - loss: 4.6706 - acc: 0.8683 - mDice: 0.1065 - val_loss: 4.0855 - val_acc: 0.9041 - val_mDice: 0.1257

Epoch 00047: val_mDice improved from 0.11321 to 0.12568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 10s - loss: 4.5879 - acc: 0.8683 - mDice: 0.1133 - val_loss: 4.4498 - val_acc: 0.9047 - val_mDice: 0.1235

Epoch 00048: val_mDice did not improve from 0.12568
Epoch 49/300
 - 10s - loss: 4.5051 - acc: 0.8683 - mDice: 0.1202 - val_loss: 4.2760 - val_acc: 0.9045 - val_mDice: 0.1329

Epoch 00049: val_mDice improved from 0.12568 to 0.13294, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 10s - loss: 4.4195 - acc: 0.8682 - mDice: 0.1263 - val_loss: 4.0072 - val_acc: 0.9048 - val_mDice: 0.1454

Epoch 00050: val_mDice improved from 0.13294 to 0.14539, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 10s - loss: 4.3480 - acc: 0.8681 - mDice: 0.1324 - val_loss: 4.0720 - val_acc: 0.9047 - val_mDice: 0.1498

Epoch 00051: val_mDice improved from 0.14539 to 0.14982, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 10s - loss: 4.2751 - acc: 0.8677 - mDice: 0.1387 - val_loss: 3.8695 - val_acc: 0.9047 - val_mDice: 0.1583

Epoch 00052: val_mDice improved from 0.14982 to 0.15835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 10s - loss: 4.1933 - acc: 0.8676 - mDice: 0.1458 - val_loss: 4.0279 - val_acc: 0.9047 - val_mDice: 0.1637

Epoch 00053: val_mDice improved from 0.15835 to 0.16370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 10s - loss: 4.1261 - acc: 0.8677 - mDice: 0.1516 - val_loss: 3.9924 - val_acc: 0.9049 - val_mDice: 0.1739

Epoch 00054: val_mDice improved from 0.16370 to 0.17387, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 10s - loss: 4.0567 - acc: 0.8680 - mDice: 0.1578 - val_loss: 4.2058 - val_acc: 0.9048 - val_mDice: 0.1676

Epoch 00055: val_mDice did not improve from 0.17387
Epoch 56/300
 - 10s - loss: 3.9953 - acc: 0.8683 - mDice: 0.1640 - val_loss: 3.7576 - val_acc: 0.9049 - val_mDice: 0.1905

Epoch 00056: val_mDice improved from 0.17387 to 0.19050, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 10s - loss: 3.9409 - acc: 0.8686 - mDice: 0.1698 - val_loss: 4.1882 - val_acc: 0.9049 - val_mDice: 0.1810

Epoch 00057: val_mDice did not improve from 0.19050
Epoch 58/300
 - 10s - loss: 3.8795 - acc: 0.8692 - mDice: 0.1769 - val_loss: 3.9712 - val_acc: 0.9048 - val_mDice: 0.1910

Epoch 00058: val_mDice improved from 0.19050 to 0.19105, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 10s - loss: 3.8332 - acc: 0.8699 - mDice: 0.1822 - val_loss: 3.9521 - val_acc: 0.9054 - val_mDice: 0.2007

Epoch 00059: val_mDice improved from 0.19105 to 0.20068, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 10s - loss: 3.7877 - acc: 0.8706 - mDice: 0.1876 - val_loss: 3.9312 - val_acc: 0.9054 - val_mDice: 0.2053

Epoch 00060: val_mDice improved from 0.20068 to 0.20534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 10s - loss: 3.7243 - acc: 0.8720 - mDice: 0.1945 - val_loss: 4.0833 - val_acc: 0.9053 - val_mDice: 0.1992

Epoch 00061: val_mDice did not improve from 0.20534
Epoch 62/300
 - 10s - loss: 3.6915 - acc: 0.8728 - mDice: 0.1987 - val_loss: 3.9718 - val_acc: 0.9076 - val_mDice: 0.2040

Epoch 00062: val_mDice did not improve from 0.20534
Epoch 63/300
 - 10s - loss: 3.6461 - acc: 0.8736 - mDice: 0.2042 - val_loss: 4.0235 - val_acc: 0.9060 - val_mDice: 0.2152

Epoch 00063: val_mDice improved from 0.20534 to 0.21521, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 10s - loss: 3.5935 - acc: 0.8744 - mDice: 0.2105 - val_loss: 3.8111 - val_acc: 0.9049 - val_mDice: 0.2222

Epoch 00064: val_mDice improved from 0.21521 to 0.22220, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 10s - loss: 3.5668 - acc: 0.8749 - mDice: 0.2143 - val_loss: 3.8252 - val_acc: 0.9052 - val_mDice: 0.2250

Epoch 00065: val_mDice improved from 0.22220 to 0.22496, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 10s - loss: 3.5159 - acc: 0.8758 - mDice: 0.2207 - val_loss: 3.8179 - val_acc: 0.9060 - val_mDice: 0.2308

Epoch 00066: val_mDice improved from 0.22496 to 0.23081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 10s - loss: 3.4865 - acc: 0.8769 - mDice: 0.2259 - val_loss: 4.2808 - val_acc: 0.9063 - val_mDice: 0.2257

Epoch 00067: val_mDice did not improve from 0.23081
Epoch 68/300
 - 10s - loss: 3.4424 - acc: 0.8784 - mDice: 0.2317 - val_loss: 3.9062 - val_acc: 0.9101 - val_mDice: 0.2404

Epoch 00068: val_mDice improved from 0.23081 to 0.24036, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 10s - loss: 3.4098 - acc: 0.8802 - mDice: 0.2369 - val_loss: 3.6866 - val_acc: 0.9155 - val_mDice: 0.2475

Epoch 00069: val_mDice improved from 0.24036 to 0.24753, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 10s - loss: 3.3658 - acc: 0.8820 - mDice: 0.2422 - val_loss: 4.0300 - val_acc: 0.9141 - val_mDice: 0.2438

Epoch 00070: val_mDice did not improve from 0.24753
Epoch 71/300
 - 10s - loss: 3.3332 - acc: 0.8835 - mDice: 0.2471 - val_loss: 3.9533 - val_acc: 0.9189 - val_mDice: 0.2446

Epoch 00071: val_mDice did not improve from 0.24753
Epoch 72/300
 - 10s - loss: 3.3007 - acc: 0.8853 - mDice: 0.2520 - val_loss: 3.8015 - val_acc: 0.9192 - val_mDice: 0.2549

Epoch 00072: val_mDice improved from 0.24753 to 0.25487, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 10s - loss: 3.2687 - acc: 0.8866 - mDice: 0.2569 - val_loss: 3.8225 - val_acc: 0.9219 - val_mDice: 0.2633

Epoch 00073: val_mDice improved from 0.25487 to 0.26331, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 10s - loss: 3.2458 - acc: 0.8877 - mDice: 0.2597 - val_loss: 3.7097 - val_acc: 0.9218 - val_mDice: 0.2684

Epoch 00074: val_mDice improved from 0.26331 to 0.26837, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 10s - loss: 3.2110 - acc: 0.8885 - mDice: 0.2650 - val_loss: 3.8230 - val_acc: 0.9223 - val_mDice: 0.2697

Epoch 00075: val_mDice improved from 0.26837 to 0.26968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 10s - loss: 3.1964 - acc: 0.8893 - mDice: 0.2672 - val_loss: 3.8729 - val_acc: 0.9210 - val_mDice: 0.2655

Epoch 00076: val_mDice did not improve from 0.26968
Epoch 77/300
 - 10s - loss: 3.1495 - acc: 0.8903 - mDice: 0.2729 - val_loss: 3.6192 - val_acc: 0.9216 - val_mDice: 0.2730

Epoch 00077: val_mDice improved from 0.26968 to 0.27296, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 78/300
 - 10s - loss: 3.1302 - acc: 0.8911 - mDice: 0.2767 - val_loss: 3.6618 - val_acc: 0.9224 - val_mDice: 0.2776

Epoch 00078: val_mDice improved from 0.27296 to 0.27759, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 10s - loss: 3.1070 - acc: 0.8922 - mDice: 0.2810 - val_loss: 3.6420 - val_acc: 0.9241 - val_mDice: 0.2876

Epoch 00079: val_mDice improved from 0.27759 to 0.28760, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 80/300
 - 10s - loss: 3.0752 - acc: 0.8930 - mDice: 0.2859 - val_loss: 3.5853 - val_acc: 0.9220 - val_mDice: 0.2846

Epoch 00080: val_mDice did not improve from 0.28760
Epoch 81/300
 - 10s - loss: 3.0500 - acc: 0.8936 - mDice: 0.2894 - val_loss: 3.5724 - val_acc: 0.9235 - val_mDice: 0.2926

Epoch 00081: val_mDice improved from 0.28760 to 0.29260, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 10s - loss: 3.0197 - acc: 0.8941 - mDice: 0.2947 - val_loss: 3.4221 - val_acc: 0.9242 - val_mDice: 0.2979

Epoch 00082: val_mDice improved from 0.29260 to 0.29788, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 83/300
 - 10s - loss: 2.9887 - acc: 0.8948 - mDice: 0.2995 - val_loss: 4.0273 - val_acc: 0.9249 - val_mDice: 0.2922

Epoch 00083: val_mDice did not improve from 0.29788
Epoch 84/300
 - 10s - loss: 2.9752 - acc: 0.8949 - mDice: 0.3023 - val_loss: 3.4564 - val_acc: 0.9243 - val_mDice: 0.3038

Epoch 00084: val_mDice improved from 0.29788 to 0.30380, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 85/300
 - 10s - loss: 2.9547 - acc: 0.8954 - mDice: 0.3055 - val_loss: 3.3729 - val_acc: 0.9257 - val_mDice: 0.3064

Epoch 00085: val_mDice improved from 0.30380 to 0.30640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 86/300
 - 10s - loss: 2.9240 - acc: 0.8958 - mDice: 0.3106 - val_loss: 3.5800 - val_acc: 0.9245 - val_mDice: 0.3046

Epoch 00086: val_mDice did not improve from 0.30640
Epoch 87/300
 - 10s - loss: 2.9025 - acc: 0.8960 - mDice: 0.3140 - val_loss: 3.6156 - val_acc: 0.9221 - val_mDice: 0.2952

Epoch 00087: val_mDice did not improve from 0.30640
Epoch 88/300
 - 10s - loss: 2.8923 - acc: 0.8963 - mDice: 0.3164 - val_loss: 3.4723 - val_acc: 0.9247 - val_mDice: 0.3128

Epoch 00088: val_mDice improved from 0.30640 to 0.31284, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 10s - loss: 2.8666 - acc: 0.8965 - mDice: 0.3195 - val_loss: 3.4659 - val_acc: 0.9255 - val_mDice: 0.3186

Epoch 00089: val_mDice improved from 0.31284 to 0.31859, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 90/300
 - 10s - loss: 2.8472 - acc: 0.8968 - mDice: 0.3241 - val_loss: 3.6870 - val_acc: 0.9262 - val_mDice: 0.3140

Epoch 00090: val_mDice did not improve from 0.31859
Epoch 91/300
 - 10s - loss: 2.8344 - acc: 0.8969 - mDice: 0.3273 - val_loss: 3.5876 - val_acc: 0.9250 - val_mDice: 0.3152

Epoch 00091: val_mDice did not improve from 0.31859
Epoch 92/300
 - 10s - loss: 2.8061 - acc: 0.8973 - mDice: 0.3307 - val_loss: 3.4431 - val_acc: 0.9258 - val_mDice: 0.3277

Epoch 00092: val_mDice improved from 0.31859 to 0.32770, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 93/300
 - 10s - loss: 2.7914 - acc: 0.8974 - mDice: 0.3329 - val_loss: 3.3088 - val_acc: 0.9269 - val_mDice: 0.3428

Epoch 00093: val_mDice improved from 0.32770 to 0.34283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 10s - loss: 2.7726 - acc: 0.8976 - mDice: 0.3360 - val_loss: 3.6534 - val_acc: 0.9266 - val_mDice: 0.3322

Epoch 00094: val_mDice did not improve from 0.34283
Epoch 95/300
 - 10s - loss: 2.7554 - acc: 0.8978 - mDice: 0.3399 - val_loss: 3.7697 - val_acc: 0.9260 - val_mDice: 0.3352

Epoch 00095: val_mDice did not improve from 0.34283
Epoch 96/300
 - 10s - loss: 2.7440 - acc: 0.8978 - mDice: 0.3421 - val_loss: 3.4615 - val_acc: 0.9265 - val_mDice: 0.3374

Epoch 00096: val_mDice did not improve from 0.34283
Epoch 97/300
 - 10s - loss: 2.7143 - acc: 0.8985 - mDice: 0.3464 - val_loss: 3.4515 - val_acc: 0.9276 - val_mDice: 0.3434

Epoch 00097: val_mDice improved from 0.34283 to 0.34343, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 10s - loss: 2.6897 - acc: 0.8990 - mDice: 0.3495 - val_loss: 3.4210 - val_acc: 0.9259 - val_mDice: 0.3425

Epoch 00098: val_mDice did not improve from 0.34343
Epoch 99/300
 - 10s - loss: 2.6803 - acc: 0.8992 - mDice: 0.3531 - val_loss: 3.3335 - val_acc: 0.9263 - val_mDice: 0.3430

Epoch 00099: val_mDice did not improve from 0.34343
Epoch 100/300
 - 10s - loss: 2.6670 - acc: 0.8994 - mDice: 0.3550 - val_loss: 3.7308 - val_acc: 0.9263 - val_mDice: 0.3425

Epoch 00100: val_mDice did not improve from 0.34343
Epoch 101/300
 - 10s - loss: 2.6660 - acc: 0.8994 - mDice: 0.3546 - val_loss: 3.4810 - val_acc: 0.9288 - val_mDice: 0.3487

Epoch 00101: val_mDice improved from 0.34343 to 0.34870, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 10s - loss: 2.6469 - acc: 0.8999 - mDice: 0.3591 - val_loss: 3.3930 - val_acc: 0.9279 - val_mDice: 0.3504

Epoch 00102: val_mDice improved from 0.34870 to 0.35040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 10s - loss: 2.6282 - acc: 0.9003 - mDice: 0.3613 - val_loss: 3.4719 - val_acc: 0.9276 - val_mDice: 0.3500

Epoch 00103: val_mDice did not improve from 0.35040
Epoch 104/300
 - 10s - loss: 2.6227 - acc: 0.9003 - mDice: 0.3633 - val_loss: 3.4869 - val_acc: 0.9275 - val_mDice: 0.3504

Epoch 00104: val_mDice improved from 0.35040 to 0.35041, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 105/300
 - 10s - loss: 2.6122 - acc: 0.9008 - mDice: 0.3650 - val_loss: 3.3887 - val_acc: 0.9275 - val_mDice: 0.3510

Epoch 00105: val_mDice improved from 0.35041 to 0.35103, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 10s - loss: 2.5971 - acc: 0.9010 - mDice: 0.3673 - val_loss: 3.5162 - val_acc: 0.9295 - val_mDice: 0.3476

Epoch 00106: val_mDice did not improve from 0.35103
Epoch 107/300
 - 10s - loss: 2.5915 - acc: 0.9011 - mDice: 0.3686 - val_loss: 3.4329 - val_acc: 0.9275 - val_mDice: 0.3430

Epoch 00107: val_mDice did not improve from 0.35103
Epoch 108/300
 - 10s - loss: 2.5796 - acc: 0.9015 - mDice: 0.3701 - val_loss: 3.3981 - val_acc: 0.9257 - val_mDice: 0.3393

Epoch 00108: val_mDice did not improve from 0.35103
Epoch 109/300
 - 10s - loss: 2.5726 - acc: 0.9016 - mDice: 0.3722 - val_loss: 3.3863 - val_acc: 0.9234 - val_mDice: 0.3347

Epoch 00109: val_mDice did not improve from 0.35103
Epoch 110/300
 - 10s - loss: 2.5679 - acc: 0.9018 - mDice: 0.3726 - val_loss: 3.4338 - val_acc: 0.9281 - val_mDice: 0.3520

Epoch 00110: val_mDice improved from 0.35103 to 0.35202, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 111/300
 - 10s - loss: 2.5563 - acc: 0.9023 - mDice: 0.3742 - val_loss: 3.4512 - val_acc: 0.9296 - val_mDice: 0.3601

Epoch 00111: val_mDice improved from 0.35202 to 0.36012, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 10s - loss: 2.5377 - acc: 0.9027 - mDice: 0.3774 - val_loss: 3.3672 - val_acc: 0.9287 - val_mDice: 0.3546

Epoch 00112: val_mDice did not improve from 0.36012
Epoch 113/300
 - 10s - loss: 2.5371 - acc: 0.9028 - mDice: 0.3780 - val_loss: 3.4265 - val_acc: 0.9305 - val_mDice: 0.3613

Epoch 00113: val_mDice improved from 0.36012 to 0.36128, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 114/300
 - 10s - loss: 2.5269 - acc: 0.9031 - mDice: 0.3799 - val_loss: 3.4800 - val_acc: 0.9286 - val_mDice: 0.3535

Epoch 00114: val_mDice did not improve from 0.36128
Epoch 115/300
 - 10s - loss: 2.5247 - acc: 0.9030 - mDice: 0.3804 - val_loss: 3.5767 - val_acc: 0.9295 - val_mDice: 0.3621

Epoch 00115: val_mDice improved from 0.36128 to 0.36214, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 116/300
 - 10s - loss: 2.5138 - acc: 0.9034 - mDice: 0.3827 - val_loss: 3.6295 - val_acc: 0.9302 - val_mDice: 0.3589

Epoch 00116: val_mDice did not improve from 0.36214
Epoch 117/300
 - 10s - loss: 2.5025 - acc: 0.9036 - mDice: 0.3844 - val_loss: 3.4307 - val_acc: 0.9329 - val_mDice: 0.3702

Epoch 00117: val_mDice improved from 0.36214 to 0.37022, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 118/300
 - 10s - loss: 2.4891 - acc: 0.9039 - mDice: 0.3876 - val_loss: 3.7328 - val_acc: 0.9292 - val_mDice: 0.3471

Epoch 00118: val_mDice did not improve from 0.37022
Epoch 119/300
 - 10s - loss: 2.4915 - acc: 0.9040 - mDice: 0.3870 - val_loss: 3.5074 - val_acc: 0.9303 - val_mDice: 0.3552

Epoch 00119: val_mDice did not improve from 0.37022
Epoch 120/300
 - 10s - loss: 2.4819 - acc: 0.9041 - mDice: 0.3886 - val_loss: 3.4637 - val_acc: 0.9310 - val_mDice: 0.3666

Epoch 00120: val_mDice did not improve from 0.37022
Epoch 121/300
 - 10s - loss: 2.4719 - acc: 0.9043 - mDice: 0.3907 - val_loss: 3.5032 - val_acc: 0.9323 - val_mDice: 0.3711

Epoch 00121: val_mDice improved from 0.37022 to 0.37107, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 10s - loss: 2.4617 - acc: 0.9045 - mDice: 0.3918 - val_loss: 3.5077 - val_acc: 0.9305 - val_mDice: 0.3582

Epoch 00122: val_mDice did not improve from 0.37107
Epoch 123/300
 - 10s - loss: 2.4675 - acc: 0.9045 - mDice: 0.3918 - val_loss: 3.2782 - val_acc: 0.9309 - val_mDice: 0.3674

Epoch 00123: val_mDice did not improve from 0.37107
Epoch 124/300
 - 10s - loss: 2.4536 - acc: 0.9050 - mDice: 0.3941 - val_loss: 3.4510 - val_acc: 0.9306 - val_mDice: 0.3559

Epoch 00124: val_mDice did not improve from 0.37107
Epoch 125/300
 - 10s - loss: 2.4536 - acc: 0.9049 - mDice: 0.3939 - val_loss: 3.4588 - val_acc: 0.9324 - val_mDice: 0.3688

Epoch 00125: val_mDice did not improve from 0.37107
Epoch 126/300
 - 10s - loss: 2.4402 - acc: 0.9054 - mDice: 0.3977 - val_loss: 3.4455 - val_acc: 0.9307 - val_mDice: 0.3613

Epoch 00126: val_mDice did not improve from 0.37107
Epoch 127/300
 - 10s - loss: 2.4393 - acc: 0.9053 - mDice: 0.3964 - val_loss: 3.3682 - val_acc: 0.9284 - val_mDice: 0.3577

Epoch 00127: val_mDice did not improve from 0.37107
Epoch 128/300
 - 10s - loss: 2.4337 - acc: 0.9056 - mDice: 0.3991 - val_loss: 3.4900 - val_acc: 0.9313 - val_mDice: 0.3615

Epoch 00128: val_mDice did not improve from 0.37107
Epoch 129/300
 - 10s - loss: 2.4322 - acc: 0.9057 - mDice: 0.3985 - val_loss: 3.2992 - val_acc: 0.9305 - val_mDice: 0.3575

Epoch 00129: val_mDice did not improve from 0.37107
Epoch 130/300
 - 10s - loss: 2.4250 - acc: 0.9062 - mDice: 0.4007 - val_loss: 3.5658 - val_acc: 0.9329 - val_mDice: 0.3750

Epoch 00130: val_mDice improved from 0.37107 to 0.37496, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 131/300
 - 10s - loss: 2.4231 - acc: 0.9066 - mDice: 0.4011 - val_loss: 3.3362 - val_acc: 0.9316 - val_mDice: 0.3671

Epoch 00131: val_mDice did not improve from 0.37496
Epoch 132/300
 - 10s - loss: 2.4098 - acc: 0.9071 - mDice: 0.4045 - val_loss: 3.3204 - val_acc: 0.9330 - val_mDice: 0.3677

Epoch 00132: val_mDice did not improve from 0.37496
Epoch 133/300
 - 10s - loss: 2.4049 - acc: 0.9072 - mDice: 0.4048 - val_loss: 3.2210 - val_acc: 0.9323 - val_mDice: 0.3676

Epoch 00133: val_mDice did not improve from 0.37496
Epoch 134/300
 - 10s - loss: 2.4088 - acc: 0.9073 - mDice: 0.4044 - val_loss: 3.3930 - val_acc: 0.9324 - val_mDice: 0.3618

Epoch 00134: val_mDice did not improve from 0.37496
Epoch 135/300
 - 10s - loss: 2.3993 - acc: 0.9076 - mDice: 0.4066 - val_loss: 3.7404 - val_acc: 0.9313 - val_mDice: 0.3581

Epoch 00135: val_mDice did not improve from 0.37496
Epoch 136/300
 - 10s - loss: 2.3950 - acc: 0.9076 - mDice: 0.4073 - val_loss: 3.4635 - val_acc: 0.9331 - val_mDice: 0.3728

Epoch 00136: val_mDice did not improve from 0.37496
Epoch 137/300
 - 10s - loss: 2.3895 - acc: 0.9077 - mDice: 0.4087 - val_loss: 3.5275 - val_acc: 0.9326 - val_mDice: 0.3624

Epoch 00137: val_mDice did not improve from 0.37496
Epoch 138/300
 - 10s - loss: 2.3874 - acc: 0.9080 - mDice: 0.4092 - val_loss: 3.7989 - val_acc: 0.9322 - val_mDice: 0.3584

Epoch 00138: val_mDice did not improve from 0.37496
Epoch 139/300
 - 10s - loss: 2.3799 - acc: 0.9081 - mDice: 0.4095 - val_loss: 3.2062 - val_acc: 0.9316 - val_mDice: 0.3693

Epoch 00139: val_mDice did not improve from 0.37496
Epoch 140/300
 - 10s - loss: 2.3754 - acc: 0.9082 - mDice: 0.4113 - val_loss: 3.4076 - val_acc: 0.9326 - val_mDice: 0.3665

Epoch 00140: val_mDice did not improve from 0.37496
Epoch 141/300
 - 10s - loss: 2.3738 - acc: 0.9081 - mDice: 0.4110 - val_loss: 3.4450 - val_acc: 0.9333 - val_mDice: 0.3696

Epoch 00141: val_mDice did not improve from 0.37496
Epoch 142/300
 - 10s - loss: 2.3692 - acc: 0.9085 - mDice: 0.4122 - val_loss: 3.2746 - val_acc: 0.9327 - val_mDice: 0.3677

Epoch 00142: val_mDice did not improve from 0.37496
Epoch 143/300
 - 10s - loss: 2.3654 - acc: 0.9084 - mDice: 0.4129 - val_loss: 3.2943 - val_acc: 0.9331 - val_mDice: 0.3680

Epoch 00143: val_mDice did not improve from 0.37496
Epoch 144/300
 - 10s - loss: 2.3590 - acc: 0.9084 - mDice: 0.4145 - val_loss: 3.5861 - val_acc: 0.9331 - val_mDice: 0.3726

Epoch 00144: val_mDice did not improve from 0.37496
Epoch 145/300
 - 10s - loss: 2.3610 - acc: 0.9087 - mDice: 0.4145 - val_loss: 3.3192 - val_acc: 0.9306 - val_mDice: 0.3572

Epoch 00145: val_mDice did not improve from 0.37496
Epoch 146/300
 - 10s - loss: 2.3545 - acc: 0.9086 - mDice: 0.4147 - val_loss: 3.4129 - val_acc: 0.9335 - val_mDice: 0.3727

Epoch 00146: val_mDice did not improve from 0.37496
Epoch 147/300
 - 10s - loss: 2.3511 - acc: 0.9087 - mDice: 0.4159 - val_loss: 3.3638 - val_acc: 0.9329 - val_mDice: 0.3721

Epoch 00147: val_mDice did not improve from 0.37496
Epoch 148/300
 - 10s - loss: 2.3478 - acc: 0.9086 - mDice: 0.4162 - val_loss: 3.9117 - val_acc: 0.9330 - val_mDice: 0.3507

Epoch 00148: val_mDice did not improve from 0.37496
Epoch 149/300
 - 10s - loss: 2.3473 - acc: 0.9090 - mDice: 0.4172 - val_loss: 3.2366 - val_acc: 0.9332 - val_mDice: 0.3765

Epoch 00149: val_mDice improved from 0.37496 to 0.37651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 150/300
 - 10s - loss: 2.3420 - acc: 0.9089 - mDice: 0.4183 - val_loss: 3.3634 - val_acc: 0.9334 - val_mDice: 0.3685

Epoch 00150: val_mDice did not improve from 0.37651
Epoch 151/300
 - 10s - loss: 2.3377 - acc: 0.9090 - mDice: 0.4197 - val_loss: 3.3201 - val_acc: 0.9353 - val_mDice: 0.3862

Epoch 00151: val_mDice improved from 0.37651 to 0.38619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 152/300
 - 10s - loss: 2.3307 - acc: 0.9091 - mDice: 0.4208 - val_loss: 3.3331 - val_acc: 0.9342 - val_mDice: 0.3755

Epoch 00152: val_mDice did not improve from 0.38619
Epoch 153/300
 - 10s - loss: 2.3212 - acc: 0.9094 - mDice: 0.4224 - val_loss: 3.3450 - val_acc: 0.9334 - val_mDice: 0.3777

Epoch 00153: val_mDice did not improve from 0.38619
Epoch 154/300
 - 10s - loss: 2.3211 - acc: 0.9093 - mDice: 0.4224 - val_loss: 3.4119 - val_acc: 0.9340 - val_mDice: 0.3751

Epoch 00154: val_mDice did not improve from 0.38619
Epoch 155/300
 - 10s - loss: 2.3184 - acc: 0.9093 - mDice: 0.4233 - val_loss: 3.7250 - val_acc: 0.9330 - val_mDice: 0.3645

Epoch 00155: val_mDice did not improve from 0.38619
Epoch 156/300
 - 10s - loss: 2.3176 - acc: 0.9095 - mDice: 0.4234 - val_loss: 4.0789 - val_acc: 0.9331 - val_mDice: 0.3587

Epoch 00156: val_mDice did not improve from 0.38619
Epoch 157/300
 - 10s - loss: 2.3173 - acc: 0.9093 - mDice: 0.4240 - val_loss: 3.6442 - val_acc: 0.9332 - val_mDice: 0.3688

Epoch 00157: val_mDice did not improve from 0.38619
Epoch 158/300
 - 10s - loss: 2.3096 - acc: 0.9095 - mDice: 0.4249 - val_loss: 3.4004 - val_acc: 0.9320 - val_mDice: 0.3686

Epoch 00158: val_mDice did not improve from 0.38619
Epoch 159/300
 - 10s - loss: 2.3100 - acc: 0.9095 - mDice: 0.4256 - val_loss: 3.3498 - val_acc: 0.9316 - val_mDice: 0.3716

Epoch 00159: val_mDice did not improve from 0.38619
Epoch 160/300
 - 10s - loss: 2.3046 - acc: 0.9097 - mDice: 0.4265 - val_loss: 3.4326 - val_acc: 0.9320 - val_mDice: 0.3697

Epoch 00160: val_mDice did not improve from 0.38619
Epoch 161/300
 - 10s - loss: 2.3017 - acc: 0.9096 - mDice: 0.4266 - val_loss: 3.3793 - val_acc: 0.9343 - val_mDice: 0.3839

Epoch 00161: val_mDice did not improve from 0.38619
Epoch 162/300
 - 10s - loss: 2.2987 - acc: 0.9098 - mDice: 0.4274 - val_loss: 3.2983 - val_acc: 0.9335 - val_mDice: 0.3772

Epoch 00162: val_mDice did not improve from 0.38619
Epoch 163/300
 - 10s - loss: 2.3008 - acc: 0.9097 - mDice: 0.4272 - val_loss: 3.3298 - val_acc: 0.9333 - val_mDice: 0.3719

Epoch 00163: val_mDice did not improve from 0.38619
Epoch 164/300
 - 10s - loss: 2.2909 - acc: 0.9099 - mDice: 0.4289 - val_loss: 3.3296 - val_acc: 0.9327 - val_mDice: 0.3724

Epoch 00164: val_mDice did not improve from 0.38619
Epoch 165/300
 - 10s - loss: 2.2937 - acc: 0.9098 - mDice: 0.4283 - val_loss: 3.3855 - val_acc: 0.9339 - val_mDice: 0.3785

Epoch 00165: val_mDice did not improve from 0.38619
Epoch 166/300
 - 10s - loss: 2.2870 - acc: 0.9101 - mDice: 0.4305 - val_loss: 3.3802 - val_acc: 0.9340 - val_mDice: 0.3780

Epoch 00166: val_mDice did not improve from 0.38619
Epoch 167/300
 - 10s - loss: 2.2787 - acc: 0.9100 - mDice: 0.4312 - val_loss: 3.1842 - val_acc: 0.9350 - val_mDice: 0.3873

Epoch 00167: val_mDice improved from 0.38619 to 0.38734, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 168/300
 - 10s - loss: 2.2933 - acc: 0.9097 - mDice: 0.4289 - val_loss: 3.2935 - val_acc: 0.9317 - val_mDice: 0.3716

Epoch 00168: val_mDice did not improve from 0.38734
Epoch 169/300
 - 10s - loss: 2.2831 - acc: 0.9101 - mDice: 0.4309 - val_loss: 3.5967 - val_acc: 0.9314 - val_mDice: 0.3590

Epoch 00169: val_mDice did not improve from 0.38734
Epoch 170/300
 - 10s - loss: 2.2779 - acc: 0.9100 - mDice: 0.4320 - val_loss: 3.3800 - val_acc: 0.9347 - val_mDice: 0.3784

Epoch 00170: val_mDice did not improve from 0.38734
Epoch 171/300
 - 10s - loss: 2.2691 - acc: 0.9102 - mDice: 0.4334 - val_loss: 3.4011 - val_acc: 0.9349 - val_mDice: 0.3907

Epoch 00171: val_mDice improved from 0.38734 to 0.39075, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 172/300
 - 10s - loss: 2.2747 - acc: 0.9101 - mDice: 0.4326 - val_loss: 3.3245 - val_acc: 0.9354 - val_mDice: 0.3867

Epoch 00172: val_mDice did not improve from 0.39075
Epoch 173/300
 - 10s - loss: 2.2738 - acc: 0.9101 - mDice: 0.4329 - val_loss: 3.4470 - val_acc: 0.9345 - val_mDice: 0.3737

Epoch 00173: val_mDice did not improve from 0.39075
Epoch 174/300
 - 10s - loss: 2.2691 - acc: 0.9101 - mDice: 0.4340 - val_loss: 3.4415 - val_acc: 0.9349 - val_mDice: 0.3870

Epoch 00174: val_mDice did not improve from 0.39075
Epoch 175/300
 - 10s - loss: 2.2688 - acc: 0.9100 - mDice: 0.4340 - val_loss: 3.6631 - val_acc: 0.9327 - val_mDice: 0.3653

Epoch 00175: val_mDice did not improve from 0.39075
Epoch 176/300
 - 10s - loss: 2.2636 - acc: 0.9103 - mDice: 0.4356 - val_loss: 3.5031 - val_acc: 0.9345 - val_mDice: 0.3801

Epoch 00176: val_mDice did not improve from 0.39075
Epoch 177/300
 - 10s - loss: 2.2602 - acc: 0.9102 - mDice: 0.4349 - val_loss: 3.4312 - val_acc: 0.9342 - val_mDice: 0.3798

Epoch 00177: val_mDice did not improve from 0.39075
Epoch 178/300
 - 10s - loss: 2.2582 - acc: 0.9103 - mDice: 0.4356 - val_loss: 3.3685 - val_acc: 0.9338 - val_mDice: 0.3771

Epoch 00178: val_mDice did not improve from 0.39075
Epoch 179/300
 - 10s - loss: 2.2587 - acc: 0.9102 - mDice: 0.4353 - val_loss: 3.4603 - val_acc: 0.9331 - val_mDice: 0.3798

Epoch 00179: val_mDice did not improve from 0.39075
Epoch 180/300
 - 10s - loss: 2.2598 - acc: 0.9101 - mDice: 0.4362 - val_loss: 3.3776 - val_acc: 0.9347 - val_mDice: 0.3797

Epoch 00180: val_mDice did not improve from 0.39075
Epoch 181/300
 - 10s - loss: 2.2519 - acc: 0.9103 - mDice: 0.4374 - val_loss: 3.5454 - val_acc: 0.9342 - val_mDice: 0.3747

Epoch 00181: val_mDice did not improve from 0.39075
Epoch 182/300
 - 10s - loss: 2.2522 - acc: 0.9103 - mDice: 0.4370 - val_loss: 3.5082 - val_acc: 0.9342 - val_mDice: 0.3789

Epoch 00182: val_mDice did not improve from 0.39075
Epoch 183/300
 - 10s - loss: 2.2463 - acc: 0.9104 - mDice: 0.4380 - val_loss: 3.5560 - val_acc: 0.9316 - val_mDice: 0.3643

Epoch 00183: val_mDice did not improve from 0.39075
Epoch 184/300
 - 10s - loss: 2.2424 - acc: 0.9105 - mDice: 0.4399 - val_loss: 3.3741 - val_acc: 0.9335 - val_mDice: 0.3817

Epoch 00184: val_mDice did not improve from 0.39075
Epoch 185/300
 - 10s - loss: 2.2330 - acc: 0.9106 - mDice: 0.4413 - val_loss: 3.3509 - val_acc: 0.9340 - val_mDice: 0.3853

Epoch 00185: val_mDice did not improve from 0.39075
Epoch 186/300
 - 10s - loss: 2.2365 - acc: 0.9106 - mDice: 0.4403 - val_loss: 3.5738 - val_acc: 0.9341 - val_mDice: 0.3778

Epoch 00186: val_mDice did not improve from 0.39075
Epoch 187/300
 - 10s - loss: 2.2379 - acc: 0.9106 - mDice: 0.4406 - val_loss: 3.4599 - val_acc: 0.9339 - val_mDice: 0.3858

Epoch 00187: val_mDice did not improve from 0.39075
Epoch 188/300
 - 10s - loss: 2.2401 - acc: 0.9103 - mDice: 0.4392 - val_loss: 3.1928 - val_acc: 0.9325 - val_mDice: 0.3823

Epoch 00188: val_mDice did not improve from 0.39075
Epoch 189/300
 - 10s - loss: 2.2300 - acc: 0.9107 - mDice: 0.4411 - val_loss: 3.3809 - val_acc: 0.9331 - val_mDice: 0.3819

Epoch 00189: val_mDice did not improve from 0.39075
Epoch 190/300
 - 10s - loss: 2.2335 - acc: 0.9106 - mDice: 0.4412 - val_loss: 3.3393 - val_acc: 0.9349 - val_mDice: 0.3902

Epoch 00190: val_mDice did not improve from 0.39075
Epoch 191/300
 - 10s - loss: 2.2289 - acc: 0.9106 - mDice: 0.4422 - val_loss: 3.3483 - val_acc: 0.9318 - val_mDice: 0.3715

Epoch 00191: val_mDice did not improve from 0.39075
Epoch 192/300
 - 10s - loss: 2.2240 - acc: 0.9106 - mDice: 0.4424 - val_loss: 3.2707 - val_acc: 0.9322 - val_mDice: 0.3795

Epoch 00192: val_mDice did not improve from 0.39075
Epoch 193/300
 - 10s - loss: 2.2240 - acc: 0.9105 - mDice: 0.4432 - val_loss: 3.3041 - val_acc: 0.9354 - val_mDice: 0.3912

Epoch 00193: val_mDice improved from 0.39075 to 0.39123, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 194/300
 - 10s - loss: 2.2156 - acc: 0.9108 - mDice: 0.4446 - val_loss: 3.3368 - val_acc: 0.9342 - val_mDice: 0.3919

Epoch 00194: val_mDice improved from 0.39123 to 0.39188, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 195/300
 - 10s - loss: 2.2214 - acc: 0.9106 - mDice: 0.4440 - val_loss: 3.4563 - val_acc: 0.9343 - val_mDice: 0.3801

Epoch 00195: val_mDice did not improve from 0.39188
Epoch 196/300
 - 10s - loss: 2.2113 - acc: 0.9107 - mDice: 0.4459 - val_loss: 3.2532 - val_acc: 0.9328 - val_mDice: 0.3819

Epoch 00196: val_mDice did not improve from 0.39188
Epoch 197/300
 - 10s - loss: 2.2107 - acc: 0.9108 - mDice: 0.4460 - val_loss: 3.7169 - val_acc: 0.9351 - val_mDice: 0.3849

Epoch 00197: val_mDice did not improve from 0.39188
Epoch 198/300
 - 10s - loss: 2.2179 - acc: 0.9105 - mDice: 0.4444 - val_loss: 3.4218 - val_acc: 0.9330 - val_mDice: 0.3799

Epoch 00198: val_mDice did not improve from 0.39188
Epoch 199/300
 - 10s - loss: 2.2113 - acc: 0.9106 - mDice: 0.4457 - val_loss: 3.0864 - val_acc: 0.9336 - val_mDice: 0.3975

Epoch 00199: val_mDice improved from 0.39188 to 0.39755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 200/300
 - 10s - loss: 2.2088 - acc: 0.9107 - mDice: 0.4464 - val_loss: 3.3953 - val_acc: 0.9336 - val_mDice: 0.3866

Epoch 00200: val_mDice did not improve from 0.39755
Epoch 201/300
 - 10s - loss: 2.2099 - acc: 0.9106 - mDice: 0.4469 - val_loss: 3.4876 - val_acc: 0.9339 - val_mDice: 0.3850

Epoch 00201: val_mDice did not improve from 0.39755
Epoch 202/300
 - 10s - loss: 2.2092 - acc: 0.9107 - mDice: 0.4464 - val_loss: 3.4412 - val_acc: 0.9349 - val_mDice: 0.3952

Epoch 00202: val_mDice did not improve from 0.39755
Epoch 203/300
 - 10s - loss: 2.2037 - acc: 0.9109 - mDice: 0.4473 - val_loss: 3.4250 - val_acc: 0.9338 - val_mDice: 0.3884

Epoch 00203: val_mDice did not improve from 0.39755
Epoch 204/300
 - 10s - loss: 2.2044 - acc: 0.9108 - mDice: 0.4471 - val_loss: 3.6381 - val_acc: 0.9349 - val_mDice: 0.3772

Epoch 00204: val_mDice did not improve from 0.39755
Epoch 205/300
 - 10s - loss: 2.1949 - acc: 0.9109 - mDice: 0.4488 - val_loss: 3.5351 - val_acc: 0.9342 - val_mDice: 0.3895

Epoch 00205: val_mDice did not improve from 0.39755
Epoch 206/300
 - 10s - loss: 2.1983 - acc: 0.9108 - mDice: 0.4483 - val_loss: 3.1894 - val_acc: 0.9340 - val_mDice: 0.3980

Epoch 00206: val_mDice improved from 0.39755 to 0.39795, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 207/300
 - 10s - loss: 2.1923 - acc: 0.9109 - mDice: 0.4495 - val_loss: 3.1810 - val_acc: 0.9341 - val_mDice: 0.3918

Epoch 00207: val_mDice did not improve from 0.39795
Epoch 208/300
 - 10s - loss: 2.1938 - acc: 0.9110 - mDice: 0.4502 - val_loss: 3.3085 - val_acc: 0.9332 - val_mDice: 0.3848

Epoch 00208: val_mDice did not improve from 0.39795
Epoch 209/300
 - 10s - loss: 2.1992 - acc: 0.9108 - mDice: 0.4489 - val_loss: 3.2855 - val_acc: 0.9342 - val_mDice: 0.3942

Epoch 00209: val_mDice did not improve from 0.39795
Epoch 210/300
 - 10s - loss: 2.1855 - acc: 0.9112 - mDice: 0.4515 - val_loss: 3.6292 - val_acc: 0.9338 - val_mDice: 0.3858

Epoch 00210: val_mDice did not improve from 0.39795
Epoch 211/300
 - 10s - loss: 2.1897 - acc: 0.9108 - mDice: 0.4502 - val_loss: 3.2759 - val_acc: 0.9338 - val_mDice: 0.3928

Epoch 00211: val_mDice did not improve from 0.39795
Epoch 212/300
 - 10s - loss: 2.1916 - acc: 0.9109 - mDice: 0.4507 - val_loss: 3.3781 - val_acc: 0.9349 - val_mDice: 0.3922

Epoch 00212: val_mDice did not improve from 0.39795
Epoch 213/300
 - 10s - loss: 2.1935 - acc: 0.9109 - mDice: 0.4505 - val_loss: 3.3102 - val_acc: 0.9332 - val_mDice: 0.3831

Epoch 00213: val_mDice did not improve from 0.39795
Epoch 214/300
 - 10s - loss: 2.1873 - acc: 0.9111 - mDice: 0.4513 - val_loss: 3.3021 - val_acc: 0.9337 - val_mDice: 0.3916

Epoch 00214: val_mDice did not improve from 0.39795
Epoch 215/300
 - 10s - loss: 2.1798 - acc: 0.9112 - mDice: 0.4522 - val_loss: 3.3228 - val_acc: 0.9335 - val_mDice: 0.3860

Epoch 00215: val_mDice did not improve from 0.39795
Epoch 216/300
 - 10s - loss: 2.1812 - acc: 0.9110 - mDice: 0.4524 - val_loss: 3.7170 - val_acc: 0.9344 - val_mDice: 0.3826

Epoch 00216: val_mDice did not improve from 0.39795
Epoch 217/300
 - 10s - loss: 2.1836 - acc: 0.9113 - mDice: 0.4525 - val_loss: 3.2915 - val_acc: 0.9343 - val_mDice: 0.3974

Epoch 00217: val_mDice did not improve from 0.39795
Epoch 218/300
 - 10s - loss: 2.1803 - acc: 0.9110 - mDice: 0.4524 - val_loss: 3.4219 - val_acc: 0.9342 - val_mDice: 0.3916

Epoch 00218: val_mDice did not improve from 0.39795
Epoch 219/300
 - 10s - loss: 2.1783 - acc: 0.9112 - mDice: 0.4530 - val_loss: 3.2523 - val_acc: 0.9348 - val_mDice: 0.3979

Epoch 00219: val_mDice did not improve from 0.39795
Epoch 220/300
 - 10s - loss: 2.1689 - acc: 0.9115 - mDice: 0.4550 - val_loss: 3.7124 - val_acc: 0.9342 - val_mDice: 0.3845

Epoch 00220: val_mDice did not improve from 0.39795
Epoch 221/300
 - 10s - loss: 2.1786 - acc: 0.9114 - mDice: 0.4530 - val_loss: 3.3238 - val_acc: 0.9335 - val_mDice: 0.3898

Epoch 00221: val_mDice did not improve from 0.39795
Epoch 222/300
 - 10s - loss: 2.1759 - acc: 0.9111 - mDice: 0.4527 - val_loss: 3.2649 - val_acc: 0.9336 - val_mDice: 0.3989

Epoch 00222: val_mDice improved from 0.39795 to 0.39891, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 223/300
 - 10s - loss: 2.1729 - acc: 0.9113 - mDice: 0.4544 - val_loss: 3.6105 - val_acc: 0.9329 - val_mDice: 0.3831

Epoch 00223: val_mDice did not improve from 0.39891
Epoch 224/300
 - 10s - loss: 2.1752 - acc: 0.9112 - mDice: 0.4534 - val_loss: 3.2327 - val_acc: 0.9354 - val_mDice: 0.4055

Epoch 00224: val_mDice improved from 0.39891 to 0.40553, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 225/300
 - 10s - loss: 2.1692 - acc: 0.9116 - mDice: 0.4555 - val_loss: 3.1273 - val_acc: 0.9339 - val_mDice: 0.3988

Epoch 00225: val_mDice did not improve from 0.40553
Epoch 226/300
 - 10s - loss: 2.1656 - acc: 0.9115 - mDice: 0.4557 - val_loss: 3.1058 - val_acc: 0.9345 - val_mDice: 0.4013

Epoch 00226: val_mDice did not improve from 0.40553
Epoch 227/300
 - 10s - loss: 2.1671 - acc: 0.9115 - mDice: 0.4548 - val_loss: 3.2197 - val_acc: 0.9349 - val_mDice: 0.4021

Epoch 00227: val_mDice did not improve from 0.40553
Epoch 228/300
 - 10s - loss: 2.1641 - acc: 0.9115 - mDice: 0.4559 - val_loss: 3.4989 - val_acc: 0.9336 - val_mDice: 0.3864

Epoch 00228: val_mDice did not improve from 0.40553
Epoch 229/300
 - 10s - loss: 2.1630 - acc: 0.9115 - mDice: 0.4554 - val_loss: 3.2295 - val_acc: 0.9323 - val_mDice: 0.3932

Epoch 00229: val_mDice did not improve from 0.40553
Epoch 230/300
 - 10s - loss: 2.1659 - acc: 0.9116 - mDice: 0.4554 - val_loss: 3.4081 - val_acc: 0.9343 - val_mDice: 0.3951

Epoch 00230: val_mDice did not improve from 0.40553
Epoch 231/300
 - 10s - loss: 2.1583 - acc: 0.9115 - mDice: 0.4565 - val_loss: 3.4474 - val_acc: 0.9337 - val_mDice: 0.3894

Epoch 00231: val_mDice did not improve from 0.40553
Epoch 232/300
 - 10s - loss: 2.1583 - acc: 0.9116 - mDice: 0.4575 - val_loss: 3.4183 - val_acc: 0.9338 - val_mDice: 0.3956

Epoch 00232: val_mDice did not improve from 0.40553
Epoch 233/300
 - 10s - loss: 2.1557 - acc: 0.9117 - mDice: 0.4575 - val_loss: 3.2990 - val_acc: 0.9345 - val_mDice: 0.4053

Epoch 00233: val_mDice did not improve from 0.40553
Epoch 234/300
 - 10s - loss: 2.1576 - acc: 0.9117 - mDice: 0.4576 - val_loss: 3.3703 - val_acc: 0.9333 - val_mDice: 0.3935

Epoch 00234: val_mDice did not improve from 0.40553
Epoch 235/300
 - 10s - loss: 2.1564 - acc: 0.9116 - mDice: 0.4573 - val_loss: 3.3463 - val_acc: 0.9342 - val_mDice: 0.4008

Epoch 00235: val_mDice did not improve from 0.40553
Epoch 236/300
 - 10s - loss: 2.1573 - acc: 0.9116 - mDice: 0.4576 - val_loss: 3.4550 - val_acc: 0.9349 - val_mDice: 0.3995

Epoch 00236: val_mDice did not improve from 0.40553
Epoch 237/300
 - 10s - loss: 2.1486 - acc: 0.9117 - mDice: 0.4588 - val_loss: 3.5559 - val_acc: 0.9334 - val_mDice: 0.3905

Epoch 00237: val_mDice did not improve from 0.40553
Epoch 238/300
 - 10s - loss: 2.1578 - acc: 0.9116 - mDice: 0.4577 - val_loss: 3.2613 - val_acc: 0.9334 - val_mDice: 0.3994

Epoch 00238: val_mDice did not improve from 0.40553
Epoch 239/300
 - 10s - loss: 2.1512 - acc: 0.9116 - mDice: 0.4584 - val_loss: 3.4532 - val_acc: 0.9340 - val_mDice: 0.3976

Epoch 00239: val_mDice did not improve from 0.40553
Epoch 240/300
 - 10s - loss: 2.1505 - acc: 0.9118 - mDice: 0.4585 - val_loss: 3.3993 - val_acc: 0.9350 - val_mDice: 0.4039

Epoch 00240: val_mDice did not improve from 0.40553
Epoch 241/300
 - 10s - loss: 2.1526 - acc: 0.9115 - mDice: 0.4581 - val_loss: 3.5395 - val_acc: 0.9340 - val_mDice: 0.3921

Epoch 00241: val_mDice did not improve from 0.40553
Epoch 242/300
 - 10s - loss: 2.1493 - acc: 0.9120 - mDice: 0.4599 - val_loss: 3.3803 - val_acc: 0.9322 - val_mDice: 0.3889

Epoch 00242: val_mDice did not improve from 0.40553
Epoch 243/300
 - 10s - loss: 2.1469 - acc: 0.9120 - mDice: 0.4604 - val_loss: 3.2474 - val_acc: 0.9336 - val_mDice: 0.3935

Epoch 00243: val_mDice did not improve from 0.40553
Epoch 244/300
 - 10s - loss: 2.1476 - acc: 0.9118 - mDice: 0.4593 - val_loss: 3.5358 - val_acc: 0.9338 - val_mDice: 0.3944

Epoch 00244: val_mDice did not improve from 0.40553
Epoch 245/300
 - 10s - loss: 2.1416 - acc: 0.9119 - mDice: 0.4608 - val_loss: 3.1559 - val_acc: 0.9354 - val_mDice: 0.4098

Epoch 00245: val_mDice improved from 0.40553 to 0.40981, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 246/300
 - 10s - loss: 2.1428 - acc: 0.9118 - mDice: 0.4602 - val_loss: 3.3095 - val_acc: 0.9343 - val_mDice: 0.4028

Epoch 00246: val_mDice did not improve from 0.40981
Epoch 247/300
 - 10s - loss: 2.1406 - acc: 0.9118 - mDice: 0.4610 - val_loss: 3.6143 - val_acc: 0.9340 - val_mDice: 0.3986

Epoch 00247: val_mDice did not improve from 0.40981
Epoch 248/300
 - 10s - loss: 2.1430 - acc: 0.9117 - mDice: 0.4604 - val_loss: 3.3924 - val_acc: 0.9347 - val_mDice: 0.3946

Epoch 00248: val_mDice did not improve from 0.40981
Epoch 249/300
 - 10s - loss: 2.1356 - acc: 0.9120 - mDice: 0.4620 - val_loss: 3.4017 - val_acc: 0.9319 - val_mDice: 0.3902

Epoch 00249: val_mDice did not improve from 0.40981
Epoch 250/300
 - 10s - loss: 2.1437 - acc: 0.9118 - mDice: 0.4606 - val_loss: 3.2574 - val_acc: 0.9340 - val_mDice: 0.4024

Epoch 00250: val_mDice did not improve from 0.40981
Epoch 251/300
 - 10s - loss: 2.1335 - acc: 0.9121 - mDice: 0.4624 - val_loss: 3.2017 - val_acc: 0.9328 - val_mDice: 0.3938

Epoch 00251: val_mDice did not improve from 0.40981
Epoch 252/300
 - 10s - loss: 2.1380 - acc: 0.9119 - mDice: 0.4620 - val_loss: 3.2552 - val_acc: 0.9344 - val_mDice: 0.3964

Epoch 00252: val_mDice did not improve from 0.40981
Epoch 253/300
 - 10s - loss: 2.1355 - acc: 0.9122 - mDice: 0.4621 - val_loss: 3.4002 - val_acc: 0.9348 - val_mDice: 0.4009

Epoch 00253: val_mDice did not improve from 0.40981
Epoch 254/300
 - 10s - loss: 2.1315 - acc: 0.9120 - mDice: 0.4624 - val_loss: 3.2248 - val_acc: 0.9350 - val_mDice: 0.4068

Epoch 00254: val_mDice did not improve from 0.40981
Epoch 255/300
 - 10s - loss: 2.1319 - acc: 0.9120 - mDice: 0.4626 - val_loss: 3.5844 - val_acc: 0.9345 - val_mDice: 0.3976

Epoch 00255: val_mDice did not improve from 0.40981
Epoch 256/300
 - 10s - loss: 2.1210 - acc: 0.9124 - mDice: 0.4649 - val_loss: 3.2811 - val_acc: 0.9352 - val_mDice: 0.4048

Epoch 00256: val_mDice did not improve from 0.40981
Epoch 257/300
 - 10s - loss: 2.1313 - acc: 0.9120 - mDice: 0.4630 - val_loss: 3.2377 - val_acc: 0.9326 - val_mDice: 0.3927

Epoch 00257: val_mDice did not improve from 0.40981
Epoch 258/300
 - 10s - loss: 2.1284 - acc: 0.9121 - mDice: 0.4636 - val_loss: 3.4189 - val_acc: 0.9322 - val_mDice: 0.3802

Epoch 00258: val_mDice did not improve from 0.40981
Epoch 259/300
 - 10s - loss: 2.1237 - acc: 0.9121 - mDice: 0.4641 - val_loss: 3.3483 - val_acc: 0.9315 - val_mDice: 0.3866

Epoch 00259: val_mDice did not improve from 0.40981
Epoch 260/300
 - 10s - loss: 2.1281 - acc: 0.9122 - mDice: 0.4636 - val_loss: 3.3296 - val_acc: 0.9346 - val_mDice: 0.4017

Epoch 00260: val_mDice did not improve from 0.40981
Epoch 261/300
 - 10s - loss: 2.1246 - acc: 0.9121 - mDice: 0.4636 - val_loss: 3.4734 - val_acc: 0.9351 - val_mDice: 0.3987

Epoch 00261: val_mDice did not improve from 0.40981
Epoch 262/300
 - 10s - loss: 2.1207 - acc: 0.9121 - mDice: 0.4655 - val_loss: 3.4333 - val_acc: 0.9334 - val_mDice: 0.3953

Epoch 00262: val_mDice did not improve from 0.40981
Epoch 263/300
 - 10s - loss: 2.1137 - acc: 0.9123 - mDice: 0.4661 - val_loss: 3.4906 - val_acc: 0.9321 - val_mDice: 0.3909

Epoch 00263: val_mDice did not improve from 0.40981
Epoch 264/300
 - 10s - loss: 2.1235 - acc: 0.9121 - mDice: 0.4641 - val_loss: 3.3019 - val_acc: 0.9329 - val_mDice: 0.3885

Epoch 00264: val_mDice did not improve from 0.40981
Epoch 265/300
 - 10s - loss: 2.1229 - acc: 0.9120 - mDice: 0.4651 - val_loss: 3.4241 - val_acc: 0.9316 - val_mDice: 0.3877

Epoch 00265: val_mDice did not improve from 0.40981
Epoch 266/300
 - 10s - loss: 2.1231 - acc: 0.9120 - mDice: 0.4649 - val_loss: 3.1881 - val_acc: 0.9345 - val_mDice: 0.4055

Epoch 00266: val_mDice did not improve from 0.40981
Epoch 267/300
 - 10s - loss: 2.1160 - acc: 0.9123 - mDice: 0.4659 - val_loss: 3.4191 - val_acc: 0.9352 - val_mDice: 0.3995

Epoch 00267: val_mDice did not improve from 0.40981
Epoch 268/300
 - 10s - loss: 2.1201 - acc: 0.9120 - mDice: 0.4654 - val_loss: 3.5113 - val_acc: 0.9325 - val_mDice: 0.3853

Epoch 00268: val_mDice did not improve from 0.40981
Epoch 269/300
 - 10s - loss: 2.1149 - acc: 0.9124 - mDice: 0.4670 - val_loss: 3.1870 - val_acc: 0.9342 - val_mDice: 0.4080

Epoch 00269: val_mDice did not improve from 0.40981
Epoch 270/300
 - 10s - loss: 2.1110 - acc: 0.9123 - mDice: 0.4667 - val_loss: 3.4319 - val_acc: 0.9341 - val_mDice: 0.3946

Epoch 00270: val_mDice did not improve from 0.40981
Epoch 271/300
 - 10s - loss: 2.1152 - acc: 0.9120 - mDice: 0.4659 - val_loss: 3.3446 - val_acc: 0.9344 - val_mDice: 0.3998

Epoch 00271: val_mDice did not improve from 0.40981
Epoch 272/300
 - 10s - loss: 2.1163 - acc: 0.9121 - mDice: 0.4662 - val_loss: 3.5934 - val_acc: 0.9336 - val_mDice: 0.4014

Epoch 00272: val_mDice did not improve from 0.40981
Epoch 273/300
 - 10s - loss: 2.1165 - acc: 0.9120 - mDice: 0.4661 - val_loss: 3.4942 - val_acc: 0.9336 - val_mDice: 0.3981

Epoch 00273: val_mDice did not improve from 0.40981
Epoch 274/300
 - 10s - loss: 2.1134 - acc: 0.9122 - mDice: 0.4675 - val_loss: 3.3468 - val_acc: 0.9325 - val_mDice: 0.3929

Epoch 00274: val_mDice did not improve from 0.40981
Epoch 275/300
 - 10s - loss: 2.1118 - acc: 0.9123 - mDice: 0.4671 - val_loss: 3.3880 - val_acc: 0.9351 - val_mDice: 0.4089

Epoch 00275: val_mDice did not improve from 0.40981
Restoring model weights from the end of the best epoch
Epoch 00275: early stopping
{'val_loss': [137.2329666046869, 78.55714591344197, 47.77061201277233, 32.07604574021839, 24.198800206184387, 19.199227103165217, 16.7986545193763, 12.476625717821575, 9.699904369456428, 8.282497213000344, 7.384301755399931, 6.713904013236363, 6.457712829822586, 6.360434999068578, 6.218603143025012, 6.029125202269781, 5.786502180000146, 5.62559131550647, 5.580896049205746, 5.385271607232945, 5.580085124997866, 5.2511192217824005, 5.2050211775515765, 5.232948803209832, 5.178943514646519, 4.991679493603962, 5.2015046408133845, 5.0580081560072445, 5.098326223414569, 5.0475669886384695, 5.631097907910035, 5.190343236284597, 5.018903790219199, 4.678163778391623, 5.055280887388757, 4.857323217870934, 4.9654231790807986, 4.6468186271155165, 4.987168002918008, 4.512552730473025, 4.433109834878927, 4.792388497053513, 4.807157790820513, 4.689929717486458, 4.286440645034115, 4.177152122282202, 4.085508066185174, 4.449787139094302, 4.275979072742519, 4.0071770472097255, 4.072028224046032, 3.8695137592121247, 4.027867813550291, 3.9923994162430367, 4.205847195660074, 3.757592085305424, 4.18822573919204, 3.971164665451007, 3.952125114255718, 3.931182537405264, 4.083300467004024, 3.971836348374685, 4.0235321356338405, 3.811098160621311, 3.825194623631736, 3.817904452482859, 4.280808575273979, 3.9061769359700738, 3.686617469636812, 4.029951986413272, 3.9532526446328986, 3.8015070124751045, 3.8225185864028477, 3.709702194047471, 3.8229547016588703, 3.872902597965939, 3.6191763825537193, 3.661844781333847, 3.641953079118615, 3.585271226197836, 3.572394366198707, 3.4221009977516674, 4.027332442590878, 3.45638385694474, 3.372948893390241, 3.579953784965688, 3.6155648727324747, 3.4722708947513077, 3.465906898269341, 3.6869763838393346, 3.5876387979923967, 3.4431395253521346, 3.308808014284642, 3.6533752851454273, 3.769700742592769, 3.461543638303521, 3.451526414469949, 3.420978596256602, 3.3335209963843226, 3.7308014439685, 3.4809626865954626, 3.393024605166699, 3.471930672015463, 3.4868532608130147, 3.3886996588359275, 3.5161688775711117, 3.432880942266257, 3.3980983992417655, 3.386263812315606, 3.433828555357953, 3.4512370391083613, 3.367169531727476, 3.426455278880894, 3.479986982819225, 3.5767191051993343, 3.6294919842676747, 3.4307042219720425, 3.732791135885886, 3.5074448391706463, 3.4636974413657473, 3.50323208004591, 3.5076766442507505, 3.2782296086439775, 3.4509620355176076, 3.4588156414351294, 3.4454987218071307, 3.368248076665969, 3.4899503036091724, 3.2992181741588174, 3.5657511031521216, 3.3362096091288898, 3.3204273733620844, 3.2209654225125197, 3.3929958565692817, 3.7403557491710497, 3.463461977607083, 3.5275301127472805, 3.798949746548065, 3.2061721400934315, 3.4075628321706537, 3.445003973807962, 3.274637165612408, 3.294308786590894, 3.586061432115024, 3.3191556444480304, 3.4128824134933806, 3.363797278365209, 3.911654829136318, 3.2366336776564517, 3.363367675049674, 3.320075391614366, 3.333071061853497, 3.345035411639228, 3.411914592653158, 3.7250423147121356, 4.078926091658927, 3.64418393759323, 3.4004265072178983, 3.349778887860122, 3.432639646609979, 3.379340353155775, 3.2982965265178965, 3.329770652710327, 3.32963762086417, 3.3855257473797318, 3.3802259681036784, 3.1841918482400833, 3.293532539646895, 3.5967320617110956, 3.3799542292863842, 3.401062896652591, 3.3244840654411485, 3.4469809413311028, 3.441483103492785, 3.663129319813812, 3.5031432147093473, 3.4312469129494967, 3.3685281878958144, 3.4602604487556077, 3.377586577220687, 3.5453614228449406, 3.5082090523182634, 3.5560319553173723, 3.3741322579749284, 3.3509495330176184, 3.573809085613383, 3.4599114530054584, 3.192828965994219, 3.380934017488644, 3.3393286789457, 3.3482750871085694, 3.2707309495835077, 3.3040733603937995, 3.3367671328818513, 3.456294445343138, 3.2532129709475806, 3.716877010717456, 3.421751811346483, 3.08637123825472, 3.3952902751486924, 3.487602507074674, 3.4412005419532457, 3.4250078886924755, 3.638104381704969, 3.5350890821289447, 3.1894111802269305, 3.1809864083216306, 3.308463486133232, 3.2855238284294805, 3.629181646941496, 3.2759016511873122, 3.378056455758356, 3.3102134463066855, 3.3020689514953467, 3.322822104119474, 3.716953582795603, 3.2914581333420108, 3.4219492521314394, 3.252347534256322, 3.7124312468139187, 3.3237986651116183, 3.264910356274673, 3.6105419230603037, 3.2327149912182773, 3.1272872515643635, 3.1057599614862177, 3.2197312373401865, 3.498868874895076, 3.229548194356972, 3.408079342650516, 3.4474061912901344, 3.4183097492814776, 3.298999942218264, 3.3703498045159948, 3.3462619090541486, 3.454951852781787, 3.5558829438828288, 3.261338916429806, 3.453163313102864, 3.3992990787540163, 3.539508765740764, 3.3803268367247212, 3.247374119281414, 3.5357745630400523, 3.155931591854564, 3.3095070799103095, 3.614281494392171, 3.392448403384714, 3.4017402397113896, 3.2573954345481027, 3.2017318026739217, 3.255178959552376, 3.400242084548587, 3.2248234375867817, 3.584395811183467, 3.281067751835854, 3.237678347465893, 3.4189369981842384, 3.3482705250027633, 3.3295850571954535, 3.473404673282944, 3.4332649780526046, 3.4906412102725533, 3.301890642293507, 3.4241373562475754, 3.188060608869862, 3.419083740606549, 3.5112584631091783, 3.1870206156745553, 3.431877950605537, 3.344613977353133, 3.593409651890397, 3.4941507060790347, 3.3468150372306504, 3.3880088227756677], 'val_acc': [0.6175389460154942, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9046520136651539, 0.904427673135485, 0.9046954966726757, 0.904143784727369, 0.9046520136651539, 0.9044780248687381, 0.9048214072272891, 0.9046588880675179, 0.9046703151294163, 0.9046611587206522, 0.9049427764756339, 0.9047527228082929, 0.904899293468112, 0.9048901115145002, 0.9047733630452838, 0.9054349944705055, 0.9054372452554249, 0.905318246001289, 0.9075823937143598, 0.9059935751415434, 0.9048649299712408, 0.9051854553676787, 0.9059661030769348, 0.9062522933596656, 0.9101213585762751, 0.9154967779204959, 0.9140956827572414, 0.9189102564539228, 0.919239912714277, 0.9218772933596656, 0.921762832573482, 0.9223351506959825, 0.9209523825418382, 0.9216186006863912, 0.922445073014214, 0.9241231452851069, 0.9220352626982189, 0.9235302209854126, 0.9241872571763539, 0.9249015677542913, 0.9243360587528774, 0.9256753524144491, 0.9245421375547137, 0.9221222656113761, 0.9247229922385443, 0.925528830006009, 0.9261561206408909, 0.9249999948910305, 0.9258447743597484, 0.9269299592290606, 0.9266025679452079, 0.9259981569789705, 0.9264972465378898, 0.9275846821921212, 0.9258997213272822, 0.9262637637910389, 0.9263484194165185, 0.9287568472680592, 0.9279235345976693, 0.9275549281211126, 0.9275251683734712, 0.9275045621962774, 0.9295123758770171, 0.9274633668717884, 0.925679936295464, 0.9234432180722555, 0.9281432969229562, 0.9296222272373381, 0.9287408334868295, 0.930512819971357, 0.9286400959605262, 0.9295100768407186, 0.930235797450656, 0.9328891691707429, 0.9291666405541557, 0.9303182391893297, 0.9309569540477934, 0.9322871026538667, 0.9304738754317874, 0.9309478175072443, 0.9306044181187948, 0.9323970022655669, 0.9307005604108175, 0.9283814288320995, 0.9313232671646845, 0.9304922081175304, 0.9328914937518892, 0.9315888030188424, 0.9330402782985142, 0.9322710377829415, 0.9323603510856628, 0.9312729154314313, 0.9331387338184175, 0.9325595072337559, 0.9322183983666557, 0.9316025773684183, 0.9325618119466872, 0.9332577898388817, 0.9326877423695156, 0.9331135465985253, 0.9331455826759338, 0.9305952560333979, 0.9335416583787828, 0.9328960747945876, 0.932957873457954, 0.9331662285895574, 0.9333951274553934, 0.9353090808505103, 0.9342330750964937, 0.9334019961811247, 0.9340430583272662, 0.9329693061964852, 0.9330952508108956, 0.9331890855516706, 0.9320283617292132, 0.9316048480215526, 0.931973431791578, 0.9342696695100694, 0.9335439687683469, 0.9332532258260817, 0.932653376034328, 0.9339422952561152, 0.9340041137876964, 0.9350206284295945, 0.9317078789075216, 0.9313507221993946, 0.9347229656719026, 0.9349061335836139, 0.9354006222316197, 0.9344780189650399, 0.9349084013984317, 0.9326534129324413, 0.934468842688061, 0.9341643679709661, 0.9338369766871134, 0.9331135806583223, 0.9346977784520104, 0.9341575333050319, 0.9342353599412101, 0.931607152734484, 0.9334638118743896, 0.9340384375481379, 0.9340590635935465, 0.9338599074454534, 0.93247937304633, 0.933145576999301, 0.9348603770846412, 0.9317674040794373, 0.9322184324264526, 0.9353983714467003, 0.9342101925895328, 0.9342536755970546, 0.9327609908013117, 0.9350824299312773, 0.9329922142482939, 0.9335805830501375, 0.933605744725182, 0.9338599102837699, 0.9348969544683184, 0.9337957614944095, 0.9348649155525934, 0.9342307618686131, 0.9340201587904067, 0.9341323120253426, 0.9332051475842794, 0.9342353627795265, 0.933818675222851, 0.9338003595670065, 0.9349038600921631, 0.933200546673366, 0.9336630247888111, 0.9335119326909384, 0.9343727174259367, 0.9342971387363616, 0.934207885038285, 0.9347939604804629, 0.9342468068713233, 0.9334592506999061, 0.9336400883538383, 0.9329052084968203, 0.9354097814787001, 0.9339056980042231, 0.9345329858007885, 0.9349244265329271, 0.933605744725182, 0.9322802168982369, 0.9342857315426781, 0.9336606945310321, 0.9338370107469105, 0.9344505270322164, 0.9332967088336036, 0.9341689461753482, 0.9348832170168558, 0.933436373869578, 0.9334225995200021, 0.934027027516138, 0.9349999825159708, 0.933999552613213, 0.9321863793191456, 0.9336286414237249, 0.9337843514624096, 0.9353639880816141, 0.9342674130485171, 0.9340224294435411, 0.934716094107855, 0.9318612615267435, 0.9340293180374872, 0.9327976391428993, 0.9344299180167062, 0.9348374803860983, 0.9350480834643046, 0.9345169407980782, 0.935164851801736, 0.9325892868496123, 0.9322435628800165, 0.9314652198836917, 0.9346016560282026, 0.9350938569931757, 0.9334019933428083, 0.9320535461107889, 0.932866306531997, 0.9316414623033433, 0.9345306952794393, 0.9351877286320641, 0.9325045914877028, 0.9341506645793006, 0.9340705275535583, 0.9344345331192017, 0.9336469968159994, 0.933646974109468, 0.9325251607667833, 0.9351487897691273], 'val_mDice': [0.012647758548458418, 0.0134646228980273, 0.013776939240328613, 0.01435491771969412, 0.0174252400590506, 0.018924537048275982, 0.01788261714058795, 0.017653398803390917, 0.018031120222682755, 0.01848131659928532, 0.018159262082051663, 0.016992073938516632, 0.017494987119876203, 0.02151368511840701, 0.022907252368029384, 0.026144053210460004, 0.027583956718444824, 0.030744773125098573, 0.031814847219114505, 0.03566339247239133, 0.03657096336107878, 0.041486100160649846, 0.042499784069756665, 0.04474394717475488, 0.04912092751779017, 0.05421109302412896, 0.053518341188984256, 0.058089549608883406, 0.057356618389132474, 0.06299066791931789, 0.049731768667697906, 0.061384172999255714, 0.06756740985881715, 0.07081187245923848, 0.06982711418753579, 0.07447779823892883, 0.07493077564452376, 0.08156422124288622, 0.07879646095846381, 0.08814468083991892, 0.09021983965344373, 0.08686180399464709, 0.08779411292856648, 0.09338899533308688, 0.10707462698753391, 0.11320557826686473, 0.12567968746381147, 0.12349717616147939, 0.13294291505146594, 0.1453856902108306, 0.1498229620268657, 0.15834884061699822, 0.1636963915966806, 0.17386566466164022, 0.1676408588176682, 0.1904959018741335, 0.18100312192525184, 0.19104823922472342, 0.20068088386740005, 0.2053412456242811, 0.1991821914201691, 0.20402370703717074, 0.21520715739045823, 0.22220008057497798, 0.22496451597128594, 0.23080959348451524, 0.22568401445945105, 0.24035540276340076, 0.2475342413499242, 0.24381109362556821, 0.24459869166215262, 0.2548722544951098, 0.26330640113779474, 0.2683694002528985, 0.26968325834189144, 0.26548774930692853, 0.27296335125962895, 0.2775883224038851, 0.28760451610599247, 0.28464138561061453, 0.29260180464812685, 0.29788166461955934, 0.2921866721340588, 0.30380106078726904, 0.30640327149913427, 0.30464049659314607, 0.29523485828013646, 0.3128371664455959, 0.31858997756526586, 0.31399347597644445, 0.31516119252358166, 0.3277014917915776, 0.3428298586181232, 0.3321967843387808, 0.335168461004893, 0.3374101980811074, 0.34342790998163675, 0.34252068684214637, 0.3429592218072641, 0.34251471147650764, 0.34869568794965744, 0.3504039716152918, 0.35004331898831187, 0.35040581935927984, 0.3510326200297901, 0.34761848194258554, 0.34304963140970185, 0.3393499290659314, 0.33474435391170637, 0.35201533280667807, 0.3601210459712006, 0.35458772789154736, 0.36128466363464085, 0.3534920233346167, 0.3621427336973803, 0.3588774853519031, 0.3702172294613861, 0.347116598770732, 0.3552435709252244, 0.3665565715304443, 0.37107127337228685, 0.35822427680804614, 0.36741463290083975, 0.3559253517360914, 0.36883777876694995, 0.36128981730767656, 0.35769366951925413, 0.36154859903312864, 0.3575048448429221, 0.37496497641716686, 0.3670664968944731, 0.3677034537707056, 0.36762561028202373, 0.3618491781609399, 0.3580812481897218, 0.372825287992046, 0.3624260038847015, 0.35835673056897666, 0.3692679467300574, 0.36645020065563066, 0.36959309592133477, 0.3677078726745787, 0.3680204034206413, 0.372575105655761, 0.3572286936853613, 0.3727161018621354, 0.37210848767842564, 0.35069091174574124, 0.37651242759256137, 0.3685269841835612, 0.3861914351582527, 0.37554333660574185, 0.37767241292056586, 0.3750637194940022, 0.3645240919930594, 0.3587456388132913, 0.368833622052556, 0.36861293390393257, 0.37162269847024054, 0.36968554166101275, 0.3838942918394293, 0.3771800510585308, 0.37191058784013703, 0.37235880589910914, 0.3785465013767992, 0.3779734816579592, 0.38734026643491926, 0.37161080078000114, 0.35895957833244685, 0.3784171303822881, 0.39074641182309106, 0.3866985442028159, 0.3736555918696381, 0.387002941043604, 0.3653053219119708, 0.38005910849287394, 0.3797572074191911, 0.3771358760339873, 0.37975679089625675, 0.3797112327246439, 0.3747139544714065, 0.37892361517463413, 0.36427544554074603, 0.3817035541647956, 0.38534452127558844, 0.37784709373400327, 0.38580596695343655, 0.38233228737399694, 0.38192805115665707, 0.39020833124717075, 0.3714764943080289, 0.37947254574724604, 0.39123051968358813, 0.39187901715437573, 0.38011713042145684, 0.38194488095385687, 0.384928126065504, 0.3799214017178331, 0.39754859180677504, 0.38660200348212603, 0.3849946725226584, 0.3951675898972012, 0.38838325831152143, 0.3771503120660782, 0.38946779764124323, 0.39795203045720146, 0.3918142669967243, 0.3848021445529802, 0.394245197730405, 0.38582067741524606, 0.39276279509067535, 0.392215547817094, 0.3831064562712397, 0.39164552216728526, 0.38603418480072704, 0.38259759740460486, 0.3974489928001449, 0.39162190808426767, 0.39792701566503164, 0.3844951112710294, 0.3898120520725137, 0.3989139986889703, 0.38310490034165834, 0.4055294751056603, 0.3987692963509333, 0.40134882731806665, 0.40205126787934986, 0.3863923741238458, 0.3932447586031187, 0.3951192565617107, 0.38939924254303887, 0.3956053361651443, 0.4053203105216935, 0.3934528621889296, 0.400759680462735, 0.3994904469166483, 0.3904590739735535, 0.399413267771403, 0.3975941411086491, 0.40391952934719266, 0.39208833163692836, 0.38894330479559447, 0.39352454333787873, 0.3943744844623974, 0.40980682309184757, 0.4027845323795364, 0.3986336423882416, 0.3945906215480396, 0.39020866386237596, 0.4023775649922235, 0.39383990885246367, 0.396432312471526, 0.40090272043432507, 0.4067818799189159, 0.39764625969387235, 0.4048211899186884, 0.3926667134676661, 0.380167086741754, 0.38662643482287723, 0.4016689484318097, 0.39865031075619517, 0.39525676021973294, 0.3909027221656981, 0.3885373654110091, 0.38766058002199444, 0.40547749896844226, 0.3994943531496184, 0.38533432604301543, 0.40796066678705667, 0.3945923823685873, 0.3998033936534609, 0.4014192141947292, 0.3981126234644935, 0.39290194195650874, 0.408901405831178], 'loss': [207.13451347549642, 126.03080871593798, 82.13861740727128, 57.04096397177801, 41.30685193473855, 31.335306894946978, 25.02165183116412, 20.861192099271722, 17.966987081453798, 15.881661040947941, 14.313905970770337, 13.11756250709841, 12.176970780458849, 11.39955560766205, 10.737627355675949, 10.159396077434245, 9.691953393014291, 9.277651325043442, 8.910216975814233, 8.576588020725518, 8.283204020391155, 8.029658610636453, 7.771387662734971, 7.55239105518783, 7.353401401494771, 7.1559658569937525, 6.964022991352511, 6.784124437361203, 6.626859837468245, 6.477193233110506, 6.326534742377778, 6.197146419847266, 6.0660059698887, 5.945345795864725, 5.841215296749529, 5.733938607494795, 5.620306227738995, 5.523557425970192, 5.425356185387986, 5.329819042190366, 5.219385398613228, 5.131226072151445, 5.0345210188819145, 4.94601833027453, 4.856281487496345, 4.758880293895588, 4.670641190839774, 4.587948287179159, 4.505134964945323, 4.4194981880401265, 4.34798073345741, 4.275098043989304, 4.193264091591718, 4.126141553534601, 4.056747334711764, 3.9953014533920097, 3.940881590772415, 3.8795184299898486, 3.8332361087004805, 3.787688334920812, 3.7243248190014766, 3.691503083427816, 3.64611247966164, 3.5935364948524224, 3.566820224356003, 3.5159438014007476, 3.4864803897132526, 3.4423557123467714, 3.4097572648044907, 3.36580330162166, 3.3331994874780295, 3.300703356531238, 3.26871353738313, 3.245759585746249, 3.210956532439906, 3.1963656116409846, 3.149513321849499, 3.130189160839174, 3.1070365499434316, 3.075247281407124, 3.050025532878569, 3.0196886092958732, 2.988747963906254, 2.975191464423214, 2.9546715382875126, 2.9239603881089473, 2.902459010743258, 2.8923450248972724, 2.8666194094291466, 2.8471926717554097, 2.8343604747238293, 2.806062663017451, 2.791436266940477, 2.7726243353542714, 2.75536069064235, 2.7440166446269534, 2.7142649224113815, 2.6897439227665876, 2.6803100865218643, 2.666977282960405, 2.6659819439056407, 2.646942412161473, 2.6282487357308555, 2.6226568189411004, 2.6122016310117275, 2.5970929473357, 2.5914838286477613, 2.5795646789838336, 2.5726218998995227, 2.56793868675015, 2.5562906978170328, 2.53773065835542, 2.5371419549884653, 2.526898875663063, 2.5247207116547496, 2.5137976612777226, 2.5025073372837388, 2.489128562281167, 2.4914614308746836, 2.4819481686541907, 2.471897019341614, 2.4617333589134267, 2.4675339269849497, 2.453637514795576, 2.453606406135184, 2.440171365818813, 2.4393403007732735, 2.433736144542602, 2.4322284393648848, 2.4250114878317466, 2.423128579102754, 2.409780015415922, 2.404903803690794, 2.408844455326824, 2.399334773429906, 2.3949552710538473, 2.3894895110627647, 2.3873974828791797, 2.379916764165203, 2.375423573588093, 2.3737757712033805, 2.3691641212038297, 2.365355095629842, 2.3590497419005394, 2.3610378935407392, 2.354517538882598, 2.3511335791846517, 2.3477530949649403, 2.347256413079742, 2.3419534182856485, 2.3376589705311126, 2.3306699437352303, 2.3212199628939167, 2.3210562329451334, 2.318432677405773, 2.317610400293474, 2.317286464734553, 2.3095830683536285, 2.3100234475327937, 2.3046471795894194, 2.301724862978825, 2.2987439875338884, 2.3008365188234223, 2.2909382547007513, 2.2937034834596446, 2.2870255342815935, 2.278661521432118, 2.293297629446475, 2.2830921256383627, 2.277863835631158, 2.269148390670491, 2.274742983291667, 2.2738247240626195, 2.269130368694212, 2.2687647982739474, 2.2636410680514945, 2.260172769998985, 2.2582404209006057, 2.2587280978535054, 2.2598334092360277, 2.251940191676903, 2.2522277500396384, 2.2463009692994254, 2.242418237796105, 2.2329701343114974, 2.2364724117229042, 2.2379270886924423, 2.240066110961396, 2.229974496150858, 2.233473196456215, 2.228868324103465, 2.223983287834262, 2.223980468777027, 2.2155668723553346, 2.2214104161226658, 2.211288846065388, 2.210661809660921, 2.217890191680323, 2.2112740705715157, 2.2088225118360563, 2.2098628446181108, 2.2092043294689665, 2.2037476540669, 2.2044386976551684, 2.1948611836960166, 2.1982531455157464, 2.1923185711723314, 2.1937967424611493, 2.1991639402943677, 2.185547505336068, 2.1897101161657835, 2.191620713096605, 2.193516738455673, 2.1873363721846433, 2.179757775519426, 2.181161601763597, 2.183554931093278, 2.180294151753848, 2.1782771604516635, 2.168873335622468, 2.1785844697505494, 2.1759304114796416, 2.172889630689266, 2.1751738210530647, 2.1691678117092392, 2.165565138288792, 2.167129149711819, 2.1641237057080405, 2.163010706670808, 2.165858361057769, 2.158270158679448, 2.1583049344446517, 2.15570164602433, 2.1576259672147633, 2.1563756887407597, 2.1573420602139515, 2.1486137765254134, 2.157847054137507, 2.1511656366976695, 2.1504814249485107, 2.1525923934129017, 2.149315659701537, 2.146862095919606, 2.147573761404974, 2.1416025921538635, 2.142836451921156, 2.1406497426039457, 2.1429978853387137, 2.135630187442221, 2.143738600291658, 2.133533469394657, 2.1380215936573985, 2.1355236702993103, 2.131537150435763, 2.13186889658182, 2.120977512002566, 2.1313431117262067, 2.128442301992876, 2.1236875368630885, 2.128148390719581, 2.124608827184247, 2.1206621471020397, 2.1136538560895626, 2.1235318619413874, 2.1229057472198485, 2.1230793187492774, 2.1159822769378316, 2.120093111911904, 2.114948471502139, 2.1109996476521813, 2.1151841772712676, 2.1163241854747272, 2.1164989218859858, 2.1134284507430814, 2.1117876547481917], 'acc': [0.4913379532973247, 0.7633576601462164, 0.8692114001780271, 0.8692392983463244, 0.8692391118562954, 0.8692395980472576, 0.8692395746167831, 0.8692396010004856, 0.8692396215926436, 0.8692396441268075, 0.869239645092065, 0.869239645816008, 0.8692396455861848, 0.8692396415068231, 0.869239645747061, 0.8692396429087446, 0.8692396435292672, 0.8692396442761926, 0.8692396903097779, 0.8692400393882238, 0.8692407327073375, 0.8692408938593595, 0.8692407792924984, 0.8692401078525527, 0.8692402465738311, 0.869240706898193, 0.8692401278471703, 0.8692397836984318, 0.8692396395763083, 0.8692396466318804, 0.8692396429202358, 0.8692396441382987, 0.8692396427363772, 0.8692396450116268, 0.8692396452644323, 0.8692396465284599, 0.869173883334598, 0.8688979513594152, 0.8690874061649743, 0.8690036847248044, 0.8689469373798425, 0.8688582575272935, 0.868737766341435, 0.86858251293845, 0.868461718857116, 0.8683604354892412, 0.8682985879884466, 0.8682969652873279, 0.8683421526636396, 0.8682424241760597, 0.8681047086497826, 0.8677366712384309, 0.8675819549552273, 0.8676608069820488, 0.8680489826850494, 0.8683413889726647, 0.8686165758380776, 0.8692219400534584, 0.8699373260775306, 0.8706212418488921, 0.8719657413504913, 0.8727894064914474, 0.8736377504604135, 0.8743782519995419, 0.8748694725617614, 0.8758262859297231, 0.8769491715699647, 0.8784202788867587, 0.8801978865707681, 0.8819612833905225, 0.8835091152838345, 0.8853224763434494, 0.8865600826999046, 0.8876695036313564, 0.8885474139862215, 0.8892782727417653, 0.8902873161994173, 0.8910872995313339, 0.89219090737243, 0.8930062057771089, 0.8935704665398216, 0.8940694941039349, 0.8948008646527443, 0.8948995076509163, 0.8953759663477784, 0.8958068935425727, 0.8959967842422883, 0.8963218179998403, 0.8965112498575537, 0.8968495803346335, 0.8969372139400845, 0.8972906782053927, 0.8973828371903652, 0.8975550444088898, 0.8977638700460224, 0.897838622061577, 0.8984953379479359, 0.8989766394446207, 0.8991999693382952, 0.8994347253860664, 0.899353897603739, 0.8999331504686825, 0.9003184962093382, 0.9003166878340629, 0.900764995350276, 0.9009510664000363, 0.9011017490564018, 0.9015425024681614, 0.9016166054860048, 0.9018059209037981, 0.902253969616115, 0.9026822087184206, 0.9028114820574666, 0.9030653788033584, 0.9030443168982665, 0.903427045791623, 0.9035941853283319, 0.9038949538017252, 0.9039582361231794, 0.904055534173051, 0.9042954341005172, 0.9044954290802777, 0.904460088938262, 0.9049812953366878, 0.9048535682710582, 0.9053686337593274, 0.9052712683482783, 0.9055885141562791, 0.9057408895293987, 0.9061897554585119, 0.9065854124694701, 0.9070753089398439, 0.9072220580688625, 0.9072895316413907, 0.9075894944925678, 0.907556057240615, 0.9076917515523589, 0.9079552174464848, 0.9080814086496888, 0.9081850123469807, 0.9081030748518809, 0.9084714823705005, 0.9083912840968776, 0.9083926739181561, 0.9087361963539425, 0.9086462717330224, 0.9086569278645152, 0.9086277570388026, 0.9090042726047148, 0.9089324850325459, 0.9089841838859285, 0.9090501526566354, 0.9093764827548826, 0.9092768224788259, 0.9092994349665189, 0.9095137522862737, 0.9093013368453597, 0.909456427533525, 0.9095077544065806, 0.9097225533782896, 0.9095992576830048, 0.9098169539667449, 0.9097460757819632, 0.909883159650689, 0.9098483302332612, 0.9101096664477984, 0.9100447791331027, 0.9097058721675289, 0.9100567397984876, 0.9100275667319988, 0.9102445231804113, 0.9100923044088146, 0.9100594275577325, 0.9100966435626037, 0.9100464041669267, 0.9102865132334974, 0.9102495508040448, 0.9103089869539471, 0.9101800595562606, 0.9101079936222417, 0.9103307708830692, 0.9103296099886721, 0.9103734982204456, 0.9105410063360981, 0.9106116848964555, 0.9105633696839601, 0.9106123263444748, 0.910308157648451, 0.9106674836921581, 0.9105725667599379, 0.9106319998991566, 0.9106180994111218, 0.9105146848927901, 0.910765820945047, 0.9105805853140798, 0.9107497616473342, 0.9107516169869787, 0.9104849557116331, 0.910571153427747, 0.9106836531211444, 0.9105682339033042, 0.9107353728309817, 0.9108775322529675, 0.9108026870441087, 0.9108953533407946, 0.9108046565599149, 0.9109279064974023, 0.9109768462429301, 0.9108335296611738, 0.9111957267366302, 0.9108361222046705, 0.9108909031364393, 0.9108978953231891, 0.9110649013670971, 0.9112215612420508, 0.9110345198672839, 0.9113162662083413, 0.9109731149485006, 0.9112185043867599, 0.9115191832291085, 0.9114092557168728, 0.9110747219763764, 0.9113151542203193, 0.9111955865329922, 0.91164658358958, 0.911502173980215, 0.9114852828374598, 0.9115241415495585, 0.9114898472524503, 0.9116008885912935, 0.9115096813156491, 0.9115789200452015, 0.911658141892012, 0.9116710043376366, 0.9115819356587206, 0.9116087625866477, 0.9117255512749227, 0.9115816988259214, 0.9116346496066676, 0.9117655009462214, 0.9114625059572378, 0.9119533775559229, 0.9119835002633311, 0.9117878354282904, 0.9118594585849482, 0.911848734241932, 0.911777222595487, 0.9116828706491101, 0.9119621341987362, 0.9118418510143573, 0.9121068918753249, 0.9119032790188801, 0.912204603342645, 0.9119633163402875, 0.9119857474400264, 0.9123672256470646, 0.9119743931401505, 0.9120810301358562, 0.9121077228470393, 0.9122036537591814, 0.9121179202284592, 0.9121454258012891, 0.9122932604197158, 0.9120840649625941, 0.912039761716101, 0.9120261849914876, 0.9123086921047654, 0.9120191109417168, 0.9123655807105521, 0.9122611952007265, 0.9119886191842275, 0.9121340712945724, 0.911989429000717, 0.9121784192646586, 0.9122816305158684], 'mDice': [0.015888470131850022, 0.015678954920105754, 0.01647763425659899, 0.017242348028075057, 0.01784826663696389, 0.017814815557196072, 0.017731574362390576, 0.01786412363101665, 0.01819710454239031, 0.018585074205308468, 0.01911356820593295, 0.019930532881898506, 0.020697507408670386, 0.02140782521479227, 0.022291011213451905, 0.02321327859200056, 0.02433262632684669, 0.02556026121915674, 0.027244464642416196, 0.029011250006910093, 0.03075083937041328, 0.032412835844801945, 0.03427196614313544, 0.036260327135026166, 0.03834559615605917, 0.04047023753396964, 0.04281003891786836, 0.045166810268131505, 0.047266168673907906, 0.0496934700167625, 0.05213347494642422, 0.05420301401643296, 0.05665828528700363, 0.05893212504123295, 0.06128314540367584, 0.0637489908522256, 0.06625530694390346, 0.06878973566338992, 0.07080339381109664, 0.0739160196868951, 0.0774483431539302, 0.08152902117262968, 0.08569900132751612, 0.08918767042511204, 0.09432619841992337, 0.10009717953062618, 0.106511777470469, 0.11333916661066887, 0.12022507512600454, 0.12633121037485515, 0.1323932417818685, 0.1387302483066971, 0.1458051927618974, 0.1516129799964273, 0.1577623733291512, 0.1639613480881181, 0.16975178694090726, 0.17691991965573534, 0.18224359292939882, 0.18756182501834093, 0.19445551806317143, 0.1987362525896091, 0.20420900726676883, 0.21052141615724848, 0.21430093094244015, 0.2206696497151818, 0.2259054718212698, 0.23167108113374557, 0.23690816033247328, 0.2422483602004725, 0.24712543301277132, 0.25204889763290994, 0.25692024400681457, 0.25971998033299254, 0.26502527357733546, 0.26716743640893226, 0.2729028847398201, 0.276654892403748, 0.28099168533416574, 0.2858888978486533, 0.28941088488549194, 0.2946568716749052, 0.29950990134726974, 0.30227756974829034, 0.3055364591711768, 0.31057199349126674, 0.31395466928195237, 0.3164332676581939, 0.3195341238631709, 0.3241422950922696, 0.32730352555094416, 0.3307433967269039, 0.3328725135760751, 0.33601441293386036, 0.3398658369854739, 0.3420519765939744, 0.346385659947661, 0.3494732129463507, 0.3530781840988499, 0.3549588833306597, 0.35458651660195156, 0.35908137955071723, 0.3612635912453023, 0.3633383884121049, 0.3649586892746215, 0.3672724232660775, 0.3685940087979208, 0.37005478803284575, 0.3721776283704318, 0.372603682388523, 0.37424716938891023, 0.3774198896627057, 0.3780317255497806, 0.3798977797181247, 0.38037981830949563, 0.3826605169695422, 0.3843542149192409, 0.3875563553309473, 0.3869833775067205, 0.3885687623659218, 0.39072128789076027, 0.3918482326739598, 0.39179305788269614, 0.39410883596183843, 0.39387965089143623, 0.3976846341340326, 0.396443766310699, 0.39911319904068154, 0.39854978386069256, 0.40066693275908366, 0.40108567240129256, 0.4044721705251193, 0.4047797501190609, 0.40435672535449524, 0.4065619617533955, 0.4073130979757675, 0.40867617815382473, 0.409194251682939, 0.40950609169324426, 0.41130252694127556, 0.4110171347253236, 0.4121761593097939, 0.4128598007412115, 0.41454053758218934, 0.41452749046788456, 0.41472138754315335, 0.4159366744077571, 0.4162019095899789, 0.41717702293984343, 0.4182731438423136, 0.4197078916305794, 0.42076433552973114, 0.42243007327036197, 0.4224445915001538, 0.42326079505772773, 0.4233703725492423, 0.4240400782952034, 0.4248807121729699, 0.42561069903678006, 0.42648843433531997, 0.4266225255843369, 0.4273767759971405, 0.42719964993604465, 0.42888238015422153, 0.4283385681857809, 0.430539813578554, 0.43122809503679216, 0.4288750186703305, 0.4308673112303254, 0.43203571831787024, 0.43335260650498897, 0.4325953275913307, 0.43294840479991054, 0.4339764303891719, 0.43398680243621746, 0.4356179452595142, 0.43490709163031643, 0.43561018646303484, 0.43526180145114013, 0.43622474338200734, 0.4373726285915051, 0.4370232354659255, 0.4379555165066709, 0.4399485989977037, 0.44125183426807535, 0.44029925175546936, 0.440647346293832, 0.43915300237574006, 0.4410655297639169, 0.44120057798521306, 0.4421940848780018, 0.44237234646676016, 0.44322305196095046, 0.44457516855004897, 0.44397116392270575, 0.4458931716267075, 0.4460216647677277, 0.44444922962376254, 0.44570079432624093, 0.4464390611278544, 0.4468685040727663, 0.44640985272604994, 0.44728290153326544, 0.4471085374068497, 0.4487905361092985, 0.44832443573903735, 0.44946125795530034, 0.45022727467776125, 0.4488770117391068, 0.4514899959230046, 0.45023747706358114, 0.4507041249646142, 0.4504644682532863, 0.45128779843268607, 0.452178714292367, 0.4524470051060712, 0.4525338049129827, 0.45236665218442257, 0.4529918432350629, 0.45498223126175913, 0.45302973763203147, 0.45267996215535333, 0.4544069516491012, 0.4533822332615335, 0.45547561772370215, 0.4557276557630396, 0.45477085679924645, 0.45591282344103456, 0.4553844598945175, 0.4554260703716201, 0.45647534195434986, 0.4574541063673675, 0.4574539336667301, 0.4575986020340266, 0.4572782594504466, 0.4575820363611299, 0.45876706514672, 0.4577214595024494, 0.4584108778270702, 0.4584932868676492, 0.4581404276923313, 0.4598937548951237, 0.4603556395438863, 0.4593039179884769, 0.46081519861751746, 0.4602221574182567, 0.46095693038450813, 0.4604496303518452, 0.4620097620376438, 0.4605924111985048, 0.46237940958453116, 0.4619902709051205, 0.462087747338008, 0.4623730087098713, 0.46255396336104476, 0.4649225441228684, 0.4630424784101777, 0.4635853796984755, 0.46413584133092667, 0.4636168501350446, 0.46364770487528306, 0.4654614137205885, 0.466114923119522, 0.46410549189719436, 0.4650831578070657, 0.464882430013076, 0.46587906571305143, 0.4654414198843068, 0.46699204267002103, 0.46666927446927414, 0.4659276414588625, 0.4661716319805904, 0.4661380156791346, 0.46752184601286967, 0.4671065353556637]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.47s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.18s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:41,  1.42s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:02,  1.49s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:59,  1.49s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:27,  1.59s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:10,  1.54s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:36,  1.64s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:02,  1.74s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:14,  1.78s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<07:59,  1.74s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:13,  1.80s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:26,  1.85s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:33,  1.88s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:30,  1.88s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:28,  1.88s/it]predicting train subjects:   5%|▌         | 15/285 [00:26<08:33,  1.90s/it]predicting train subjects:   6%|▌         | 16/285 [00:28<08:31,  1.90s/it]predicting train subjects:   6%|▌         | 17/285 [00:30<08:34,  1.92s/it]predicting train subjects:   6%|▋         | 18/285 [00:32<08:33,  1.92s/it]predicting train subjects:   7%|▋         | 19/285 [00:34<08:38,  1.95s/it]predicting train subjects:   7%|▋         | 20/285 [00:36<08:38,  1.96s/it]predicting train subjects:   7%|▋         | 21/285 [00:38<08:32,  1.94s/it]predicting train subjects:   8%|▊         | 22/285 [00:40<08:30,  1.94s/it]predicting train subjects:   8%|▊         | 23/285 [00:42<08:30,  1.95s/it]predicting train subjects:   8%|▊         | 24/285 [00:44<08:26,  1.94s/it]predicting train subjects:   9%|▉         | 25/285 [00:46<08:23,  1.94s/it]predicting train subjects:   9%|▉         | 26/285 [00:48<08:13,  1.90s/it]predicting train subjects:   9%|▉         | 27/285 [00:49<08:07,  1.89s/it]predicting train subjects:  10%|▉         | 28/285 [00:51<08:10,  1.91s/it]predicting train subjects:  10%|█         | 29/285 [00:53<07:55,  1.86s/it]predicting train subjects:  11%|█         | 30/285 [00:55<07:45,  1.82s/it]predicting train subjects:  11%|█         | 31/285 [00:57<07:36,  1.80s/it]predicting train subjects:  11%|█         | 32/285 [00:58<07:37,  1.81s/it]predicting train subjects:  12%|█▏        | 33/285 [01:00<07:33,  1.80s/it]predicting train subjects:  12%|█▏        | 34/285 [01:02<07:28,  1.78s/it]predicting train subjects:  12%|█▏        | 35/285 [01:04<07:27,  1.79s/it]predicting train subjects:  13%|█▎        | 36/285 [01:05<07:25,  1.79s/it]predicting train subjects:  13%|█▎        | 37/285 [01:07<07:23,  1.79s/it]predicting train subjects:  13%|█▎        | 38/285 [01:09<07:29,  1.82s/it]predicting train subjects:  14%|█▎        | 39/285 [01:11<07:33,  1.84s/it]predicting train subjects:  14%|█▍        | 40/285 [01:13<07:37,  1.87s/it]predicting train subjects:  14%|█▍        | 41/285 [01:15<07:36,  1.87s/it]predicting train subjects:  15%|█▍        | 42/285 [01:17<07:34,  1.87s/it]predicting train subjects:  15%|█▌        | 43/285 [01:19<07:34,  1.88s/it]predicting train subjects:  15%|█▌        | 44/285 [01:21<07:36,  1.89s/it]predicting train subjects:  16%|█▌        | 45/285 [01:23<07:39,  1.91s/it]predicting train subjects:  16%|█▌        | 46/285 [01:24<07:19,  1.84s/it]predicting train subjects:  16%|█▋        | 47/285 [01:26<06:58,  1.76s/it]predicting train subjects:  17%|█▋        | 48/285 [01:27<06:42,  1.70s/it]predicting train subjects:  17%|█▋        | 49/285 [01:29<06:39,  1.69s/it]predicting train subjects:  18%|█▊        | 50/285 [01:31<06:32,  1.67s/it]predicting train subjects:  18%|█▊        | 51/285 [01:32<06:30,  1.67s/it]predicting train subjects:  18%|█▊        | 52/285 [01:34<06:28,  1.67s/it]predicting train subjects:  19%|█▊        | 53/285 [01:36<06:25,  1.66s/it]predicting train subjects:  19%|█▉        | 54/285 [01:37<06:20,  1.65s/it]predicting train subjects:  19%|█▉        | 55/285 [01:39<06:19,  1.65s/it]predicting train subjects:  20%|█▉        | 56/285 [01:40<06:16,  1.64s/it]predicting train subjects:  20%|██        | 57/285 [01:42<06:17,  1.66s/it]predicting train subjects:  20%|██        | 58/285 [01:44<06:14,  1.65s/it]predicting train subjects:  21%|██        | 59/285 [01:45<06:09,  1.64s/it]predicting train subjects:  21%|██        | 60/285 [01:47<06:08,  1.64s/it]predicting train subjects:  21%|██▏       | 61/285 [01:49<06:08,  1.64s/it]predicting train subjects:  22%|██▏       | 62/285 [01:50<06:02,  1.63s/it]predicting train subjects:  22%|██▏       | 63/285 [01:52<06:01,  1.63s/it]predicting train subjects:  22%|██▏       | 64/285 [01:54<06:00,  1.63s/it]predicting train subjects:  23%|██▎       | 65/285 [01:55<06:15,  1.71s/it]predicting train subjects:  23%|██▎       | 66/285 [01:57<06:19,  1.73s/it]predicting train subjects:  24%|██▎       | 67/285 [01:59<06:14,  1.72s/it]predicting train subjects:  24%|██▍       | 68/285 [02:01<06:12,  1.71s/it]predicting train subjects:  24%|██▍       | 69/285 [02:02<06:10,  1.71s/it]predicting train subjects:  25%|██▍       | 70/285 [02:04<06:04,  1.70s/it]predicting train subjects:  25%|██▍       | 71/285 [02:06<06:08,  1.72s/it]predicting train subjects:  25%|██▌       | 72/285 [02:07<06:04,  1.71s/it]predicting train subjects:  26%|██▌       | 73/285 [02:09<06:00,  1.70s/it]predicting train subjects:  26%|██▌       | 74/285 [02:11<05:56,  1.69s/it]predicting train subjects:  26%|██▋       | 75/285 [02:12<05:52,  1.68s/it]predicting train subjects:  27%|██▋       | 76/285 [02:14<05:48,  1.67s/it]predicting train subjects:  27%|██▋       | 77/285 [02:16<05:47,  1.67s/it]predicting train subjects:  27%|██▋       | 78/285 [02:17<05:41,  1.65s/it]predicting train subjects:  28%|██▊       | 79/285 [02:19<05:39,  1.65s/it]predicting train subjects:  28%|██▊       | 80/285 [02:21<05:33,  1.63s/it]predicting train subjects:  28%|██▊       | 81/285 [02:22<05:30,  1.62s/it]predicting train subjects:  29%|██▉       | 82/285 [02:24<05:27,  1.62s/it]predicting train subjects:  29%|██▉       | 83/285 [02:25<05:25,  1.61s/it]predicting train subjects:  29%|██▉       | 84/285 [02:27<05:20,  1.59s/it]predicting train subjects:  30%|██▉       | 85/285 [02:29<05:31,  1.66s/it]predicting train subjects:  30%|███       | 86/285 [02:31<05:43,  1.72s/it]predicting train subjects:  31%|███       | 87/285 [02:32<05:45,  1.74s/it]predicting train subjects:  31%|███       | 88/285 [02:34<05:44,  1.75s/it]predicting train subjects:  31%|███       | 89/285 [02:36<05:48,  1.78s/it]predicting train subjects:  32%|███▏      | 90/285 [02:38<05:55,  1.82s/it]predicting train subjects:  32%|███▏      | 91/285 [02:40<05:52,  1.82s/it]predicting train subjects:  32%|███▏      | 92/285 [02:42<05:50,  1.82s/it]predicting train subjects:  33%|███▎      | 93/285 [02:44<05:59,  1.87s/it]predicting train subjects:  33%|███▎      | 94/285 [02:46<05:59,  1.88s/it]predicting train subjects:  33%|███▎      | 95/285 [02:47<06:01,  1.90s/it]predicting train subjects:  34%|███▎      | 96/285 [02:49<06:02,  1.92s/it]predicting train subjects:  34%|███▍      | 97/285 [02:51<05:56,  1.90s/it]predicting train subjects:  34%|███▍      | 98/285 [02:53<05:51,  1.88s/it]predicting train subjects:  35%|███▍      | 99/285 [02:55<05:50,  1.88s/it]predicting train subjects:  35%|███▌      | 100/285 [02:57<05:53,  1.91s/it]predicting train subjects:  35%|███▌      | 101/285 [02:59<05:51,  1.91s/it]predicting train subjects:  36%|███▌      | 102/285 [03:01<05:42,  1.87s/it]predicting train subjects:  36%|███▌      | 103/285 [03:02<05:35,  1.84s/it]predicting train subjects:  36%|███▋      | 104/285 [03:04<05:25,  1.80s/it]predicting train subjects:  37%|███▋      | 105/285 [03:06<05:20,  1.78s/it]predicting train subjects:  37%|███▋      | 106/285 [03:08<05:15,  1.76s/it]predicting train subjects:  38%|███▊      | 107/285 [03:09<05:11,  1.75s/it]predicting train subjects:  38%|███▊      | 108/285 [03:11<05:08,  1.74s/it]predicting train subjects:  38%|███▊      | 109/285 [03:13<05:10,  1.76s/it]predicting train subjects:  39%|███▊      | 110/285 [03:15<05:05,  1.75s/it]predicting train subjects:  39%|███▉      | 111/285 [03:16<05:02,  1.74s/it]predicting train subjects:  39%|███▉      | 112/285 [03:18<04:57,  1.72s/it]predicting train subjects:  40%|███▉      | 113/285 [03:20<04:55,  1.72s/it]predicting train subjects:  40%|████      | 114/285 [03:21<04:54,  1.72s/it]predicting train subjects:  40%|████      | 115/285 [03:23<04:54,  1.74s/it]predicting train subjects:  41%|████      | 116/285 [03:25<04:55,  1.75s/it]predicting train subjects:  41%|████      | 117/285 [03:27<04:54,  1.76s/it]predicting train subjects:  41%|████▏     | 118/285 [03:29<04:56,  1.77s/it]predicting train subjects:  42%|████▏     | 119/285 [03:30<04:58,  1.80s/it]predicting train subjects:  42%|████▏     | 120/285 [03:32<05:00,  1.82s/it]predicting train subjects:  42%|████▏     | 121/285 [03:34<04:51,  1.78s/it]predicting train subjects:  43%|████▎     | 122/285 [03:35<04:34,  1.68s/it]predicting train subjects:  43%|████▎     | 123/285 [03:37<04:22,  1.62s/it]predicting train subjects:  44%|████▎     | 124/285 [03:39<04:24,  1.64s/it]predicting train subjects:  44%|████▍     | 125/285 [03:40<04:15,  1.60s/it]predicting train subjects:  44%|████▍     | 126/285 [03:42<04:15,  1.61s/it]predicting train subjects:  45%|████▍     | 127/285 [03:43<04:12,  1.60s/it]predicting train subjects:  45%|████▍     | 128/285 [03:45<04:11,  1.60s/it]predicting train subjects:  45%|████▌     | 129/285 [03:46<04:07,  1.59s/it]predicting train subjects:  46%|████▌     | 130/285 [03:48<04:05,  1.59s/it]predicting train subjects:  46%|████▌     | 131/285 [03:50<04:03,  1.58s/it]predicting train subjects:  46%|████▋     | 132/285 [03:51<04:01,  1.58s/it]predicting train subjects:  47%|████▋     | 133/285 [03:53<03:59,  1.58s/it]predicting train subjects:  47%|████▋     | 134/285 [03:54<04:00,  1.60s/it]predicting train subjects:  47%|████▋     | 135/285 [03:56<03:57,  1.58s/it]predicting train subjects:  48%|████▊     | 136/285 [03:57<03:55,  1.58s/it]predicting train subjects:  48%|████▊     | 137/285 [03:59<03:54,  1.58s/it]predicting train subjects:  48%|████▊     | 138/285 [04:01<03:52,  1.58s/it]predicting train subjects:  49%|████▉     | 139/285 [04:02<03:50,  1.58s/it]predicting train subjects:  49%|████▉     | 140/285 [04:04<03:46,  1.56s/it]predicting train subjects:  49%|████▉     | 141/285 [04:05<03:44,  1.56s/it]predicting train subjects:  50%|████▉     | 142/285 [04:07<03:34,  1.50s/it]predicting train subjects:  50%|█████     | 143/285 [04:08<03:28,  1.47s/it]predicting train subjects:  51%|█████     | 144/285 [04:09<03:24,  1.45s/it]predicting train subjects:  51%|█████     | 145/285 [04:11<03:21,  1.44s/it]predicting train subjects:  51%|█████     | 146/285 [04:12<03:17,  1.42s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:14<03:17,  1.43s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:15<03:18,  1.45s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:17<03:14,  1.43s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:18<03:11,  1.42s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:19<03:07,  1.40s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:21<03:07,  1.41s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:22<03:04,  1.40s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:24<03:02,  1.40s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:25<03:03,  1.41s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:27<03:07,  1.45s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:28<03:10,  1.49s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:30<03:07,  1.48s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:31<03:06,  1.48s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:33<03:03,  1.47s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:34<03:02,  1.47s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:35<03:00,  1.47s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:37<03:01,  1.48s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:38<02:57,  1.47s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:40<02:56,  1.47s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:41<02:53,  1.46s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:43<02:54,  1.48s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:44<02:49,  1.45s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:46<02:45,  1.43s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:47<02:40,  1.39s/it]predicting train subjects:  60%|██████    | 171/285 [04:48<02:39,  1.40s/it]predicting train subjects:  60%|██████    | 172/285 [04:50<02:33,  1.36s/it]predicting train subjects:  61%|██████    | 173/285 [04:51<02:30,  1.35s/it]predicting train subjects:  61%|██████    | 174/285 [04:52<02:31,  1.36s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:54<02:32,  1.38s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:55<02:30,  1.38s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:56<02:27,  1.37s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:58<02:23,  1.34s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:59<02:26,  1.38s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:00<02:22,  1.35s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:02<02:21,  1.36s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:03<02:20,  1.36s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:05<02:22,  1.40s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:06<02:24,  1.43s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:08<02:20,  1.40s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:09<02:20,  1.42s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:10<02:16,  1.40s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:12<02:14,  1.39s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:13<02:14,  1.40s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:14<02:10,  1.38s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:16<02:07,  1.36s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:17<02:05,  1.35s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:19<02:05,  1.36s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:20<02:02,  1.35s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:21<01:59,  1.33s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:23<02:04,  1.40s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:24<02:08,  1.46s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:26<02:09,  1.49s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:27<02:11,  1.53s/it]predicting train subjects:  70%|███████   | 200/285 [05:29<02:11,  1.54s/it]predicting train subjects:  71%|███████   | 201/285 [05:31<02:11,  1.56s/it]predicting train subjects:  71%|███████   | 202/285 [05:32<02:11,  1.59s/it]predicting train subjects:  71%|███████   | 203/285 [05:34<02:09,  1.58s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:36<02:10,  1.61s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:37<02:09,  1.62s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:39<02:07,  1.62s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:40<02:06,  1.63s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:42<02:06,  1.65s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:44<02:03,  1.63s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:45<02:02,  1.63s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:47<01:59,  1.62s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:49<01:57,  1.61s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:50<01:54,  1.59s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:51<01:47,  1.52s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:53<01:44,  1.49s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:54<01:40,  1.46s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:56<01:39,  1.46s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:57<01:36,  1.44s/it]predicting train subjects:  77%|███████▋  | 219/285 [05:58<01:33,  1.42s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:00<01:32,  1.43s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:01<01:30,  1.42s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:03<01:27,  1.40s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:04<01:26,  1.39s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:05<01:23,  1.37s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:07<01:21,  1.37s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:08<01:21,  1.39s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:09<01:20,  1.38s/it]predicting train subjects:  80%|████████  | 228/285 [06:11<01:21,  1.43s/it]predicting train subjects:  80%|████████  | 229/285 [06:12<01:19,  1.42s/it]predicting train subjects:  81%|████████  | 230/285 [06:14<01:18,  1.42s/it]predicting train subjects:  81%|████████  | 231/285 [06:15<01:16,  1.41s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:17<01:20,  1.52s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:19<01:23,  1.60s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:21<01:24,  1.65s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:22<01:24,  1.69s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:24<01:23,  1.71s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:26<01:22,  1.71s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:28<01:21,  1.72s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:29<01:19,  1.72s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:31<01:18,  1.75s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:33<01:16,  1.75s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:35<01:15,  1.75s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:36<01:14,  1.77s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:38<01:13,  1.79s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:40<01:11,  1.79s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:42<01:08,  1.76s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:43<01:06,  1.75s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:45<01:04,  1.75s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:47<01:02,  1.74s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:48<00:57,  1.63s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:50<00:52,  1.55s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:51<00:48,  1.48s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:52<00:45,  1.44s/it]predicting train subjects:  89%|████████▉ | 254/285 [06:54<00:43,  1.40s/it]predicting train subjects:  89%|████████▉ | 255/285 [06:55<00:41,  1.38s/it]predicting train subjects:  90%|████████▉ | 256/285 [06:56<00:39,  1.36s/it]predicting train subjects:  90%|█████████ | 257/285 [06:58<00:38,  1.36s/it]predicting train subjects:  91%|█████████ | 258/285 [06:59<00:37,  1.38s/it]predicting train subjects:  91%|█████████ | 259/285 [07:00<00:35,  1.36s/it]predicting train subjects:  91%|█████████ | 260/285 [07:02<00:33,  1.35s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:03<00:32,  1.34s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:04<00:30,  1.33s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:06<00:29,  1.32s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:07<00:28,  1.34s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:08<00:27,  1.36s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:10<00:26,  1.37s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:11<00:24,  1.35s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:13<00:25,  1.50s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:15<00:25,  1.62s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:17<00:25,  1.67s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:18<00:23,  1.71s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:20<00:22,  1.74s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:22<00:21,  1.78s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:24<00:20,  1.82s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:26<00:18,  1.84s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:28<00:16,  1.83s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:30<00:14,  1.81s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:31<00:12,  1.79s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:33<00:10,  1.78s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:35<00:08,  1.79s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:37<00:07,  1.81s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:39<00:05,  1.82s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:40<00:03,  1.82s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:42<00:01,  1.82s/it]predicting train subjects: 100%|██████████| 285/285 [07:44<00:00,  1.82s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:36,  1.61s/it]Loading train:   1%|          | 2/285 [00:03<07:29,  1.59s/it]Loading train:   1%|          | 3/285 [00:04<07:08,  1.52s/it]Loading train:   1%|▏         | 4/285 [00:06<07:18,  1.56s/it]Loading train:   2%|▏         | 5/285 [00:07<06:50,  1.47s/it]Loading train:   2%|▏         | 6/285 [00:09<07:06,  1.53s/it]Loading train:   2%|▏         | 7/285 [00:10<07:30,  1.62s/it]Loading train:   3%|▎         | 8/285 [00:12<07:26,  1.61s/it]Loading train:   3%|▎         | 9/285 [00:13<06:58,  1.52s/it]Loading train:   4%|▎         | 10/285 [00:14<06:26,  1.41s/it]Loading train:   4%|▍         | 11/285 [00:16<06:17,  1.38s/it]Loading train:   4%|▍         | 12/285 [00:17<05:55,  1.30s/it]Loading train:   5%|▍         | 13/285 [00:18<05:44,  1.27s/it]Loading train:   5%|▍         | 14/285 [00:19<05:33,  1.23s/it]Loading train:   5%|▌         | 15/285 [00:20<05:30,  1.22s/it]Loading train:   6%|▌         | 16/285 [00:22<05:45,  1.29s/it]Loading train:   6%|▌         | 17/285 [00:23<05:43,  1.28s/it]Loading train:   6%|▋         | 18/285 [00:24<05:44,  1.29s/it]Loading train:   7%|▋         | 19/285 [00:26<05:51,  1.32s/it]Loading train:   7%|▋         | 20/285 [00:27<05:49,  1.32s/it]Loading train:   7%|▋         | 21/285 [00:28<05:36,  1.27s/it]Loading train:   8%|▊         | 22/285 [00:30<05:35,  1.28s/it]Loading train:   8%|▊         | 23/285 [00:31<05:42,  1.31s/it]Loading train:   8%|▊         | 24/285 [00:32<05:41,  1.31s/it]Loading train:   9%|▉         | 25/285 [00:34<05:33,  1.28s/it]Loading train:   9%|▉         | 26/285 [00:35<05:32,  1.28s/it]Loading train:   9%|▉         | 27/285 [00:36<05:31,  1.29s/it]Loading train:  10%|▉         | 28/285 [00:37<05:27,  1.27s/it]Loading train:  10%|█         | 29/285 [00:39<05:22,  1.26s/it]Loading train:  11%|█         | 30/285 [00:40<05:11,  1.22s/it]Loading train:  11%|█         | 31/285 [00:41<04:58,  1.18s/it]Loading train:  11%|█         | 32/285 [00:42<04:48,  1.14s/it]Loading train:  12%|█▏        | 33/285 [00:43<04:51,  1.16s/it]Loading train:  12%|█▏        | 34/285 [00:44<04:44,  1.13s/it]Loading train:  12%|█▏        | 35/285 [00:45<04:35,  1.10s/it]Loading train:  13%|█▎        | 36/285 [00:46<04:34,  1.10s/it]Loading train:  13%|█▎        | 37/285 [00:47<04:34,  1.11s/it]Loading train:  13%|█▎        | 38/285 [00:48<04:20,  1.05s/it]Loading train:  14%|█▎        | 39/285 [00:49<04:13,  1.03s/it]Loading train:  14%|█▍        | 40/285 [00:50<04:14,  1.04s/it]Loading train:  14%|█▍        | 41/285 [00:51<04:16,  1.05s/it]Loading train:  15%|█▍        | 42/285 [00:53<04:20,  1.07s/it]Loading train:  15%|█▌        | 43/285 [00:54<04:21,  1.08s/it]Loading train:  15%|█▌        | 44/285 [00:55<04:16,  1.07s/it]Loading train:  16%|█▌        | 45/285 [00:56<04:17,  1.07s/it]Loading train:  16%|█▌        | 46/285 [00:57<04:05,  1.03s/it]Loading train:  16%|█▋        | 47/285 [00:58<04:12,  1.06s/it]Loading train:  17%|█▋        | 48/285 [00:59<04:05,  1.04s/it]Loading train:  17%|█▋        | 49/285 [01:00<03:54,  1.00it/s]Loading train:  18%|█▊        | 50/285 [01:01<04:04,  1.04s/it]Loading train:  18%|█▊        | 51/285 [01:02<04:16,  1.10s/it]Loading train:  18%|█▊        | 52/285 [01:03<04:27,  1.15s/it]Loading train:  19%|█▊        | 53/285 [01:04<04:23,  1.14s/it]Loading train:  19%|█▉        | 54/285 [01:05<04:12,  1.09s/it]Loading train:  19%|█▉        | 55/285 [01:06<04:09,  1.08s/it]Loading train:  20%|█▉        | 56/285 [01:08<04:05,  1.07s/it]Loading train:  20%|██        | 57/285 [01:08<03:57,  1.04s/it]Loading train:  20%|██        | 58/285 [01:09<03:52,  1.02s/it]Loading train:  21%|██        | 59/285 [01:11<03:56,  1.05s/it]Loading train:  21%|██        | 60/285 [01:12<03:53,  1.04s/it]Loading train:  21%|██▏       | 61/285 [01:13<03:45,  1.01s/it]Loading train:  22%|██▏       | 62/285 [01:14<03:45,  1.01s/it]Loading train:  22%|██▏       | 63/285 [01:15<03:45,  1.01s/it]Loading train:  22%|██▏       | 64/285 [01:16<04:24,  1.20s/it]Loading train:  23%|██▎       | 65/285 [01:18<04:48,  1.31s/it]Loading train:  23%|██▎       | 66/285 [01:19<04:56,  1.35s/it]Loading train:  24%|██▎       | 67/285 [01:20<04:36,  1.27s/it]Loading train:  24%|██▍       | 68/285 [01:21<04:20,  1.20s/it]Loading train:  24%|██▍       | 69/285 [01:22<04:05,  1.13s/it]Loading train:  25%|██▍       | 70/285 [01:23<03:49,  1.07s/it]Loading train:  25%|██▍       | 71/285 [01:24<03:55,  1.10s/it]Loading train:  25%|██▌       | 72/285 [01:25<03:51,  1.09s/it]Loading train:  26%|██▌       | 73/285 [01:26<03:42,  1.05s/it]Loading train:  26%|██▌       | 74/285 [01:27<03:34,  1.02s/it]Loading train:  26%|██▋       | 75/285 [01:28<03:35,  1.03s/it]Loading train:  27%|██▋       | 76/285 [01:30<03:38,  1.05s/it]Loading train:  27%|██▋       | 77/285 [01:30<03:29,  1.01s/it]Loading train:  27%|██▋       | 78/285 [01:31<03:21,  1.03it/s]Loading train:  28%|██▊       | 79/285 [01:32<03:16,  1.05it/s]Loading train:  28%|██▊       | 80/285 [01:33<03:08,  1.09it/s]Loading train:  28%|██▊       | 81/285 [01:34<03:05,  1.10it/s]Loading train:  29%|██▉       | 82/285 [01:35<03:10,  1.07it/s]Loading train:  29%|██▉       | 83/285 [01:36<03:10,  1.06it/s]Loading train:  29%|██▉       | 84/285 [01:37<03:14,  1.04it/s]Loading train:  30%|██▉       | 85/285 [01:38<03:31,  1.06s/it]Loading train:  30%|███       | 86/285 [01:39<03:36,  1.09s/it]Loading train:  31%|███       | 87/285 [01:40<03:36,  1.09s/it]Loading train:  31%|███       | 88/285 [01:42<03:48,  1.16s/it]Loading train:  31%|███       | 89/285 [01:43<03:50,  1.18s/it]Loading train:  32%|███▏      | 90/285 [01:44<03:47,  1.17s/it]Loading train:  32%|███▏      | 91/285 [01:45<03:38,  1.13s/it]Loading train:  32%|███▏      | 92/285 [01:46<03:32,  1.10s/it]Loading train:  33%|███▎      | 93/285 [01:47<03:37,  1.14s/it]Loading train:  33%|███▎      | 94/285 [01:49<03:35,  1.13s/it]Loading train:  33%|███▎      | 95/285 [01:50<03:33,  1.12s/it]Loading train:  34%|███▎      | 96/285 [01:51<03:33,  1.13s/it]Loading train:  34%|███▍      | 97/285 [01:52<03:25,  1.09s/it]Loading train:  34%|███▍      | 98/285 [01:53<03:30,  1.13s/it]Loading train:  35%|███▍      | 99/285 [01:54<03:31,  1.14s/it]Loading train:  35%|███▌      | 100/285 [01:55<03:36,  1.17s/it]Loading train:  35%|███▌      | 101/285 [01:56<03:28,  1.13s/it]Loading train:  36%|███▌      | 102/285 [01:58<03:30,  1.15s/it]Loading train:  36%|███▌      | 103/285 [01:59<03:29,  1.15s/it]Loading train:  36%|███▋      | 104/285 [02:00<03:20,  1.11s/it]Loading train:  37%|███▋      | 105/285 [02:01<03:24,  1.13s/it]Loading train:  37%|███▋      | 106/285 [02:02<03:16,  1.10s/it]Loading train:  38%|███▊      | 107/285 [02:03<03:15,  1.10s/it]Loading train:  38%|███▊      | 108/285 [02:04<03:15,  1.10s/it]Loading train:  38%|███▊      | 109/285 [02:05<03:16,  1.11s/it]Loading train:  39%|███▊      | 110/285 [02:06<03:09,  1.08s/it]Loading train:  39%|███▉      | 111/285 [02:07<02:59,  1.03s/it]Loading train:  39%|███▉      | 112/285 [02:08<02:55,  1.02s/it]Loading train:  40%|███▉      | 113/285 [02:09<02:53,  1.01s/it]Loading train:  40%|████      | 114/285 [02:10<02:50,  1.01it/s]Loading train:  40%|████      | 115/285 [02:11<02:46,  1.02it/s]Loading train:  41%|████      | 116/285 [02:12<02:48,  1.00it/s]Loading train:  41%|████      | 117/285 [02:13<02:52,  1.03s/it]Loading train:  41%|████▏     | 118/285 [02:14<02:55,  1.05s/it]Loading train:  42%|████▏     | 119/285 [02:16<03:02,  1.10s/it]Loading train:  42%|████▏     | 120/285 [02:17<03:07,  1.13s/it]Loading train:  42%|████▏     | 121/285 [02:18<03:18,  1.21s/it]Loading train:  43%|████▎     | 122/285 [02:19<03:19,  1.23s/it]Loading train:  43%|████▎     | 123/285 [02:21<03:23,  1.25s/it]Loading train:  44%|████▎     | 124/285 [02:22<03:10,  1.18s/it]Loading train:  44%|████▍     | 125/285 [02:23<03:01,  1.13s/it]Loading train:  44%|████▍     | 126/285 [02:24<02:50,  1.07s/it]Loading train:  45%|████▍     | 127/285 [02:25<02:52,  1.09s/it]Loading train:  45%|████▍     | 128/285 [02:26<02:45,  1.05s/it]Loading train:  45%|████▌     | 129/285 [02:27<02:41,  1.04s/it]Loading train:  46%|████▌     | 130/285 [02:28<02:34,  1.00it/s]Loading train:  46%|████▌     | 131/285 [02:29<02:32,  1.01it/s]Loading train:  46%|████▋     | 132/285 [02:30<02:25,  1.05it/s]Loading train:  47%|████▋     | 133/285 [02:31<02:22,  1.07it/s]Loading train:  47%|████▋     | 134/285 [02:31<02:18,  1.09it/s]Loading train:  47%|████▋     | 135/285 [02:32<02:16,  1.10it/s]Loading train:  48%|████▊     | 136/285 [02:33<02:13,  1.12it/s]Loading train:  48%|████▊     | 137/285 [02:34<02:11,  1.12it/s]Loading train:  48%|████▊     | 138/285 [02:35<02:15,  1.08it/s]Loading train:  49%|████▉     | 139/285 [02:36<02:17,  1.06it/s]Loading train:  49%|████▉     | 140/285 [02:37<02:22,  1.02it/s]Loading train:  49%|████▉     | 141/285 [02:38<02:19,  1.03it/s]Loading train:  50%|████▉     | 142/285 [02:39<02:24,  1.01s/it]Loading train:  50%|█████     | 143/285 [02:40<02:20,  1.01it/s]Loading train:  51%|█████     | 144/285 [02:41<02:21,  1.00s/it]Loading train:  51%|█████     | 145/285 [02:42<02:17,  1.02it/s]Loading train:  51%|█████     | 146/285 [02:43<02:20,  1.01s/it]Loading train:  52%|█████▏    | 147/285 [02:44<02:11,  1.05it/s]Loading train:  52%|█████▏    | 148/285 [02:45<02:12,  1.03it/s]Loading train:  52%|█████▏    | 149/285 [02:46<02:08,  1.06it/s]Loading train:  53%|█████▎    | 150/285 [02:47<02:04,  1.09it/s]Loading train:  53%|█████▎    | 151/285 [02:48<02:03,  1.08it/s]Loading train:  53%|█████▎    | 152/285 [02:48<02:01,  1.10it/s]Loading train:  54%|█████▎    | 153/285 [02:49<01:58,  1.11it/s]Loading train:  54%|█████▍    | 154/285 [02:50<01:57,  1.12it/s]Loading train:  54%|█████▍    | 155/285 [02:51<01:55,  1.13it/s]Loading train:  55%|█████▍    | 156/285 [02:52<01:53,  1.14it/s]Loading train:  55%|█████▌    | 157/285 [02:53<02:00,  1.06it/s]Loading train:  55%|█████▌    | 158/285 [02:54<01:55,  1.09it/s]Loading train:  56%|█████▌    | 159/285 [02:55<01:56,  1.08it/s]Loading train:  56%|█████▌    | 160/285 [02:56<01:55,  1.09it/s]Loading train:  56%|█████▋    | 161/285 [02:57<01:50,  1.12it/s]Loading train:  57%|█████▋    | 162/285 [02:58<01:51,  1.10it/s]Loading train:  57%|█████▋    | 163/285 [02:58<01:50,  1.10it/s]Loading train:  58%|█████▊    | 164/285 [02:59<01:44,  1.16it/s]Loading train:  58%|█████▊    | 165/285 [03:00<01:46,  1.13it/s]Loading train:  58%|█████▊    | 166/285 [03:01<01:47,  1.10it/s]Loading train:  59%|█████▊    | 167/285 [03:02<01:50,  1.07it/s]Loading train:  59%|█████▉    | 168/285 [03:03<01:49,  1.07it/s]Loading train:  59%|█████▉    | 169/285 [03:04<01:50,  1.05it/s]Loading train:  60%|█████▉    | 170/285 [03:05<01:49,  1.05it/s]Loading train:  60%|██████    | 171/285 [03:06<01:47,  1.06it/s]Loading train:  60%|██████    | 172/285 [03:07<01:40,  1.12it/s]Loading train:  61%|██████    | 173/285 [03:08<01:43,  1.08it/s]Loading train:  61%|██████    | 174/285 [03:09<01:43,  1.07it/s]Loading train:  61%|██████▏   | 175/285 [03:10<01:41,  1.09it/s]Loading train:  62%|██████▏   | 176/285 [03:10<01:37,  1.12it/s]Loading train:  62%|██████▏   | 177/285 [03:11<01:36,  1.12it/s]Loading train:  62%|██████▏   | 178/285 [03:12<01:40,  1.06it/s]Loading train:  63%|██████▎   | 179/285 [03:13<01:44,  1.01it/s]Loading train:  63%|██████▎   | 180/285 [03:14<01:41,  1.04it/s]Loading train:  64%|██████▎   | 181/285 [03:15<01:40,  1.03it/s]Loading train:  64%|██████▍   | 182/285 [03:16<01:37,  1.06it/s]Loading train:  64%|██████▍   | 183/285 [03:17<01:39,  1.03it/s]Loading train:  65%|██████▍   | 184/285 [03:18<01:42,  1.02s/it]Loading train:  65%|██████▍   | 185/285 [03:19<01:40,  1.01s/it]Loading train:  65%|██████▌   | 186/285 [03:20<01:37,  1.01it/s]Loading train:  66%|██████▌   | 187/285 [03:21<01:36,  1.02it/s]Loading train:  66%|██████▌   | 188/285 [03:22<01:35,  1.02it/s]Loading train:  66%|██████▋   | 189/285 [03:23<01:32,  1.04it/s]Loading train:  67%|██████▋   | 190/285 [03:24<01:35,  1.01s/it]Loading train:  67%|██████▋   | 191/285 [03:25<01:33,  1.01it/s]Loading train:  67%|██████▋   | 192/285 [03:26<01:33,  1.00s/it]Loading train:  68%|██████▊   | 193/285 [03:27<01:28,  1.04it/s]Loading train:  68%|██████▊   | 194/285 [03:28<01:28,  1.03it/s]Loading train:  68%|██████▊   | 195/285 [03:29<01:29,  1.01it/s]Loading train:  69%|██████▉   | 196/285 [03:30<01:29,  1.01s/it]Loading train:  69%|██████▉   | 197/285 [03:31<01:32,  1.05s/it]Loading train:  69%|██████▉   | 198/285 [03:32<01:32,  1.06s/it]Loading train:  70%|██████▉   | 199/285 [03:33<01:25,  1.01it/s]Loading train:  70%|███████   | 200/285 [03:34<01:24,  1.01it/s]Loading train:  71%|███████   | 201/285 [03:35<01:25,  1.02s/it]Loading train:  71%|███████   | 202/285 [03:36<01:22,  1.00it/s]Loading train:  71%|███████   | 203/285 [03:37<01:22,  1.01s/it]Loading train:  72%|███████▏  | 204/285 [03:38<01:20,  1.00it/s]Loading train:  72%|███████▏  | 205/285 [03:39<01:21,  1.02s/it]Loading train:  72%|███████▏  | 206/285 [03:40<01:21,  1.03s/it]Loading train:  73%|███████▎  | 207/285 [03:42<01:22,  1.06s/it]Loading train:  73%|███████▎  | 208/285 [03:43<01:25,  1.11s/it]Loading train:  73%|███████▎  | 209/285 [03:44<01:26,  1.13s/it]Loading train:  74%|███████▎  | 210/285 [03:45<01:22,  1.10s/it]Loading train:  74%|███████▍  | 211/285 [03:46<01:18,  1.05s/it]Loading train:  74%|███████▍  | 212/285 [03:47<01:16,  1.05s/it]Loading train:  75%|███████▍  | 213/285 [03:48<01:17,  1.08s/it]Loading train:  75%|███████▌  | 214/285 [03:49<01:14,  1.05s/it]Loading train:  75%|███████▌  | 215/285 [03:50<01:09,  1.00it/s]Loading train:  76%|███████▌  | 216/285 [03:51<01:07,  1.02it/s]Loading train:  76%|███████▌  | 217/285 [03:52<01:05,  1.04it/s]Loading train:  76%|███████▋  | 218/285 [03:53<01:03,  1.06it/s]Loading train:  77%|███████▋  | 219/285 [03:54<01:02,  1.06it/s]Loading train:  77%|███████▋  | 220/285 [03:54<00:59,  1.10it/s]Loading train:  78%|███████▊  | 221/285 [03:55<00:58,  1.10it/s]Loading train:  78%|███████▊  | 222/285 [03:56<00:57,  1.10it/s]Loading train:  78%|███████▊  | 223/285 [03:57<00:56,  1.11it/s]Loading train:  79%|███████▊  | 224/285 [03:58<00:53,  1.13it/s]Loading train:  79%|███████▉  | 225/285 [03:59<00:51,  1.17it/s]Loading train:  79%|███████▉  | 226/285 [04:00<00:51,  1.14it/s]Loading train:  80%|███████▉  | 227/285 [04:01<00:50,  1.15it/s]Loading train:  80%|████████  | 228/285 [04:01<00:50,  1.13it/s]Loading train:  80%|████████  | 229/285 [04:02<00:48,  1.15it/s]Loading train:  81%|████████  | 230/285 [04:03<00:49,  1.12it/s]Loading train:  81%|████████  | 231/285 [04:04<00:47,  1.13it/s]Loading train:  81%|████████▏ | 232/285 [04:05<00:49,  1.06it/s]Loading train:  82%|████████▏ | 233/285 [04:06<00:51,  1.01it/s]Loading train:  82%|████████▏ | 234/285 [04:07<00:53,  1.04s/it]Loading train:  82%|████████▏ | 235/285 [04:09<00:53,  1.08s/it]Loading train:  83%|████████▎ | 236/285 [04:10<00:54,  1.12s/it]Loading train:  83%|████████▎ | 237/285 [04:11<00:53,  1.11s/it]Loading train:  84%|████████▎ | 238/285 [04:12<00:52,  1.12s/it]Loading train:  84%|████████▍ | 239/285 [04:13<00:51,  1.12s/it]Loading train:  84%|████████▍ | 240/285 [04:14<00:48,  1.08s/it]Loading train:  85%|████████▍ | 241/285 [04:15<00:49,  1.13s/it]Loading train:  85%|████████▍ | 242/285 [04:17<00:49,  1.16s/it]Loading train:  85%|████████▌ | 243/285 [04:18<00:50,  1.19s/it]Loading train:  86%|████████▌ | 244/285 [04:19<00:47,  1.15s/it]Loading train:  86%|████████▌ | 245/285 [04:20<00:46,  1.17s/it]Loading train:  86%|████████▋ | 246/285 [04:21<00:44,  1.15s/it]Loading train:  87%|████████▋ | 247/285 [04:23<00:44,  1.16s/it]Loading train:  87%|████████▋ | 248/285 [04:24<00:41,  1.11s/it]Loading train:  87%|████████▋ | 249/285 [04:25<00:40,  1.12s/it]Loading train:  88%|████████▊ | 250/285 [04:26<00:37,  1.06s/it]Loading train:  88%|████████▊ | 251/285 [04:26<00:33,  1.01it/s]Loading train:  88%|████████▊ | 252/285 [04:27<00:32,  1.02it/s]Loading train:  89%|████████▉ | 253/285 [04:28<00:30,  1.04it/s]Loading train:  89%|████████▉ | 254/285 [04:29<00:29,  1.06it/s]Loading train:  89%|████████▉ | 255/285 [04:30<00:28,  1.06it/s]Loading train:  90%|████████▉ | 256/285 [04:31<00:27,  1.05it/s]Loading train:  90%|█████████ | 257/285 [04:32<00:25,  1.10it/s]Loading train:  91%|█████████ | 258/285 [04:33<00:23,  1.13it/s]Loading train:  91%|█████████ | 259/285 [04:34<00:22,  1.13it/s]Loading train:  91%|█████████ | 260/285 [04:34<00:21,  1.17it/s]Loading train:  92%|█████████▏| 261/285 [04:35<00:21,  1.13it/s]Loading train:  92%|█████████▏| 262/285 [04:36<00:20,  1.13it/s]Loading train:  92%|█████████▏| 263/285 [04:37<00:19,  1.12it/s]Loading train:  93%|█████████▎| 264/285 [04:38<00:18,  1.11it/s]Loading train:  93%|█████████▎| 265/285 [04:39<00:18,  1.08it/s]Loading train:  93%|█████████▎| 266/285 [04:40<00:17,  1.11it/s]Loading train:  94%|█████████▎| 267/285 [04:41<00:16,  1.10it/s]Loading train:  94%|█████████▍| 268/285 [04:42<00:16,  1.01it/s]Loading train:  94%|█████████▍| 269/285 [04:43<00:16,  1.01s/it]Loading train:  95%|█████████▍| 270/285 [04:44<00:15,  1.04s/it]Loading train:  95%|█████████▌| 271/285 [04:45<00:15,  1.11s/it]Loading train:  95%|█████████▌| 272/285 [04:47<00:14,  1.11s/it]Loading train:  96%|█████████▌| 273/285 [04:48<00:13,  1.10s/it]Loading train:  96%|█████████▌| 274/285 [04:49<00:11,  1.08s/it]Loading train:  96%|█████████▋| 275/285 [04:50<00:10,  1.05s/it]Loading train:  97%|█████████▋| 276/285 [04:51<00:09,  1.05s/it]Loading train:  97%|█████████▋| 277/285 [04:52<00:08,  1.05s/it]Loading train:  98%|█████████▊| 278/285 [04:53<00:07,  1.09s/it]Loading train:  98%|█████████▊| 279/285 [04:54<00:06,  1.10s/it]Loading train:  98%|█████████▊| 280/285 [04:55<00:05,  1.14s/it]Loading train:  99%|█████████▊| 281/285 [04:56<00:04,  1.14s/it]Loading train:  99%|█████████▉| 282/285 [04:58<00:03,  1.13s/it]Loading train:  99%|█████████▉| 283/285 [04:59<00:02,  1.15s/it]Loading train: 100%|█████████▉| 284/285 [05:00<00:01,  1.14s/it]Loading train: 100%|██████████| 285/285 [05:01<00:00,  1.12s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:09, 30.00it/s]concatenating: train:  11%|█         | 31/285 [00:00<00:06, 40.86it/s]concatenating: train:  20%|██        | 57/285 [00:00<00:04, 54.63it/s]concatenating: train:  28%|██▊       | 79/285 [00:00<00:02, 70.50it/s]concatenating: train:  38%|███▊      | 107/285 [00:00<00:01, 90.85it/s]concatenating: train:  49%|████▉     | 139/285 [00:00<00:01, 115.70it/s]concatenating: train:  58%|█████▊    | 164/285 [00:00<00:00, 137.33it/s]concatenating: train:  66%|██████▌   | 188/285 [00:00<00:00, 138.61it/s]concatenating: train:  78%|███████▊  | 221/285 [00:01<00:00, 167.69it/s]concatenating: train:  89%|████████▉ | 255/285 [00:01<00:00, 197.03it/s]concatenating: train:  99%|█████████▉| 283/285 [00:01<00:00, 189.87it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 222.93it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.42s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.42s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 66.18it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:26,  1.36s/it]Loading train:   1%|          | 2/285 [00:02<06:31,  1.38s/it]Loading train:   1%|          | 3/285 [00:04<06:27,  1.37s/it]Loading train:   1%|▏         | 4/285 [00:05<06:45,  1.44s/it]Loading train:   2%|▏         | 5/285 [00:06<06:25,  1.38s/it]Loading train:   2%|▏         | 6/285 [00:08<06:40,  1.43s/it]Loading train:   2%|▏         | 7/285 [00:10<06:54,  1.49s/it]Loading train:   3%|▎         | 8/285 [00:11<06:58,  1.51s/it]Loading train:   3%|▎         | 9/285 [00:13<06:45,  1.47s/it]Loading train:   4%|▎         | 10/285 [00:14<06:15,  1.36s/it]Loading train:   4%|▍         | 11/285 [00:15<05:43,  1.25s/it]Loading train:   4%|▍         | 12/285 [00:16<05:24,  1.19s/it]Loading train:   5%|▍         | 13/285 [00:17<05:06,  1.13s/it]Loading train:   5%|▍         | 14/285 [00:18<05:00,  1.11s/it]Loading train:   5%|▌         | 15/285 [00:19<05:10,  1.15s/it]Loading train:   6%|▌         | 16/285 [00:20<05:03,  1.13s/it]Loading train:   6%|▌         | 17/285 [00:21<05:03,  1.13s/it]Loading train:   6%|▋         | 18/285 [00:22<04:56,  1.11s/it]Loading train:   7%|▋         | 19/285 [00:23<04:47,  1.08s/it]Loading train:   7%|▋         | 20/285 [00:24<04:41,  1.06s/it]Loading train:   7%|▋         | 21/285 [00:25<04:40,  1.06s/it]Loading train:   8%|▊         | 22/285 [00:27<05:00,  1.14s/it]Loading train:   8%|▊         | 23/285 [00:28<05:10,  1.18s/it]Loading train:   8%|▊         | 24/285 [00:29<05:00,  1.15s/it]Loading train:   9%|▉         | 25/285 [00:30<04:54,  1.13s/it]Loading train:   9%|▉         | 26/285 [00:31<04:41,  1.09s/it]Loading train:   9%|▉         | 27/285 [00:32<04:42,  1.09s/it]Loading train:  10%|▉         | 28/285 [00:34<04:57,  1.16s/it]Loading train:  10%|█         | 29/285 [00:35<04:39,  1.09s/it]Loading train:  11%|█         | 30/285 [00:36<04:37,  1.09s/it]Loading train:  11%|█         | 31/285 [00:37<04:31,  1.07s/it]Loading train:  11%|█         | 32/285 [00:38<04:37,  1.10s/it]Loading train:  12%|█▏        | 33/285 [00:39<04:30,  1.07s/it]Loading train:  12%|█▏        | 34/285 [00:40<04:18,  1.03s/it]Loading train:  12%|█▏        | 35/285 [00:41<04:19,  1.04s/it]Loading train:  13%|█▎        | 36/285 [00:42<04:12,  1.02s/it]Loading train:  13%|█▎        | 37/285 [00:43<04:05,  1.01it/s]Loading train:  13%|█▎        | 38/285 [00:44<04:04,  1.01it/s]Loading train:  14%|█▎        | 39/285 [00:45<04:10,  1.02s/it]Loading train:  14%|█▍        | 40/285 [00:46<04:22,  1.07s/it]Loading train:  14%|█▍        | 41/285 [00:47<04:26,  1.09s/it]Loading train:  15%|█▍        | 42/285 [00:48<04:36,  1.14s/it]Loading train:  15%|█▌        | 43/285 [00:49<04:21,  1.08s/it]Loading train:  15%|█▌        | 44/285 [00:50<04:12,  1.05s/it]Loading train:  16%|█▌        | 45/285 [00:51<04:12,  1.05s/it]Loading train:  16%|█▌        | 46/285 [00:52<04:04,  1.02s/it]Loading train:  16%|█▋        | 47/285 [00:53<03:58,  1.00s/it]Loading train:  17%|█▋        | 48/285 [00:54<03:42,  1.06it/s]Loading train:  17%|█▋        | 49/285 [00:55<03:30,  1.12it/s]Loading train:  18%|█▊        | 50/285 [00:56<03:24,  1.15it/s]Loading train:  18%|█▊        | 51/285 [00:57<03:25,  1.14it/s]Loading train:  18%|█▊        | 52/285 [00:57<03:24,  1.14it/s]Loading train:  19%|█▊        | 53/285 [00:58<03:25,  1.13it/s]Loading train:  19%|█▉        | 54/285 [00:59<03:43,  1.03it/s]Loading train:  19%|█▉        | 55/285 [01:00<03:44,  1.02it/s]Loading train:  20%|█▉        | 56/285 [01:01<03:38,  1.05it/s]Loading train:  20%|██        | 57/285 [01:02<03:46,  1.01it/s]Loading train:  20%|██        | 58/285 [01:04<04:00,  1.06s/it]Loading train:  21%|██        | 59/285 [01:05<03:44,  1.01it/s]Loading train:  21%|██        | 60/285 [01:06<03:47,  1.01s/it]Loading train:  21%|██▏       | 61/285 [01:06<03:34,  1.04it/s]Loading train:  22%|██▏       | 62/285 [01:07<03:29,  1.06it/s]Loading train:  22%|██▏       | 63/285 [01:08<03:20,  1.11it/s]Loading train:  22%|██▏       | 64/285 [01:10<03:53,  1.05s/it]Loading train:  23%|██▎       | 65/285 [01:11<04:35,  1.25s/it]Loading train:  23%|██▎       | 66/285 [01:13<04:47,  1.31s/it]Loading train:  24%|██▎       | 67/285 [01:14<04:25,  1.22s/it]Loading train:  24%|██▍       | 68/285 [01:15<04:05,  1.13s/it]Loading train:  24%|██▍       | 69/285 [01:16<03:50,  1.07s/it]Loading train:  25%|██▍       | 70/285 [01:16<03:36,  1.01s/it]Loading train:  25%|██▍       | 71/285 [01:17<03:28,  1.03it/s]Loading train:  25%|██▌       | 72/285 [01:18<03:26,  1.03it/s]Loading train:  26%|██▌       | 73/285 [01:19<03:17,  1.07it/s]Loading train:  26%|██▌       | 74/285 [01:20<03:10,  1.11it/s]Loading train:  26%|██▋       | 75/285 [01:21<03:06,  1.13it/s]Loading train:  27%|██▋       | 76/285 [01:22<03:16,  1.06it/s]Loading train:  27%|██▋       | 77/285 [01:23<03:22,  1.03it/s]Loading train:  27%|██▋       | 78/285 [01:24<03:15,  1.06it/s]Loading train:  28%|██▊       | 79/285 [01:25<03:28,  1.01s/it]Loading train:  28%|██▊       | 80/285 [01:26<03:16,  1.04it/s]Loading train:  28%|██▊       | 81/285 [01:27<03:08,  1.08it/s]Loading train:  29%|██▉       | 82/285 [01:28<03:25,  1.01s/it]Loading train:  29%|██▉       | 83/285 [01:29<03:53,  1.15s/it]Loading train:  29%|██▉       | 84/285 [01:31<03:58,  1.19s/it]Loading train:  30%|██▉       | 85/285 [01:32<04:20,  1.30s/it]Loading train:  30%|███       | 86/285 [01:33<04:13,  1.27s/it]Loading train:  31%|███       | 87/285 [01:35<04:28,  1.36s/it]Loading train:  31%|███       | 88/285 [01:37<04:42,  1.43s/it]Loading train:  31%|███       | 89/285 [01:38<04:16,  1.31s/it]Loading train:  32%|███▏      | 90/285 [01:39<04:43,  1.45s/it]Loading train:  32%|███▏      | 91/285 [01:41<04:39,  1.44s/it]Loading train:  32%|███▏      | 92/285 [01:42<04:40,  1.45s/it]Loading train:  33%|███▎      | 93/285 [01:44<04:46,  1.49s/it]Loading train:  33%|███▎      | 94/285 [01:45<04:36,  1.45s/it]Loading train:  33%|███▎      | 95/285 [01:47<04:34,  1.45s/it]Loading train:  34%|███▎      | 96/285 [01:48<04:32,  1.44s/it]Loading train:  34%|███▍      | 97/285 [01:50<04:32,  1.45s/it]Loading train:  34%|███▍      | 98/285 [01:51<04:22,  1.40s/it]Loading train:  35%|███▍      | 99/285 [01:52<04:14,  1.37s/it]Loading train:  35%|███▌      | 100/285 [01:53<04:08,  1.34s/it]Loading train:  35%|███▌      | 101/285 [01:55<04:24,  1.44s/it]Loading train:  36%|███▌      | 102/285 [01:57<04:25,  1.45s/it]Loading train:  36%|███▌      | 103/285 [01:58<04:02,  1.33s/it]Loading train:  36%|███▋      | 104/285 [01:58<03:38,  1.20s/it]Loading train:  37%|███▋      | 105/285 [02:00<03:40,  1.22s/it]Loading train:  37%|███▋      | 106/285 [02:01<03:21,  1.13s/it]Loading train:  38%|███▊      | 107/285 [02:02<03:09,  1.07s/it]Loading train:  38%|███▊      | 108/285 [02:02<03:01,  1.02s/it]Loading train:  38%|███▊      | 109/285 [02:03<02:53,  1.01it/s]Loading train:  39%|███▊      | 110/285 [02:04<02:43,  1.07it/s]Loading train:  39%|███▉      | 111/285 [02:05<02:40,  1.09it/s]Loading train:  39%|███▉      | 112/285 [02:06<02:39,  1.09it/s]Loading train:  40%|███▉      | 113/285 [02:07<02:29,  1.15it/s]Loading train:  40%|████      | 114/285 [02:08<02:26,  1.17it/s]Loading train:  40%|████      | 115/285 [02:09<02:30,  1.13it/s]Loading train:  41%|████      | 116/285 [02:10<02:34,  1.10it/s]Loading train:  41%|████      | 117/285 [02:10<02:34,  1.08it/s]Loading train:  41%|████▏     | 118/285 [02:11<02:28,  1.12it/s]Loading train:  42%|████▏     | 119/285 [02:12<02:25,  1.14it/s]Loading train:  42%|████▏     | 120/285 [02:13<02:25,  1.13it/s]Loading train:  42%|████▏     | 121/285 [02:14<02:48,  1.03s/it]Loading train:  43%|████▎     | 122/285 [02:15<02:49,  1.04s/it]Loading train:  43%|████▎     | 123/285 [02:17<02:52,  1.07s/it]Loading train:  44%|████▎     | 124/285 [02:17<02:43,  1.02s/it]Loading train:  44%|████▍     | 125/285 [02:18<02:29,  1.07it/s]Loading train:  44%|████▍     | 126/285 [02:19<02:22,  1.11it/s]Loading train:  45%|████▍     | 127/285 [02:20<02:22,  1.11it/s]Loading train:  45%|████▍     | 128/285 [02:21<02:21,  1.11it/s]Loading train:  45%|████▌     | 129/285 [02:22<02:19,  1.12it/s]Loading train:  46%|████▌     | 130/285 [02:22<02:12,  1.17it/s]Loading train:  46%|████▌     | 131/285 [02:23<02:10,  1.18it/s]Loading train:  46%|████▋     | 132/285 [02:24<02:07,  1.20it/s]Loading train:  47%|████▋     | 133/285 [02:25<02:04,  1.22it/s]Loading train:  47%|████▋     | 134/285 [02:26<02:07,  1.19it/s]Loading train:  47%|████▋     | 135/285 [02:27<02:08,  1.17it/s]Loading train:  48%|████▊     | 136/285 [02:28<02:06,  1.18it/s]Loading train:  48%|████▊     | 137/285 [02:28<02:06,  1.17it/s]Loading train:  48%|████▊     | 138/285 [02:29<02:03,  1.19it/s]Loading train:  49%|████▉     | 139/285 [02:30<02:00,  1.21it/s]Loading train:  49%|████▉     | 140/285 [02:31<02:01,  1.20it/s]Loading train:  49%|████▉     | 141/285 [02:32<01:57,  1.22it/s]Loading train:  50%|████▉     | 142/285 [02:33<01:59,  1.20it/s]Loading train:  50%|█████     | 143/285 [02:33<02:00,  1.18it/s]Loading train:  51%|█████     | 144/285 [02:34<01:54,  1.24it/s]Loading train:  51%|█████     | 145/285 [02:35<01:50,  1.27it/s]Loading train:  51%|█████     | 146/285 [02:36<01:47,  1.29it/s]Loading train:  52%|█████▏    | 147/285 [02:36<01:48,  1.27it/s]Loading train:  52%|█████▏    | 148/285 [02:37<01:45,  1.30it/s]Loading train:  52%|█████▏    | 149/285 [02:38<01:40,  1.36it/s]Loading train:  53%|█████▎    | 150/285 [02:38<01:36,  1.39it/s]Loading train:  53%|█████▎    | 151/285 [02:39<01:37,  1.37it/s]Loading train:  53%|█████▎    | 152/285 [02:40<01:35,  1.39it/s]Loading train:  54%|█████▎    | 153/285 [02:41<01:33,  1.41it/s]Loading train:  54%|█████▍    | 154/285 [02:41<01:34,  1.38it/s]Loading train:  54%|█████▍    | 155/285 [02:42<01:37,  1.33it/s]Loading train:  55%|█████▍    | 156/285 [02:43<01:33,  1.38it/s]Loading train:  55%|█████▌    | 157/285 [02:43<01:29,  1.43it/s]Loading train:  55%|█████▌    | 158/285 [02:44<01:34,  1.34it/s]Loading train:  56%|█████▌    | 159/285 [02:45<01:32,  1.36it/s]Loading train:  56%|█████▌    | 160/285 [02:46<01:33,  1.33it/s]Loading train:  56%|█████▋    | 161/285 [02:47<01:34,  1.32it/s]Loading train:  57%|█████▋    | 162/285 [02:47<01:32,  1.33it/s]Loading train:  57%|█████▋    | 163/285 [02:48<01:29,  1.36it/s]Loading train:  58%|█████▊    | 164/285 [02:49<01:27,  1.38it/s]Loading train:  58%|█████▊    | 165/285 [02:50<01:28,  1.35it/s]Loading train:  58%|█████▊    | 166/285 [02:50<01:26,  1.38it/s]Loading train:  59%|█████▊    | 167/285 [02:51<01:25,  1.38it/s]Loading train:  59%|█████▉    | 168/285 [02:52<01:26,  1.36it/s]Loading train:  59%|█████▉    | 169/285 [02:52<01:27,  1.33it/s]Loading train:  60%|█████▉    | 170/285 [02:53<01:25,  1.34it/s]Loading train:  60%|██████    | 171/285 [02:54<01:24,  1.35it/s]Loading train:  60%|██████    | 172/285 [02:55<01:24,  1.34it/s]Loading train:  61%|██████    | 173/285 [02:55<01:22,  1.36it/s]Loading train:  61%|██████    | 174/285 [02:56<01:20,  1.38it/s]Loading train:  61%|██████▏   | 175/285 [02:57<01:21,  1.36it/s]Loading train:  62%|██████▏   | 176/285 [02:58<01:19,  1.37it/s]Loading train:  62%|██████▏   | 177/285 [02:58<01:20,  1.34it/s]Loading train:  62%|██████▏   | 178/285 [02:59<01:20,  1.32it/s]Loading train:  63%|██████▎   | 179/285 [03:00<01:24,  1.26it/s]Loading train:  63%|██████▎   | 180/285 [03:01<01:21,  1.28it/s]Loading train:  64%|██████▎   | 181/285 [03:02<01:24,  1.23it/s]Loading train:  64%|██████▍   | 182/285 [03:02<01:19,  1.29it/s]Loading train:  64%|██████▍   | 183/285 [03:03<01:24,  1.20it/s]Loading train:  65%|██████▍   | 184/285 [03:04<01:21,  1.24it/s]Loading train:  65%|██████▍   | 185/285 [03:05<01:19,  1.27it/s]Loading train:  65%|██████▌   | 186/285 [03:06<01:31,  1.09it/s]Loading train:  66%|██████▌   | 187/285 [03:07<01:34,  1.04it/s]Loading train:  66%|██████▌   | 188/285 [03:08<01:33,  1.04it/s]Loading train:  66%|██████▋   | 189/285 [03:09<01:30,  1.06it/s]Loading train:  67%|██████▋   | 190/285 [03:10<01:26,  1.09it/s]Loading train:  67%|██████▋   | 191/285 [03:11<01:22,  1.14it/s]Loading train:  67%|██████▋   | 192/285 [03:11<01:19,  1.17it/s]Loading train:  68%|██████▊   | 193/285 [03:12<01:17,  1.19it/s]Loading train:  68%|██████▊   | 194/285 [03:13<01:15,  1.20it/s]Loading train:  68%|██████▊   | 195/285 [03:14<01:17,  1.17it/s]Loading train:  69%|██████▉   | 196/285 [03:15<01:17,  1.15it/s]Loading train:  69%|██████▉   | 197/285 [03:16<01:16,  1.15it/s]Loading train:  69%|██████▉   | 198/285 [03:17<01:15,  1.15it/s]Loading train:  70%|██████▉   | 199/285 [03:17<01:14,  1.15it/s]Loading train:  70%|███████   | 200/285 [03:18<01:12,  1.18it/s]Loading train:  71%|███████   | 201/285 [03:19<01:10,  1.19it/s]Loading train:  71%|███████   | 202/285 [03:20<01:09,  1.20it/s]Loading train:  71%|███████   | 203/285 [03:21<01:09,  1.19it/s]Loading train:  72%|███████▏  | 204/285 [03:22<01:08,  1.17it/s]Loading train:  72%|███████▏  | 205/285 [03:22<01:08,  1.17it/s]Loading train:  72%|███████▏  | 206/285 [03:23<01:07,  1.17it/s]Loading train:  73%|███████▎  | 207/285 [03:24<01:06,  1.17it/s]Loading train:  73%|███████▎  | 208/285 [03:25<01:05,  1.17it/s]Loading train:  73%|███████▎  | 209/285 [03:26<01:03,  1.19it/s]Loading train:  74%|███████▎  | 210/285 [03:27<01:05,  1.15it/s]Loading train:  74%|███████▍  | 211/285 [03:28<01:01,  1.20it/s]Loading train:  74%|███████▍  | 212/285 [03:28<01:02,  1.17it/s]Loading train:  75%|███████▍  | 213/285 [03:29<00:59,  1.21it/s]Loading train:  75%|███████▌  | 214/285 [03:30<00:59,  1.19it/s]Loading train:  75%|███████▌  | 215/285 [03:31<00:55,  1.25it/s]Loading train:  76%|███████▌  | 216/285 [03:31<00:53,  1.28it/s]Loading train:  76%|███████▌  | 217/285 [03:32<00:53,  1.27it/s]Loading train:  76%|███████▋  | 218/285 [03:33<00:52,  1.28it/s]Loading train:  77%|███████▋  | 219/285 [03:34<00:50,  1.30it/s]Loading train:  77%|███████▋  | 220/285 [03:35<00:51,  1.27it/s]Loading train:  78%|███████▊  | 221/285 [03:35<00:49,  1.30it/s]Loading train:  78%|███████▊  | 222/285 [03:36<00:48,  1.29it/s]Loading train:  78%|███████▊  | 223/285 [03:37<00:48,  1.28it/s]Loading train:  79%|███████▊  | 224/285 [03:38<00:47,  1.30it/s]Loading train:  79%|███████▉  | 225/285 [03:38<00:44,  1.34it/s]Loading train:  79%|███████▉  | 226/285 [03:39<00:45,  1.29it/s]Loading train:  80%|███████▉  | 227/285 [03:40<00:45,  1.28it/s]Loading train:  80%|████████  | 228/285 [03:41<00:43,  1.30it/s]Loading train:  80%|████████  | 229/285 [03:42<00:43,  1.29it/s]Loading train:  81%|████████  | 230/285 [03:42<00:43,  1.27it/s]Loading train:  81%|████████  | 231/285 [03:43<00:42,  1.28it/s]Loading train:  81%|████████▏ | 232/285 [03:44<00:45,  1.17it/s]Loading train:  82%|████████▏ | 233/285 [03:45<00:45,  1.14it/s]Loading train:  82%|████████▏ | 234/285 [03:46<00:49,  1.04it/s]Loading train:  82%|████████▏ | 235/285 [03:47<00:49,  1.00it/s]Loading train:  83%|████████▎ | 236/285 [03:48<00:49,  1.02s/it]Loading train:  83%|████████▎ | 237/285 [03:49<00:48,  1.00s/it]Loading train:  84%|████████▎ | 238/285 [03:50<00:44,  1.05it/s]Loading train:  84%|████████▍ | 239/285 [03:51<00:42,  1.08it/s]Loading train:  84%|████████▍ | 240/285 [03:52<00:41,  1.08it/s]Loading train:  85%|████████▍ | 241/285 [03:53<00:39,  1.10it/s]Loading train:  85%|████████▍ | 242/285 [03:54<00:41,  1.03it/s]Loading train:  85%|████████▌ | 243/285 [03:55<00:42,  1.02s/it]Loading train:  86%|████████▌ | 244/285 [03:56<00:41,  1.01s/it]Loading train:  86%|████████▌ | 245/285 [03:57<00:40,  1.00s/it]Loading train:  86%|████████▋ | 246/285 [03:58<00:38,  1.02it/s]Loading train:  87%|████████▋ | 247/285 [03:59<00:37,  1.02it/s]Loading train:  87%|████████▋ | 248/285 [04:00<00:36,  1.01it/s]Loading train:  87%|████████▋ | 249/285 [04:01<00:34,  1.04it/s]Loading train:  88%|████████▊ | 250/285 [04:02<00:31,  1.11it/s]Loading train:  88%|████████▊ | 251/285 [04:02<00:28,  1.21it/s]Loading train:  88%|████████▊ | 252/285 [04:03<00:26,  1.25it/s]Loading train:  89%|████████▉ | 253/285 [04:04<00:24,  1.30it/s]Loading train:  89%|████████▉ | 254/285 [04:04<00:22,  1.35it/s]Loading train:  89%|████████▉ | 255/285 [04:05<00:21,  1.39it/s]Loading train:  90%|████████▉ | 256/285 [04:06<00:20,  1.39it/s]Loading train:  90%|█████████ | 257/285 [04:07<00:20,  1.39it/s]Loading train:  91%|█████████ | 258/285 [04:07<00:21,  1.28it/s]Loading train:  91%|█████████ | 259/285 [04:08<00:20,  1.28it/s]Loading train:  91%|█████████ | 260/285 [04:09<00:19,  1.31it/s]Loading train:  92%|█████████▏| 261/285 [04:10<00:18,  1.28it/s]Loading train:  92%|█████████▏| 262/285 [04:11<00:17,  1.30it/s]Loading train:  92%|█████████▏| 263/285 [04:11<00:16,  1.30it/s]Loading train:  93%|█████████▎| 264/285 [04:12<00:16,  1.28it/s]Loading train:  93%|█████████▎| 265/285 [04:13<00:15,  1.33it/s]Loading train:  93%|█████████▎| 266/285 [04:14<00:14,  1.27it/s]Loading train:  94%|█████████▎| 267/285 [04:15<00:14,  1.25it/s]Loading train:  94%|█████████▍| 268/285 [04:15<00:14,  1.21it/s]Loading train:  94%|█████████▍| 269/285 [04:16<00:13,  1.16it/s]Loading train:  95%|█████████▍| 270/285 [04:17<00:13,  1.10it/s]Loading train:  95%|█████████▌| 271/285 [04:18<00:12,  1.11it/s]Loading train:  95%|█████████▌| 272/285 [04:19<00:11,  1.09it/s]Loading train:  96%|█████████▌| 273/285 [04:20<00:11,  1.06it/s]Loading train:  96%|█████████▌| 274/285 [04:21<00:10,  1.05it/s]Loading train:  96%|█████████▋| 275/285 [04:22<00:09,  1.01it/s]Loading train:  97%|█████████▋| 276/285 [04:23<00:08,  1.02it/s]Loading train:  97%|█████████▋| 277/285 [04:24<00:07,  1.01it/s]Loading train:  98%|█████████▊| 278/285 [04:25<00:06,  1.02it/s]Loading train:  98%|█████████▊| 279/285 [04:26<00:05,  1.05it/s]Loading train:  98%|█████████▊| 280/285 [04:27<00:04,  1.04it/s]Loading train:  99%|█████████▊| 281/285 [04:28<00:03,  1.04it/s]Loading train:  99%|█████████▉| 282/285 [04:29<00:02,  1.06it/s]Loading train:  99%|█████████▉| 283/285 [04:30<00:01,  1.04it/s]Loading train: 100%|█████████▉| 284/285 [04:31<00:00,  1.06it/s]Loading train: 100%|██████████| 285/285 [04:32<00:00,  1.09it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:  10%|█         | 29/285 [00:00<00:00, 289.26it/s]concatenating: train:  21%|██▏       | 61/285 [00:00<00:00, 294.55it/s]concatenating: train:  32%|███▏      | 91/285 [00:00<00:00, 282.61it/s]concatenating: train:  41%|████      | 116/285 [00:00<00:00, 270.20it/s]concatenating: train:  52%|█████▏    | 149/285 [00:00<00:00, 284.05it/s]concatenating: train:  64%|██████▍   | 183/285 [00:00<00:00, 297.12it/s]concatenating: train:  77%|███████▋  | 219/285 [00:00<00:00, 312.29it/s]concatenating: train:  89%|████████▉ | 253/285 [00:00<00:00, 319.23it/s]concatenating: train: 100%|██████████| 285/285 [00:00<00:00, 310.47it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.28s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 708.10it/s]2019-07-05 18:54:15.425669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 18:54:15.425786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 18:54:15.425802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 18:54:15.425812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 18:54:15.426319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:13,  2.94it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:10,  3.46it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:10,  3.34it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:07,  4.26it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:08,  3.85it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:06,  4.36it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:07,  3.85it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  4.88it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:05,  4.09it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:04,  4.51it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:05,  3.99it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  4.75it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:04<00:02,  5.14it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:04<00:03,  4.21it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  4.72it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:05<00:02,  4.02it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:05<00:01,  4.79it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:05<00:01,  5.04it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:06<00:01,  4.22it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:06<00:00,  4.64it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:06<00:00,  3.79it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:06<00:00,  5.73it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   200         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 20)   3620        dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 20)   80          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 20)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 20)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   3620        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 20)   3620        dropout_15[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 52, 80, 20)   80          conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 52, 80, 20)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 52, 80, 20)   0           activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 52, 80, 13)   273         dropout_16[0][0]                 
==================================================================================================
Total params: 198,913
Trainable params: 55,273
Non-trainable params: 143,640
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 18s - loss: 191.6731 - acc: 0.4727 - mDice: 0.0196 - val_loss: 93.5157 - val_acc: 0.9047 - val_mDice: 0.0166

Epoch 00001: val_mDice improved from -inf to 0.01656, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 85.5584 - acc: 0.8055 - mDice: 0.0221 - val_loss: 39.9320 - val_acc: 0.9047 - val_mDice: 0.0189

Epoch 00002: val_mDice improved from 0.01656 to 0.01894, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 11s - loss: 40.4568 - acc: 0.8527 - mDice: 0.0228 - val_loss: 18.0405 - val_acc: 0.9047 - val_mDice: 0.0173

Epoch 00003: val_mDice did not improve from 0.01894
Epoch 4/300
 - 11s - loss: 23.4767 - acc: 0.8663 - mDice: 0.0225 - val_loss: 11.2901 - val_acc: 0.9047 - val_mDice: 0.0174

Epoch 00004: val_mDice did not improve from 0.01894
Epoch 5/300
 - 11s - loss: 16.3965 - acc: 0.8688 - mDice: 0.0231 - val_loss: 8.7972 - val_acc: 0.9047 - val_mDice: 0.0205

Epoch 00005: val_mDice improved from 0.01894 to 0.02048, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 11s - loss: 12.9210 - acc: 0.8691 - mDice: 0.0246 - val_loss: 7.4411 - val_acc: 0.9047 - val_mDice: 0.0221

Epoch 00006: val_mDice improved from 0.02048 to 0.02213, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 11s - loss: 10.9478 - acc: 0.8692 - mDice: 0.0267 - val_loss: 6.6753 - val_acc: 0.9047 - val_mDice: 0.0253

Epoch 00007: val_mDice improved from 0.02213 to 0.02526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 9.6606 - acc: 0.8692 - mDice: 0.0292 - val_loss: 6.0868 - val_acc: 0.9047 - val_mDice: 0.0295

Epoch 00008: val_mDice improved from 0.02526 to 0.02947, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 11s - loss: 8.7171 - acc: 0.8692 - mDice: 0.0325 - val_loss: 5.8292 - val_acc: 0.9047 - val_mDice: 0.0272

Epoch 00009: val_mDice did not improve from 0.02947
Epoch 10/300
 - 11s - loss: 7.9597 - acc: 0.8692 - mDice: 0.0364 - val_loss: 5.4504 - val_acc: 0.9047 - val_mDice: 0.0396

Epoch 00010: val_mDice improved from 0.02947 to 0.03961, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 7.3713 - acc: 0.8692 - mDice: 0.0411 - val_loss: 6.2095 - val_acc: 0.9047 - val_mDice: 0.0464

Epoch 00011: val_mDice improved from 0.03961 to 0.04643, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 11s - loss: 6.9009 - acc: 0.8692 - mDice: 0.0460 - val_loss: 5.0508 - val_acc: 0.9047 - val_mDice: 0.0519

Epoch 00012: val_mDice improved from 0.04643 to 0.05193, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 6.5255 - acc: 0.8692 - mDice: 0.0512 - val_loss: 4.8304 - val_acc: 0.9047 - val_mDice: 0.0636

Epoch 00013: val_mDice improved from 0.05193 to 0.06361, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 6.2078 - acc: 0.8692 - mDice: 0.0568 - val_loss: 4.6579 - val_acc: 0.9047 - val_mDice: 0.0684

Epoch 00014: val_mDice improved from 0.06361 to 0.06839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 11s - loss: 5.9062 - acc: 0.8692 - mDice: 0.0634 - val_loss: 4.5462 - val_acc: 0.9048 - val_mDice: 0.0758

Epoch 00015: val_mDice improved from 0.06839 to 0.07584, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 11s - loss: 5.6358 - acc: 0.8694 - mDice: 0.0709 - val_loss: 4.4823 - val_acc: 0.9048 - val_mDice: 0.0840

Epoch 00016: val_mDice improved from 0.07584 to 0.08399, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 5.3949 - acc: 0.8702 - mDice: 0.0788 - val_loss: 4.4600 - val_acc: 0.9059 - val_mDice: 0.0953

Epoch 00017: val_mDice improved from 0.08399 to 0.09531, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 5.1747 - acc: 0.8716 - mDice: 0.0878 - val_loss: 4.0191 - val_acc: 0.9074 - val_mDice: 0.1147

Epoch 00018: val_mDice improved from 0.09531 to 0.11469, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 11s - loss: 4.9621 - acc: 0.8733 - mDice: 0.0978 - val_loss: 3.9697 - val_acc: 0.9095 - val_mDice: 0.1249

Epoch 00019: val_mDice improved from 0.11469 to 0.12492, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 4.7652 - acc: 0.8747 - mDice: 0.1089 - val_loss: 3.9707 - val_acc: 0.9098 - val_mDice: 0.1382

Epoch 00020: val_mDice improved from 0.12492 to 0.13819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 4.5585 - acc: 0.8763 - mDice: 0.1224 - val_loss: 3.8098 - val_acc: 0.9107 - val_mDice: 0.1532

Epoch 00021: val_mDice improved from 0.13819 to 0.15325, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 11s - loss: 4.3617 - acc: 0.8777 - mDice: 0.1366 - val_loss: 3.6820 - val_acc: 0.9135 - val_mDice: 0.1699

Epoch 00022: val_mDice improved from 0.15325 to 0.16990, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 11s - loss: 4.1856 - acc: 0.8794 - mDice: 0.1516 - val_loss: 3.8922 - val_acc: 0.9148 - val_mDice: 0.1800

Epoch 00023: val_mDice improved from 0.16990 to 0.17999, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 4.0140 - acc: 0.8810 - mDice: 0.1680 - val_loss: 3.8821 - val_acc: 0.9165 - val_mDice: 0.1891

Epoch 00024: val_mDice improved from 0.17999 to 0.18908, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 11s - loss: 3.8686 - acc: 0.8831 - mDice: 0.1842 - val_loss: 3.2808 - val_acc: 0.9192 - val_mDice: 0.2205

Epoch 00025: val_mDice improved from 0.18908 to 0.22045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 11s - loss: 3.7236 - acc: 0.8853 - mDice: 0.2010 - val_loss: 3.2638 - val_acc: 0.9194 - val_mDice: 0.2291

Epoch 00026: val_mDice improved from 0.22045 to 0.22905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 3.5863 - acc: 0.8874 - mDice: 0.2169 - val_loss: 3.1444 - val_acc: 0.9204 - val_mDice: 0.2462

Epoch 00027: val_mDice improved from 0.22905 to 0.24616, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 11s - loss: 3.4731 - acc: 0.8894 - mDice: 0.2320 - val_loss: 3.1425 - val_acc: 0.9208 - val_mDice: 0.2490

Epoch 00028: val_mDice improved from 0.24616 to 0.24904, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 11s - loss: 3.3716 - acc: 0.8914 - mDice: 0.2456 - val_loss: 3.0882 - val_acc: 0.9217 - val_mDice: 0.2714

Epoch 00029: val_mDice improved from 0.24904 to 0.27142, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 11s - loss: 3.2602 - acc: 0.8938 - mDice: 0.2608 - val_loss: 2.9360 - val_acc: 0.9243 - val_mDice: 0.2926

Epoch 00030: val_mDice improved from 0.27142 to 0.29260, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 11s - loss: 3.1635 - acc: 0.8958 - mDice: 0.2743 - val_loss: 3.0308 - val_acc: 0.9229 - val_mDice: 0.2929

Epoch 00031: val_mDice improved from 0.29260 to 0.29285, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 11s - loss: 3.0940 - acc: 0.8974 - mDice: 0.2863 - val_loss: 2.9226 - val_acc: 0.9278 - val_mDice: 0.3200

Epoch 00032: val_mDice improved from 0.29285 to 0.32005, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 3.0250 - acc: 0.8987 - mDice: 0.2971 - val_loss: 3.0142 - val_acc: 0.9262 - val_mDice: 0.3119

Epoch 00033: val_mDice did not improve from 0.32005
Epoch 34/300
 - 11s - loss: 2.9647 - acc: 0.8999 - mDice: 0.3062 - val_loss: 2.8358 - val_acc: 0.9291 - val_mDice: 0.3325

Epoch 00034: val_mDice improved from 0.32005 to 0.33252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 2.8994 - acc: 0.9010 - mDice: 0.3166 - val_loss: 2.9792 - val_acc: 0.9295 - val_mDice: 0.3314

Epoch 00035: val_mDice did not improve from 0.33252
Epoch 36/300
 - 11s - loss: 2.8369 - acc: 0.9020 - mDice: 0.3270 - val_loss: 2.9358 - val_acc: 0.9317 - val_mDice: 0.3429

Epoch 00036: val_mDice improved from 0.33252 to 0.34288, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 11s - loss: 2.7795 - acc: 0.9030 - mDice: 0.3375 - val_loss: 3.5020 - val_acc: 0.9297 - val_mDice: 0.3276

Epoch 00037: val_mDice did not improve from 0.34288
Epoch 38/300
 - 11s - loss: 2.7485 - acc: 0.9039 - mDice: 0.3440 - val_loss: 2.8201 - val_acc: 0.9318 - val_mDice: 0.3642

Epoch 00038: val_mDice improved from 0.34288 to 0.36415, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 11s - loss: 2.7029 - acc: 0.9051 - mDice: 0.3517 - val_loss: 2.9339 - val_acc: 0.9296 - val_mDice: 0.3577

Epoch 00039: val_mDice did not improve from 0.36415
Epoch 40/300
 - 11s - loss: 2.6615 - acc: 0.9060 - mDice: 0.3599 - val_loss: 3.1232 - val_acc: 0.9332 - val_mDice: 0.3546

Epoch 00040: val_mDice did not improve from 0.36415
Epoch 41/300
 - 11s - loss: 2.6285 - acc: 0.9067 - mDice: 0.3659 - val_loss: 3.1136 - val_acc: 0.9334 - val_mDice: 0.3616

Epoch 00041: val_mDice did not improve from 0.36415
Epoch 42/300
 - 11s - loss: 2.5827 - acc: 0.9077 - mDice: 0.3736 - val_loss: 3.0528 - val_acc: 0.9330 - val_mDice: 0.3698

Epoch 00042: val_mDice improved from 0.36415 to 0.36985, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 11s - loss: 2.5586 - acc: 0.9083 - mDice: 0.3788 - val_loss: 2.7590 - val_acc: 0.9341 - val_mDice: 0.3857

Epoch 00043: val_mDice improved from 0.36985 to 0.38568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 11s - loss: 2.5111 - acc: 0.9093 - mDice: 0.3867 - val_loss: 3.1449 - val_acc: 0.9324 - val_mDice: 0.3615

Epoch 00044: val_mDice did not improve from 0.38568
Epoch 45/300
 - 11s - loss: 2.4947 - acc: 0.9099 - mDice: 0.3904 - val_loss: 2.9288 - val_acc: 0.9303 - val_mDice: 0.3792

Epoch 00045: val_mDice did not improve from 0.38568
Epoch 46/300
 - 11s - loss: 2.4550 - acc: 0.9106 - mDice: 0.3979 - val_loss: 3.0384 - val_acc: 0.9361 - val_mDice: 0.3909

Epoch 00046: val_mDice improved from 0.38568 to 0.39094, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 11s - loss: 2.4347 - acc: 0.9109 - mDice: 0.4012 - val_loss: 3.0132 - val_acc: 0.9360 - val_mDice: 0.3946

Epoch 00047: val_mDice improved from 0.39094 to 0.39459, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 11s - loss: 2.4042 - acc: 0.9115 - mDice: 0.4074 - val_loss: 2.9779 - val_acc: 0.9346 - val_mDice: 0.3957

Epoch 00048: val_mDice improved from 0.39459 to 0.39566, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 11s - loss: 2.3741 - acc: 0.9121 - mDice: 0.4135 - val_loss: 2.9393 - val_acc: 0.9367 - val_mDice: 0.4034

Epoch 00049: val_mDice improved from 0.39566 to 0.40337, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 11s - loss: 2.3418 - acc: 0.9130 - mDice: 0.4197 - val_loss: 2.8866 - val_acc: 0.9316 - val_mDice: 0.3881

Epoch 00050: val_mDice did not improve from 0.40337
Epoch 51/300
 - 11s - loss: 2.3293 - acc: 0.9131 - mDice: 0.4224 - val_loss: 2.9339 - val_acc: 0.9358 - val_mDice: 0.3956

Epoch 00051: val_mDice did not improve from 0.40337
Epoch 52/300
 - 11s - loss: 2.3027 - acc: 0.9138 - mDice: 0.4278 - val_loss: 2.9652 - val_acc: 0.9313 - val_mDice: 0.3987

Epoch 00052: val_mDice did not improve from 0.40337
Epoch 53/300
 - 11s - loss: 2.2760 - acc: 0.9146 - mDice: 0.4328 - val_loss: 3.2613 - val_acc: 0.9372 - val_mDice: 0.3950

Epoch 00053: val_mDice did not improve from 0.40337
Epoch 54/300
 - 11s - loss: 2.2663 - acc: 0.9150 - mDice: 0.4347 - val_loss: 2.8669 - val_acc: 0.9382 - val_mDice: 0.4134

Epoch 00054: val_mDice improved from 0.40337 to 0.41342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 11s - loss: 2.2282 - acc: 0.9158 - mDice: 0.4423 - val_loss: 3.1131 - val_acc: 0.9361 - val_mDice: 0.4014

Epoch 00055: val_mDice did not improve from 0.41342
Epoch 56/300
 - 11s - loss: 2.2170 - acc: 0.9162 - mDice: 0.4452 - val_loss: 3.0652 - val_acc: 0.9369 - val_mDice: 0.4065

Epoch 00056: val_mDice did not improve from 0.41342
Epoch 57/300
 - 11s - loss: 2.1988 - acc: 0.9170 - mDice: 0.4491 - val_loss: 2.9333 - val_acc: 0.9362 - val_mDice: 0.4095

Epoch 00057: val_mDice did not improve from 0.41342
Epoch 58/300
 - 11s - loss: 2.1700 - acc: 0.9182 - mDice: 0.4547 - val_loss: 2.9293 - val_acc: 0.9355 - val_mDice: 0.4096

Epoch 00058: val_mDice did not improve from 0.41342
Epoch 59/300
 - 11s - loss: 2.1675 - acc: 0.9184 - mDice: 0.4558 - val_loss: 3.1660 - val_acc: 0.9387 - val_mDice: 0.4048

Epoch 00059: val_mDice did not improve from 0.41342
Epoch 60/300
 - 11s - loss: 2.1383 - acc: 0.9190 - mDice: 0.4616 - val_loss: 3.0044 - val_acc: 0.9352 - val_mDice: 0.4089

Epoch 00060: val_mDice did not improve from 0.41342
Epoch 61/300
 - 11s - loss: 2.1225 - acc: 0.9195 - mDice: 0.4642 - val_loss: 3.6489 - val_acc: 0.9368 - val_mDice: 0.3944

Epoch 00061: val_mDice did not improve from 0.41342
Epoch 62/300
 - 11s - loss: 2.1119 - acc: 0.9196 - mDice: 0.4673 - val_loss: 2.9341 - val_acc: 0.9351 - val_mDice: 0.4174

Epoch 00062: val_mDice improved from 0.41342 to 0.41744, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 11s - loss: 2.0978 - acc: 0.9201 - mDice: 0.4703 - val_loss: 3.1361 - val_acc: 0.9370 - val_mDice: 0.4113

Epoch 00063: val_mDice did not improve from 0.41744
Epoch 64/300
 - 11s - loss: 2.0801 - acc: 0.9205 - mDice: 0.4737 - val_loss: 3.0050 - val_acc: 0.9335 - val_mDice: 0.4138

Epoch 00064: val_mDice did not improve from 0.41744
Epoch 65/300
 - 11s - loss: 2.0620 - acc: 0.9210 - mDice: 0.4771 - val_loss: 2.9777 - val_acc: 0.9364 - val_mDice: 0.4235

Epoch 00065: val_mDice improved from 0.41744 to 0.42346, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 11s - loss: 2.0542 - acc: 0.9213 - mDice: 0.4798 - val_loss: 3.3350 - val_acc: 0.9395 - val_mDice: 0.4172

Epoch 00066: val_mDice did not improve from 0.42346
Epoch 67/300
 - 11s - loss: 2.0350 - acc: 0.9216 - mDice: 0.4833 - val_loss: 3.2639 - val_acc: 0.9350 - val_mDice: 0.4080

Epoch 00067: val_mDice did not improve from 0.42346
Epoch 68/300
 - 11s - loss: 2.0239 - acc: 0.9219 - mDice: 0.4855 - val_loss: 3.0162 - val_acc: 0.9358 - val_mDice: 0.4271

Epoch 00068: val_mDice improved from 0.42346 to 0.42713, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 11s - loss: 2.0161 - acc: 0.9222 - mDice: 0.4877 - val_loss: 2.9930 - val_acc: 0.9363 - val_mDice: 0.4147

Epoch 00069: val_mDice did not improve from 0.42713
Epoch 70/300
 - 11s - loss: 2.0013 - acc: 0.9224 - mDice: 0.4907 - val_loss: 3.1600 - val_acc: 0.9407 - val_mDice: 0.4226

Epoch 00070: val_mDice did not improve from 0.42713
Epoch 71/300
 - 11s - loss: 1.9897 - acc: 0.9227 - mDice: 0.4934 - val_loss: 3.1123 - val_acc: 0.9386 - val_mDice: 0.4230

Epoch 00071: val_mDice did not improve from 0.42713
Epoch 72/300
 - 11s - loss: 1.9769 - acc: 0.9233 - mDice: 0.4964 - val_loss: 3.1459 - val_acc: 0.9402 - val_mDice: 0.4227

Epoch 00072: val_mDice did not improve from 0.42713
Epoch 73/300
 - 11s - loss: 1.9740 - acc: 0.9231 - mDice: 0.4970 - val_loss: 3.1051 - val_acc: 0.9380 - val_mDice: 0.4223

Epoch 00073: val_mDice did not improve from 0.42713
Epoch 74/300
 - 11s - loss: 1.9580 - acc: 0.9235 - mDice: 0.4998 - val_loss: 3.6357 - val_acc: 0.9388 - val_mDice: 0.4116

Epoch 00074: val_mDice did not improve from 0.42713
Epoch 75/300
 - 11s - loss: 1.9487 - acc: 0.9239 - mDice: 0.5025 - val_loss: 3.0807 - val_acc: 0.9337 - val_mDice: 0.4157

Epoch 00075: val_mDice did not improve from 0.42713
Epoch 76/300
 - 11s - loss: 1.9402 - acc: 0.9241 - mDice: 0.5046 - val_loss: 3.2284 - val_acc: 0.9388 - val_mDice: 0.4188

Epoch 00076: val_mDice did not improve from 0.42713
Epoch 77/300
 - 11s - loss: 1.9353 - acc: 0.9244 - mDice: 0.5050 - val_loss: 3.2848 - val_acc: 0.9394 - val_mDice: 0.4254

Epoch 00077: val_mDice did not improve from 0.42713
Epoch 78/300
 - 11s - loss: 1.9229 - acc: 0.9244 - mDice: 0.5076 - val_loss: 3.2107 - val_acc: 0.9335 - val_mDice: 0.3987

Epoch 00078: val_mDice did not improve from 0.42713
Epoch 79/300
 - 11s - loss: 1.9084 - acc: 0.9250 - mDice: 0.5115 - val_loss: 3.4493 - val_acc: 0.9339 - val_mDice: 0.4131

Epoch 00079: val_mDice did not improve from 0.42713
Epoch 80/300
 - 11s - loss: 1.9084 - acc: 0.9251 - mDice: 0.5114 - val_loss: 3.5627 - val_acc: 0.9400 - val_mDice: 0.4184

Epoch 00080: val_mDice did not improve from 0.42713
Epoch 81/300
 - 11s - loss: 1.8903 - acc: 0.9253 - mDice: 0.5150 - val_loss: 3.1224 - val_acc: 0.9336 - val_mDice: 0.4329

Epoch 00081: val_mDice improved from 0.42713 to 0.43288, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 11s - loss: 1.8960 - acc: 0.9253 - mDice: 0.5132 - val_loss: 3.0891 - val_acc: 0.9400 - val_mDice: 0.4375

Epoch 00082: val_mDice improved from 0.43288 to 0.43747, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 83/300
 - 11s - loss: 1.8838 - acc: 0.9257 - mDice: 0.5154 - val_loss: 3.3731 - val_acc: 0.9409 - val_mDice: 0.4329

Epoch 00083: val_mDice did not improve from 0.43747
Epoch 84/300
 - 11s - loss: 1.8744 - acc: 0.9260 - mDice: 0.5183 - val_loss: 3.2080 - val_acc: 0.9384 - val_mDice: 0.4292

Epoch 00084: val_mDice did not improve from 0.43747
Epoch 85/300
 - 11s - loss: 1.8723 - acc: 0.9262 - mDice: 0.5188 - val_loss: 3.2395 - val_acc: 0.9339 - val_mDice: 0.4237

Epoch 00085: val_mDice did not improve from 0.43747
Epoch 86/300
 - 11s - loss: 1.8593 - acc: 0.9266 - mDice: 0.5215 - val_loss: 3.3493 - val_acc: 0.9399 - val_mDice: 0.4287

Epoch 00086: val_mDice did not improve from 0.43747
Epoch 87/300
 - 11s - loss: 1.8495 - acc: 0.9268 - mDice: 0.5233 - val_loss: 3.2494 - val_acc: 0.9400 - val_mDice: 0.4277

Epoch 00087: val_mDice did not improve from 0.43747
Epoch 88/300
 - 11s - loss: 1.8422 - acc: 0.9269 - mDice: 0.5246 - val_loss: 3.3256 - val_acc: 0.9405 - val_mDice: 0.4363

Epoch 00088: val_mDice did not improve from 0.43747
Epoch 89/300
 - 11s - loss: 1.8433 - acc: 0.9268 - mDice: 0.5255 - val_loss: 3.2416 - val_acc: 0.9366 - val_mDice: 0.4196

Epoch 00089: val_mDice did not improve from 0.43747
Epoch 90/300
 - 11s - loss: 1.8403 - acc: 0.9265 - mDice: 0.5258 - val_loss: 3.0679 - val_acc: 0.9413 - val_mDice: 0.4380

Epoch 00090: val_mDice improved from 0.43747 to 0.43800, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 11s - loss: 1.8284 - acc: 0.9272 - mDice: 0.5285 - val_loss: 3.2291 - val_acc: 0.9387 - val_mDice: 0.4336

Epoch 00091: val_mDice did not improve from 0.43800
Epoch 92/300
 - 11s - loss: 1.8135 - acc: 0.9276 - mDice: 0.5316 - val_loss: 3.4315 - val_acc: 0.9414 - val_mDice: 0.4321

Epoch 00092: val_mDice did not improve from 0.43800
Epoch 93/300
 - 11s - loss: 1.8076 - acc: 0.9276 - mDice: 0.5322 - val_loss: 3.2055 - val_acc: 0.9381 - val_mDice: 0.4313

Epoch 00093: val_mDice did not improve from 0.43800
Epoch 94/300
 - 11s - loss: 1.8101 - acc: 0.9277 - mDice: 0.5330 - val_loss: 3.3342 - val_acc: 0.9388 - val_mDice: 0.4250

Epoch 00094: val_mDice did not improve from 0.43800
Epoch 95/300
 - 11s - loss: 1.7976 - acc: 0.9279 - mDice: 0.5357 - val_loss: 3.1571 - val_acc: 0.9380 - val_mDice: 0.4386

Epoch 00095: val_mDice improved from 0.43800 to 0.43858, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 11s - loss: 1.7931 - acc: 0.9278 - mDice: 0.5364 - val_loss: 3.3976 - val_acc: 0.9316 - val_mDice: 0.4097

Epoch 00096: val_mDice did not improve from 0.43858
Epoch 97/300
 - 11s - loss: 1.7949 - acc: 0.9277 - mDice: 0.5360 - val_loss: 3.5313 - val_acc: 0.9376 - val_mDice: 0.4250

Epoch 00097: val_mDice did not improve from 0.43858
Epoch 98/300
 - 11s - loss: 1.7870 - acc: 0.9279 - mDice: 0.5375 - val_loss: 3.1399 - val_acc: 0.9399 - val_mDice: 0.4394

Epoch 00098: val_mDice improved from 0.43858 to 0.43940, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 11s - loss: 1.7798 - acc: 0.9283 - mDice: 0.5397 - val_loss: 3.5181 - val_acc: 0.9414 - val_mDice: 0.4330

Epoch 00099: val_mDice did not improve from 0.43940
Epoch 100/300
 - 11s - loss: 1.7688 - acc: 0.9284 - mDice: 0.5414 - val_loss: 3.4332 - val_acc: 0.9415 - val_mDice: 0.4284

Epoch 00100: val_mDice did not improve from 0.43940
Epoch 101/300
 - 11s - loss: 1.7696 - acc: 0.9283 - mDice: 0.5416 - val_loss: 3.3912 - val_acc: 0.9400 - val_mDice: 0.4315

Epoch 00101: val_mDice did not improve from 0.43940
Epoch 102/300
 - 11s - loss: 1.7699 - acc: 0.9283 - mDice: 0.5410 - val_loss: 3.3762 - val_acc: 0.9324 - val_mDice: 0.4305

Epoch 00102: val_mDice did not improve from 0.43940
Epoch 103/300
 - 11s - loss: 1.7624 - acc: 0.9283 - mDice: 0.5432 - val_loss: 3.2798 - val_acc: 0.9410 - val_mDice: 0.4407

Epoch 00103: val_mDice improved from 0.43940 to 0.44074, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 104/300
 - 11s - loss: 1.7624 - acc: 0.9285 - mDice: 0.5434 - val_loss: 3.2638 - val_acc: 0.9410 - val_mDice: 0.4471

Epoch 00104: val_mDice improved from 0.44074 to 0.44706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 105/300
 - 11s - loss: 1.7547 - acc: 0.9288 - mDice: 0.5449 - val_loss: 3.0707 - val_acc: 0.9390 - val_mDice: 0.4435

Epoch 00105: val_mDice did not improve from 0.44706
Epoch 106/300
 - 11s - loss: 1.7416 - acc: 0.9291 - mDice: 0.5480 - val_loss: 3.4594 - val_acc: 0.9404 - val_mDice: 0.4410

Epoch 00106: val_mDice did not improve from 0.44706
Epoch 107/300
 - 11s - loss: 1.7513 - acc: 0.9287 - mDice: 0.5459 - val_loss: 3.2336 - val_acc: 0.9412 - val_mDice: 0.4514

Epoch 00107: val_mDice improved from 0.44706 to 0.45142, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 11s - loss: 1.7337 - acc: 0.9293 - mDice: 0.5493 - val_loss: 3.1412 - val_acc: 0.9360 - val_mDice: 0.4352

Epoch 00108: val_mDice did not improve from 0.45142
Epoch 109/300
 - 11s - loss: 1.7378 - acc: 0.9292 - mDice: 0.5488 - val_loss: 3.1267 - val_acc: 0.9384 - val_mDice: 0.4472

Epoch 00109: val_mDice did not improve from 0.45142
Epoch 110/300
 - 11s - loss: 1.7289 - acc: 0.9294 - mDice: 0.5501 - val_loss: 3.3347 - val_acc: 0.9358 - val_mDice: 0.4314

Epoch 00110: val_mDice did not improve from 0.45142
Epoch 111/300
 - 11s - loss: 1.7247 - acc: 0.9295 - mDice: 0.5514 - val_loss: 3.3767 - val_acc: 0.9352 - val_mDice: 0.4432

Epoch 00111: val_mDice did not improve from 0.45142
Epoch 112/300
 - 11s - loss: 1.7198 - acc: 0.9296 - mDice: 0.5527 - val_loss: 3.3769 - val_acc: 0.9381 - val_mDice: 0.4365

Epoch 00112: val_mDice did not improve from 0.45142
Epoch 113/300
 - 11s - loss: 1.7141 - acc: 0.9297 - mDice: 0.5541 - val_loss: 3.1734 - val_acc: 0.9396 - val_mDice: 0.4427

Epoch 00113: val_mDice did not improve from 0.45142
Epoch 114/300
 - 11s - loss: 1.7185 - acc: 0.9297 - mDice: 0.5528 - val_loss: 3.4345 - val_acc: 0.9395 - val_mDice: 0.4372

Epoch 00114: val_mDice did not improve from 0.45142
Epoch 115/300
 - 11s - loss: 1.7132 - acc: 0.9298 - mDice: 0.5542 - val_loss: 3.4498 - val_acc: 0.9379 - val_mDice: 0.4422

Epoch 00115: val_mDice did not improve from 0.45142
Epoch 116/300
 - 11s - loss: 1.7059 - acc: 0.9297 - mDice: 0.5558 - val_loss: 3.2720 - val_acc: 0.9392 - val_mDice: 0.4409

Epoch 00116: val_mDice did not improve from 0.45142
Epoch 117/300
 - 11s - loss: 1.6975 - acc: 0.9302 - mDice: 0.5581 - val_loss: 3.2656 - val_acc: 0.9396 - val_mDice: 0.4364

Epoch 00117: val_mDice did not improve from 0.45142
Epoch 118/300
 - 11s - loss: 1.6976 - acc: 0.9301 - mDice: 0.5578 - val_loss: 3.3781 - val_acc: 0.9395 - val_mDice: 0.4325

Epoch 00118: val_mDice did not improve from 0.45142
Epoch 119/300
 - 11s - loss: 1.6953 - acc: 0.9302 - mDice: 0.5587 - val_loss: 3.6956 - val_acc: 0.9398 - val_mDice: 0.4335

Epoch 00119: val_mDice did not improve from 0.45142
Epoch 120/300
 - 11s - loss: 1.6924 - acc: 0.9302 - mDice: 0.5595 - val_loss: 3.6670 - val_acc: 0.9407 - val_mDice: 0.4378

Epoch 00120: val_mDice did not improve from 0.45142
Epoch 121/300
 - 11s - loss: 1.6905 - acc: 0.9303 - mDice: 0.5595 - val_loss: 3.3298 - val_acc: 0.9388 - val_mDice: 0.4429

Epoch 00121: val_mDice did not improve from 0.45142
Epoch 122/300
 - 11s - loss: 1.6857 - acc: 0.9305 - mDice: 0.5598 - val_loss: 3.3882 - val_acc: 0.9359 - val_mDice: 0.4301

Epoch 00122: val_mDice did not improve from 0.45142
Epoch 123/300
 - 11s - loss: 1.6810 - acc: 0.9305 - mDice: 0.5618 - val_loss: 3.3147 - val_acc: 0.9362 - val_mDice: 0.4422

Epoch 00123: val_mDice did not improve from 0.45142
Epoch 124/300
 - 11s - loss: 1.6780 - acc: 0.9307 - mDice: 0.5622 - val_loss: 3.2899 - val_acc: 0.9384 - val_mDice: 0.4460

Epoch 00124: val_mDice did not improve from 0.45142
Epoch 125/300
 - 11s - loss: 1.6713 - acc: 0.9307 - mDice: 0.5640 - val_loss: 3.3069 - val_acc: 0.9388 - val_mDice: 0.4502

Epoch 00125: val_mDice did not improve from 0.45142
Epoch 126/300
 - 11s - loss: 1.6690 - acc: 0.9309 - mDice: 0.5645 - val_loss: 3.3068 - val_acc: 0.9387 - val_mDice: 0.4399

Epoch 00126: val_mDice did not improve from 0.45142
Epoch 127/300
 - 11s - loss: 1.6648 - acc: 0.9309 - mDice: 0.5649 - val_loss: 3.2570 - val_acc: 0.9362 - val_mDice: 0.4447

Epoch 00127: val_mDice did not improve from 0.45142
Epoch 128/300
 - 11s - loss: 1.6655 - acc: 0.9309 - mDice: 0.5661 - val_loss: 3.1111 - val_acc: 0.9369 - val_mDice: 0.4479

Epoch 00128: val_mDice did not improve from 0.45142
Epoch 129/300
 - 11s - loss: 1.6591 - acc: 0.9312 - mDice: 0.5670 - val_loss: 3.2607 - val_acc: 0.9384 - val_mDice: 0.4425

Epoch 00129: val_mDice did not improve from 0.45142
Epoch 130/300
 - 11s - loss: 1.6535 - acc: 0.9313 - mDice: 0.5685 - val_loss: 3.4921 - val_acc: 0.9374 - val_mDice: 0.4479

Epoch 00130: val_mDice did not improve from 0.45142
Epoch 131/300
 - 11s - loss: 1.6504 - acc: 0.9314 - mDice: 0.5686 - val_loss: 3.5092 - val_acc: 0.9388 - val_mDice: 0.4418

Epoch 00131: val_mDice did not improve from 0.45142
Epoch 132/300
 - 11s - loss: 1.6644 - acc: 0.9312 - mDice: 0.5658 - val_loss: 3.3863 - val_acc: 0.9413 - val_mDice: 0.4403

Epoch 00132: val_mDice did not improve from 0.45142
Epoch 133/300
 - 11s - loss: 1.6559 - acc: 0.9314 - mDice: 0.5681 - val_loss: 3.2413 - val_acc: 0.9391 - val_mDice: 0.4424

Epoch 00133: val_mDice did not improve from 0.45142
Epoch 134/300
 - 11s - loss: 1.6451 - acc: 0.9317 - mDice: 0.5703 - val_loss: 3.5147 - val_acc: 0.9396 - val_mDice: 0.4480

Epoch 00134: val_mDice did not improve from 0.45142
Epoch 135/300
 - 11s - loss: 1.6419 - acc: 0.9318 - mDice: 0.5712 - val_loss: 3.5334 - val_acc: 0.9358 - val_mDice: 0.4382

Epoch 00135: val_mDice did not improve from 0.45142
Epoch 136/300
 - 11s - loss: 1.6383 - acc: 0.9317 - mDice: 0.5717 - val_loss: 3.3681 - val_acc: 0.9408 - val_mDice: 0.4443

Epoch 00136: val_mDice did not improve from 0.45142
Epoch 137/300
 - 11s - loss: 1.6344 - acc: 0.9319 - mDice: 0.5728 - val_loss: 3.4593 - val_acc: 0.9414 - val_mDice: 0.4539

Epoch 00137: val_mDice improved from 0.45142 to 0.45392, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 11s - loss: 1.6372 - acc: 0.9318 - mDice: 0.5722 - val_loss: 3.5437 - val_acc: 0.9381 - val_mDice: 0.4466

Epoch 00138: val_mDice did not improve from 0.45392
Epoch 139/300
 - 11s - loss: 1.6378 - acc: 0.9321 - mDice: 0.5725 - val_loss: 3.2437 - val_acc: 0.9415 - val_mDice: 0.4540

Epoch 00139: val_mDice improved from 0.45392 to 0.45404, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 140/300
 - 11s - loss: 1.6266 - acc: 0.9322 - mDice: 0.5752 - val_loss: 3.2536 - val_acc: 0.9394 - val_mDice: 0.4444

Epoch 00140: val_mDice did not improve from 0.45404
Epoch 141/300
 - 11s - loss: 1.6263 - acc: 0.9322 - mDice: 0.5747 - val_loss: 3.3099 - val_acc: 0.9388 - val_mDice: 0.4422

Epoch 00141: val_mDice did not improve from 0.45404
Epoch 142/300
 - 11s - loss: 1.6224 - acc: 0.9323 - mDice: 0.5761 - val_loss: 3.3001 - val_acc: 0.9386 - val_mDice: 0.4386

Epoch 00142: val_mDice did not improve from 0.45404
Epoch 143/300
 - 11s - loss: 1.6215 - acc: 0.9322 - mDice: 0.5764 - val_loss: 3.2315 - val_acc: 0.9370 - val_mDice: 0.4433

Epoch 00143: val_mDice did not improve from 0.45404
Epoch 144/300
 - 11s - loss: 1.6215 - acc: 0.9322 - mDice: 0.5765 - val_loss: 3.5611 - val_acc: 0.9396 - val_mDice: 0.4420

Epoch 00144: val_mDice did not improve from 0.45404
Epoch 145/300
 - 11s - loss: 1.6124 - acc: 0.9326 - mDice: 0.5780 - val_loss: 3.4147 - val_acc: 0.9398 - val_mDice: 0.4538

Epoch 00145: val_mDice did not improve from 0.45404
Epoch 146/300
 - 11s - loss: 1.6147 - acc: 0.9324 - mDice: 0.5778 - val_loss: 3.4025 - val_acc: 0.9409 - val_mDice: 0.4552

Epoch 00146: val_mDice improved from 0.45404 to 0.45521, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 147/300
 - 11s - loss: 1.6151 - acc: 0.9325 - mDice: 0.5777 - val_loss: 3.3949 - val_acc: 0.9360 - val_mDice: 0.4509

Epoch 00147: val_mDice did not improve from 0.45521
Epoch 148/300
 - 11s - loss: 1.6110 - acc: 0.9325 - mDice: 0.5783 - val_loss: 3.3407 - val_acc: 0.9390 - val_mDice: 0.4388

Epoch 00148: val_mDice did not improve from 0.45521
Epoch 149/300
 - 11s - loss: 1.6073 - acc: 0.9324 - mDice: 0.5792 - val_loss: 3.2538 - val_acc: 0.9384 - val_mDice: 0.4545

Epoch 00149: val_mDice did not improve from 0.45521
Epoch 150/300
 - 11s - loss: 1.6081 - acc: 0.9326 - mDice: 0.5797 - val_loss: 3.4668 - val_acc: 0.9354 - val_mDice: 0.4400

Epoch 00150: val_mDice did not improve from 0.45521
Epoch 151/300
 - 11s - loss: 1.6037 - acc: 0.9327 - mDice: 0.5801 - val_loss: 3.5142 - val_acc: 0.9369 - val_mDice: 0.4418

Epoch 00151: val_mDice did not improve from 0.45521
Epoch 152/300
 - 11s - loss: 1.6046 - acc: 0.9327 - mDice: 0.5802 - val_loss: 3.2475 - val_acc: 0.9412 - val_mDice: 0.4507

Epoch 00152: val_mDice did not improve from 0.45521
Epoch 153/300
 - 11s - loss: 1.6076 - acc: 0.9327 - mDice: 0.5795 - val_loss: 3.3643 - val_acc: 0.9407 - val_mDice: 0.4530

Epoch 00153: val_mDice did not improve from 0.45521
Epoch 154/300
 - 11s - loss: 1.5986 - acc: 0.9328 - mDice: 0.5814 - val_loss: 3.3156 - val_acc: 0.9389 - val_mDice: 0.4569

Epoch 00154: val_mDice improved from 0.45521 to 0.45690, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 155/300
 - 11s - loss: 1.5977 - acc: 0.9329 - mDice: 0.5825 - val_loss: 3.4791 - val_acc: 0.9369 - val_mDice: 0.4318

Epoch 00155: val_mDice did not improve from 0.45690
Epoch 156/300
 - 11s - loss: 1.5977 - acc: 0.9327 - mDice: 0.5817 - val_loss: 3.3031 - val_acc: 0.9374 - val_mDice: 0.4620

Epoch 00156: val_mDice improved from 0.45690 to 0.46201, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 157/300
 - 11s - loss: 1.5933 - acc: 0.9331 - mDice: 0.5837 - val_loss: 3.3151 - val_acc: 0.9389 - val_mDice: 0.4622

Epoch 00157: val_mDice improved from 0.46201 to 0.46215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 158/300
 - 11s - loss: 1.5923 - acc: 0.9331 - mDice: 0.5837 - val_loss: 3.5187 - val_acc: 0.9381 - val_mDice: 0.4510

Epoch 00158: val_mDice did not improve from 0.46215
Epoch 159/300
 - 11s - loss: 1.5853 - acc: 0.9333 - mDice: 0.5852 - val_loss: 3.3067 - val_acc: 0.9383 - val_mDice: 0.4513

Epoch 00159: val_mDice did not improve from 0.46215
Epoch 160/300
 - 11s - loss: 1.5869 - acc: 0.9332 - mDice: 0.5848 - val_loss: 3.3018 - val_acc: 0.9401 - val_mDice: 0.4667

Epoch 00160: val_mDice improved from 0.46215 to 0.46672, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 161/300
 - 11s - loss: 1.5888 - acc: 0.9331 - mDice: 0.5847 - val_loss: 3.3972 - val_acc: 0.9340 - val_mDice: 0.4435

Epoch 00161: val_mDice did not improve from 0.46672
Epoch 162/300
 - 11s - loss: 1.5838 - acc: 0.9332 - mDice: 0.5853 - val_loss: 3.4134 - val_acc: 0.9355 - val_mDice: 0.4406

Epoch 00162: val_mDice did not improve from 0.46672
Epoch 163/300
 - 11s - loss: 1.5795 - acc: 0.9334 - mDice: 0.5869 - val_loss: 3.3360 - val_acc: 0.9344 - val_mDice: 0.4392

Epoch 00163: val_mDice did not improve from 0.46672
Epoch 164/300
 - 11s - loss: 1.5760 - acc: 0.9335 - mDice: 0.5875 - val_loss: 3.3396 - val_acc: 0.9346 - val_mDice: 0.4581

Epoch 00164: val_mDice did not improve from 0.46672
Epoch 165/300
 - 11s - loss: 1.5829 - acc: 0.9334 - mDice: 0.5864 - val_loss: 3.3423 - val_acc: 0.9399 - val_mDice: 0.4445

Epoch 00165: val_mDice did not improve from 0.46672
Epoch 166/300
 - 11s - loss: 1.5844 - acc: 0.9332 - mDice: 0.5862 - val_loss: 3.5485 - val_acc: 0.9342 - val_mDice: 0.4350

Epoch 00166: val_mDice did not improve from 0.46672
Epoch 167/300
 - 11s - loss: 1.5749 - acc: 0.9335 - mDice: 0.5879 - val_loss: 3.2642 - val_acc: 0.9395 - val_mDice: 0.4632

Epoch 00167: val_mDice did not improve from 0.46672
Epoch 168/300
 - 11s - loss: 1.5701 - acc: 0.9336 - mDice: 0.5883 - val_loss: 3.2929 - val_acc: 0.9400 - val_mDice: 0.4547

Epoch 00168: val_mDice did not improve from 0.46672
Epoch 169/300
 - 11s - loss: 1.5664 - acc: 0.9339 - mDice: 0.5908 - val_loss: 3.4232 - val_acc: 0.9379 - val_mDice: 0.4462

Epoch 00169: val_mDice did not improve from 0.46672
Epoch 170/300
 - 11s - loss: 1.5746 - acc: 0.9337 - mDice: 0.5884 - val_loss: 3.4122 - val_acc: 0.9379 - val_mDice: 0.4428

Epoch 00170: val_mDice did not improve from 0.46672
Epoch 171/300
 - 11s - loss: 1.5665 - acc: 0.9337 - mDice: 0.5895 - val_loss: 3.6232 - val_acc: 0.9377 - val_mDice: 0.4449

Epoch 00171: val_mDice did not improve from 0.46672
Epoch 172/300
 - 11s - loss: 1.5671 - acc: 0.9338 - mDice: 0.5905 - val_loss: 3.6058 - val_acc: 0.9372 - val_mDice: 0.4383

Epoch 00172: val_mDice did not improve from 0.46672
Epoch 173/300
 - 11s - loss: 1.5691 - acc: 0.9339 - mDice: 0.5893 - val_loss: 3.4999 - val_acc: 0.9386 - val_mDice: 0.4532

Epoch 00173: val_mDice did not improve from 0.46672
Epoch 174/300
 - 11s - loss: 1.5654 - acc: 0.9338 - mDice: 0.5902 - val_loss: 3.4799 - val_acc: 0.9388 - val_mDice: 0.4541

Epoch 00174: val_mDice did not improve from 0.46672
Epoch 175/300
 - 11s - loss: 1.5525 - acc: 0.9344 - mDice: 0.5939 - val_loss: 3.2302 - val_acc: 0.9396 - val_mDice: 0.4613

Epoch 00175: val_mDice did not improve from 0.46672
Epoch 176/300
 - 11s - loss: 1.5611 - acc: 0.9339 - mDice: 0.5913 - val_loss: 3.6868 - val_acc: 0.9381 - val_mDice: 0.4377

Epoch 00176: val_mDice did not improve from 0.46672
Epoch 177/300
 - 11s - loss: 1.5570 - acc: 0.9341 - mDice: 0.5927 - val_loss: 3.4671 - val_acc: 0.9368 - val_mDice: 0.4525

Epoch 00177: val_mDice did not improve from 0.46672
Epoch 178/300
 - 11s - loss: 1.5464 - acc: 0.9343 - mDice: 0.5948 - val_loss: 3.7532 - val_acc: 0.9372 - val_mDice: 0.4397

Epoch 00178: val_mDice did not improve from 0.46672
Epoch 179/300
 - 11s - loss: 1.5507 - acc: 0.9343 - mDice: 0.5947 - val_loss: 3.4441 - val_acc: 0.9365 - val_mDice: 0.4424

Epoch 00179: val_mDice did not improve from 0.46672
Epoch 180/300
 - 11s - loss: 1.5542 - acc: 0.9343 - mDice: 0.5945 - val_loss: 3.4511 - val_acc: 0.9405 - val_mDice: 0.4598

Epoch 00180: val_mDice did not improve from 0.46672
Epoch 181/300
 - 11s - loss: 1.5545 - acc: 0.9342 - mDice: 0.5934 - val_loss: 3.3854 - val_acc: 0.9373 - val_mDice: 0.4508

Epoch 00181: val_mDice did not improve from 0.46672
Epoch 182/300
 - 11s - loss: 1.5488 - acc: 0.9345 - mDice: 0.5949 - val_loss: 3.4691 - val_acc: 0.9401 - val_mDice: 0.4572

Epoch 00182: val_mDice did not improve from 0.46672
Epoch 183/300
 - 11s - loss: 1.5446 - acc: 0.9345 - mDice: 0.5957 - val_loss: 3.5372 - val_acc: 0.9409 - val_mDice: 0.4569

Epoch 00183: val_mDice did not improve from 0.46672
Epoch 184/300
 - 11s - loss: 1.5463 - acc: 0.9346 - mDice: 0.5956 - val_loss: 3.3922 - val_acc: 0.9346 - val_mDice: 0.4460

Epoch 00184: val_mDice did not improve from 0.46672
Epoch 185/300
 - 11s - loss: 1.5389 - acc: 0.9347 - mDice: 0.5968 - val_loss: 3.3913 - val_acc: 0.9370 - val_mDice: 0.4440

Epoch 00185: val_mDice did not improve from 0.46672
Epoch 186/300
 - 11s - loss: 1.5376 - acc: 0.9348 - mDice: 0.5972 - val_loss: 3.4519 - val_acc: 0.9371 - val_mDice: 0.4544

Epoch 00186: val_mDice did not improve from 0.46672
Epoch 187/300
 - 11s - loss: 1.5397 - acc: 0.9347 - mDice: 0.5970 - val_loss: 3.3341 - val_acc: 0.9355 - val_mDice: 0.4454

Epoch 00187: val_mDice did not improve from 0.46672
Epoch 188/300
 - 11s - loss: 1.5419 - acc: 0.9346 - mDice: 0.5965 - val_loss: 3.8062 - val_acc: 0.9367 - val_mDice: 0.4406

Epoch 00188: val_mDice did not improve from 0.46672
Epoch 189/300
 - 11s - loss: 1.5421 - acc: 0.9346 - mDice: 0.5971 - val_loss: 3.6404 - val_acc: 0.9395 - val_mDice: 0.4511

Epoch 00189: val_mDice did not improve from 0.46672
Epoch 190/300
 - 11s - loss: 1.5422 - acc: 0.9346 - mDice: 0.5965 - val_loss: 3.2697 - val_acc: 0.9359 - val_mDice: 0.4538

Epoch 00190: val_mDice did not improve from 0.46672
Restoring model weights from the end of the best epoch
Epoch 00190: early stopping
{'val_loss': [93.51568428675334, 39.93195616631281, 18.04052762190501, 11.290100009668441, 8.797249463342485, 7.441070589990843, 6.67525402492001, 6.086773239785717, 5.829169305484919, 5.450409425156457, 6.209536672348068, 5.050783742574, 4.830384776528392, 4.657943311723924, 4.546159025813852, 4.482341604750781, 4.460042927148087, 4.019065956452063, 3.9696977174885215, 3.970711785324273, 3.8098470954490558, 3.682008539193443, 3.8922125752571795, 3.8820860129559325, 3.2808091834719693, 3.2638416514687596, 3.1443546421471096, 3.1425387324499234, 3.0881840273560512, 2.935980963432008, 3.0308417435735464, 2.9225658285653306, 3.0142408337532762, 2.835828483282101, 2.979209181231757, 2.9358447438017246, 3.5020119594631804, 2.820126911758312, 2.933881734054358, 3.1232223837592064, 3.1135629168711603, 3.0527763242966364, 2.7590355579297814, 3.144944088178731, 2.9288018940105323, 3.0383980591958832, 3.013248375106958, 2.9778820341231214, 2.93933099424023, 2.886607386570956, 2.933940095871332, 2.9651751698748696, 3.2612582282163203, 2.866938874591142, 3.113129052355708, 3.065241555128956, 2.9332883782418713, 2.9293164033442736, 3.165957616486897, 3.0043919214845767, 3.6488894388035296, 2.934148220367552, 3.1361462549262105, 3.0050355874534165, 2.977669642839049, 3.334973391855047, 3.2639119945510866, 3.016176900737697, 2.9930077790770504, 3.1600150394563875, 3.1123191912303723, 3.1459006808387735, 3.105062159820504, 3.6357492554844137, 3.080710548109242, 3.2283967808997702, 3.284773776440748, 3.2107365335825655, 3.4493164436536885, 3.5627150066596056, 3.1224197949770662, 3.089068541170231, 3.3730755942607566, 3.2080323654670444, 3.239532543035845, 3.349279240705073, 3.2493652844402408, 3.325627067215031, 3.2416416163600625, 3.067917121485585, 3.2291357326986536, 3.4315379468635436, 3.2054506526550366, 3.3342118162573096, 3.1570902446817075, 3.3976361324478472, 3.5313143121034263, 3.1398782626991824, 3.518082380937856, 3.4332425699879727, 3.391174066989195, 3.376190260895306, 3.2798054532724477, 3.2638150191023234, 3.070736507752112, 3.459399888868488, 3.2336499093632614, 3.1411972543047297, 3.1267238058415905, 3.3346673193875525, 3.3767111880616065, 3.3769345081278255, 3.1734337346805703, 3.434515820660939, 3.449750271448422, 3.2720112574198064, 3.265574167748647, 3.3780637482136844, 3.695603210589893, 3.6670419921921122, 3.329762402611474, 3.3881904795499787, 3.314715069452567, 3.2898933032882356, 3.306928322362226, 3.3068174750854573, 3.25703120739421, 3.1111381027758833, 3.260652557148465, 3.4921395188818374, 3.5092064978865287, 3.3862980181306956, 3.241310178657018, 3.514689939229616, 3.533374337922959, 3.3680619471041218, 3.4593219689226578, 3.5437348864174316, 3.243691679312005, 3.2536382275145677, 3.3098808911495974, 3.300132893912849, 3.2315102726487175, 3.561114586135816, 3.4146709529062114, 3.4025275046005845, 3.3949323789039183, 3.340652963656577, 3.25377052840555, 3.4667550172390684, 3.514202418382324, 3.247530488962574, 3.364277495270861, 3.3156476019926013, 3.4790829158875916, 3.3031012186603177, 3.315109979117378, 3.5186790223455144, 3.3066520679830793, 3.3018380735690394, 3.397161397329044, 3.4134425985671224, 3.336026263024126, 3.339571273974365, 3.3422828075431643, 3.5484861562117223, 3.2641769066630375, 3.292864998758194, 3.4231959931729805, 3.41217777811523, 3.6232064524000243, 3.605788320036871, 3.4999279135039876, 3.479894172001098, 3.2302192083692978, 3.6867520511815592, 3.467148913691441, 3.753202861352336, 3.4441230344541727, 3.451057086299573, 3.385372945063171, 3.4691115827964887, 3.5372295740858783, 3.392155267830406, 3.3912662808295515, 3.451854579106328, 3.3341265041824606, 3.8061863387535726, 3.64044389654217, 3.26971481696126], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047756507283166, 0.9048466086387634, 0.9059157314754668, 0.9074061257498605, 0.909468860853286, 0.9098030924797058, 0.9107188440504528, 0.9135118949980963, 0.9148282947994414, 0.916504130476997, 0.9192055861155192, 0.9194459943544298, 0.9203960526557196, 0.9208287398020426, 0.9217422235579718, 0.9242582604998634, 0.9228754838307699, 0.9277724538530622, 0.9261904727844965, 0.9291117446763175, 0.9294574175562177, 0.9317284708931333, 0.929727588381086, 0.9317536779812404, 0.9296268536930993, 0.9331959713073004, 0.9334203317051842, 0.9330448990776425, 0.9341300584021068, 0.9324130302383786, 0.9303273615382966, 0.9361195138522557, 0.9360485360735938, 0.934583340372358, 0.9366506366502672, 0.9316414679799762, 0.9358219050225758, 0.9313140937260219, 0.9372298405283973, 0.9381502015250069, 0.9360714015506563, 0.9368681538672674, 0.9362339632851737, 0.9354830781618754, 0.9386996456554958, 0.935235823903765, 0.9368246055784679, 0.9350824356079102, 0.9370489716529846, 0.9335004431860787, 0.9363919241087777, 0.9394940251395816, 0.9349748066493443, 0.9358355942226592, 0.9363415950820559, 0.9406662015687852, 0.9386103720892043, 0.9401671120098659, 0.9380128326870146, 0.9387934775579543, 0.9336652755737305, 0.938782030627841, 0.9393978686559767, 0.933511924175989, 0.9338644459134057, 0.9400320592380705, 0.9335989015442985, 0.9400343384061541, 0.9409043107713971, 0.93838141645704, 0.9338621724219549, 0.9399015789940244, 0.940041184425354, 0.9404922184490022, 0.9365567962328593, 0.9413484590394157, 0.9387156537600926, 0.9414010927790687, 0.9380815199443272, 0.9387614329655966, 0.9380288407916114, 0.9315544764200846, 0.9375893104644049, 0.939860360963004, 0.9414331657545907, 0.9415292910167149, 0.9399656596637908, 0.9324015435718355, 0.9410164895511809, 0.9410096179871332, 0.9390224615732828, 0.9403685728708903, 0.9412111015546889, 0.9359958540825617, 0.9383608329863775, 0.9358424856549218, 0.9351625669570196, 0.9381089976855687, 0.9395649972416106, 0.9394780312265668, 0.9379372767039708, 0.9392193044934954, 0.9395695953142076, 0.9395215454555693, 0.939775634379614, 0.9406547773452032, 0.9388209723290943, 0.9358516619319007, 0.9361882124628339, 0.9383584942136493, 0.9387889163834708, 0.9386698773929051, 0.9362385670344034, 0.9368635614713033, 0.9384317710286095, 0.9374381928216844, 0.9388438576743716, 0.9413461827096485, 0.939125466914404, 0.939578754561288, 0.9358333150545756, 0.9407921092850822, 0.9413759140741258, 0.9380952403658912, 0.9415247042973837, 0.9393566648165385, 0.9388301542827061, 0.9386446731431144, 0.9369757118679228, 0.939565034139724, 0.9398260230109805, 0.9409272046316237, 0.9360164943195525, 0.939040774390811, 0.9383836899484906, 0.9354098155384972, 0.9368841761634463, 0.9411835925919669, 0.9407051489466712, 0.9388805145309085, 0.9369070529937744, 0.9373947012992132, 0.938859882808867, 0.9380517289752052, 0.938344765277136, 0.9401419503348214, 0.9340132588431949, 0.9354578881036668, 0.934356689453125, 0.934615379288083, 0.939926734992436, 0.934200990767706, 0.9394711539858863, 0.940011427516029, 0.9379441142082214, 0.9378663074402582, 0.9376511148044041, 0.9372115560940334, 0.938589748882112, 0.9388278268632435, 0.9396153716813951, 0.9380654522350856, 0.9368475278218588, 0.9372344556308928, 0.9365246977124896, 0.9404899477958679, 0.9373260197185335, 0.9401053161848159, 0.9409249084336417, 0.9345993683451698, 0.9370375502677191, 0.9371062318483988, 0.935453284354437, 0.9367147570564633, 0.9394894753183637, 0.9359111984570821], 'val_mDice': [0.016560683301317373, 0.018936002072656437, 0.017263642767266857, 0.017411852421771203, 0.02048473015782379, 0.02212675822721351, 0.025259982461908033, 0.029466981112602212, 0.027163646316954067, 0.03961393835821322, 0.046428534911856764, 0.051926632766567525, 0.06361017828541142, 0.06839452586358502, 0.07584130276171934, 0.0839892194295923, 0.09530983270988577, 0.11469252700252193, 0.12491667337183442, 0.13819409924603643, 0.15324712810771807, 0.16990215473231815, 0.1799866742942305, 0.18907671927341393, 0.22045277360649335, 0.2290549281807173, 0.24616005236194247, 0.24904274612310387, 0.27141842370231944, 0.29260274093775523, 0.292853617153707, 0.3200497197075969, 0.31190023775256814, 0.33252105498242945, 0.33135972828382537, 0.3428767619743234, 0.32755120657384396, 0.36415293777272817, 0.3577109222256002, 0.35463661789184525, 0.3616055937572604, 0.3698485738464764, 0.3856829284202485, 0.36145804290260586, 0.3792194699247678, 0.3909405970147678, 0.39459470836889177, 0.3956551686638877, 0.40337200675691876, 0.38805864715860006, 0.39557242677325294, 0.39870635065294446, 0.39495403692126274, 0.41341906785964966, 0.4013678137035597, 0.4065080506815797, 0.4095284142309711, 0.409602423508962, 0.40478242685397464, 0.4089139426747958, 0.39443268059265046, 0.4174439509709676, 0.4113472281467347, 0.41384882550863994, 0.4234620187254179, 0.4171820680300395, 0.40795628336213885, 0.4271265250586328, 0.41472420983371283, 0.4225846901535988, 0.42295155958050773, 0.4227486712237199, 0.4222710615112668, 0.41157480755022596, 0.4156660918323767, 0.41881846654273214, 0.42542878077143714, 0.3986528597417332, 0.41308886149809476, 0.4183773588211763, 0.4328755775377864, 0.43747254815839587, 0.4328765484193961, 0.4291765721780913, 0.4236678074867952, 0.42865055568871047, 0.42769473755643483, 0.43625910402763457, 0.419574787574155, 0.4380006426501842, 0.4336027559779939, 0.432131471910647, 0.43131019458884284, 0.4249636254140309, 0.43857780276309877, 0.40967682961906704, 0.4249840220879941, 0.4394038970626536, 0.4329943816576685, 0.42837865703872274, 0.43147028237581253, 0.43053700863605454, 0.4407401255198887, 0.4470583682968503, 0.44351739223514286, 0.44095109757922946, 0.4514242559671402, 0.4352387984593709, 0.4471991296325411, 0.43139190244532766, 0.4431736980165754, 0.43652779626704397, 0.44268847771343733, 0.43721718713641167, 0.44216141175656093, 0.44087066643294837, 0.4364292405190922, 0.43253525843222934, 0.43351612115899724, 0.4378087815074694, 0.44292627345947994, 0.43013354142506915, 0.44224113190457937, 0.4460394401990232, 0.4501756207928771, 0.4398781100199336, 0.4446773064278421, 0.44792516103812624, 0.4424555931417715, 0.4479171637268293, 0.4418088708605085, 0.44030722195193883, 0.4424295459120047, 0.4479662078831877, 0.43818679274547667, 0.444278311871347, 0.45391768625094775, 0.44658223787943524, 0.4540379712624209, 0.4443565358718236, 0.44218296131917406, 0.4386296917994817, 0.4432943466873396, 0.4420358215769132, 0.4537745024121943, 0.4552110971084663, 0.4509219988470986, 0.4388078541627952, 0.4544767112958999, 0.4399747291491145, 0.44182109158663524, 0.4507274865394547, 0.45303348700205487, 0.4568984877495539, 0.4318448957942781, 0.46200633315103395, 0.4621520854887508, 0.4509680349202383, 0.4512686768458003, 0.46672305358307703, 0.4434846696399507, 0.4406163185125306, 0.43922151908988044, 0.4580605858493419, 0.4444854639115788, 0.4349966531708127, 0.46316862957818167, 0.4546921599124159, 0.4461933620983646, 0.44277527768697056, 0.44489824807360057, 0.4382971855146544, 0.453193051474435, 0.4541072831267402, 0.4612719455645198, 0.43769538438036326, 0.45250015812260763, 0.43966125164713177, 0.4424402073380493, 0.45979120174334165, 0.4508321036895116, 0.4572334246976035, 0.45692472780744237, 0.44597955438352765, 0.44404117532429244, 0.4544455696429525, 0.44536400036442847, 0.4405653281580834, 0.4510581078273909, 0.45383680221580325], 'loss': [191.67314103381906, 85.5583777191415, 40.45677899201987, 23.476743753277685, 16.396485526138402, 12.920975841399398, 10.947817512668118, 9.660609772882411, 8.717110976562441, 7.959695061944182, 7.371281401187624, 6.900916052296692, 6.525477520873006, 6.207751926622893, 5.906150530592104, 5.635829348497039, 5.394918730706178, 5.174720355856274, 4.962078158557127, 4.765173778934747, 4.558521542173143, 4.361729052201479, 4.185557485660975, 4.013990505136505, 3.8685880651450097, 3.7236175282005632, 3.586319321585869, 3.473055048762936, 3.3715839892516644, 3.2601788362510775, 3.163508636771725, 3.094000363731421, 3.0250033899748145, 2.964669313508007, 2.89937093833841, 2.8369422456720375, 2.779471939368953, 2.748502564609499, 2.7028984038476334, 2.661538873675795, 2.6285423176445346, 2.5827236948662327, 2.5586333474098018, 2.5110931976787936, 2.494660558695046, 2.45495367992446, 2.434724955124871, 2.404227791987151, 2.374148615504681, 2.3418189496324775, 2.3292695603344007, 2.3026580258051923, 2.2760286835638666, 2.266307425999977, 2.2281726241134736, 2.2170207223290994, 2.1988052624324994, 2.1700014850871283, 2.1674537418755992, 2.1382694514656837, 2.122528219972246, 2.1118840829057914, 2.0977525075828267, 2.080110486518539, 2.0619848163366457, 2.0541679380899316, 2.035035452701141, 2.023924467052043, 2.016098663299925, 2.0013484413370546, 1.9897411087700598, 1.9768606284599164, 1.9740334759563845, 1.95799075562025, 1.9487207513198426, 1.940225065317553, 1.9353368908586313, 1.9229095317689078, 1.9083787457249357, 1.908404728408859, 1.890289190212196, 1.896011768197287, 1.8838335712773735, 1.8744482507084246, 1.8723235626268506, 1.859251400788717, 1.8494867807825521, 1.8422227629995631, 1.8432713434924641, 1.8402781257607332, 1.828439620847943, 1.813544713487877, 1.8076231518484895, 1.8101336102920482, 1.7975911460953318, 1.7930964259069873, 1.7949044320632526, 1.7870390193642094, 1.779767385286611, 1.7688128711723605, 1.7696147971652436, 1.7699098567730984, 1.762429503087297, 1.7624264058290888, 1.7547118965775517, 1.741642349989446, 1.7513288829927387, 1.7337247557968263, 1.7378451853375778, 1.7288705556406918, 1.724728544415227, 1.7197664623571218, 1.7141131814422004, 1.7184792720492414, 1.7132158336415098, 1.7058792310296995, 1.6974504516835798, 1.6976108237316734, 1.6953380624292351, 1.69237273284551, 1.6904693772297776, 1.6857424632234796, 1.6809807374386836, 1.6779914631052055, 1.6712521261301991, 1.6689558334380004, 1.6647963898706373, 1.665506081956187, 1.659065587938418, 1.6534719400744184, 1.6503764552281315, 1.6644232436276272, 1.6559207802083604, 1.6451348802913286, 1.6418613554862784, 1.6383327817052735, 1.634367296087597, 1.637197935261753, 1.637777849385475, 1.6265681444943607, 1.6263463543815972, 1.6224267594681463, 1.6215089464408252, 1.6215455852677327, 1.6124101352756002, 1.614717376181586, 1.6151075070594256, 1.6110334240312358, 1.6072918121953081, 1.6081221242021775, 1.6036875139019129, 1.604613067765031, 1.6075590794684456, 1.59856231189174, 1.5976680363215117, 1.5976793056097889, 1.59332445698803, 1.5923185174074939, 1.585264917120484, 1.586865426971431, 1.5888256178394962, 1.5838371397604574, 1.5794896485834469, 1.575951911338106, 1.5829463108716177, 1.5844408087540114, 1.5749345214145822, 1.5700876679704532, 1.5663681597706898, 1.574552001212175, 1.5665288335536518, 1.5670998705497523, 1.5691327000252655, 1.5653530133351992, 1.5525465616031673, 1.5611128699970265, 1.55698571994352, 1.5464382967738395, 1.550720321412764, 1.554220012520374, 1.554519023811608, 1.548805546342281, 1.5446078614138767, 1.5463416467679127, 1.5389066888161287, 1.5376172188230623, 1.539692146092774, 1.54190570620505, 1.5421268870105673, 1.5421565515539517], 'acc': [0.47269403120904663, 0.8055310221060361, 0.8526695613941981, 0.8663130652152988, 0.8688176597775764, 0.8691227900768167, 0.8692391802401862, 0.8692396447588213, 0.8692396419779607, 0.8692396228796534, 0.8692394779071841, 0.8692389712734491, 0.8692372123905981, 0.8692032444194399, 0.8691720954595514, 0.8693701707147002, 0.8701546068294489, 0.8716157575046164, 0.8732937429253251, 0.874652052479533, 0.8763134514111095, 0.8776743999905546, 0.8794086736499264, 0.8810285005193468, 0.8830705198773718, 0.8852667915515411, 0.8873900505465443, 0.889384774153279, 0.8914159737456987, 0.8938235466319838, 0.8957966035653889, 0.8974046613730193, 0.8986838621764094, 0.8998690773945727, 0.9009900864135422, 0.9020061259534471, 0.9030098801567441, 0.9039468837078537, 0.9051218298167789, 0.9060057677789669, 0.9066705665125055, 0.9077422428572364, 0.9083156288417061, 0.9092667405375953, 0.9098588495969726, 0.9105622319442495, 0.9109295522268048, 0.9115219157119465, 0.912128626403952, 0.9129721966924202, 0.913118227428615, 0.9138196329172347, 0.9145776568085144, 0.9150359701485171, 0.9158394777846056, 0.9162370836272827, 0.9170054388869031, 0.9181744355054082, 0.9183596934866809, 0.9189907800192912, 0.9195046165625163, 0.9196366899639098, 0.9200731363741447, 0.920529572426388, 0.9209706026946554, 0.9213078907626843, 0.9215830109793717, 0.9219134210942682, 0.9222097882696814, 0.9224414800503636, 0.9227353928749654, 0.9233122569461629, 0.923123448047193, 0.9235098702796788, 0.9239028372561424, 0.9240748650631924, 0.9243929436470195, 0.9243603650030199, 0.9249537487767302, 0.9251135878993156, 0.92533819049637, 0.9252662883343576, 0.9257179785951475, 0.9260109183713378, 0.9261688327591658, 0.9265831035641041, 0.9268075868714308, 0.9269368858472539, 0.9268098848275603, 0.9265422256139516, 0.927172267772523, 0.9275530723448041, 0.9275652832186978, 0.9277086492239315, 0.927890106952326, 0.9277787879985584, 0.9276963916499651, 0.9279469240601177, 0.9282580067800239, 0.9284130495500537, 0.9283025865038167, 0.9283453405236656, 0.9282971674453231, 0.9285290684455481, 0.9287756422661347, 0.9291434471998368, 0.9287408352564681, 0.9292866287374855, 0.9291973928621975, 0.9294082542501803, 0.9294518905382798, 0.9295709228294995, 0.9296900498078742, 0.9296983470341271, 0.9297774544060058, 0.9297149592864392, 0.9301764025540166, 0.9300885113052764, 0.9301574972752824, 0.9301881035996329, 0.9302653361543516, 0.9304690825955266, 0.9305190675654392, 0.9306811063717573, 0.9306657461502291, 0.9308604343401252, 0.9309457295635474, 0.9309461272036338, 0.931204515280741, 0.9313346214530692, 0.9313839079328463, 0.9312158935741581, 0.9314359767441118, 0.9316794936059871, 0.9318054055969954, 0.93171711879956, 0.9318992498754329, 0.931828250206112, 0.9321342833083053, 0.9322459253015513, 0.9321752031092609, 0.9322798000449318, 0.9322243060752194, 0.9321851431921265, 0.932602491835213, 0.9324278454801722, 0.9324703919926612, 0.9325426888760056, 0.9324254834953015, 0.9326499493935215, 0.932654143620806, 0.9327360783580275, 0.9326747441443493, 0.9327723453195196, 0.9328556447123801, 0.9326803290087845, 0.9330997367297751, 0.9330720701887816, 0.9333197794048823, 0.9332177735303485, 0.9331199434053147, 0.9332030367272103, 0.9334202520254836, 0.9334741505166343, 0.9333858865866065, 0.9332313760755976, 0.9334955344844559, 0.9335531432986696, 0.9338930705872854, 0.9336829965735576, 0.9337232004063241, 0.9337728101554026, 0.9339266460549057, 0.933752233292157, 0.9343688141433218, 0.9339485906649123, 0.9341112350898141, 0.934292346348256, 0.9342830539416183, 0.9343415854155086, 0.9342018135462236, 0.9344811029279763, 0.9345071273477914, 0.9346348024848709, 0.9346519963507343, 0.9348020808118052, 0.9347354602395442, 0.9346261370818647, 0.9346022221286885, 0.9346132319810557], 'mDice': [0.019551976184400723, 0.022062394748608317, 0.02275327104606138, 0.022549844276018517, 0.023075381730916025, 0.024550100708395063, 0.026651728041477234, 0.029165622539974855, 0.03250941481848477, 0.03644235019399891, 0.041097195121291424, 0.045986400676217176, 0.05115466338661518, 0.05676487711598838, 0.06337279138082205, 0.07085727796558933, 0.07880322105000422, 0.08779275816932955, 0.09781813086607517, 0.10890157315222909, 0.12239600371275652, 0.13664353246033387, 0.15161276126220458, 0.1680275073096498, 0.18421421012300182, 0.2010365125644361, 0.2168965047555368, 0.23201585591632276, 0.24556666373034444, 0.26081988146499513, 0.2743040810137878, 0.2863017685592163, 0.2970721092335815, 0.3062080612687309, 0.3165998449726833, 0.32701874382008195, 0.3374563686553973, 0.3439800361401547, 0.3516920017373339, 0.35986208558909943, 0.3659018995342674, 0.373620606201886, 0.37882626513444734, 0.38667603980168824, 0.39042716685209244, 0.39788944033590934, 0.40121674711175204, 0.407360539331541, 0.41350852173372615, 0.419662902758304, 0.4223926395999459, 0.4278208992948692, 0.4327717912560234, 0.43474341337451455, 0.44231096792593383, 0.44522132633025185, 0.4490743681532766, 0.45468990325766856, 0.4557623974109905, 0.46162939229658717, 0.4642234657549412, 0.46727787287726624, 0.47032250600994635, 0.4737425948961268, 0.4771102947181696, 0.47977552619402364, 0.48331518349721986, 0.4855410942216358, 0.4876894031286653, 0.4907357539426437, 0.4933509835623261, 0.4964429706490198, 0.4970437260935342, 0.49978242404019185, 0.5024922886679851, 0.5046330759560692, 0.504983106990391, 0.5075969985622143, 0.5115178313838141, 0.5113842951118406, 0.5149632044120724, 0.5132131815094559, 0.5153803053976875, 0.518302131623047, 0.5187766999667013, 0.5214880058272211, 0.5232936612024963, 0.5245756739615842, 0.5254844645555709, 0.5258028486433667, 0.5284683067536157, 0.5316070837967194, 0.5322086727180209, 0.532955155830887, 0.5356563121706667, 0.5363561388888708, 0.5359714778259768, 0.5374583289736793, 0.5397260635373035, 0.5413653104089693, 0.541552658505768, 0.5410360918124103, 0.5431515091826559, 0.5434457028971624, 0.5448930559416646, 0.5480278962743703, 0.545882227047574, 0.54931797366715, 0.5487914843000335, 0.5501346525967592, 0.5514102288011368, 0.5527445934447154, 0.5541242144805102, 0.5528093759140712, 0.5542153681039305, 0.5558254808641385, 0.5580833469692948, 0.5577815203899268, 0.558691493941337, 0.5594613914065033, 0.5594708065387255, 0.5597970244947462, 0.561801316744104, 0.5622439959937722, 0.5639642149077828, 0.5645240229265939, 0.564942523064309, 0.5660658382096225, 0.5670118118518346, 0.5685197023819563, 0.568565804879744, 0.5658373467628497, 0.5681081083851879, 0.5702798643943223, 0.5711934385971685, 0.5717395073948786, 0.5728448071253651, 0.5721992835020407, 0.5724610504397128, 0.575179016070718, 0.5746727338962434, 0.5760723262686295, 0.5764429619345841, 0.576543005877477, 0.5780035623217998, 0.5777614221468077, 0.5777429671836355, 0.5782743051191367, 0.5791678053808277, 0.5797425535388459, 0.5801264065089243, 0.5802050032826179, 0.579539421748306, 0.5814216292063027, 0.5824541265228614, 0.5817489677802065, 0.5836711696445861, 0.5837324681207581, 0.585224825360997, 0.584827425026218, 0.5847174513654854, 0.585280530572696, 0.5868885404277174, 0.5874812259870801, 0.5863774775056819, 0.5861767591114607, 0.5879468468887075, 0.5883123500971796, 0.5908290311990042, 0.588447252656537, 0.5894814174092986, 0.5905155740607927, 0.5893382515915562, 0.5901977473746749, 0.5938714113634447, 0.5913179718232325, 0.5926770614203355, 0.5948305640097457, 0.5946620001897707, 0.5944696939678717, 0.5934280466086514, 0.5948913917620563, 0.5957488276858722, 0.5955669267779259, 0.5967823723986448, 0.5971517091383933, 0.5970085364489375, 0.5965011019524706, 0.5970991801406599, 0.596547053284054]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.57s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.29s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.07s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:13,  1.53s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:33,  1.60s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:28,  1.59s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<08:05,  1.73s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:47,  1.67s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:20,  1.79s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:37,  1.86s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:55,  1.93s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:40,  1.89s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<08:57,  1.96s/it]predicting train subjects:   4%|▍         | 11/285 [00:20<09:05,  1.99s/it]predicting train subjects:   4%|▍         | 12/285 [00:22<09:16,  2.04s/it]predicting train subjects:   5%|▍         | 13/285 [00:24<09:15,  2.04s/it]predicting train subjects:   5%|▍         | 14/285 [00:26<09:11,  2.03s/it]predicting train subjects:   5%|▌         | 15/285 [00:28<09:08,  2.03s/it]predicting train subjects:   6%|▌         | 16/285 [00:30<09:03,  2.02s/it]predicting train subjects:   6%|▌         | 17/285 [00:32<08:59,  2.01s/it]predicting train subjects:   6%|▋         | 18/285 [00:34<09:02,  2.03s/it]predicting train subjects:   7%|▋         | 19/285 [00:36<09:03,  2.04s/it]predicting train subjects:   7%|▋         | 20/285 [00:39<09:07,  2.06s/it]predicting train subjects:   7%|▋         | 21/285 [00:41<08:57,  2.03s/it]predicting train subjects:   8%|▊         | 22/285 [00:43<08:47,  2.01s/it]predicting train subjects:   8%|▊         | 23/285 [00:44<08:43,  2.00s/it]predicting train subjects:   8%|▊         | 24/285 [00:46<08:38,  1.99s/it]predicting train subjects:   9%|▉         | 25/285 [00:48<08:32,  1.97s/it]predicting train subjects:   9%|▉         | 26/285 [00:50<08:33,  1.98s/it]predicting train subjects:   9%|▉         | 27/285 [00:52<08:25,  1.96s/it]predicting train subjects:  10%|▉         | 28/285 [00:54<08:14,  1.92s/it]predicting train subjects:  10%|█         | 29/285 [00:56<08:12,  1.92s/it]predicting train subjects:  11%|█         | 30/285 [00:58<08:05,  1.91s/it]predicting train subjects:  11%|█         | 31/285 [01:00<08:01,  1.89s/it]predicting train subjects:  11%|█         | 32/285 [01:02<07:54,  1.88s/it]predicting train subjects:  12%|█▏        | 33/285 [01:03<07:49,  1.86s/it]predicting train subjects:  12%|█▏        | 34/285 [01:05<07:44,  1.85s/it]predicting train subjects:  12%|█▏        | 35/285 [01:07<07:43,  1.86s/it]predicting train subjects:  13%|█▎        | 36/285 [01:09<07:44,  1.87s/it]predicting train subjects:  13%|█▎        | 37/285 [01:11<07:51,  1.90s/it]predicting train subjects:  13%|█▎        | 38/285 [01:13<07:42,  1.87s/it]predicting train subjects:  14%|█▎        | 39/285 [01:15<07:40,  1.87s/it]predicting train subjects:  14%|█▍        | 40/285 [01:17<07:39,  1.87s/it]predicting train subjects:  14%|█▍        | 41/285 [01:18<07:32,  1.86s/it]predicting train subjects:  15%|█▍        | 42/285 [01:20<07:31,  1.86s/it]predicting train subjects:  15%|█▌        | 43/285 [01:22<07:22,  1.83s/it]predicting train subjects:  15%|█▌        | 44/285 [01:24<07:30,  1.87s/it]predicting train subjects:  16%|█▌        | 45/285 [01:26<07:25,  1.86s/it]predicting train subjects:  16%|█▌        | 46/285 [01:27<07:01,  1.76s/it]predicting train subjects:  16%|█▋        | 47/285 [01:29<06:46,  1.71s/it]predicting train subjects:  17%|█▋        | 48/285 [01:31<06:36,  1.67s/it]predicting train subjects:  17%|█▋        | 49/285 [01:32<06:26,  1.64s/it]predicting train subjects:  18%|█▊        | 50/285 [01:34<06:15,  1.60s/it]predicting train subjects:  18%|█▊        | 51/285 [01:35<06:13,  1.60s/it]predicting train subjects:  18%|█▊        | 52/285 [01:37<06:08,  1.58s/it]predicting train subjects:  19%|█▊        | 53/285 [01:38<06:04,  1.57s/it]predicting train subjects:  19%|█▉        | 54/285 [01:40<06:04,  1.58s/it]predicting train subjects:  19%|█▉        | 55/285 [01:42<06:12,  1.62s/it]predicting train subjects:  20%|█▉        | 56/285 [01:43<06:11,  1.62s/it]predicting train subjects:  20%|██        | 57/285 [01:45<06:04,  1.60s/it]predicting train subjects:  20%|██        | 58/285 [01:46<05:58,  1.58s/it]predicting train subjects:  21%|██        | 59/285 [01:48<05:54,  1.57s/it]predicting train subjects:  21%|██        | 60/285 [01:49<05:52,  1.57s/it]predicting train subjects:  21%|██▏       | 61/285 [01:51<05:56,  1.59s/it]predicting train subjects:  22%|██▏       | 62/285 [01:53<05:57,  1.60s/it]predicting train subjects:  22%|██▏       | 63/285 [01:54<06:03,  1.64s/it]predicting train subjects:  22%|██▏       | 64/285 [01:56<06:08,  1.67s/it]predicting train subjects:  23%|██▎       | 65/285 [01:58<06:30,  1.78s/it]predicting train subjects:  23%|██▎       | 66/285 [02:00<06:33,  1.80s/it]predicting train subjects:  24%|██▎       | 67/285 [02:02<06:23,  1.76s/it]predicting train subjects:  24%|██▍       | 68/285 [02:03<06:13,  1.72s/it]predicting train subjects:  24%|██▍       | 69/285 [02:05<06:15,  1.74s/it]predicting train subjects:  25%|██▍       | 70/285 [02:07<06:09,  1.72s/it]predicting train subjects:  25%|██▍       | 71/285 [02:08<06:07,  1.72s/it]predicting train subjects:  25%|██▌       | 72/285 [02:10<06:02,  1.70s/it]predicting train subjects:  26%|██▌       | 73/285 [02:12<05:56,  1.68s/it]predicting train subjects:  26%|██▌       | 74/285 [02:13<05:53,  1.67s/it]predicting train subjects:  26%|██▋       | 75/285 [02:15<05:50,  1.67s/it]predicting train subjects:  27%|██▋       | 76/285 [02:17<05:50,  1.68s/it]predicting train subjects:  27%|██▋       | 77/285 [02:18<05:44,  1.66s/it]predicting train subjects:  27%|██▋       | 78/285 [02:20<05:43,  1.66s/it]predicting train subjects:  28%|██▊       | 79/285 [02:22<05:40,  1.66s/it]predicting train subjects:  28%|██▊       | 80/285 [02:23<05:39,  1.65s/it]predicting train subjects:  28%|██▊       | 81/285 [02:25<05:38,  1.66s/it]predicting train subjects:  29%|██▉       | 82/285 [02:27<05:36,  1.66s/it]predicting train subjects:  29%|██▉       | 83/285 [02:28<05:36,  1.67s/it]predicting train subjects:  29%|██▉       | 84/285 [02:30<05:39,  1.69s/it]predicting train subjects:  30%|██▉       | 85/285 [02:32<05:51,  1.76s/it]predicting train subjects:  30%|███       | 86/285 [02:34<05:53,  1.78s/it]predicting train subjects:  31%|███       | 87/285 [02:36<06:00,  1.82s/it]predicting train subjects:  31%|███       | 88/285 [02:38<06:02,  1.84s/it]predicting train subjects:  31%|███       | 89/285 [02:39<05:58,  1.83s/it]predicting train subjects:  32%|███▏      | 90/285 [02:41<05:56,  1.83s/it]predicting train subjects:  32%|███▏      | 91/285 [02:43<05:51,  1.81s/it]predicting train subjects:  32%|███▏      | 92/285 [02:45<05:46,  1.80s/it]predicting train subjects:  33%|███▎      | 93/285 [02:47<05:48,  1.82s/it]predicting train subjects:  33%|███▎      | 94/285 [02:49<05:49,  1.83s/it]predicting train subjects:  33%|███▎      | 95/285 [02:50<05:48,  1.83s/it]predicting train subjects:  34%|███▎      | 96/285 [02:52<05:59,  1.90s/it]predicting train subjects:  34%|███▍      | 97/285 [02:54<05:52,  1.88s/it]predicting train subjects:  34%|███▍      | 98/285 [02:56<05:49,  1.87s/it]predicting train subjects:  35%|███▍      | 99/285 [02:58<05:51,  1.89s/it]predicting train subjects:  35%|███▌      | 100/285 [03:00<05:43,  1.86s/it]predicting train subjects:  35%|███▌      | 101/285 [03:02<05:44,  1.87s/it]predicting train subjects:  36%|███▌      | 102/285 [03:04<05:45,  1.89s/it]predicting train subjects:  36%|███▌      | 103/285 [03:06<05:45,  1.90s/it]predicting train subjects:  36%|███▋      | 104/285 [03:07<05:39,  1.88s/it]predicting train subjects:  37%|███▋      | 105/285 [03:09<05:36,  1.87s/it]predicting train subjects:  37%|███▋      | 106/285 [03:11<05:35,  1.87s/it]predicting train subjects:  38%|███▊      | 107/285 [03:13<05:33,  1.88s/it]predicting train subjects:  38%|███▊      | 108/285 [03:15<05:28,  1.85s/it]predicting train subjects:  38%|███▊      | 109/285 [03:17<05:24,  1.84s/it]predicting train subjects:  39%|███▊      | 110/285 [03:18<05:18,  1.82s/it]predicting train subjects:  39%|███▉      | 111/285 [03:20<05:13,  1.80s/it]predicting train subjects:  39%|███▉      | 112/285 [03:22<05:07,  1.78s/it]predicting train subjects:  40%|███▉      | 113/285 [03:24<05:03,  1.77s/it]predicting train subjects:  40%|████      | 114/285 [03:25<04:56,  1.74s/it]predicting train subjects:  40%|████      | 115/285 [03:27<04:56,  1.74s/it]predicting train subjects:  41%|████      | 116/285 [03:29<04:56,  1.75s/it]predicting train subjects:  41%|████      | 117/285 [03:31<04:52,  1.74s/it]predicting train subjects:  41%|████▏     | 118/285 [03:32<04:56,  1.78s/it]predicting train subjects:  42%|████▏     | 119/285 [03:34<04:54,  1.77s/it]predicting train subjects:  42%|████▏     | 120/285 [03:36<04:53,  1.78s/it]predicting train subjects:  42%|████▏     | 121/285 [03:38<04:44,  1.74s/it]predicting train subjects:  43%|████▎     | 122/285 [03:39<04:31,  1.67s/it]predicting train subjects:  43%|████▎     | 123/285 [03:41<04:19,  1.60s/it]predicting train subjects:  44%|████▎     | 124/285 [03:42<04:20,  1.62s/it]predicting train subjects:  44%|████▍     | 125/285 [03:44<04:21,  1.63s/it]predicting train subjects:  44%|████▍     | 126/285 [03:46<04:21,  1.64s/it]predicting train subjects:  45%|████▍     | 127/285 [03:47<04:18,  1.63s/it]predicting train subjects:  45%|████▍     | 128/285 [03:49<04:17,  1.64s/it]predicting train subjects:  45%|████▌     | 129/285 [03:50<04:14,  1.63s/it]predicting train subjects:  46%|████▌     | 130/285 [03:52<04:13,  1.63s/it]predicting train subjects:  46%|████▌     | 131/285 [03:54<04:11,  1.63s/it]predicting train subjects:  46%|████▋     | 132/285 [03:55<04:08,  1.63s/it]predicting train subjects:  47%|████▋     | 133/285 [03:57<04:07,  1.63s/it]predicting train subjects:  47%|████▋     | 134/285 [03:59<04:04,  1.62s/it]predicting train subjects:  47%|████▋     | 135/285 [04:00<04:02,  1.62s/it]predicting train subjects:  48%|████▊     | 136/285 [04:02<04:00,  1.62s/it]predicting train subjects:  48%|████▊     | 137/285 [04:03<03:59,  1.62s/it]predicting train subjects:  48%|████▊     | 138/285 [04:05<03:58,  1.62s/it]predicting train subjects:  49%|████▉     | 139/285 [04:07<03:56,  1.62s/it]predicting train subjects:  49%|████▉     | 140/285 [04:08<03:53,  1.61s/it]predicting train subjects:  49%|████▉     | 141/285 [04:10<03:51,  1.61s/it]predicting train subjects:  50%|████▉     | 142/285 [04:11<03:44,  1.57s/it]predicting train subjects:  50%|█████     | 143/285 [04:13<03:38,  1.54s/it]predicting train subjects:  51%|█████     | 144/285 [04:14<03:32,  1.50s/it]predicting train subjects:  51%|█████     | 145/285 [04:16<03:31,  1.51s/it]predicting train subjects:  51%|█████     | 146/285 [04:17<03:27,  1.49s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:19<03:24,  1.48s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:20<03:23,  1.48s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:22<03:20,  1.47s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:23<03:19,  1.48s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:25<03:17,  1.48s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:26<03:15,  1.47s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:27<03:14,  1.47s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:29<03:12,  1.47s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:30<03:10,  1.46s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:32<03:06,  1.45s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:33<03:07,  1.46s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:35<03:04,  1.45s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:36<03:04,  1.46s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:38<03:02,  1.46s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:39<03:00,  1.46s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:41<02:59,  1.46s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:42<02:59,  1.47s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:44<02:58,  1.48s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:45<02:57,  1.48s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:47<02:55,  1.48s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:48<02:52,  1.46s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:49<02:52,  1.47s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:51<02:51,  1.48s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:52<02:49,  1.48s/it]predicting train subjects:  60%|██████    | 171/285 [04:54<02:48,  1.48s/it]predicting train subjects:  60%|██████    | 172/285 [04:55<02:46,  1.47s/it]predicting train subjects:  61%|██████    | 173/285 [04:57<02:43,  1.46s/it]predicting train subjects:  61%|██████    | 174/285 [04:58<02:42,  1.47s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:00<02:41,  1.47s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:01<02:38,  1.46s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:03<02:36,  1.45s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:04<02:36,  1.46s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:05<02:32,  1.43s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:07<02:28,  1.42s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:08<02:27,  1.42s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:10<02:25,  1.42s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:11<02:24,  1.41s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:12<02:20,  1.39s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:14<02:17,  1.38s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:15<02:16,  1.38s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:17<02:16,  1.39s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:18<02:15,  1.39s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:19<02:15,  1.41s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:21<02:14,  1.41s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:22<02:11,  1.40s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:24<02:10,  1.40s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:25<02:08,  1.40s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:26<02:05,  1.38s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:28<02:03,  1.38s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:29<02:08,  1.45s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:31<02:13,  1.52s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:33<02:14,  1.54s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:34<02:14,  1.56s/it]predicting train subjects:  70%|███████   | 200/285 [05:36<02:13,  1.57s/it]predicting train subjects:  71%|███████   | 201/285 [05:37<02:12,  1.58s/it]predicting train subjects:  71%|███████   | 202/285 [05:39<02:13,  1.61s/it]predicting train subjects:  71%|███████   | 203/285 [05:41<02:10,  1.60s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:42<02:08,  1.59s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:44<02:08,  1.61s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:45<02:06,  1.59s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:47<02:07,  1.63s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:49<02:03,  1.60s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:50<02:00,  1.59s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:52<02:01,  1.62s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:53<01:58,  1.60s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:55<01:57,  1.62s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:57<01:54,  1.59s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:58<01:48,  1.53s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:59<01:42,  1.47s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:01<01:39,  1.44s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:02<01:36,  1.41s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:03<01:34,  1.40s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:05<01:32,  1.41s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:06<01:30,  1.40s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:08<01:29,  1.40s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:09<01:28,  1.40s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:10<01:26,  1.40s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:12<01:25,  1.39s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:13<01:24,  1.41s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:15<01:22,  1.40s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:16<01:21,  1.41s/it]predicting train subjects:  80%|████████  | 228/285 [06:17<01:19,  1.39s/it]predicting train subjects:  80%|████████  | 229/285 [06:19<01:18,  1.40s/it]predicting train subjects:  81%|████████  | 230/285 [06:20<01:17,  1.41s/it]predicting train subjects:  81%|████████  | 231/285 [06:22<01:15,  1.40s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:23<01:19,  1.50s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:25<01:22,  1.59s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:27<01:24,  1.65s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:29<01:24,  1.68s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:31<01:24,  1.72s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:32<01:23,  1.74s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:34<01:22,  1.76s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:36<01:21,  1.78s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:38<01:18,  1.76s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:39<01:17,  1.75s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:41<01:16,  1.79s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:43<01:15,  1.81s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:45<01:14,  1.82s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:47<01:12,  1.82s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:49<01:11,  1.83s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:51<01:10,  1.84s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:52<01:08,  1.84s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:54<01:05,  1.83s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:56<00:59,  1.69s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:57<00:54,  1.60s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:58<00:51,  1.55s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:00<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:01<00:46,  1.49s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:03<00:43,  1.45s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:04<00:42,  1.45s/it]predicting train subjects:  90%|█████████ | 257/285 [07:06<00:40,  1.44s/it]predicting train subjects:  91%|█████████ | 258/285 [07:07<00:38,  1.42s/it]predicting train subjects:  91%|█████████ | 259/285 [07:08<00:36,  1.42s/it]predicting train subjects:  91%|█████████ | 260/285 [07:10<00:35,  1.43s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:11<00:34,  1.42s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:13<00:32,  1.42s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:14<00:31,  1.41s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:15<00:29,  1.41s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:17<00:27,  1.40s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:18<00:26,  1.40s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:19<00:25,  1.39s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:21<00:26,  1.54s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:23<00:25,  1.62s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:25<00:25,  1.68s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:27<00:24,  1.73s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:29<00:22,  1.76s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:31<00:21,  1.78s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:32<00:20,  1.83s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:34<00:18,  1.87s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:36<00:16,  1.88s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:38<00:14,  1.87s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:40<00:13,  1.89s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:42<00:11,  1.89s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:44<00:09,  1.90s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:46<00:07,  1.88s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:48<00:05,  1.86s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:49<00:03,  1.85s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:51<00:01,  1.84s/it]predicting train subjects: 100%|██████████| 285/285 [07:53<00:00,  1.83s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:53,  1.46s/it]Loading train:   1%|          | 2/285 [00:03<07:06,  1.51s/it]Loading train:   1%|          | 3/285 [00:04<06:53,  1.47s/it]Loading train:   1%|▏         | 4/285 [00:06<07:09,  1.53s/it]Loading train:   2%|▏         | 5/285 [00:07<06:51,  1.47s/it]Loading train:   2%|▏         | 6/285 [00:09<07:06,  1.53s/it]Loading train:   2%|▏         | 7/285 [00:11<07:39,  1.65s/it]Loading train:   3%|▎         | 8/285 [00:12<07:41,  1.66s/it]Loading train:   3%|▎         | 9/285 [00:14<07:13,  1.57s/it]Loading train:   4%|▎         | 10/285 [00:15<06:34,  1.43s/it]Loading train:   4%|▍         | 11/285 [00:16<06:19,  1.38s/it]Loading train:   4%|▍         | 12/285 [00:17<06:13,  1.37s/it]Loading train:   5%|▍         | 13/285 [00:19<06:04,  1.34s/it]Loading train:   5%|▍         | 14/285 [00:20<06:11,  1.37s/it]Loading train:   5%|▌         | 15/285 [00:22<06:17,  1.40s/it]Loading train:   6%|▌         | 16/285 [00:23<06:37,  1.48s/it]Loading train:   6%|▌         | 17/285 [00:25<06:44,  1.51s/it]Loading train:   6%|▋         | 18/285 [00:26<06:56,  1.56s/it]Loading train:   7%|▋         | 19/285 [00:28<06:58,  1.57s/it]Loading train:   7%|▋         | 20/285 [00:30<06:51,  1.55s/it]Loading train:   7%|▋         | 21/285 [00:31<06:28,  1.47s/it]Loading train:   8%|▊         | 22/285 [00:32<06:08,  1.40s/it]Loading train:   8%|▊         | 23/285 [00:33<05:45,  1.32s/it]Loading train:   8%|▊         | 24/285 [00:35<05:48,  1.34s/it]Loading train:   9%|▉         | 25/285 [00:36<05:34,  1.29s/it]Loading train:   9%|▉         | 26/285 [00:37<05:35,  1.30s/it]Loading train:   9%|▉         | 27/285 [00:38<05:24,  1.26s/it]Loading train:  10%|▉         | 28/285 [00:39<05:09,  1.20s/it]Loading train:  10%|█         | 29/285 [00:40<04:58,  1.17s/it]Loading train:  11%|█         | 30/285 [00:42<05:02,  1.19s/it]Loading train:  11%|█         | 31/285 [00:43<04:52,  1.15s/it]Loading train:  11%|█         | 32/285 [00:44<04:35,  1.09s/it]Loading train:  12%|█▏        | 33/285 [00:45<04:32,  1.08s/it]Loading train:  12%|█▏        | 34/285 [00:46<04:42,  1.12s/it]Loading train:  12%|█▏        | 35/285 [00:47<04:48,  1.16s/it]Loading train:  13%|█▎        | 36/285 [00:48<04:42,  1.13s/it]Loading train:  13%|█▎        | 37/285 [00:49<04:44,  1.15s/it]Loading train:  13%|█▎        | 38/285 [00:50<04:33,  1.11s/it]Loading train:  14%|█▎        | 39/285 [00:52<04:34,  1.12s/it]Loading train:  14%|█▍        | 40/285 [00:53<04:30,  1.10s/it]Loading train:  14%|█▍        | 41/285 [00:54<04:28,  1.10s/it]Loading train:  15%|█▍        | 42/285 [00:55<04:25,  1.09s/it]Loading train:  15%|█▌        | 43/285 [00:56<04:32,  1.13s/it]Loading train:  15%|█▌        | 44/285 [00:57<04:33,  1.13s/it]Loading train:  16%|█▌        | 45/285 [00:58<04:20,  1.09s/it]Loading train:  16%|█▌        | 46/285 [00:59<04:11,  1.05s/it]Loading train:  16%|█▋        | 47/285 [01:00<04:00,  1.01s/it]Loading train:  17%|█▋        | 48/285 [01:01<03:50,  1.03it/s]Loading train:  17%|█▋        | 49/285 [01:02<03:38,  1.08it/s]Loading train:  18%|█▊        | 50/285 [01:03<03:33,  1.10it/s]Loading train:  18%|█▊        | 51/285 [01:04<03:38,  1.07it/s]Loading train:  18%|█▊        | 52/285 [01:05<03:42,  1.05it/s]Loading train:  19%|█▊        | 53/285 [01:06<03:49,  1.01it/s]Loading train:  19%|█▉        | 54/285 [01:07<03:53,  1.01s/it]Loading train:  19%|█▉        | 55/285 [01:08<03:52,  1.01s/it]Loading train:  20%|█▉        | 56/285 [01:09<03:56,  1.03s/it]Loading train:  20%|██        | 57/285 [01:10<03:59,  1.05s/it]Loading train:  20%|██        | 58/285 [01:11<03:52,  1.02s/it]Loading train:  21%|██        | 59/285 [01:12<03:44,  1.01it/s]Loading train:  21%|██        | 60/285 [01:13<03:38,  1.03it/s]Loading train:  21%|██▏       | 61/285 [01:14<03:38,  1.02it/s]Loading train:  22%|██▏       | 62/285 [01:15<03:37,  1.03it/s]Loading train:  22%|██▏       | 63/285 [01:16<03:34,  1.04it/s]Loading train:  22%|██▏       | 64/285 [01:17<04:08,  1.13s/it]Loading train:  23%|██▎       | 65/285 [01:19<04:43,  1.29s/it]Loading train:  23%|██▎       | 66/285 [01:20<04:46,  1.31s/it]Loading train:  24%|██▎       | 67/285 [01:21<04:34,  1.26s/it]Loading train:  24%|██▍       | 68/285 [01:22<04:16,  1.18s/it]Loading train:  24%|██▍       | 69/285 [01:23<04:04,  1.13s/it]Loading train:  25%|██▍       | 70/285 [01:24<04:07,  1.15s/it]Loading train:  25%|██▍       | 71/285 [01:26<04:04,  1.14s/it]Loading train:  25%|██▌       | 72/285 [01:27<03:58,  1.12s/it]Loading train:  26%|██▌       | 73/285 [01:28<03:46,  1.07s/it]Loading train:  26%|██▌       | 74/285 [01:29<03:49,  1.09s/it]Loading train:  26%|██▋       | 75/285 [01:30<03:49,  1.09s/it]Loading train:  27%|██▋       | 76/285 [01:31<03:46,  1.08s/it]Loading train:  27%|██▋       | 77/285 [01:32<03:44,  1.08s/it]Loading train:  27%|██▋       | 78/285 [01:33<03:40,  1.07s/it]Loading train:  28%|██▊       | 79/285 [01:34<03:41,  1.08s/it]Loading train:  28%|██▊       | 80/285 [01:35<03:45,  1.10s/it]Loading train:  28%|██▊       | 81/285 [01:36<03:37,  1.07s/it]Loading train:  29%|██▉       | 82/285 [01:37<03:31,  1.04s/it]Loading train:  29%|██▉       | 83/285 [01:38<03:31,  1.05s/it]Loading train:  29%|██▉       | 84/285 [01:39<03:32,  1.06s/it]Loading train:  30%|██▉       | 85/285 [01:40<03:34,  1.07s/it]Loading train:  30%|███       | 86/285 [01:42<03:37,  1.09s/it]Loading train:  31%|███       | 87/285 [01:43<03:34,  1.09s/it]Loading train:  31%|███       | 88/285 [01:44<03:38,  1.11s/it]Loading train:  31%|███       | 89/285 [01:45<03:38,  1.12s/it]Loading train:  32%|███▏      | 90/285 [01:46<03:37,  1.11s/it]Loading train:  32%|███▏      | 91/285 [01:47<03:38,  1.13s/it]Loading train:  32%|███▏      | 92/285 [01:48<03:32,  1.10s/it]Loading train:  33%|███▎      | 93/285 [01:49<03:31,  1.10s/it]Loading train:  33%|███▎      | 94/285 [01:50<03:25,  1.08s/it]Loading train:  33%|███▎      | 95/285 [01:52<03:23,  1.07s/it]Loading train:  34%|███▎      | 96/285 [01:53<03:23,  1.08s/it]Loading train:  34%|███▍      | 97/285 [01:54<03:20,  1.07s/it]Loading train:  34%|███▍      | 98/285 [01:55<03:21,  1.08s/it]Loading train:  35%|███▍      | 99/285 [01:56<03:24,  1.10s/it]Loading train:  35%|███▌      | 100/285 [01:57<03:21,  1.09s/it]Loading train:  35%|███▌      | 101/285 [01:58<03:18,  1.08s/it]Loading train:  36%|███▌      | 102/285 [01:59<03:20,  1.09s/it]Loading train:  36%|███▌      | 103/285 [02:00<03:18,  1.09s/it]Loading train:  36%|███▋      | 104/285 [02:01<03:19,  1.10s/it]Loading train:  37%|███▋      | 105/285 [02:02<03:18,  1.10s/it]Loading train:  37%|███▋      | 106/285 [02:04<03:17,  1.10s/it]Loading train:  38%|███▊      | 107/285 [02:05<03:14,  1.09s/it]Loading train:  38%|███▊      | 108/285 [02:06<03:17,  1.11s/it]Loading train:  38%|███▊      | 109/285 [02:07<03:12,  1.10s/it]Loading train:  39%|███▊      | 110/285 [02:08<03:05,  1.06s/it]Loading train:  39%|███▉      | 111/285 [02:09<03:09,  1.09s/it]Loading train:  39%|███▉      | 112/285 [02:10<03:08,  1.09s/it]Loading train:  40%|███▉      | 113/285 [02:11<03:08,  1.10s/it]Loading train:  40%|████      | 114/285 [02:12<03:08,  1.10s/it]Loading train:  40%|████      | 115/285 [02:13<03:07,  1.10s/it]Loading train:  41%|████      | 116/285 [02:14<03:05,  1.10s/it]Loading train:  41%|████      | 117/285 [02:16<03:03,  1.09s/it]Loading train:  41%|████▏     | 118/285 [02:17<03:05,  1.11s/it]Loading train:  42%|████▏     | 119/285 [02:18<03:06,  1.12s/it]Loading train:  42%|████▏     | 120/285 [02:19<02:58,  1.08s/it]Loading train:  42%|████▏     | 121/285 [02:20<03:14,  1.18s/it]Loading train:  43%|████▎     | 122/285 [02:22<03:22,  1.24s/it]Loading train:  43%|████▎     | 123/285 [02:23<03:30,  1.30s/it]Loading train:  44%|████▎     | 124/285 [02:24<03:12,  1.20s/it]Loading train:  44%|████▍     | 125/285 [02:25<03:03,  1.15s/it]Loading train:  44%|████▍     | 126/285 [02:26<02:52,  1.09s/it]Loading train:  45%|████▍     | 127/285 [02:27<02:41,  1.02s/it]Loading train:  45%|████▍     | 128/285 [02:28<02:45,  1.06s/it]Loading train:  45%|████▌     | 129/285 [02:29<02:43,  1.05s/it]Loading train:  46%|████▌     | 130/285 [02:30<02:36,  1.01s/it]Loading train:  46%|████▌     | 131/285 [02:31<02:35,  1.01s/it]Loading train:  46%|████▋     | 132/285 [02:32<02:33,  1.00s/it]Loading train:  47%|████▋     | 133/285 [02:33<02:29,  1.02it/s]Loading train:  47%|████▋     | 134/285 [02:34<02:29,  1.01it/s]Loading train:  47%|████▋     | 135/285 [02:35<02:31,  1.01s/it]Loading train:  48%|████▊     | 136/285 [02:36<02:26,  1.02it/s]Loading train:  48%|████▊     | 137/285 [02:37<02:26,  1.01it/s]Loading train:  48%|████▊     | 138/285 [02:38<02:29,  1.02s/it]Loading train:  49%|████▉     | 139/285 [02:39<02:28,  1.02s/it]Loading train:  49%|████▉     | 140/285 [02:40<02:28,  1.03s/it]Loading train:  49%|████▉     | 141/285 [02:41<02:23,  1.00it/s]Loading train:  50%|████▉     | 142/285 [02:42<02:21,  1.01it/s]Loading train:  50%|█████     | 143/285 [02:43<02:18,  1.02it/s]Loading train:  51%|█████     | 144/285 [02:44<02:18,  1.02it/s]Loading train:  51%|█████     | 145/285 [02:45<02:13,  1.05it/s]Loading train:  51%|█████     | 146/285 [02:46<02:10,  1.07it/s]Loading train:  52%|█████▏    | 147/285 [02:47<02:13,  1.03it/s]Loading train:  52%|█████▏    | 148/285 [02:48<02:13,  1.03it/s]Loading train:  52%|█████▏    | 149/285 [02:49<02:08,  1.06it/s]Loading train:  53%|█████▎    | 150/285 [02:49<02:02,  1.11it/s]Loading train:  53%|█████▎    | 151/285 [02:50<02:00,  1.11it/s]Loading train:  53%|█████▎    | 152/285 [02:51<01:59,  1.11it/s]Loading train:  54%|█████▎    | 153/285 [02:52<01:59,  1.10it/s]Loading train:  54%|█████▍    | 154/285 [02:53<01:53,  1.15it/s]Loading train:  54%|█████▍    | 155/285 [02:54<01:51,  1.16it/s]Loading train:  55%|█████▍    | 156/285 [02:55<01:50,  1.17it/s]Loading train:  55%|█████▌    | 157/285 [02:55<01:49,  1.16it/s]Loading train:  55%|█████▌    | 158/285 [02:56<01:54,  1.11it/s]Loading train:  56%|█████▌    | 159/285 [02:57<01:49,  1.15it/s]Loading train:  56%|█████▌    | 160/285 [02:58<01:52,  1.11it/s]Loading train:  56%|█████▋    | 161/285 [02:59<01:48,  1.14it/s]Loading train:  57%|█████▋    | 162/285 [03:00<01:50,  1.11it/s]Loading train:  57%|█████▋    | 163/285 [03:01<01:49,  1.11it/s]Loading train:  58%|█████▊    | 164/285 [03:02<01:50,  1.10it/s]Loading train:  58%|█████▊    | 165/285 [03:03<01:52,  1.07it/s]Loading train:  58%|█████▊    | 166/285 [03:04<01:54,  1.04it/s]Loading train:  59%|█████▊    | 167/285 [03:05<01:51,  1.06it/s]Loading train:  59%|█████▉    | 168/285 [03:06<01:48,  1.08it/s]Loading train:  59%|█████▉    | 169/285 [03:06<01:42,  1.13it/s]Loading train:  60%|█████▉    | 170/285 [03:07<01:41,  1.14it/s]Loading train:  60%|██████    | 171/285 [03:08<01:37,  1.17it/s]Loading train:  60%|██████    | 172/285 [03:09<01:41,  1.12it/s]Loading train:  61%|██████    | 173/285 [03:10<01:41,  1.10it/s]Loading train:  61%|██████    | 174/285 [03:11<01:42,  1.08it/s]Loading train:  61%|██████▏   | 175/285 [03:12<01:44,  1.05it/s]Loading train:  62%|██████▏   | 176/285 [03:13<01:44,  1.05it/s]Loading train:  62%|██████▏   | 177/285 [03:14<01:40,  1.07it/s]Loading train:  62%|██████▏   | 178/285 [03:15<01:39,  1.08it/s]Loading train:  63%|██████▎   | 179/285 [03:16<01:35,  1.11it/s]Loading train:  63%|██████▎   | 180/285 [03:16<01:34,  1.12it/s]Loading train:  64%|██████▎   | 181/285 [03:17<01:31,  1.13it/s]Loading train:  64%|██████▍   | 182/285 [03:18<01:27,  1.17it/s]Loading train:  64%|██████▍   | 183/285 [03:19<01:32,  1.10it/s]Loading train:  65%|██████▍   | 184/285 [03:20<01:27,  1.16it/s]Loading train:  65%|██████▍   | 185/285 [03:21<01:25,  1.17it/s]Loading train:  65%|██████▌   | 186/285 [03:22<01:25,  1.15it/s]Loading train:  66%|██████▌   | 187/285 [03:22<01:24,  1.16it/s]Loading train:  66%|██████▌   | 188/285 [03:23<01:25,  1.14it/s]Loading train:  66%|██████▋   | 189/285 [03:24<01:24,  1.13it/s]Loading train:  67%|██████▋   | 190/285 [03:25<01:22,  1.15it/s]Loading train:  67%|██████▋   | 191/285 [03:26<01:20,  1.16it/s]Loading train:  67%|██████▋   | 192/285 [03:27<01:21,  1.14it/s]Loading train:  68%|██████▊   | 193/285 [03:28<01:36,  1.05s/it]Loading train:  68%|██████▊   | 194/285 [03:29<01:34,  1.04s/it]Loading train:  68%|██████▊   | 195/285 [03:30<01:29,  1.00it/s]Loading train:  69%|██████▉   | 196/285 [03:31<01:29,  1.01s/it]Loading train:  69%|██████▉   | 197/285 [03:32<01:32,  1.06s/it]Loading train:  69%|██████▉   | 198/285 [03:33<01:31,  1.05s/it]Loading train:  70%|██████▉   | 199/285 [03:35<01:35,  1.11s/it]Loading train:  70%|███████   | 200/285 [03:36<01:32,  1.09s/it]Loading train:  71%|███████   | 201/285 [03:37<01:26,  1.03s/it]Loading train:  71%|███████   | 202/285 [03:38<01:26,  1.04s/it]Loading train:  71%|███████   | 203/285 [03:39<01:29,  1.09s/it]Loading train:  72%|███████▏  | 204/285 [03:40<01:28,  1.09s/it]Loading train:  72%|███████▏  | 205/285 [03:41<01:27,  1.10s/it]Loading train:  72%|███████▏  | 206/285 [03:42<01:27,  1.11s/it]Loading train:  73%|███████▎  | 207/285 [03:43<01:23,  1.08s/it]Loading train:  73%|███████▎  | 208/285 [03:44<01:20,  1.05s/it]Loading train:  73%|███████▎  | 209/285 [03:45<01:18,  1.03s/it]Loading train:  74%|███████▎  | 210/285 [03:46<01:14,  1.00it/s]Loading train:  74%|███████▍  | 211/285 [03:47<01:14,  1.01s/it]Loading train:  74%|███████▍  | 212/285 [03:48<01:14,  1.02s/it]Loading train:  75%|███████▍  | 213/285 [03:49<01:11,  1.00it/s]Loading train:  75%|███████▌  | 214/285 [03:50<01:09,  1.02it/s]Loading train:  75%|███████▌  | 215/285 [03:51<01:10,  1.00s/it]Loading train:  76%|███████▌  | 216/285 [03:52<01:10,  1.02s/it]Loading train:  76%|███████▌  | 217/285 [03:53<01:06,  1.02it/s]Loading train:  76%|███████▋  | 218/285 [03:54<01:05,  1.02it/s]Loading train:  77%|███████▋  | 219/285 [03:55<01:03,  1.04it/s]Loading train:  77%|███████▋  | 220/285 [03:56<01:02,  1.04it/s]Loading train:  78%|███████▊  | 221/285 [03:57<01:02,  1.02it/s]Loading train:  78%|███████▊  | 222/285 [03:58<01:00,  1.04it/s]Loading train:  78%|███████▊  | 223/285 [03:59<01:05,  1.06s/it]Loading train:  79%|███████▊  | 224/285 [04:00<01:02,  1.03s/it]Loading train:  79%|███████▉  | 225/285 [04:01<00:59,  1.02it/s]Loading train:  79%|███████▉  | 226/285 [04:02<00:55,  1.06it/s]Loading train:  80%|███████▉  | 227/285 [04:03<00:53,  1.09it/s]Loading train:  80%|████████  | 228/285 [04:04<00:53,  1.07it/s]Loading train:  80%|████████  | 229/285 [04:05<00:52,  1.08it/s]Loading train:  81%|████████  | 230/285 [04:06<00:49,  1.11it/s]Loading train:  81%|████████  | 231/285 [04:06<00:49,  1.10it/s]Loading train:  81%|████████▏ | 232/285 [04:08<00:51,  1.04it/s]Loading train:  82%|████████▏ | 233/285 [04:09<00:50,  1.02it/s]Loading train:  82%|████████▏ | 234/285 [04:10<00:49,  1.03it/s]Loading train:  82%|████████▏ | 235/285 [04:11<00:49,  1.00it/s]Loading train:  83%|████████▎ | 236/285 [04:12<00:48,  1.00it/s]Loading train:  83%|████████▎ | 237/285 [04:13<00:47,  1.00it/s]Loading train:  84%|████████▎ | 238/285 [04:14<00:49,  1.05s/it]Loading train:  84%|████████▍ | 239/285 [04:15<00:50,  1.09s/it]Loading train:  84%|████████▍ | 240/285 [04:16<00:48,  1.09s/it]Loading train:  85%|████████▍ | 241/285 [04:17<00:48,  1.11s/it]Loading train:  85%|████████▍ | 242/285 [04:18<00:47,  1.11s/it]Loading train:  85%|████████▌ | 243/285 [04:19<00:44,  1.07s/it]Loading train:  86%|████████▌ | 244/285 [04:20<00:42,  1.05s/it]Loading train:  86%|████████▌ | 245/285 [04:21<00:40,  1.00s/it]Loading train:  86%|████████▋ | 246/285 [04:22<00:40,  1.05s/it]Loading train:  87%|████████▋ | 247/285 [04:23<00:41,  1.10s/it]Loading train:  87%|████████▋ | 248/285 [04:25<00:40,  1.10s/it]Loading train:  87%|████████▋ | 249/285 [04:26<00:40,  1.11s/it]Loading train:  88%|████████▊ | 250/285 [04:27<00:37,  1.06s/it]Loading train:  88%|████████▊ | 251/285 [04:28<00:34,  1.02s/it]Loading train:  88%|████████▊ | 252/285 [04:29<00:33,  1.00s/it]Loading train:  89%|████████▉ | 253/285 [04:29<00:31,  1.03it/s]Loading train:  89%|████████▉ | 254/285 [04:30<00:30,  1.02it/s]Loading train:  89%|████████▉ | 255/285 [04:31<00:28,  1.05it/s]Loading train:  90%|████████▉ | 256/285 [04:32<00:26,  1.08it/s]Loading train:  90%|█████████ | 257/285 [04:33<00:25,  1.08it/s]Loading train:  91%|█████████ | 258/285 [04:34<00:24,  1.09it/s]Loading train:  91%|█████████ | 259/285 [04:35<00:23,  1.09it/s]Loading train:  91%|█████████ | 260/285 [04:36<00:23,  1.08it/s]Loading train:  92%|█████████▏| 261/285 [04:37<00:22,  1.08it/s]Loading train:  92%|█████████▏| 262/285 [04:38<00:20,  1.10it/s]Loading train:  92%|█████████▏| 263/285 [04:39<00:19,  1.12it/s]Loading train:  93%|█████████▎| 264/285 [04:39<00:18,  1.15it/s]Loading train:  93%|█████████▎| 265/285 [04:40<00:17,  1.15it/s]Loading train:  93%|█████████▎| 266/285 [04:41<00:16,  1.13it/s]Loading train:  94%|█████████▎| 267/285 [04:42<00:15,  1.13it/s]Loading train:  94%|█████████▍| 268/285 [04:43<00:16,  1.02it/s]Loading train:  94%|█████████▍| 269/285 [04:44<00:16,  1.06s/it]Loading train:  95%|█████████▍| 270/285 [04:46<00:16,  1.08s/it]Loading train:  95%|█████████▌| 271/285 [04:47<00:15,  1.14s/it]Loading train:  95%|█████████▌| 272/285 [04:48<00:15,  1.19s/it]Loading train:  96%|█████████▌| 273/285 [04:49<00:13,  1.12s/it]Loading train:  96%|█████████▌| 274/285 [04:50<00:11,  1.07s/it]Loading train:  96%|█████████▋| 275/285 [04:51<00:10,  1.08s/it]Loading train:  97%|█████████▋| 276/285 [04:52<00:09,  1.07s/it]Loading train:  97%|█████████▋| 277/285 [04:53<00:08,  1.05s/it]Loading train:  98%|█████████▊| 278/285 [04:54<00:07,  1.03s/it]Loading train:  98%|█████████▊| 279/285 [04:55<00:06,  1.04s/it]Loading train:  98%|█████████▊| 280/285 [04:56<00:05,  1.04s/it]Loading train:  99%|█████████▊| 281/285 [04:57<00:04,  1.07s/it]Loading train:  99%|█████████▉| 282/285 [04:59<00:03,  1.07s/it]Loading train:  99%|█████████▉| 283/285 [05:00<00:02,  1.05s/it]Loading train: 100%|█████████▉| 284/285 [05:01<00:01,  1.06s/it]Loading train: 100%|██████████| 285/285 [05:02<00:00,  1.08s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:10, 26.07it/s]concatenating: train:   3%|▎         | 8/285 [00:00<00:09, 29.77it/s]concatenating: train:   5%|▌         | 15/285 [00:00<00:07, 35.31it/s]concatenating: train:   8%|▊         | 24/285 [00:00<00:06, 43.16it/s]concatenating: train:  17%|█▋        | 49/285 [00:00<00:04, 57.37it/s]concatenating: train:  26%|██▌       | 74/285 [00:00<00:02, 74.59it/s]concatenating: train:  36%|███▌      | 103/285 [00:00<00:01, 95.79it/s]concatenating: train:  46%|████▋     | 132/285 [00:00<00:01, 119.86it/s]concatenating: train:  57%|█████▋    | 163/285 [00:00<00:00, 146.57it/s]concatenating: train:  68%|██████▊   | 193/285 [00:01<00:00, 172.76it/s]concatenating: train:  79%|███████▉  | 225/285 [00:01<00:00, 199.39it/s]concatenating: train:  89%|████████▉ | 255/285 [00:01<00:00, 220.48it/s]concatenating: train: 100%|█████████▉| 284/285 [00:01<00:00, 207.47it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 201.69it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.53s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 79.56it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:33,  1.17s/it]Loading train:   1%|          | 2/285 [00:02<05:59,  1.27s/it]Loading train:   1%|          | 3/285 [00:03<05:58,  1.27s/it]Loading train:   1%|▏         | 4/285 [00:05<06:21,  1.36s/it]Loading train:   2%|▏         | 5/285 [00:06<06:03,  1.30s/it]Loading train:   2%|▏         | 6/285 [00:08<06:22,  1.37s/it]Loading train:   2%|▏         | 7/285 [00:09<06:37,  1.43s/it]Loading train:   3%|▎         | 8/285 [00:11<06:49,  1.48s/it]Loading train:   3%|▎         | 9/285 [00:12<06:36,  1.44s/it]Loading train:   4%|▎         | 10/285 [00:13<06:08,  1.34s/it]Loading train:   4%|▍         | 11/285 [00:14<05:34,  1.22s/it]Loading train:   4%|▍         | 12/285 [00:15<05:23,  1.18s/it]Loading train:   5%|▍         | 13/285 [00:16<05:13,  1.15s/it]Loading train:   5%|▍         | 14/285 [00:17<04:54,  1.09s/it]Loading train:   5%|▌         | 15/285 [00:18<04:45,  1.06s/it]Loading train:   6%|▌         | 16/285 [00:19<04:45,  1.06s/it]Loading train:   6%|▌         | 17/285 [00:20<04:36,  1.03s/it]Loading train:   6%|▋         | 18/285 [00:21<04:32,  1.02s/it]Loading train:   7%|▋         | 19/285 [00:22<04:28,  1.01s/it]Loading train:   7%|▋         | 20/285 [00:23<04:22,  1.01it/s]Loading train:   7%|▋         | 21/285 [00:24<04:24,  1.00s/it]Loading train:   8%|▊         | 22/285 [00:25<04:26,  1.01s/it]Loading train:   8%|▊         | 23/285 [00:26<04:15,  1.02it/s]Loading train:   8%|▊         | 24/285 [00:27<04:18,  1.01it/s]Loading train:   9%|▉         | 25/285 [00:28<04:15,  1.02it/s]Loading train:   9%|▉         | 26/285 [00:29<04:17,  1.01it/s]Loading train:   9%|▉         | 27/285 [00:30<04:12,  1.02it/s]Loading train:  10%|▉         | 28/285 [00:31<04:23,  1.02s/it]Loading train:  10%|█         | 29/285 [00:32<04:18,  1.01s/it]Loading train:  11%|█         | 30/285 [00:33<04:10,  1.02it/s]Loading train:  11%|█         | 31/285 [00:34<04:05,  1.03it/s]Loading train:  11%|█         | 32/285 [00:35<04:02,  1.04it/s]Loading train:  12%|█▏        | 33/285 [00:36<04:00,  1.05it/s]Loading train:  12%|█▏        | 34/285 [00:37<03:54,  1.07it/s]Loading train:  12%|█▏        | 35/285 [00:38<03:54,  1.07it/s]Loading train:  13%|█▎        | 36/285 [00:39<03:54,  1.06it/s]Loading train:  13%|█▎        | 37/285 [00:40<03:50,  1.08it/s]Loading train:  13%|█▎        | 38/285 [00:41<03:45,  1.10it/s]Loading train:  14%|█▎        | 39/285 [00:42<03:53,  1.05it/s]Loading train:  14%|█▍        | 40/285 [00:43<04:01,  1.02it/s]Loading train:  14%|█▍        | 41/285 [00:44<03:50,  1.06it/s]Loading train:  15%|█▍        | 42/285 [00:44<03:43,  1.09it/s]Loading train:  15%|█▌        | 43/285 [00:45<03:36,  1.12it/s]Loading train:  15%|█▌        | 44/285 [00:46<03:39,  1.10it/s]Loading train:  16%|█▌        | 45/285 [00:47<03:38,  1.10it/s]Loading train:  16%|█▌        | 46/285 [00:48<03:29,  1.14it/s]Loading train:  16%|█▋        | 47/285 [00:49<03:22,  1.18it/s]Loading train:  17%|█▋        | 48/285 [00:49<03:13,  1.22it/s]Loading train:  17%|█▋        | 49/285 [00:50<03:08,  1.25it/s]Loading train:  18%|█▊        | 50/285 [00:51<03:05,  1.26it/s]Loading train:  18%|█▊        | 51/285 [00:52<03:05,  1.26it/s]Loading train:  18%|█▊        | 52/285 [00:53<03:01,  1.29it/s]Loading train:  19%|█▊        | 53/285 [00:53<03:06,  1.24it/s]Loading train:  19%|█▉        | 54/285 [00:54<03:02,  1.27it/s]Loading train:  19%|█▉        | 55/285 [00:55<02:57,  1.30it/s]Loading train:  20%|█▉        | 56/285 [00:56<02:57,  1.29it/s]Loading train:  20%|██        | 57/285 [00:56<02:55,  1.30it/s]Loading train:  20%|██        | 58/285 [00:57<02:59,  1.27it/s]Loading train:  21%|██        | 59/285 [00:58<02:54,  1.29it/s]Loading train:  21%|██        | 60/285 [00:59<02:51,  1.31it/s]Loading train:  21%|██▏       | 61/285 [01:00<02:56,  1.27it/s]Loading train:  22%|██▏       | 62/285 [01:00<03:01,  1.23it/s]Loading train:  22%|██▏       | 63/285 [01:01<03:00,  1.23it/s]Loading train:  22%|██▏       | 64/285 [01:03<03:35,  1.03it/s]Loading train:  23%|██▎       | 65/285 [01:04<04:17,  1.17s/it]Loading train:  23%|██▎       | 66/285 [01:05<04:16,  1.17s/it]Loading train:  24%|██▎       | 67/285 [01:06<03:47,  1.04s/it]Loading train:  24%|██▍       | 68/285 [01:07<03:34,  1.01it/s]Loading train:  24%|██▍       | 69/285 [01:08<03:29,  1.03it/s]Loading train:  25%|██▍       | 70/285 [01:09<03:16,  1.09it/s]Loading train:  25%|██▍       | 71/285 [01:10<03:09,  1.13it/s]Loading train:  25%|██▌       | 72/285 [01:10<02:59,  1.19it/s]Loading train:  26%|██▌       | 73/285 [01:11<03:01,  1.17it/s]Loading train:  26%|██▌       | 74/285 [01:12<03:03,  1.15it/s]Loading train:  26%|██▋       | 75/285 [01:13<02:55,  1.20it/s]Loading train:  27%|██▋       | 76/285 [01:14<02:52,  1.21it/s]Loading train:  27%|██▋       | 77/285 [01:14<02:51,  1.21it/s]Loading train:  27%|██▋       | 78/285 [01:15<02:47,  1.24it/s]Loading train:  28%|██▊       | 79/285 [01:16<02:53,  1.19it/s]Loading train:  28%|██▊       | 80/285 [01:17<02:54,  1.17it/s]Loading train:  28%|██▊       | 81/285 [01:18<02:51,  1.19it/s]Loading train:  29%|██▉       | 82/285 [01:19<02:52,  1.18it/s]Loading train:  29%|██▉       | 83/285 [01:20<02:57,  1.13it/s]Loading train:  29%|██▉       | 84/285 [01:21<03:03,  1.10it/s]Loading train:  30%|██▉       | 85/285 [01:22<03:15,  1.02it/s]Loading train:  30%|███       | 86/285 [01:23<03:24,  1.03s/it]Loading train:  31%|███       | 87/285 [01:24<03:26,  1.04s/it]Loading train:  31%|███       | 88/285 [01:25<03:24,  1.04s/it]Loading train:  31%|███       | 89/285 [01:26<03:19,  1.02s/it]Loading train:  32%|███▏      | 90/285 [01:27<03:18,  1.02s/it]Loading train:  32%|███▏      | 91/285 [01:28<03:18,  1.02s/it]Loading train:  32%|███▏      | 92/285 [01:29<03:14,  1.01s/it]Loading train:  33%|███▎      | 93/285 [01:30<03:22,  1.05s/it]Loading train:  33%|███▎      | 94/285 [01:31<03:28,  1.09s/it]Loading train:  33%|███▎      | 95/285 [01:32<03:26,  1.09s/it]Loading train:  34%|███▎      | 96/285 [01:33<03:20,  1.06s/it]Loading train:  34%|███▍      | 97/285 [01:34<03:14,  1.03s/it]Loading train:  34%|███▍      | 98/285 [01:35<03:07,  1.00s/it]Loading train:  35%|███▍      | 99/285 [01:36<03:05,  1.00it/s]Loading train:  35%|███▌      | 100/285 [01:37<03:05,  1.00s/it]Loading train:  35%|███▌      | 101/285 [01:38<03:09,  1.03s/it]Loading train:  36%|███▌      | 102/285 [01:39<03:10,  1.04s/it]Loading train:  36%|███▌      | 103/285 [01:41<03:16,  1.08s/it]Loading train:  36%|███▋      | 104/285 [01:42<03:09,  1.05s/it]Loading train:  37%|███▋      | 105/285 [01:43<03:04,  1.03s/it]Loading train:  37%|███▋      | 106/285 [01:44<03:00,  1.01s/it]Loading train:  38%|███▊      | 107/285 [01:44<02:52,  1.03it/s]Loading train:  38%|███▊      | 108/285 [01:45<02:48,  1.05it/s]Loading train:  38%|███▊      | 109/285 [01:46<02:46,  1.06it/s]Loading train:  39%|███▊      | 110/285 [01:47<02:47,  1.05it/s]Loading train:  39%|███▉      | 111/285 [01:48<02:45,  1.05it/s]Loading train:  39%|███▉      | 112/285 [01:49<02:43,  1.06it/s]Loading train:  40%|███▉      | 113/285 [01:50<02:38,  1.08it/s]Loading train:  40%|████      | 114/285 [01:51<02:38,  1.08it/s]Loading train:  40%|████      | 115/285 [01:52<02:38,  1.08it/s]Loading train:  41%|████      | 116/285 [01:53<02:44,  1.03it/s]Loading train:  41%|████      | 117/285 [01:54<02:49,  1.01s/it]Loading train:  41%|████▏     | 118/285 [01:55<02:43,  1.02it/s]Loading train:  42%|████▏     | 119/285 [01:56<02:44,  1.01it/s]Loading train:  42%|████▏     | 120/285 [01:57<02:43,  1.01it/s]Loading train:  42%|████▏     | 121/285 [01:58<03:03,  1.12s/it]Loading train:  43%|████▎     | 122/285 [02:00<03:05,  1.14s/it]Loading train:  43%|████▎     | 123/285 [02:01<03:09,  1.17s/it]Loading train:  44%|████▎     | 124/285 [02:02<02:54,  1.08s/it]Loading train:  44%|████▍     | 125/285 [02:03<02:43,  1.02s/it]Loading train:  44%|████▍     | 126/285 [02:03<02:35,  1.02it/s]Loading train:  45%|████▍     | 127/285 [02:04<02:29,  1.06it/s]Loading train:  45%|████▍     | 128/285 [02:05<02:31,  1.04it/s]Loading train:  45%|████▌     | 129/285 [02:06<02:32,  1.02it/s]Loading train:  46%|████▌     | 130/285 [02:07<02:25,  1.07it/s]Loading train:  46%|████▌     | 131/285 [02:08<02:24,  1.06it/s]Loading train:  46%|████▋     | 132/285 [02:09<02:20,  1.09it/s]Loading train:  47%|████▋     | 133/285 [02:10<02:21,  1.07it/s]Loading train:  47%|████▋     | 134/285 [02:11<02:17,  1.10it/s]Loading train:  47%|████▋     | 135/285 [02:12<02:14,  1.12it/s]Loading train:  48%|████▊     | 136/285 [02:13<02:18,  1.08it/s]Loading train:  48%|████▊     | 137/285 [02:14<02:19,  1.06it/s]Loading train:  48%|████▊     | 138/285 [02:15<02:17,  1.07it/s]Loading train:  49%|████▉     | 139/285 [02:16<02:16,  1.07it/s]Loading train:  49%|████▉     | 140/285 [02:17<02:31,  1.04s/it]Loading train:  49%|████▉     | 141/285 [02:18<02:33,  1.06s/it]Loading train:  50%|████▉     | 142/285 [02:19<02:32,  1.06s/it]Loading train:  50%|█████     | 143/285 [02:20<02:18,  1.02it/s]Loading train:  51%|█████     | 144/285 [02:21<02:09,  1.09it/s]Loading train:  51%|█████     | 145/285 [02:21<02:09,  1.08it/s]Loading train:  51%|█████     | 146/285 [02:22<02:02,  1.14it/s]Loading train:  52%|█████▏    | 147/285 [02:23<02:03,  1.12it/s]Loading train:  52%|█████▏    | 148/285 [02:24<02:02,  1.12it/s]Loading train:  52%|█████▏    | 149/285 [02:25<01:59,  1.14it/s]Loading train:  53%|█████▎    | 150/285 [02:26<02:01,  1.11it/s]Loading train:  53%|█████▎    | 151/285 [02:27<01:56,  1.15it/s]Loading train:  53%|█████▎    | 152/285 [02:27<01:53,  1.17it/s]Loading train:  54%|█████▎    | 153/285 [02:28<01:48,  1.22it/s]Loading train:  54%|█████▍    | 154/285 [02:29<01:47,  1.22it/s]Loading train:  54%|█████▍    | 155/285 [02:30<01:45,  1.23it/s]Loading train:  55%|█████▍    | 156/285 [02:31<01:45,  1.22it/s]Loading train:  55%|█████▌    | 157/285 [02:32<01:47,  1.19it/s]Loading train:  55%|█████▌    | 158/285 [02:32<01:46,  1.20it/s]Loading train:  56%|█████▌    | 159/285 [02:33<01:42,  1.23it/s]Loading train:  56%|█████▌    | 160/285 [02:34<01:45,  1.18it/s]Loading train:  56%|█████▋    | 161/285 [02:35<01:42,  1.21it/s]Loading train:  57%|█████▋    | 162/285 [02:36<01:39,  1.23it/s]Loading train:  57%|█████▋    | 163/285 [02:37<01:41,  1.21it/s]Loading train:  58%|█████▊    | 164/285 [02:37<01:43,  1.17it/s]Loading train:  58%|█████▊    | 165/285 [02:38<01:40,  1.19it/s]Loading train:  58%|█████▊    | 166/285 [02:39<01:43,  1.15it/s]Loading train:  59%|█████▊    | 167/285 [02:40<01:39,  1.18it/s]Loading train:  59%|█████▉    | 168/285 [02:41<01:38,  1.19it/s]Loading train:  59%|█████▉    | 169/285 [02:41<01:32,  1.25it/s]Loading train:  60%|█████▉    | 170/285 [02:42<01:31,  1.26it/s]Loading train:  60%|██████    | 171/285 [02:43<01:30,  1.25it/s]Loading train:  60%|██████    | 172/285 [02:44<01:29,  1.27it/s]Loading train:  61%|██████    | 173/285 [02:45<01:32,  1.21it/s]Loading train:  61%|██████    | 174/285 [02:46<01:38,  1.13it/s]Loading train:  61%|██████▏   | 175/285 [02:47<01:35,  1.16it/s]Loading train:  62%|██████▏   | 176/285 [02:48<01:45,  1.03it/s]Loading train:  62%|██████▏   | 177/285 [02:49<01:37,  1.10it/s]Loading train:  62%|██████▏   | 178/285 [02:49<01:37,  1.10it/s]Loading train:  63%|██████▎   | 179/285 [02:50<01:32,  1.15it/s]Loading train:  63%|██████▎   | 180/285 [02:51<01:29,  1.18it/s]Loading train:  64%|██████▎   | 181/285 [02:52<01:44,  1.00s/it]Loading train:  64%|██████▍   | 182/285 [02:53<01:42,  1.01it/s]Loading train:  64%|██████▍   | 183/285 [02:54<01:41,  1.00it/s]Loading train:  65%|██████▍   | 184/285 [02:55<01:41,  1.00s/it]Loading train:  65%|██████▍   | 185/285 [02:57<01:45,  1.06s/it]Loading train:  65%|██████▌   | 186/285 [02:58<01:48,  1.10s/it]Loading train:  66%|██████▌   | 187/285 [02:59<01:51,  1.14s/it]Loading train:  66%|██████▌   | 188/285 [03:01<02:01,  1.25s/it]Loading train:  66%|██████▋   | 189/285 [03:02<01:57,  1.22s/it]Loading train:  67%|██████▋   | 190/285 [03:03<02:02,  1.29s/it]Loading train:  67%|██████▋   | 191/285 [03:04<01:59,  1.27s/it]Loading train:  67%|██████▋   | 192/285 [03:06<02:06,  1.36s/it]Loading train:  68%|██████▊   | 193/285 [03:07<02:06,  1.37s/it]Loading train:  68%|██████▊   | 194/285 [03:09<02:05,  1.38s/it]Loading train:  68%|██████▊   | 195/285 [03:10<01:54,  1.28s/it]Loading train:  69%|██████▉   | 196/285 [03:11<01:51,  1.25s/it]Loading train:  69%|██████▉   | 197/285 [03:12<01:55,  1.31s/it]Loading train:  69%|██████▉   | 198/285 [03:14<01:59,  1.37s/it]Loading train:  70%|██████▉   | 199/285 [03:15<01:44,  1.22s/it]Loading train:  70%|███████   | 200/285 [03:16<01:33,  1.10s/it]Loading train:  71%|███████   | 201/285 [03:17<01:28,  1.05s/it]Loading train:  71%|███████   | 202/285 [03:17<01:22,  1.00it/s]Loading train:  71%|███████   | 203/285 [03:18<01:17,  1.06it/s]Loading train:  72%|███████▏  | 204/285 [03:19<01:11,  1.14it/s]Loading train:  72%|███████▏  | 205/285 [03:20<01:06,  1.20it/s]Loading train:  72%|███████▏  | 206/285 [03:20<01:03,  1.24it/s]Loading train:  73%|███████▎  | 207/285 [03:21<01:03,  1.23it/s]Loading train:  73%|███████▎  | 208/285 [03:22<01:03,  1.21it/s]Loading train:  73%|███████▎  | 209/285 [03:23<01:01,  1.23it/s]Loading train:  74%|███████▎  | 210/285 [03:24<01:01,  1.22it/s]Loading train:  74%|███████▍  | 211/285 [03:25<00:59,  1.24it/s]Loading train:  74%|███████▍  | 212/285 [03:25<01:00,  1.21it/s]Loading train:  75%|███████▍  | 213/285 [03:26<00:58,  1.22it/s]Loading train:  75%|███████▌  | 214/285 [03:27<00:57,  1.24it/s]Loading train:  75%|███████▌  | 215/285 [03:28<00:53,  1.30it/s]Loading train:  76%|███████▌  | 216/285 [03:28<00:53,  1.28it/s]Loading train:  76%|███████▌  | 217/285 [03:29<00:51,  1.33it/s]Loading train:  76%|███████▋  | 218/285 [03:30<00:50,  1.32it/s]Loading train:  77%|███████▋  | 219/285 [03:31<00:49,  1.32it/s]Loading train:  77%|███████▋  | 220/285 [03:31<00:49,  1.30it/s]Loading train:  78%|███████▊  | 221/285 [03:32<00:48,  1.33it/s]Loading train:  78%|███████▊  | 222/285 [03:33<00:46,  1.35it/s]Loading train:  78%|███████▊  | 223/285 [03:34<00:44,  1.39it/s]Loading train:  79%|███████▊  | 224/285 [03:34<00:44,  1.36it/s]Loading train:  79%|███████▉  | 225/285 [03:35<00:44,  1.36it/s]Loading train:  79%|███████▉  | 226/285 [03:36<00:43,  1.37it/s]Loading train:  80%|███████▉  | 227/285 [03:37<00:42,  1.37it/s]Loading train:  80%|████████  | 228/285 [03:37<00:41,  1.36it/s]Loading train:  80%|████████  | 229/285 [03:38<00:40,  1.38it/s]Loading train:  81%|████████  | 230/285 [03:39<00:40,  1.37it/s]Loading train:  81%|████████  | 231/285 [03:39<00:39,  1.38it/s]Loading train:  81%|████████▏ | 232/285 [03:40<00:42,  1.24it/s]Loading train:  82%|████████▏ | 233/285 [03:41<00:43,  1.18it/s]Loading train:  82%|████████▏ | 234/285 [03:42<00:44,  1.15it/s]Loading train:  82%|████████▏ | 235/285 [03:43<00:44,  1.13it/s]Loading train:  83%|████████▎ | 236/285 [03:44<00:44,  1.10it/s]Loading train:  83%|████████▎ | 237/285 [03:45<00:43,  1.10it/s]Loading train:  84%|████████▎ | 238/285 [03:46<00:42,  1.11it/s]Loading train:  84%|████████▍ | 239/285 [03:47<00:41,  1.11it/s]Loading train:  84%|████████▍ | 240/285 [03:48<00:40,  1.12it/s]Loading train:  85%|████████▍ | 241/285 [03:49<00:39,  1.10it/s]Loading train:  85%|████████▍ | 242/285 [03:50<00:39,  1.08it/s]Loading train:  85%|████████▌ | 243/285 [03:51<00:38,  1.09it/s]Loading train:  86%|████████▌ | 244/285 [03:52<00:40,  1.02it/s]Loading train:  86%|████████▌ | 245/285 [03:53<00:47,  1.20s/it]Loading train:  86%|████████▋ | 246/285 [03:54<00:43,  1.12s/it]Loading train:  87%|████████▋ | 247/285 [03:55<00:40,  1.07s/it]Loading train:  87%|████████▋ | 248/285 [03:56<00:38,  1.03s/it]Loading train:  87%|████████▋ | 249/285 [03:57<00:35,  1.00it/s]Loading train:  88%|████████▊ | 250/285 [03:58<00:32,  1.07it/s]Loading train:  88%|████████▊ | 251/285 [03:59<00:29,  1.13it/s]Loading train:  88%|████████▊ | 252/285 [03:59<00:28,  1.18it/s]Loading train:  89%|████████▉ | 253/285 [04:00<00:26,  1.23it/s]Loading train:  89%|████████▉ | 254/285 [04:01<00:25,  1.19it/s]Loading train:  89%|████████▉ | 255/285 [04:02<00:24,  1.24it/s]Loading train:  90%|████████▉ | 256/285 [04:03<00:22,  1.27it/s]Loading train:  90%|█████████ | 257/285 [04:03<00:22,  1.25it/s]Loading train:  91%|█████████ | 258/285 [04:04<00:21,  1.26it/s]Loading train:  91%|█████████ | 259/285 [04:05<00:19,  1.33it/s]Loading train:  91%|█████████ | 260/285 [04:06<00:18,  1.34it/s]Loading train:  92%|█████████▏| 261/285 [04:06<00:18,  1.30it/s]Loading train:  92%|█████████▏| 262/285 [04:07<00:18,  1.28it/s]Loading train:  92%|█████████▏| 263/285 [04:08<00:16,  1.34it/s]Loading train:  93%|█████████▎| 264/285 [04:09<00:15,  1.32it/s]Loading train:  93%|█████████▎| 265/285 [04:09<00:15,  1.32it/s]Loading train:  93%|█████████▎| 266/285 [04:10<00:14,  1.35it/s]Loading train:  94%|█████████▎| 267/285 [04:11<00:13,  1.37it/s]Loading train:  94%|█████████▍| 268/285 [04:12<00:13,  1.23it/s]Loading train:  94%|█████████▍| 269/285 [04:13<00:14,  1.13it/s]Loading train:  95%|█████████▍| 270/285 [04:14<00:13,  1.10it/s]Loading train:  95%|█████████▌| 271/285 [04:15<00:12,  1.11it/s]Loading train:  95%|█████████▌| 272/285 [04:16<00:11,  1.09it/s]Loading train:  96%|█████████▌| 273/285 [04:17<00:11,  1.08it/s]Loading train:  96%|█████████▌| 274/285 [04:18<00:10,  1.10it/s]Loading train:  96%|█████████▋| 275/285 [04:19<00:09,  1.06it/s]Loading train:  97%|█████████▋| 276/285 [04:19<00:08,  1.05it/s]Loading train:  97%|█████████▋| 277/285 [04:20<00:07,  1.05it/s]Loading train:  98%|█████████▊| 278/285 [04:21<00:06,  1.08it/s]Loading train:  98%|█████████▊| 279/285 [04:22<00:05,  1.06it/s]Loading train:  98%|█████████▊| 280/285 [04:23<00:04,  1.07it/s]Loading train:  99%|█████████▊| 281/285 [04:24<00:03,  1.08it/s]Loading train:  99%|█████████▉| 282/285 [04:25<00:02,  1.06it/s]Loading train:  99%|█████████▉| 283/285 [04:26<00:02,  1.01s/it]Loading train: 100%|█████████▉| 284/285 [04:27<00:00,  1.01it/s]Loading train: 100%|██████████| 285/285 [04:28<00:00,  1.04it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:11, 24.82it/s]concatenating: train:  11%|█         | 30/285 [00:00<00:07, 34.09it/s]concatenating: train:  14%|█▎        | 39/285 [00:00<00:05, 41.75it/s]concatenating: train:  25%|██▍       | 70/285 [00:00<00:03, 56.31it/s]concatenating: train:  36%|███▌      | 102/285 [00:00<00:02, 74.81it/s]concatenating: train:  49%|████▉     | 139/285 [00:00<00:01, 98.22it/s]concatenating: train:  61%|██████▏   | 175/285 [00:00<00:00, 125.48it/s]concatenating: train:  73%|███████▎  | 209/285 [00:00<00:00, 154.70it/s]concatenating: train:  86%|████████▌ | 245/285 [00:00<00:00, 186.32it/s]concatenating: train:  99%|█████████▉| 282/285 [00:01<00:00, 218.27it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 272.87it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.39s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.33s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 190.92it/s]2019-07-05 19:47:14.673644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 19:47:14.673767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 19:47:14.673785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 19:47:14.673794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 19:47:14.674261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:14,  2.68it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:11,  3.20it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:11,  3.05it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:08,  3.91it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:08,  3.47it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:02<00:07,  3.86it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:08,  3.36it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:05,  4.22it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:03<00:06,  3.64it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:05,  4.08it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:05,  3.48it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:04<00:04,  4.21it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:04<00:03,  4.53it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:05<00:03,  3.80it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:05<00:02,  4.14it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:05<00:03,  3.47it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:06<00:01,  4.18it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:06<00:01,  4.42it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:07<00:01,  3.62it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:07<00:00,  3.99it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:07<00:00,  3.40it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:07<00:00,  5.09it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 30)   300         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 30)   120         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 30)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 30)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 30)   8130        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 30)   120         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 30)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 30)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 30)   8130        dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 30)   120         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 30)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 30)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   5420        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 30)   5430        dropout_15[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 52, 80, 30)   120         conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 52, 80, 30)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 52, 80, 30)   0           activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 52, 80, 13)   403         dropout_16[0][0]                 
==================================================================================================
Total params: 211,933
Trainable params: 68,213
Non-trainable params: 143,720
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 21s - loss: 198.0753 - acc: 0.2163 - mDice: 0.0192 - val_loss: 83.1004 - val_acc: 0.6604 - val_mDice: 0.0143

Epoch 00001: val_mDice improved from -inf to 0.01431, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 68.7682 - acc: 0.7958 - mDice: 0.0209 - val_loss: 39.2918 - val_acc: 0.9047 - val_mDice: 0.0182

Epoch 00002: val_mDice improved from 0.01431 to 0.01818, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 12s - loss: 30.1006 - acc: 0.8615 - mDice: 0.0205 - val_loss: 18.4633 - val_acc: 0.9047 - val_mDice: 0.0192

Epoch 00003: val_mDice improved from 0.01818 to 0.01924, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 12s - loss: 18.0475 - acc: 0.8669 - mDice: 0.0209 - val_loss: 11.0969 - val_acc: 0.9047 - val_mDice: 0.0199

Epoch 00004: val_mDice improved from 0.01924 to 0.01991, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 12s - loss: 13.1574 - acc: 0.8681 - mDice: 0.0217 - val_loss: 8.8891 - val_acc: 0.9047 - val_mDice: 0.0206

Epoch 00005: val_mDice improved from 0.01991 to 0.02058, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 12s - loss: 10.7570 - acc: 0.8689 - mDice: 0.0231 - val_loss: 7.4758 - val_acc: 0.9047 - val_mDice: 0.0247

Epoch 00006: val_mDice improved from 0.02058 to 0.02472, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 12s - loss: 9.3350 - acc: 0.8691 - mDice: 0.0260 - val_loss: 6.2666 - val_acc: 0.9047 - val_mDice: 0.0322

Epoch 00007: val_mDice improved from 0.02472 to 0.03218, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 12s - loss: 8.4270 - acc: 0.8692 - mDice: 0.0294 - val_loss: 5.8598 - val_acc: 0.9047 - val_mDice: 0.0352

Epoch 00008: val_mDice improved from 0.03218 to 0.03516, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 12s - loss: 7.7618 - acc: 0.8692 - mDice: 0.0336 - val_loss: 5.5784 - val_acc: 0.9047 - val_mDice: 0.0379

Epoch 00009: val_mDice improved from 0.03516 to 0.03794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 12s - loss: 7.2592 - acc: 0.8692 - mDice: 0.0383 - val_loss: 5.4707 - val_acc: 0.9047 - val_mDice: 0.0420

Epoch 00010: val_mDice improved from 0.03794 to 0.04198, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 12s - loss: 6.8694 - acc: 0.8692 - mDice: 0.0424 - val_loss: 5.5723 - val_acc: 0.9047 - val_mDice: 0.0415

Epoch 00011: val_mDice did not improve from 0.04198
Epoch 12/300
 - 12s - loss: 6.5252 - acc: 0.8692 - mDice: 0.0469 - val_loss: 5.7613 - val_acc: 0.9047 - val_mDice: 0.0409

Epoch 00012: val_mDice did not improve from 0.04198
Epoch 13/300
 - 12s - loss: 6.2086 - acc: 0.8692 - mDice: 0.0520 - val_loss: 4.9333 - val_acc: 0.9047 - val_mDice: 0.0583

Epoch 00013: val_mDice improved from 0.04198 to 0.05827, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 12s - loss: 5.9157 - acc: 0.8692 - mDice: 0.0592 - val_loss: 4.6680 - val_acc: 0.9047 - val_mDice: 0.0703

Epoch 00014: val_mDice improved from 0.05827 to 0.07030, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 12s - loss: 5.6534 - acc: 0.8692 - mDice: 0.0679 - val_loss: 4.6154 - val_acc: 0.9047 - val_mDice: 0.0763

Epoch 00015: val_mDice improved from 0.07030 to 0.07633, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 12s - loss: 5.3864 - acc: 0.8691 - mDice: 0.0774 - val_loss: 5.2844 - val_acc: 0.9047 - val_mDice: 0.0603

Epoch 00016: val_mDice did not improve from 0.07633
Epoch 17/300
 - 12s - loss: 5.1344 - acc: 0.8691 - mDice: 0.0872 - val_loss: 4.2683 - val_acc: 0.9047 - val_mDice: 0.0999

Epoch 00017: val_mDice improved from 0.07633 to 0.09994, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 12s - loss: 4.8919 - acc: 0.8691 - mDice: 0.0977 - val_loss: 4.1153 - val_acc: 0.9046 - val_mDice: 0.1154

Epoch 00018: val_mDice improved from 0.09994 to 0.11543, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 12s - loss: 4.6634 - acc: 0.8695 - mDice: 0.1110 - val_loss: 4.0021 - val_acc: 0.9044 - val_mDice: 0.1327

Epoch 00019: val_mDice improved from 0.11543 to 0.13275, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 12s - loss: 4.4418 - acc: 0.8704 - mDice: 0.1279 - val_loss: 4.1987 - val_acc: 0.9038 - val_mDice: 0.1400

Epoch 00020: val_mDice improved from 0.13275 to 0.14003, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 12s - loss: 4.2491 - acc: 0.8717 - mDice: 0.1449 - val_loss: 3.7913 - val_acc: 0.9047 - val_mDice: 0.1702

Epoch 00021: val_mDice improved from 0.14003 to 0.17018, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 12s - loss: 4.0799 - acc: 0.8734 - mDice: 0.1612 - val_loss: 4.2079 - val_acc: 0.9050 - val_mDice: 0.1656

Epoch 00022: val_mDice did not improve from 0.17018
Epoch 23/300
 - 12s - loss: 3.9194 - acc: 0.8754 - mDice: 0.1765 - val_loss: 3.7141 - val_acc: 0.9097 - val_mDice: 0.2021

Epoch 00023: val_mDice improved from 0.17018 to 0.20213, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 12s - loss: 3.7570 - acc: 0.8779 - mDice: 0.1941 - val_loss: 3.4895 - val_acc: 0.9113 - val_mDice: 0.2173

Epoch 00024: val_mDice improved from 0.20213 to 0.21729, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 12s - loss: 3.6011 - acc: 0.8804 - mDice: 0.2120 - val_loss: 3.4181 - val_acc: 0.9124 - val_mDice: 0.2395

Epoch 00025: val_mDice improved from 0.21729 to 0.23945, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 12s - loss: 3.4451 - acc: 0.8832 - mDice: 0.2311 - val_loss: 3.3062 - val_acc: 0.9170 - val_mDice: 0.2612

Epoch 00026: val_mDice improved from 0.23945 to 0.26122, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 12s - loss: 3.3113 - acc: 0.8863 - mDice: 0.2492 - val_loss: 3.2515 - val_acc: 0.9193 - val_mDice: 0.2719

Epoch 00027: val_mDice improved from 0.26122 to 0.27192, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 12s - loss: 3.1978 - acc: 0.8886 - mDice: 0.2657 - val_loss: 3.0015 - val_acc: 0.9216 - val_mDice: 0.2981

Epoch 00028: val_mDice improved from 0.27192 to 0.29810, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 12s - loss: 3.0940 - acc: 0.8908 - mDice: 0.2807 - val_loss: 3.0452 - val_acc: 0.9194 - val_mDice: 0.2972

Epoch 00029: val_mDice did not improve from 0.29810
Epoch 30/300
 - 12s - loss: 3.0124 - acc: 0.8928 - mDice: 0.2933 - val_loss: 3.1150 - val_acc: 0.9244 - val_mDice: 0.3112

Epoch 00030: val_mDice improved from 0.29810 to 0.31123, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 12s - loss: 2.9194 - acc: 0.8952 - mDice: 0.3086 - val_loss: 3.1845 - val_acc: 0.9255 - val_mDice: 0.3190

Epoch 00031: val_mDice improved from 0.31123 to 0.31904, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 12s - loss: 2.8466 - acc: 0.8970 - mDice: 0.3206 - val_loss: 2.9181 - val_acc: 0.9267 - val_mDice: 0.3388

Epoch 00032: val_mDice improved from 0.31904 to 0.33884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 12s - loss: 2.7845 - acc: 0.8984 - mDice: 0.3312 - val_loss: 2.9307 - val_acc: 0.9221 - val_mDice: 0.3406

Epoch 00033: val_mDice improved from 0.33884 to 0.34060, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 12s - loss: 2.7185 - acc: 0.9003 - mDice: 0.3429 - val_loss: 2.9176 - val_acc: 0.9258 - val_mDice: 0.3481

Epoch 00034: val_mDice improved from 0.34060 to 0.34815, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 12s - loss: 2.6643 - acc: 0.9016 - mDice: 0.3529 - val_loss: 3.3350 - val_acc: 0.9273 - val_mDice: 0.3299

Epoch 00035: val_mDice did not improve from 0.34815
Epoch 36/300
 - 12s - loss: 2.6081 - acc: 0.9032 - mDice: 0.3632 - val_loss: 2.9356 - val_acc: 0.9293 - val_mDice: 0.3590

Epoch 00036: val_mDice improved from 0.34815 to 0.35895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 12s - loss: 2.5583 - acc: 0.9049 - mDice: 0.3740 - val_loss: 3.1765 - val_acc: 0.9289 - val_mDice: 0.3598

Epoch 00037: val_mDice improved from 0.35895 to 0.35979, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 12s - loss: 2.5028 - acc: 0.9066 - mDice: 0.3838 - val_loss: 2.8872 - val_acc: 0.9221 - val_mDice: 0.3760

Epoch 00038: val_mDice improved from 0.35979 to 0.37603, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 12s - loss: 2.4566 - acc: 0.9083 - mDice: 0.3925 - val_loss: 2.9921 - val_acc: 0.9239 - val_mDice: 0.3637

Epoch 00039: val_mDice did not improve from 0.37603
Epoch 40/300
 - 12s - loss: 2.4338 - acc: 0.9097 - mDice: 0.3989 - val_loss: 2.9882 - val_acc: 0.9292 - val_mDice: 0.3809

Epoch 00040: val_mDice improved from 0.37603 to 0.38089, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 12s - loss: 2.3888 - acc: 0.9111 - mDice: 0.4068 - val_loss: 3.0134 - val_acc: 0.9301 - val_mDice: 0.3844

Epoch 00041: val_mDice improved from 0.38089 to 0.38436, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 12s - loss: 2.3436 - acc: 0.9129 - mDice: 0.4156 - val_loss: 3.0040 - val_acc: 0.9321 - val_mDice: 0.3875

Epoch 00042: val_mDice improved from 0.38436 to 0.38750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 12s - loss: 2.3123 - acc: 0.9139 - mDice: 0.4221 - val_loss: 2.9857 - val_acc: 0.9250 - val_mDice: 0.3832

Epoch 00043: val_mDice did not improve from 0.38750
Epoch 44/300
 - 12s - loss: 2.2760 - acc: 0.9153 - mDice: 0.4295 - val_loss: 2.9659 - val_acc: 0.9264 - val_mDice: 0.3893

Epoch 00044: val_mDice improved from 0.38750 to 0.38928, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 12s - loss: 2.2428 - acc: 0.9165 - mDice: 0.4365 - val_loss: 2.8862 - val_acc: 0.9299 - val_mDice: 0.4034

Epoch 00045: val_mDice improved from 0.38928 to 0.40339, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 12s - loss: 2.2172 - acc: 0.9176 - mDice: 0.4428 - val_loss: 2.8240 - val_acc: 0.9319 - val_mDice: 0.4110

Epoch 00046: val_mDice improved from 0.40339 to 0.41101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 12s - loss: 2.1901 - acc: 0.9186 - mDice: 0.4486 - val_loss: 2.7567 - val_acc: 0.9291 - val_mDice: 0.4159

Epoch 00047: val_mDice improved from 0.41101 to 0.41591, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 12s - loss: 2.1640 - acc: 0.9199 - mDice: 0.4547 - val_loss: 3.1134 - val_acc: 0.9345 - val_mDice: 0.4075

Epoch 00048: val_mDice did not improve from 0.41591
Epoch 49/300
 - 12s - loss: 2.1454 - acc: 0.9203 - mDice: 0.4588 - val_loss: 2.8617 - val_acc: 0.9281 - val_mDice: 0.4101

Epoch 00049: val_mDice did not improve from 0.41591
Epoch 50/300
 - 12s - loss: 2.1127 - acc: 0.9215 - mDice: 0.4653 - val_loss: 2.9288 - val_acc: 0.9296 - val_mDice: 0.4114

Epoch 00050: val_mDice did not improve from 0.41591
Epoch 51/300
 - 12s - loss: 2.0971 - acc: 0.9220 - mDice: 0.4692 - val_loss: 3.0406 - val_acc: 0.9284 - val_mDice: 0.4039

Epoch 00051: val_mDice did not improve from 0.41591
Epoch 52/300
 - 12s - loss: 2.0736 - acc: 0.9227 - mDice: 0.4741 - val_loss: 3.3614 - val_acc: 0.9339 - val_mDice: 0.4064

Epoch 00052: val_mDice did not improve from 0.41591
Epoch 53/300
 - 12s - loss: 2.0470 - acc: 0.9238 - mDice: 0.4803 - val_loss: 2.8285 - val_acc: 0.9273 - val_mDice: 0.4235

Epoch 00053: val_mDice improved from 0.41591 to 0.42354, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 12s - loss: 2.0297 - acc: 0.9244 - mDice: 0.4845 - val_loss: 3.1137 - val_acc: 0.9356 - val_mDice: 0.4221

Epoch 00054: val_mDice did not improve from 0.42354
Epoch 55/300
 - 12s - loss: 2.0090 - acc: 0.9249 - mDice: 0.4889 - val_loss: 2.9403 - val_acc: 0.9362 - val_mDice: 0.4333

Epoch 00055: val_mDice improved from 0.42354 to 0.43334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 12s - loss: 1.9884 - acc: 0.9257 - mDice: 0.4933 - val_loss: 3.1053 - val_acc: 0.9379 - val_mDice: 0.4289

Epoch 00056: val_mDice did not improve from 0.43334
Epoch 57/300
 - 12s - loss: 1.9689 - acc: 0.9264 - mDice: 0.4981 - val_loss: 3.1263 - val_acc: 0.9317 - val_mDice: 0.4202

Epoch 00057: val_mDice did not improve from 0.43334
Epoch 58/300
 - 12s - loss: 1.9589 - acc: 0.9266 - mDice: 0.5008 - val_loss: 3.1825 - val_acc: 0.9267 - val_mDice: 0.4156

Epoch 00058: val_mDice did not improve from 0.43334
Epoch 59/300
 - 12s - loss: 1.9347 - acc: 0.9273 - mDice: 0.5058 - val_loss: 3.1423 - val_acc: 0.9379 - val_mDice: 0.4326

Epoch 00059: val_mDice did not improve from 0.43334
Epoch 60/300
 - 12s - loss: 1.9181 - acc: 0.9279 - mDice: 0.5098 - val_loss: 2.9338 - val_acc: 0.9337 - val_mDice: 0.4315

Epoch 00060: val_mDice did not improve from 0.43334
Epoch 61/300
 - 12s - loss: 1.9108 - acc: 0.9280 - mDice: 0.5115 - val_loss: 3.2719 - val_acc: 0.9307 - val_mDice: 0.4135

Epoch 00061: val_mDice did not improve from 0.43334
Epoch 62/300
 - 12s - loss: 1.8941 - acc: 0.9285 - mDice: 0.5155 - val_loss: 2.8426 - val_acc: 0.9365 - val_mDice: 0.4445

Epoch 00062: val_mDice improved from 0.43334 to 0.44452, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 12s - loss: 1.8772 - acc: 0.9289 - mDice: 0.5191 - val_loss: 3.0895 - val_acc: 0.9348 - val_mDice: 0.4342

Epoch 00063: val_mDice did not improve from 0.44452
Epoch 64/300
 - 12s - loss: 1.8665 - acc: 0.9293 - mDice: 0.5220 - val_loss: 2.9096 - val_acc: 0.9339 - val_mDice: 0.4347

Epoch 00064: val_mDice did not improve from 0.44452
Epoch 65/300
 - 12s - loss: 1.8482 - acc: 0.9297 - mDice: 0.5252 - val_loss: 3.0180 - val_acc: 0.9304 - val_mDice: 0.4343

Epoch 00065: val_mDice did not improve from 0.44452
Epoch 66/300
 - 12s - loss: 1.8350 - acc: 0.9300 - mDice: 0.5289 - val_loss: 3.0662 - val_acc: 0.9315 - val_mDice: 0.4342

Epoch 00066: val_mDice did not improve from 0.44452
Epoch 67/300
 - 12s - loss: 1.8309 - acc: 0.9302 - mDice: 0.5305 - val_loss: 3.1271 - val_acc: 0.9306 - val_mDice: 0.4290

Epoch 00067: val_mDice did not improve from 0.44452
Epoch 68/300
 - 12s - loss: 1.8174 - acc: 0.9306 - mDice: 0.5328 - val_loss: 2.9182 - val_acc: 0.9352 - val_mDice: 0.4479

Epoch 00068: val_mDice improved from 0.44452 to 0.44785, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 12s - loss: 1.8033 - acc: 0.9311 - mDice: 0.5362 - val_loss: 3.1632 - val_acc: 0.9343 - val_mDice: 0.4374

Epoch 00069: val_mDice did not improve from 0.44785
Epoch 70/300
 - 12s - loss: 1.7985 - acc: 0.9311 - mDice: 0.5369 - val_loss: 2.9709 - val_acc: 0.9349 - val_mDice: 0.4489

Epoch 00070: val_mDice improved from 0.44785 to 0.44895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 71/300
 - 12s - loss: 1.7799 - acc: 0.9316 - mDice: 0.5414 - val_loss: 3.2252 - val_acc: 0.9344 - val_mDice: 0.4395

Epoch 00071: val_mDice did not improve from 0.44895
Epoch 72/300
 - 12s - loss: 1.7687 - acc: 0.9318 - mDice: 0.5432 - val_loss: 3.3112 - val_acc: 0.9366 - val_mDice: 0.4354

Epoch 00072: val_mDice did not improve from 0.44895
Epoch 73/300
 - 12s - loss: 1.7601 - acc: 0.9322 - mDice: 0.5455 - val_loss: 3.0433 - val_acc: 0.9361 - val_mDice: 0.4521

Epoch 00073: val_mDice improved from 0.44895 to 0.45213, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 12s - loss: 1.7484 - acc: 0.9325 - mDice: 0.5486 - val_loss: 2.9945 - val_acc: 0.9349 - val_mDice: 0.4496

Epoch 00074: val_mDice did not improve from 0.45213
Epoch 75/300
 - 12s - loss: 1.7464 - acc: 0.9324 - mDice: 0.5496 - val_loss: 3.0107 - val_acc: 0.9362 - val_mDice: 0.4565

Epoch 00075: val_mDice improved from 0.45213 to 0.45651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 12s - loss: 1.7323 - acc: 0.9328 - mDice: 0.5527 - val_loss: 2.9659 - val_acc: 0.9319 - val_mDice: 0.4466

Epoch 00076: val_mDice did not improve from 0.45651
Epoch 77/300
 - 12s - loss: 1.7311 - acc: 0.9329 - mDice: 0.5527 - val_loss: 3.0204 - val_acc: 0.9360 - val_mDice: 0.4521

Epoch 00077: val_mDice did not improve from 0.45651
Epoch 78/300
 - 12s - loss: 1.7170 - acc: 0.9331 - mDice: 0.5558 - val_loss: 2.9067 - val_acc: 0.9394 - val_mDice: 0.4622

Epoch 00078: val_mDice improved from 0.45651 to 0.46225, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 12s - loss: 1.7035 - acc: 0.9335 - mDice: 0.5593 - val_loss: 3.0509 - val_acc: 0.9381 - val_mDice: 0.4565

Epoch 00079: val_mDice did not improve from 0.46225
Epoch 80/300
 - 12s - loss: 1.6938 - acc: 0.9337 - mDice: 0.5612 - val_loss: 3.0409 - val_acc: 0.9377 - val_mDice: 0.4551

Epoch 00080: val_mDice did not improve from 0.46225
Epoch 81/300
 - 12s - loss: 1.6918 - acc: 0.9338 - mDice: 0.5621 - val_loss: 3.0037 - val_acc: 0.9380 - val_mDice: 0.4556

Epoch 00081: val_mDice did not improve from 0.46225
Epoch 82/300
 - 12s - loss: 1.6889 - acc: 0.9337 - mDice: 0.5626 - val_loss: 3.1542 - val_acc: 0.9324 - val_mDice: 0.4357

Epoch 00082: val_mDice did not improve from 0.46225
Epoch 83/300
 - 12s - loss: 1.6763 - acc: 0.9338 - mDice: 0.5643 - val_loss: 3.1806 - val_acc: 0.9412 - val_mDice: 0.4533

Epoch 00083: val_mDice did not improve from 0.46225
Epoch 84/300
 - 12s - loss: 1.6686 - acc: 0.9342 - mDice: 0.5669 - val_loss: 3.0400 - val_acc: 0.9332 - val_mDice: 0.4484

Epoch 00084: val_mDice did not improve from 0.46225
Epoch 85/300
 - 12s - loss: 1.6605 - acc: 0.9343 - mDice: 0.5689 - val_loss: 3.1499 - val_acc: 0.9388 - val_mDice: 0.4548

Epoch 00085: val_mDice did not improve from 0.46225
Epoch 86/300
 - 12s - loss: 1.6516 - acc: 0.9345 - mDice: 0.5712 - val_loss: 3.1545 - val_acc: 0.9332 - val_mDice: 0.4354

Epoch 00086: val_mDice did not improve from 0.46225
Epoch 87/300
 - 12s - loss: 1.6462 - acc: 0.9347 - mDice: 0.5720 - val_loss: 3.0104 - val_acc: 0.9328 - val_mDice: 0.4614

Epoch 00087: val_mDice did not improve from 0.46225
Epoch 88/300
 - 12s - loss: 1.6394 - acc: 0.9349 - mDice: 0.5740 - val_loss: 3.1302 - val_acc: 0.9396 - val_mDice: 0.4586

Epoch 00088: val_mDice did not improve from 0.46225
Epoch 89/300
 - 12s - loss: 1.6452 - acc: 0.9347 - mDice: 0.5728 - val_loss: 3.1448 - val_acc: 0.9356 - val_mDice: 0.4551

Epoch 00089: val_mDice did not improve from 0.46225
Epoch 90/300
 - 12s - loss: 1.6361 - acc: 0.9350 - mDice: 0.5747 - val_loss: 3.0392 - val_acc: 0.9398 - val_mDice: 0.4640

Epoch 00090: val_mDice improved from 0.46225 to 0.46403, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 12s - loss: 1.6202 - acc: 0.9355 - mDice: 0.5784 - val_loss: 3.2572 - val_acc: 0.9376 - val_mDice: 0.4505

Epoch 00091: val_mDice did not improve from 0.46403
Epoch 92/300
 - 12s - loss: 1.6122 - acc: 0.9356 - mDice: 0.5807 - val_loss: 3.0969 - val_acc: 0.9386 - val_mDice: 0.4568

Epoch 00092: val_mDice did not improve from 0.46403
Epoch 93/300
 - 12s - loss: 1.6129 - acc: 0.9356 - mDice: 0.5805 - val_loss: 3.1486 - val_acc: 0.9372 - val_mDice: 0.4618

Epoch 00093: val_mDice did not improve from 0.46403
Epoch 94/300
 - 12s - loss: 1.5996 - acc: 0.9359 - mDice: 0.5834 - val_loss: 2.9994 - val_acc: 0.9384 - val_mDice: 0.4618

Epoch 00094: val_mDice did not improve from 0.46403
Epoch 95/300
 - 12s - loss: 1.5928 - acc: 0.9362 - mDice: 0.5849 - val_loss: 3.0315 - val_acc: 0.9398 - val_mDice: 0.4651

Epoch 00095: val_mDice improved from 0.46403 to 0.46510, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 12s - loss: 1.5877 - acc: 0.9363 - mDice: 0.5869 - val_loss: 3.0569 - val_acc: 0.9384 - val_mDice: 0.4548

Epoch 00096: val_mDice did not improve from 0.46510
Epoch 97/300
 - 12s - loss: 1.5825 - acc: 0.9364 - mDice: 0.5876 - val_loss: 3.0616 - val_acc: 0.9344 - val_mDice: 0.4506

Epoch 00097: val_mDice did not improve from 0.46510
Epoch 98/300
 - 12s - loss: 1.5870 - acc: 0.9363 - mDice: 0.5868 - val_loss: 3.0847 - val_acc: 0.9371 - val_mDice: 0.4697

Epoch 00098: val_mDice improved from 0.46510 to 0.46968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 12s - loss: 1.5731 - acc: 0.9366 - mDice: 0.5895 - val_loss: 3.3062 - val_acc: 0.9348 - val_mDice: 0.4480

Epoch 00099: val_mDice did not improve from 0.46968
Epoch 100/300
 - 12s - loss: 1.5688 - acc: 0.9368 - mDice: 0.5912 - val_loss: 2.9399 - val_acc: 0.9385 - val_mDice: 0.4654

Epoch 00100: val_mDice did not improve from 0.46968
Epoch 101/300
 - 12s - loss: 1.5641 - acc: 0.9369 - mDice: 0.5927 - val_loss: 3.0441 - val_acc: 0.9381 - val_mDice: 0.4667

Epoch 00101: val_mDice did not improve from 0.46968
Epoch 102/300
 - 12s - loss: 1.5689 - acc: 0.9368 - mDice: 0.5908 - val_loss: 2.9917 - val_acc: 0.9395 - val_mDice: 0.4648

Epoch 00102: val_mDice did not improve from 0.46968
Epoch 103/300
 - 12s - loss: 1.5571 - acc: 0.9371 - mDice: 0.5936 - val_loss: 3.0409 - val_acc: 0.9360 - val_mDice: 0.4668

Epoch 00103: val_mDice did not improve from 0.46968
Epoch 104/300
 - 12s - loss: 1.5541 - acc: 0.9372 - mDice: 0.5947 - val_loss: 3.1466 - val_acc: 0.9376 - val_mDice: 0.4620

Epoch 00104: val_mDice did not improve from 0.46968
Epoch 105/300
 - 12s - loss: 1.5428 - acc: 0.9376 - mDice: 0.5970 - val_loss: 3.1118 - val_acc: 0.9344 - val_mDice: 0.4440

Epoch 00105: val_mDice did not improve from 0.46968
Epoch 106/300
 - 12s - loss: 1.5419 - acc: 0.9376 - mDice: 0.5973 - val_loss: 3.2053 - val_acc: 0.9365 - val_mDice: 0.4638

Epoch 00106: val_mDice did not improve from 0.46968
Epoch 107/300
 - 12s - loss: 1.5431 - acc: 0.9377 - mDice: 0.5971 - val_loss: 3.1561 - val_acc: 0.9332 - val_mDice: 0.4575

Epoch 00107: val_mDice did not improve from 0.46968
Epoch 108/300
 - 12s - loss: 1.5319 - acc: 0.9379 - mDice: 0.5999 - val_loss: 3.0862 - val_acc: 0.9394 - val_mDice: 0.4663

Epoch 00108: val_mDice did not improve from 0.46968
Epoch 109/300
 - 12s - loss: 1.5272 - acc: 0.9380 - mDice: 0.6009 - val_loss: 3.2064 - val_acc: 0.9363 - val_mDice: 0.4645

Epoch 00109: val_mDice did not improve from 0.46968
Epoch 110/300
 - 12s - loss: 1.5190 - acc: 0.9383 - mDice: 0.6028 - val_loss: 3.1178 - val_acc: 0.9403 - val_mDice: 0.4663

Epoch 00110: val_mDice did not improve from 0.46968
Epoch 111/300
 - 12s - loss: 1.5184 - acc: 0.9383 - mDice: 0.6035 - val_loss: 3.2108 - val_acc: 0.9414 - val_mDice: 0.4642

Epoch 00111: val_mDice did not improve from 0.46968
Epoch 112/300
 - 12s - loss: 1.5160 - acc: 0.9384 - mDice: 0.6040 - val_loss: 3.2212 - val_acc: 0.9370 - val_mDice: 0.4568

Epoch 00112: val_mDice did not improve from 0.46968
Epoch 113/300
 - 12s - loss: 1.5110 - acc: 0.9383 - mDice: 0.6042 - val_loss: 3.0813 - val_acc: 0.9415 - val_mDice: 0.4687

Epoch 00113: val_mDice did not improve from 0.46968
Epoch 114/300
 - 12s - loss: 1.5055 - acc: 0.9386 - mDice: 0.6056 - val_loss: 3.0321 - val_acc: 0.9411 - val_mDice: 0.4688

Epoch 00114: val_mDice did not improve from 0.46968
Epoch 115/300
 - 12s - loss: 1.5016 - acc: 0.9388 - mDice: 0.6067 - val_loss: 3.1695 - val_acc: 0.9406 - val_mDice: 0.4666

Epoch 00115: val_mDice did not improve from 0.46968
Epoch 116/300
 - 12s - loss: 1.4937 - acc: 0.9389 - mDice: 0.6082 - val_loss: 3.1010 - val_acc: 0.9414 - val_mDice: 0.4702

Epoch 00116: val_mDice improved from 0.46968 to 0.47015, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 117/300
 - 12s - loss: 1.4969 - acc: 0.9388 - mDice: 0.6078 - val_loss: 3.2350 - val_acc: 0.9418 - val_mDice: 0.4686

Epoch 00117: val_mDice did not improve from 0.47015
Epoch 118/300
 - 12s - loss: 1.4883 - acc: 0.9392 - mDice: 0.6100 - val_loss: 3.1619 - val_acc: 0.9375 - val_mDice: 0.4644

Epoch 00118: val_mDice did not improve from 0.47015
Epoch 119/300
 - 12s - loss: 1.4865 - acc: 0.9392 - mDice: 0.6102 - val_loss: 3.0916 - val_acc: 0.9406 - val_mDice: 0.4720

Epoch 00119: val_mDice improved from 0.47015 to 0.47201, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 120/300
 - 12s - loss: 1.4844 - acc: 0.9393 - mDice: 0.6110 - val_loss: 2.9427 - val_acc: 0.9430 - val_mDice: 0.4787

Epoch 00120: val_mDice improved from 0.47201 to 0.47874, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 121/300
 - 12s - loss: 1.4831 - acc: 0.9393 - mDice: 0.6112 - val_loss: 3.1607 - val_acc: 0.9391 - val_mDice: 0.4606

Epoch 00121: val_mDice did not improve from 0.47874
Epoch 122/300
 - 12s - loss: 1.4892 - acc: 0.9392 - mDice: 0.6098 - val_loss: 3.1293 - val_acc: 0.9400 - val_mDice: 0.4722

Epoch 00122: val_mDice did not improve from 0.47874
Epoch 123/300
 - 12s - loss: 1.4753 - acc: 0.9393 - mDice: 0.6125 - val_loss: 3.1856 - val_acc: 0.9401 - val_mDice: 0.4696

Epoch 00123: val_mDice did not improve from 0.47874
Epoch 124/300
 - 12s - loss: 1.4696 - acc: 0.9395 - mDice: 0.6141 - val_loss: 3.4126 - val_acc: 0.9382 - val_mDice: 0.4466

Epoch 00124: val_mDice did not improve from 0.47874
Epoch 125/300
 - 12s - loss: 1.4676 - acc: 0.9395 - mDice: 0.6142 - val_loss: 3.1925 - val_acc: 0.9399 - val_mDice: 0.4703

Epoch 00125: val_mDice did not improve from 0.47874
Epoch 126/300
 - 12s - loss: 1.4665 - acc: 0.9395 - mDice: 0.6146 - val_loss: 3.0947 - val_acc: 0.9387 - val_mDice: 0.4702

Epoch 00126: val_mDice did not improve from 0.47874
Epoch 127/300
 - 12s - loss: 1.4669 - acc: 0.9396 - mDice: 0.6141 - val_loss: 3.0684 - val_acc: 0.9420 - val_mDice: 0.4832

Epoch 00127: val_mDice improved from 0.47874 to 0.48324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 128/300
 - 12s - loss: 1.4654 - acc: 0.9396 - mDice: 0.6158 - val_loss: 3.0639 - val_acc: 0.9369 - val_mDice: 0.4694

Epoch 00128: val_mDice did not improve from 0.48324
Epoch 129/300
 - 12s - loss: 1.4571 - acc: 0.9399 - mDice: 0.6171 - val_loss: 3.2375 - val_acc: 0.9366 - val_mDice: 0.4690

Epoch 00129: val_mDice did not improve from 0.48324
Epoch 130/300
 - 12s - loss: 1.4569 - acc: 0.9400 - mDice: 0.6171 - val_loss: 3.1145 - val_acc: 0.9401 - val_mDice: 0.4720

Epoch 00130: val_mDice did not improve from 0.48324
Epoch 131/300
 - 12s - loss: 1.4613 - acc: 0.9398 - mDice: 0.6162 - val_loss: 3.2364 - val_acc: 0.9398 - val_mDice: 0.4612

Epoch 00131: val_mDice did not improve from 0.48324
Epoch 132/300
 - 12s - loss: 1.4484 - acc: 0.9401 - mDice: 0.6191 - val_loss: 3.1328 - val_acc: 0.9399 - val_mDice: 0.4665

Epoch 00132: val_mDice did not improve from 0.48324
Epoch 133/300
 - 12s - loss: 1.4407 - acc: 0.9403 - mDice: 0.6210 - val_loss: 3.1599 - val_acc: 0.9428 - val_mDice: 0.4757

Epoch 00133: val_mDice did not improve from 0.48324
Epoch 134/300
 - 12s - loss: 1.4441 - acc: 0.9402 - mDice: 0.6206 - val_loss: 3.1625 - val_acc: 0.9368 - val_mDice: 0.4612

Epoch 00134: val_mDice did not improve from 0.48324
Epoch 135/300
 - 12s - loss: 1.4424 - acc: 0.9402 - mDice: 0.6206 - val_loss: 2.9531 - val_acc: 0.9386 - val_mDice: 0.4731

Epoch 00135: val_mDice did not improve from 0.48324
Epoch 136/300
 - 12s - loss: 1.4376 - acc: 0.9403 - mDice: 0.6217 - val_loss: 3.1936 - val_acc: 0.9360 - val_mDice: 0.4649

Epoch 00136: val_mDice did not improve from 0.48324
Epoch 137/300
 - 12s - loss: 1.4340 - acc: 0.9403 - mDice: 0.6227 - val_loss: 3.4002 - val_acc: 0.9428 - val_mDice: 0.4612

Epoch 00137: val_mDice did not improve from 0.48324
Epoch 138/300
 - 12s - loss: 1.4346 - acc: 0.9403 - mDice: 0.6227 - val_loss: 3.1197 - val_acc: 0.9357 - val_mDice: 0.4710

Epoch 00138: val_mDice did not improve from 0.48324
Epoch 139/300
 - 12s - loss: 1.4302 - acc: 0.9404 - mDice: 0.6238 - val_loss: 3.1066 - val_acc: 0.9413 - val_mDice: 0.4711

Epoch 00139: val_mDice did not improve from 0.48324
Epoch 140/300
 - 12s - loss: 1.4273 - acc: 0.9404 - mDice: 0.6244 - val_loss: 3.5060 - val_acc: 0.9371 - val_mDice: 0.4570

Epoch 00140: val_mDice did not improve from 0.48324
Epoch 141/300
 - 12s - loss: 1.4292 - acc: 0.9403 - mDice: 0.6236 - val_loss: 3.0460 - val_acc: 0.9378 - val_mDice: 0.4760

Epoch 00141: val_mDice did not improve from 0.48324
Epoch 142/300
 - 12s - loss: 1.4240 - acc: 0.9405 - mDice: 0.6255 - val_loss: 3.0943 - val_acc: 0.9409 - val_mDice: 0.4758

Epoch 00142: val_mDice did not improve from 0.48324
Epoch 143/300
 - 12s - loss: 1.4235 - acc: 0.9405 - mDice: 0.6254 - val_loss: 3.1629 - val_acc: 0.9441 - val_mDice: 0.4773

Epoch 00143: val_mDice did not improve from 0.48324
Epoch 144/300
 - 12s - loss: 1.4148 - acc: 0.9407 - mDice: 0.6271 - val_loss: 3.0437 - val_acc: 0.9407 - val_mDice: 0.4756

Epoch 00144: val_mDice did not improve from 0.48324
Epoch 145/300
 - 12s - loss: 1.4107 - acc: 0.9408 - mDice: 0.6283 - val_loss: 3.0692 - val_acc: 0.9386 - val_mDice: 0.4784

Epoch 00145: val_mDice did not improve from 0.48324
Epoch 146/300
 - 12s - loss: 1.4155 - acc: 0.9408 - mDice: 0.6271 - val_loss: 3.1626 - val_acc: 0.9428 - val_mDice: 0.4780

Epoch 00146: val_mDice did not improve from 0.48324
Epoch 147/300
 - 12s - loss: 1.4106 - acc: 0.9409 - mDice: 0.6284 - val_loss: 3.0977 - val_acc: 0.9399 - val_mDice: 0.4733

Epoch 00147: val_mDice did not improve from 0.48324
Epoch 148/300
 - 12s - loss: 1.4110 - acc: 0.9409 - mDice: 0.6282 - val_loss: 3.2500 - val_acc: 0.9392 - val_mDice: 0.4693

Epoch 00148: val_mDice did not improve from 0.48324
Epoch 149/300
 - 12s - loss: 1.4037 - acc: 0.9411 - mDice: 0.6298 - val_loss: 3.3625 - val_acc: 0.9414 - val_mDice: 0.4690

Epoch 00149: val_mDice did not improve from 0.48324
Epoch 150/300
 - 12s - loss: 1.3991 - acc: 0.9413 - mDice: 0.6314 - val_loss: 3.3752 - val_acc: 0.9424 - val_mDice: 0.4656

Epoch 00150: val_mDice did not improve from 0.48324
Epoch 151/300
 - 12s - loss: 1.4042 - acc: 0.9410 - mDice: 0.6301 - val_loss: 3.0514 - val_acc: 0.9433 - val_mDice: 0.4803

Epoch 00151: val_mDice did not improve from 0.48324
Epoch 152/300
 - 12s - loss: 1.3982 - acc: 0.9412 - mDice: 0.6310 - val_loss: 3.1915 - val_acc: 0.9388 - val_mDice: 0.4723

Epoch 00152: val_mDice did not improve from 0.48324
Epoch 153/300
 - 12s - loss: 1.3968 - acc: 0.9413 - mDice: 0.6321 - val_loss: 3.1610 - val_acc: 0.9386 - val_mDice: 0.4666

Epoch 00153: val_mDice did not improve from 0.48324
Epoch 154/300
 - 12s - loss: 1.3978 - acc: 0.9412 - mDice: 0.6309 - val_loss: 2.9777 - val_acc: 0.9410 - val_mDice: 0.4838

Epoch 00154: val_mDice improved from 0.48324 to 0.48383, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 155/300
 - 12s - loss: 1.3956 - acc: 0.9413 - mDice: 0.6323 - val_loss: 3.1129 - val_acc: 0.9420 - val_mDice: 0.4753

Epoch 00155: val_mDice did not improve from 0.48383
Epoch 156/300
 - 12s - loss: 1.3990 - acc: 0.9413 - mDice: 0.6312 - val_loss: 3.1172 - val_acc: 0.9426 - val_mDice: 0.4809

Epoch 00156: val_mDice did not improve from 0.48383
Epoch 157/300
 - 12s - loss: 1.3897 - acc: 0.9415 - mDice: 0.6342 - val_loss: 2.9805 - val_acc: 0.9423 - val_mDice: 0.4859

Epoch 00157: val_mDice improved from 0.48383 to 0.48592, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 158/300
 - 12s - loss: 1.3844 - acc: 0.9415 - mDice: 0.6350 - val_loss: 2.9928 - val_acc: 0.9427 - val_mDice: 0.4809

Epoch 00158: val_mDice did not improve from 0.48592
Epoch 159/300
 - 12s - loss: 1.3827 - acc: 0.9416 - mDice: 0.6353 - val_loss: 3.0465 - val_acc: 0.9442 - val_mDice: 0.4866

Epoch 00159: val_mDice improved from 0.48592 to 0.48663, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 160/300
 - 12s - loss: 1.3807 - acc: 0.9417 - mDice: 0.6360 - val_loss: 3.2413 - val_acc: 0.9424 - val_mDice: 0.4704

Epoch 00160: val_mDice did not improve from 0.48663
Epoch 161/300
 - 12s - loss: 1.3861 - acc: 0.9415 - mDice: 0.6346 - val_loss: 3.0831 - val_acc: 0.9417 - val_mDice: 0.4800

Epoch 00161: val_mDice did not improve from 0.48663
Epoch 162/300
 - 12s - loss: 1.3875 - acc: 0.9417 - mDice: 0.6344 - val_loss: 3.2127 - val_acc: 0.9415 - val_mDice: 0.4756

Epoch 00162: val_mDice did not improve from 0.48663
Epoch 163/300
 - 12s - loss: 1.3825 - acc: 0.9417 - mDice: 0.6354 - val_loss: 3.2108 - val_acc: 0.9416 - val_mDice: 0.4744

Epoch 00163: val_mDice did not improve from 0.48663
Epoch 164/300
 - 12s - loss: 1.3772 - acc: 0.9418 - mDice: 0.6372 - val_loss: 3.1067 - val_acc: 0.9423 - val_mDice: 0.4803

Epoch 00164: val_mDice did not improve from 0.48663
Epoch 165/300
 - 12s - loss: 1.3734 - acc: 0.9419 - mDice: 0.6370 - val_loss: 3.2695 - val_acc: 0.9420 - val_mDice: 0.4771

Epoch 00165: val_mDice did not improve from 0.48663
Epoch 166/300
 - 12s - loss: 1.3770 - acc: 0.9418 - mDice: 0.6369 - val_loss: 3.2361 - val_acc: 0.9405 - val_mDice: 0.4710

Epoch 00166: val_mDice did not improve from 0.48663
Epoch 167/300
 - 12s - loss: 1.3721 - acc: 0.9419 - mDice: 0.6375 - val_loss: 3.2557 - val_acc: 0.9426 - val_mDice: 0.4747

Epoch 00167: val_mDice did not improve from 0.48663
Epoch 168/300
 - 12s - loss: 1.3705 - acc: 0.9420 - mDice: 0.6384 - val_loss: 3.2175 - val_acc: 0.9432 - val_mDice: 0.4779

Epoch 00168: val_mDice did not improve from 0.48663
Epoch 169/300
 - 12s - loss: 1.3731 - acc: 0.9418 - mDice: 0.6381 - val_loss: 3.4917 - val_acc: 0.9433 - val_mDice: 0.4674

Epoch 00169: val_mDice did not improve from 0.48663
Epoch 170/300
 - 12s - loss: 1.3678 - acc: 0.9422 - mDice: 0.6390 - val_loss: 3.2541 - val_acc: 0.9424 - val_mDice: 0.4773

Epoch 00170: val_mDice did not improve from 0.48663
Epoch 171/300
 - 12s - loss: 1.3614 - acc: 0.9422 - mDice: 0.6411 - val_loss: 3.3011 - val_acc: 0.9437 - val_mDice: 0.4707

Epoch 00171: val_mDice did not improve from 0.48663
Epoch 172/300
 - 12s - loss: 1.3662 - acc: 0.9420 - mDice: 0.6394 - val_loss: 3.1888 - val_acc: 0.9422 - val_mDice: 0.4840

Epoch 00172: val_mDice did not improve from 0.48663
Epoch 173/300
 - 12s - loss: 1.3614 - acc: 0.9421 - mDice: 0.6409 - val_loss: 3.3341 - val_acc: 0.9405 - val_mDice: 0.4699

Epoch 00173: val_mDice did not improve from 0.48663
Epoch 174/300
 - 12s - loss: 1.3615 - acc: 0.9421 - mDice: 0.6406 - val_loss: 3.0038 - val_acc: 0.9417 - val_mDice: 0.4889

Epoch 00174: val_mDice improved from 0.48663 to 0.48887, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 175/300
 - 12s - loss: 1.3604 - acc: 0.9423 - mDice: 0.6409 - val_loss: 3.2157 - val_acc: 0.9427 - val_mDice: 0.4827

Epoch 00175: val_mDice did not improve from 0.48887
Epoch 176/300
 - 12s - loss: 1.3562 - acc: 0.9423 - mDice: 0.6423 - val_loss: 3.2730 - val_acc: 0.9433 - val_mDice: 0.4832

Epoch 00176: val_mDice did not improve from 0.48887
Epoch 177/300
 - 12s - loss: 1.3644 - acc: 0.9420 - mDice: 0.6403 - val_loss: 3.2513 - val_acc: 0.9396 - val_mDice: 0.4797

Epoch 00177: val_mDice did not improve from 0.48887
Epoch 178/300
 - 12s - loss: 1.3593 - acc: 0.9423 - mDice: 0.6419 - val_loss: 3.4081 - val_acc: 0.9425 - val_mDice: 0.4709

Epoch 00178: val_mDice did not improve from 0.48887
Epoch 179/300
 - 12s - loss: 1.3520 - acc: 0.9425 - mDice: 0.6433 - val_loss: 3.3788 - val_acc: 0.9425 - val_mDice: 0.4731

Epoch 00179: val_mDice did not improve from 0.48887
Epoch 180/300
 - 12s - loss: 1.3579 - acc: 0.9421 - mDice: 0.6418 - val_loss: 3.2535 - val_acc: 0.9403 - val_mDice: 0.4808

Epoch 00180: val_mDice did not improve from 0.48887
Epoch 181/300
 - 12s - loss: 1.3515 - acc: 0.9425 - mDice: 0.6433 - val_loss: 3.0484 - val_acc: 0.9387 - val_mDice: 0.4793

Epoch 00181: val_mDice did not improve from 0.48887
Epoch 182/300
 - 12s - loss: 1.3520 - acc: 0.9424 - mDice: 0.6432 - val_loss: 3.4053 - val_acc: 0.9408 - val_mDice: 0.4712

Epoch 00182: val_mDice did not improve from 0.48887
Epoch 183/300
 - 12s - loss: 1.3487 - acc: 0.9426 - mDice: 0.6445 - val_loss: 3.0942 - val_acc: 0.9376 - val_mDice: 0.4728

Epoch 00183: val_mDice did not improve from 0.48887
Epoch 184/300
 - 12s - loss: 1.3525 - acc: 0.9424 - mDice: 0.6430 - val_loss: 3.2001 - val_acc: 0.9390 - val_mDice: 0.4705

Epoch 00184: val_mDice did not improve from 0.48887
Epoch 185/300
 - 12s - loss: 1.3401 - acc: 0.9428 - mDice: 0.6465 - val_loss: 3.0745 - val_acc: 0.9426 - val_mDice: 0.4848

Epoch 00185: val_mDice did not improve from 0.48887
Epoch 186/300
 - 12s - loss: 1.3415 - acc: 0.9427 - mDice: 0.6464 - val_loss: 3.0099 - val_acc: 0.9412 - val_mDice: 0.4832

Epoch 00186: val_mDice did not improve from 0.48887
Epoch 187/300
 - 12s - loss: 1.3458 - acc: 0.9426 - mDice: 0.6449 - val_loss: 3.3242 - val_acc: 0.9414 - val_mDice: 0.4733

Epoch 00187: val_mDice did not improve from 0.48887
Epoch 188/300
 - 12s - loss: 1.3445 - acc: 0.9426 - mDice: 0.6446 - val_loss: 3.3724 - val_acc: 0.9407 - val_mDice: 0.4711

Epoch 00188: val_mDice did not improve from 0.48887
Epoch 189/300
 - 12s - loss: 1.3421 - acc: 0.9426 - mDice: 0.6464 - val_loss: 3.3387 - val_acc: 0.9419 - val_mDice: 0.4776

Epoch 00189: val_mDice did not improve from 0.48887
Epoch 190/300
 - 12s - loss: 1.3366 - acc: 0.9429 - mDice: 0.6470 - val_loss: 3.1135 - val_acc: 0.9383 - val_mDice: 0.4781

Epoch 00190: val_mDice did not improve from 0.48887
Epoch 191/300
 - 12s - loss: 1.3382 - acc: 0.9428 - mDice: 0.6470 - val_loss: 3.0990 - val_acc: 0.9435 - val_mDice: 0.4859

Epoch 00191: val_mDice did not improve from 0.48887
Epoch 192/300
 - 12s - loss: 1.3418 - acc: 0.9426 - mDice: 0.6457 - val_loss: 3.0762 - val_acc: 0.9438 - val_mDice: 0.4935

Epoch 00192: val_mDice improved from 0.48887 to 0.49351, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 193/300
 - 12s - loss: 1.3345 - acc: 0.9429 - mDice: 0.6472 - val_loss: 3.0377 - val_acc: 0.9431 - val_mDice: 0.4838

Epoch 00193: val_mDice did not improve from 0.49351
Epoch 194/300
 - 12s - loss: 1.3419 - acc: 0.9428 - mDice: 0.6462 - val_loss: 2.9944 - val_acc: 0.9401 - val_mDice: 0.4829

Epoch 00194: val_mDice did not improve from 0.49351
Epoch 195/300
 - 12s - loss: 1.3353 - acc: 0.9428 - mDice: 0.6471 - val_loss: 3.3541 - val_acc: 0.9411 - val_mDice: 0.4722

Epoch 00195: val_mDice did not improve from 0.49351
Epoch 196/300
 - 12s - loss: 1.3367 - acc: 0.9427 - mDice: 0.6472 - val_loss: 3.2307 - val_acc: 0.9399 - val_mDice: 0.4716

Epoch 00196: val_mDice did not improve from 0.49351
Epoch 197/300
 - 12s - loss: 1.3289 - acc: 0.9430 - mDice: 0.6487 - val_loss: 3.0763 - val_acc: 0.9415 - val_mDice: 0.4770

Epoch 00197: val_mDice did not improve from 0.49351
Epoch 198/300
 - 12s - loss: 1.3363 - acc: 0.9429 - mDice: 0.6469 - val_loss: 3.0907 - val_acc: 0.9384 - val_mDice: 0.4764

Epoch 00198: val_mDice did not improve from 0.49351
Epoch 199/300
 - 12s - loss: 1.3278 - acc: 0.9431 - mDice: 0.6496 - val_loss: 3.0466 - val_acc: 0.9405 - val_mDice: 0.4849

Epoch 00199: val_mDice did not improve from 0.49351
Epoch 200/300
 - 12s - loss: 1.3308 - acc: 0.9429 - mDice: 0.6486 - val_loss: 3.0807 - val_acc: 0.9418 - val_mDice: 0.4867

Epoch 00200: val_mDice did not improve from 0.49351
Epoch 201/300
 - 12s - loss: 1.3287 - acc: 0.9431 - mDice: 0.6491 - val_loss: 3.2508 - val_acc: 0.9403 - val_mDice: 0.4821

Epoch 00201: val_mDice did not improve from 0.49351
Epoch 202/300
 - 12s - loss: 1.3301 - acc: 0.9431 - mDice: 0.6490 - val_loss: 3.0135 - val_acc: 0.9418 - val_mDice: 0.4867

Epoch 00202: val_mDice did not improve from 0.49351
Epoch 203/300
 - 12s - loss: 1.3242 - acc: 0.9431 - mDice: 0.6499 - val_loss: 3.0760 - val_acc: 0.9436 - val_mDice: 0.4878

Epoch 00203: val_mDice did not improve from 0.49351
Epoch 204/300
 - 12s - loss: 1.3209 - acc: 0.9432 - mDice: 0.6514 - val_loss: 3.0520 - val_acc: 0.9406 - val_mDice: 0.4799

Epoch 00204: val_mDice did not improve from 0.49351
Epoch 205/300
 - 12s - loss: 1.3213 - acc: 0.9433 - mDice: 0.6511 - val_loss: 3.1740 - val_acc: 0.9371 - val_mDice: 0.4762

Epoch 00205: val_mDice did not improve from 0.49351
Epoch 206/300
 - 12s - loss: 1.3241 - acc: 0.9432 - mDice: 0.6504 - val_loss: 3.2593 - val_acc: 0.9433 - val_mDice: 0.4820

Epoch 00206: val_mDice did not improve from 0.49351
Epoch 207/300
 - 12s - loss: 1.3244 - acc: 0.9431 - mDice: 0.6502 - val_loss: 3.1760 - val_acc: 0.9382 - val_mDice: 0.4720

Epoch 00207: val_mDice did not improve from 0.49351
Epoch 208/300
 - 12s - loss: 1.3208 - acc: 0.9433 - mDice: 0.6510 - val_loss: 3.0543 - val_acc: 0.9433 - val_mDice: 0.4846

Epoch 00208: val_mDice did not improve from 0.49351
Epoch 209/300
 - 12s - loss: 1.3167 - acc: 0.9434 - mDice: 0.6521 - val_loss: 3.0901 - val_acc: 0.9426 - val_mDice: 0.4914

Epoch 00209: val_mDice did not improve from 0.49351
Epoch 210/300
 - 12s - loss: 1.3181 - acc: 0.9434 - mDice: 0.6518 - val_loss: 3.1329 - val_acc: 0.9384 - val_mDice: 0.4752

Epoch 00210: val_mDice did not improve from 0.49351
Epoch 211/300
 - 12s - loss: 1.3192 - acc: 0.9434 - mDice: 0.6513 - val_loss: 3.1160 - val_acc: 0.9435 - val_mDice: 0.4891

Epoch 00211: val_mDice did not improve from 0.49351
Epoch 212/300
 - 12s - loss: 1.3205 - acc: 0.9432 - mDice: 0.6512 - val_loss: 3.2860 - val_acc: 0.9421 - val_mDice: 0.4806

Epoch 00212: val_mDice did not improve from 0.49351
Epoch 213/300
 - 12s - loss: 1.3139 - acc: 0.9435 - mDice: 0.6527 - val_loss: 3.0789 - val_acc: 0.9394 - val_mDice: 0.4847

Epoch 00213: val_mDice did not improve from 0.49351
Epoch 214/300
 - 12s - loss: 1.3131 - acc: 0.9435 - mDice: 0.6532 - val_loss: 3.1276 - val_acc: 0.9425 - val_mDice: 0.4884

Epoch 00214: val_mDice did not improve from 0.49351
Epoch 215/300
 - 12s - loss: 1.3199 - acc: 0.9435 - mDice: 0.6513 - val_loss: 3.3100 - val_acc: 0.9399 - val_mDice: 0.4782

Epoch 00215: val_mDice did not improve from 0.49351
Epoch 216/300
 - 12s - loss: 1.3143 - acc: 0.9435 - mDice: 0.6526 - val_loss: 3.0283 - val_acc: 0.9423 - val_mDice: 0.4921

Epoch 00216: val_mDice did not improve from 0.49351
Epoch 217/300
 - 12s - loss: 1.3156 - acc: 0.9435 - mDice: 0.6527 - val_loss: 3.4089 - val_acc: 0.9419 - val_mDice: 0.4724

Epoch 00217: val_mDice did not improve from 0.49351
Epoch 218/300
 - 12s - loss: 1.3095 - acc: 0.9437 - mDice: 0.6538 - val_loss: 3.5095 - val_acc: 0.9403 - val_mDice: 0.4659

Epoch 00218: val_mDice did not improve from 0.49351
Epoch 219/300
 - 12s - loss: 1.3083 - acc: 0.9436 - mDice: 0.6537 - val_loss: 3.0915 - val_acc: 0.9404 - val_mDice: 0.4892

Epoch 00219: val_mDice did not improve from 0.49351
Epoch 220/300
 - 12s - loss: 1.3055 - acc: 0.9436 - mDice: 0.6545 - val_loss: 3.3565 - val_acc: 0.9411 - val_mDice: 0.4738

Epoch 00220: val_mDice did not improve from 0.49351
Epoch 221/300
 - 12s - loss: 1.3108 - acc: 0.9436 - mDice: 0.6539 - val_loss: 3.3588 - val_acc: 0.9393 - val_mDice: 0.4715

Epoch 00221: val_mDice did not improve from 0.49351
Epoch 222/300
 - 12s - loss: 1.3078 - acc: 0.9438 - mDice: 0.6544 - val_loss: 3.1808 - val_acc: 0.9437 - val_mDice: 0.4874

Epoch 00222: val_mDice did not improve from 0.49351
Restoring model weights from the end of the best epoch
Epoch 00222: early stopping
{'val_loss': [83.10041268666585, 39.2918487844013, 18.463282156558265, 11.096933633089066, 8.889081925153732, 7.47575194353149, 6.26660512032963, 5.859821202499526, 5.5783601596596695, 5.470710954673233, 5.572342562090073, 5.761345867865852, 4.933315119040864, 4.667996460305793, 4.6153569633052465, 5.2844337205446905, 4.268288991635754, 4.115299979756985, 4.002117419349296, 4.198672740409772, 3.7913197794308267, 4.2078750272769305, 3.7140562230987206, 3.4895419867797974, 3.418101427677487, 3.306204606113689, 3.251455158970895, 3.0014907141615237, 3.045197105744765, 3.115012175359187, 3.184466984921268, 2.9180636737673056, 2.9306889207856286, 2.917605293160748, 3.334971956775657, 2.935559396099831, 3.1765112191704765, 2.8871576863207986, 2.9921356997497024, 2.9882330243874873, 3.0134321468483125, 3.0040319035539316, 2.9856728792101856, 2.96589124007594, 2.8861532021047815, 2.8239868520537303, 2.7567459644217576, 3.1133894347363995, 2.8616972551902844, 2.928797496349684, 3.0405706207578382, 3.3613995430281474, 2.8284675172929252, 3.113691772689067, 2.9402975378380645, 3.1052512269733206, 3.126321589263777, 3.1824923431323398, 3.142256680122089, 2.933825327748699, 3.2719400598933652, 2.8425791490645635, 3.089531808248943, 2.909621839433731, 3.017987973965882, 3.066219831639457, 3.127095417784793, 2.9181753471937206, 3.1632153556149984, 2.9709235763931203, 3.225222932751335, 3.311196182810125, 3.043334249806191, 2.994466351300833, 3.0107051856106235, 2.965912783429736, 3.020368146200088, 2.9066897480349456, 3.0509244332178715, 3.0408515180904594, 3.003653929568827, 3.154211296522546, 3.1806254655211452, 3.0400350804085887, 3.1498791397371817, 3.154549273773141, 3.0103588098482716, 3.130199252206477, 3.14479572933522, 3.0391561265901794, 3.2571946535525576, 3.0969198320859244, 3.148584513747621, 2.999365288298577, 3.0315417752024674, 3.0568605375564877, 3.0615752129372034, 3.0847197554207275, 3.3062483054097918, 2.939886591729841, 3.0441266884362057, 2.9916703789716674, 3.040918890963353, 3.1465555170462247, 3.1117982222876024, 3.2053275683539963, 3.1561233788684366, 3.0861778337331045, 3.2064487482199358, 3.117786630921598, 3.210768391200829, 3.221159802306266, 3.0812543270932067, 3.032089159690908, 3.1695049977861345, 3.1009564058899524, 3.234962445484208, 3.161858587104472, 3.0916240299785778, 2.942740442076077, 3.1607330274280336, 3.1292952313974856, 3.1855734587159183, 3.412602780032016, 3.1924625694796087, 3.094699982598069, 3.0684265522064553, 3.0639361131803264, 3.237508150150201, 3.114536844127412, 3.2363743644340763, 3.1328040125469365, 3.15992873574474, 3.1625110658683946, 2.9530723957522285, 3.1936269137077034, 3.400227956518176, 3.119731998825002, 3.1066433002595746, 3.50598806244809, 3.0460065224296633, 3.094338490450311, 3.1628501631230828, 3.043665341744643, 3.0691722199276446, 3.1626115206717733, 3.0976747958255664, 3.250038392664421, 3.362450579513929, 3.375243490251402, 3.0513939223961817, 3.1914613469991657, 3.161012410008836, 2.9777065932839397, 3.112865474208125, 3.1171881460274258, 2.98045430764822, 2.9927561894014834, 3.0464838176211786, 3.2413449327550117, 3.0831342622682096, 3.2126524201594293, 3.2107741404546513, 3.106723267008506, 3.269483727846472, 3.2361356775010273, 3.255749179094675, 3.2175009288780747, 3.4916776192507575, 3.254098450787188, 3.301104694671397, 3.1888478887932643, 3.3340683819814805, 3.003849989712416, 3.215650382562585, 3.272981603496841, 3.2512637258374264, 3.408091973535539, 3.3788446348293553, 3.2534609057807495, 3.048405953372518, 3.4052752888362323, 3.094152011487278, 3.2001492609048174, 3.074515869087052, 3.009888183724667, 3.324166494244266, 3.372392632555039, 3.33869304468057, 3.1135491974252676, 3.098970624152571, 3.076165367671776, 3.0376973295185183, 2.9943681042197916, 3.3540757053664754, 3.230727105046667, 3.0763253020122647, 3.0907277134247124, 3.046630660781548, 3.0807271096411917, 3.2507919904199385, 3.013494488428391, 3.0759503263536665, 3.0519796017823473, 3.173970072352815, 3.259332217231748, 3.176025478036276, 3.0543331156839573, 3.09005290700034, 3.1329039892048707, 3.1160026919540194, 3.285954227094494, 3.0789018637456356, 3.1276164392364167, 3.310002366058706, 3.0283095368317197, 3.408944814088976, 3.509495624873255, 3.0914718247284845, 3.3565266857455884, 3.3587603031081104, 3.180805312691345], 'val_acc': [0.6604441489492144, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9046130719639006, 0.9044047395388285, 0.9037866109893435, 0.9046840866406759, 0.9049702457019261, 0.9097366957437425, 0.9112591459637597, 0.912364954040164, 0.9170489765348888, 0.9193406615938459, 0.921568234761556, 0.919436806724185, 0.9244436820348104, 0.9255013976778302, 0.9266757993471055, 0.9221130836577642, 0.9257600534529913, 0.9272550486382984, 0.9292902946472168, 0.928912565821693, 0.9221085423514956, 0.9239011208216349, 0.9292285044987997, 0.9300915627252488, 0.932069602466765, 0.9249839924630665, 0.9264171378953117, 0.9298672250338963, 0.9318979070300147, 0.9291117475146339, 0.9344574468476432, 0.9281158532415118, 0.9295558759144374, 0.9284180317606244, 0.9339239909535363, 0.9273099615460351, 0.9356021001225426, 0.9361767626944042, 0.9378708799680074, 0.9316598120189848, 0.926746788479033, 0.9379212317012605, 0.9337271082968939, 0.9306799230121431, 0.936462918917338, 0.9348397396859669, 0.9339171165511722, 0.9303892084530422, 0.9315476389158339, 0.9305769091560727, 0.9352037793114072, 0.9342605329695202, 0.9348695022719247, 0.9343956198011126, 0.9365613290241787, 0.9360805948575338, 0.9349290217672076, 0.9361973262968517, 0.9318726971035912, 0.9359890080633617, 0.9394184947013855, 0.9381158153216044, 0.9376991731779916, 0.9380448801176888, 0.9324427672794887, 0.9411927802222115, 0.9331936694326854, 0.93879805292402, 0.933216603029342, 0.9327747140611921, 0.9395672934395927, 0.9356066868418739, 0.9398443301518759, 0.937630485920679, 0.9386103465443566, 0.9372229604494005, 0.9384019999277025, 0.9397962547483898, 0.9384203099069142, 0.934434501897721, 0.9370718683515277, 0.9348054159255255, 0.9384912905238924, 0.9381158351898193, 0.9395054777463278, 0.9359913014230274, 0.9375618185315814, 0.9344024459520975, 0.9365224270593553, 0.9331570579892113, 0.9394436762446449, 0.9363346979731605, 0.9403273576781863, 0.941444612684704, 0.9369826118151346, 0.9414903635070437, 0.9411012132962545, 0.940608955564953, 0.9414194084349132, 0.9417857045219058, 0.9374954189573016, 0.9406479199727377, 0.9429807492664882, 0.9390865195365179, 0.9399519108590626, 0.9401305119196574, 0.9381982854434422, 0.9398580335435414, 0.9386996428171793, 0.9420146629923866, 0.9369070558320909, 0.9365819777761187, 0.9401327683812096, 0.9397687968753633, 0.9398557600520906, 0.9427632973307655, 0.9367971420288086, 0.9386057513100761, 0.9359638463883173, 0.9428388135773795, 0.935737152894338, 0.941259176958175, 0.9370558432170323, 0.9377701310884385, 0.9409363468488058, 0.9441323450633458, 0.940709684576307, 0.9385645730154855, 0.9427976040613084, 0.9399473269780477, 0.9391781233605885, 0.9413621908142453, 0.9423717828024001, 0.9433104481015887, 0.9388392908232552, 0.9386378044173831, 0.9409569331577846, 0.9419757212911334, 0.9425915933790661, 0.9422550314948672, 0.9427472721962702, 0.9441918361754644, 0.9423946880158924, 0.9417238831520081, 0.9415109923907689, 0.9416163166364034, 0.9422527608417329, 0.9419803193637303, 0.9404601908865429, 0.9426373896144685, 0.9432417580059597, 0.9432715035620189, 0.9424175875527518, 0.9437110849789211, 0.9422413110733032, 0.9405151265008109, 0.9416712238675072, 0.9427426741236732, 0.9432737940833682, 0.939631428037371, 0.9424793890544346, 0.9424518971216111, 0.9402907263664972, 0.9386538295518785, 0.9408424979164487, 0.9375572346505665, 0.9389766426313491, 0.9425961375236511, 0.9412065205119905, 0.9414262629690624, 0.9406822295415969, 0.9419001851763044, 0.9382623916580564, 0.9435141824540638, 0.9438003812517438, 0.9430814867927915, 0.9401282043684096, 0.941066841284434, 0.9399496572358268, 0.9414972379094079, 0.9383539585840135, 0.9405059473855155, 0.9417719642321268, 0.9402976177987599, 0.94184981073652, 0.9436263470422654, 0.9406135763440814, 0.9370787569454738, 0.9432623499915713, 0.9382142907097226, 0.9433425012088957, 0.9426076241901943, 0.9383699553353446, 0.9435004620324998, 0.9421176768484569, 0.9394230530375526, 0.94252290896007, 0.939931313196818, 0.942342011701493, 0.9418773083459764, 0.9403136400949388, 0.9404281207493373, 0.9411057631174723, 0.939251374630701, 0.9436950570061093], 'val_mDice': [0.014305407103771964, 0.01818152646800237, 0.019244390539824963, 0.019914687212024416, 0.020579487085342407, 0.024720700834656044, 0.032183301324645676, 0.035160857518868785, 0.03793775312425125, 0.04197688422919739, 0.041542504648012776, 0.040936255550366785, 0.05826514247538788, 0.07030087088545163, 0.07632750028833038, 0.060322737809093224, 0.09993591305932828, 0.11543340094032742, 0.13274638403561853, 0.14002786902710795, 0.1701796426038657, 0.16557999973052315, 0.202127946585062, 0.21728756314232237, 0.23945050801904427, 0.2612159599860509, 0.271923795253748, 0.2980992355871768, 0.29718788376166705, 0.31122787580603645, 0.31903792163800626, 0.3388362558824675, 0.34060014021538554, 0.3481495375079768, 0.32992504585889126, 0.35895152390003204, 0.3597909396602994, 0.37602775331054417, 0.36366171212423415, 0.3808872744973217, 0.3843616483112176, 0.38749650972230093, 0.3831696189230397, 0.3892831869778179, 0.4033943691423961, 0.41100921606024104, 0.4159088030102707, 0.4074706844098511, 0.41006199625276385, 0.41142954180638, 0.4039117995472181, 0.40643271803855896, 0.4235440944986684, 0.4220578580030373, 0.43334049536358743, 0.4288982179548059, 0.4201820169885953, 0.41562635114505175, 0.4326216725721246, 0.4315197081083343, 0.41348776043880553, 0.4445224010518619, 0.43422320591551916, 0.43467856304986136, 0.4343388265087491, 0.43420365417287465, 0.42903610762386096, 0.44785001625617343, 0.4373753819437254, 0.44894935314853984, 0.43949748291855767, 0.4354126439208076, 0.4521312605412233, 0.4496386962987128, 0.45650637504600344, 0.44663837410154794, 0.45209925355655806, 0.46224940816561383, 0.4564955841217722, 0.4550577324061167, 0.4555623047053814, 0.43569704109714147, 0.45332255249931697, 0.44835469250877696, 0.45482549479319934, 0.43540445236223085, 0.4614043498323077, 0.45858110664855867, 0.4551052298574221, 0.4640341010831651, 0.45051183125802446, 0.45683303262506214, 0.4618051935519491, 0.4618153827530997, 0.465098758538564, 0.45477286335967837, 0.45064268864336465, 0.4696776105889252, 0.4480457871797539, 0.4654415480437733, 0.4667242300652322, 0.4648304535519509, 0.46679934717359994, 0.4620404133484477, 0.44395078257435844, 0.4637542413104148, 0.45748849709828693, 0.4663312965560527, 0.46454758995345663, 0.4663392821592944, 0.46416611525984036, 0.45682541210026967, 0.4687086134439423, 0.46877939218566533, 0.4665876279274623, 0.4701531272204149, 0.4686498272986639, 0.46440518949003445, 0.4720131825833094, 0.47874294061745915, 0.4605969885985057, 0.47222032859211877, 0.46959031532917705, 0.44655540620996836, 0.4703492720921834, 0.47024771198630333, 0.48323990156253177, 0.4694000549969219, 0.468959632728781, 0.47199557278127896, 0.461200336260455, 0.46652245557024363, 0.4756686712304751, 0.46122484955759274, 0.4731353535538628, 0.46489706812869935, 0.46121178123922574, 0.4709856052483831, 0.47106025084143593, 0.4569978245667049, 0.4760180078446865, 0.47584289294623194, 0.4773171820810863, 0.4755802661890075, 0.47839476434247835, 0.47799317308125044, 0.4732764923856372, 0.4692522484276976, 0.4690475155200277, 0.46555498082722935, 0.48026550170921145, 0.4722596240185556, 0.46659942326091586, 0.48382563250405447, 0.4753457893218313, 0.4808735441239107, 0.4859181809283438, 0.4809194306532542, 0.486626572906971, 0.47043182026772273, 0.4799992176038878, 0.4756248662514346, 0.47443158073084696, 0.4802651870108786, 0.4771403455663295, 0.4710346825775646, 0.47468494517462595, 0.4778894782066345, 0.4673570642868678, 0.477281410601877, 0.47072875393288477, 0.483978629644428, 0.46985398907037007, 0.48887331198368755, 0.48271589460117476, 0.48320500801006955, 0.4797485944415842, 0.47090554148668334, 0.47314896026537534, 0.4807726831308433, 0.4793014198186852, 0.47116784077315105, 0.4727649110413733, 0.4704764290224938, 0.4848053373751186, 0.48319070289532345, 0.47333467893657233, 0.4710791994418417, 0.47761730122424306, 0.4780654529375689, 0.4859162629360244, 0.49351191981917336, 0.4838213122316769, 0.48289411798829124, 0.47222824678534553, 0.4716084475318591, 0.47697596305183004, 0.4763908510406812, 0.4849403541357744, 0.48669505385415895, 0.4820847532578877, 0.48665474053649677, 0.48778531132709413, 0.4798521667364098, 0.47615710362082436, 0.48200609623676255, 0.47198629361532984, 0.48460627098878223, 0.49140476807951927, 0.4752319735430536, 0.48910730828841525, 0.4806298645479338, 0.4846775687876202, 0.4883597219983737, 0.47815464649881634, 0.49207487897503943, 0.4724044604670434, 0.465876543628318, 0.48915843275331317, 0.47380489926962627, 0.4714608926858221, 0.48737443132059916], 'loss': [198.07533816765425, 68.76824825302309, 30.100639595883372, 18.047546806283968, 13.157402408865345, 10.757003976771522, 9.33501661805167, 8.426979779415745, 7.761804456032378, 7.259206650044295, 6.86943807339011, 6.525190886399501, 6.20861235330485, 5.915712369069764, 5.653433820611598, 5.386357792269087, 5.134384045463917, 4.891900739528229, 4.6634124985590635, 4.441813887487102, 4.249148475105693, 4.079867085027906, 3.919360405359513, 3.7570488732836025, 3.6010970851923196, 3.445088037480181, 3.311318858685189, 3.1978229614542424, 3.0939816217880294, 3.0124196607252527, 2.9194087404861877, 2.846624245458838, 2.7844709497852227, 2.7185488699120044, 2.6642843225408157, 2.6080606085775626, 2.5582567242176886, 2.5028388693265105, 2.456584754237463, 2.4337819366526783, 2.3887904480700093, 2.343565829415668, 2.3123320555535267, 2.2760154851856824, 2.2427564913858355, 2.21717388206579, 2.1901092930568167, 2.163962724421648, 2.1454281310297882, 2.1126548482292944, 2.0971479727763627, 2.0736182954514994, 2.0469879634306336, 2.0296521318517304, 2.0090324520628093, 1.9883882972990499, 1.968937488886486, 1.9588805933965754, 1.9347057797119571, 1.9180662608316736, 1.910815131900994, 1.8940697259150974, 1.8771876858819168, 1.86652970077308, 1.8481569632597687, 1.8349884778106054, 1.8308788447658613, 1.8173929551490267, 1.8032542161452465, 1.7984826138926444, 1.7798539091907808, 1.7686581193584916, 1.7600758062384365, 1.7484131101608644, 1.7463949182623266, 1.7323251951170584, 1.7311192626596439, 1.7169577447302153, 1.703455902809855, 1.6937730294651026, 1.6917669037897416, 1.6889356873227104, 1.6763092456627884, 1.668626428546762, 1.660536705769623, 1.6516031093488936, 1.646182082557531, 1.6394388485166271, 1.6452491963463571, 1.6361428595747542, 1.6201568437445386, 1.612203498738842, 1.612858954022012, 1.5996045803136625, 1.5928280995901964, 1.5876908133387082, 1.582496709508291, 1.5870353420828265, 1.5730590993599187, 1.5687686535166205, 1.5641205034344234, 1.5689327683043384, 1.557101431048305, 1.554059202073051, 1.5427875169237752, 1.5418713799740094, 1.5431399118102813, 1.531941708122027, 1.5271740172992718, 1.519018422323131, 1.5183880273974515, 1.5160060858069206, 1.5110498537143024, 1.5054893901588509, 1.5016189568162723, 1.4937191069964983, 1.4968888531858253, 1.4882818610539021, 1.4864622249018602, 1.4843572513754253, 1.4831360688622297, 1.4891588876121922, 1.4753420010039588, 1.4696213232889463, 1.4675726382195065, 1.466506384667896, 1.4668848677271704, 1.4654198495643502, 1.4570691608293267, 1.456852750808535, 1.4613358470799263, 1.4484188598728787, 1.4407448057542767, 1.4441184161921468, 1.4423918593520486, 1.437561627341668, 1.4339755483047798, 1.4345893513519732, 1.4301582502220231, 1.4273466710665563, 1.4292058304184914, 1.4239921566606695, 1.423491120775104, 1.4147826885703627, 1.4106837645853925, 1.415510705791964, 1.410639758488795, 1.410996343465768, 1.4037368567848978, 1.3991024245272625, 1.4041623245987194, 1.3981871844854845, 1.3967685710264581, 1.3978237438367378, 1.3955536072990076, 1.3990198319556282, 1.389711865322517, 1.3844331562208811, 1.3826921190488355, 1.3806739628648215, 1.38606164375895, 1.38751782940283, 1.3824604897111143, 1.3772157926377429, 1.373437220199457, 1.3769892653633868, 1.372119013206426, 1.3704718269087433, 1.3730791367867652, 1.3677918513730838, 1.361355902832874, 1.366188028356531, 1.3613686991124019, 1.3614605048361463, 1.3603697977292186, 1.3562490486767749, 1.3643617221034514, 1.3593139200397188, 1.3520110296403278, 1.3578599130047846, 1.3515370757634502, 1.3520070351638707, 1.3486956750493944, 1.3525456250093106, 1.3401099351276569, 1.3415030770984853, 1.3458348941269789, 1.3445376371955835, 1.3420970794413163, 1.3365729180562695, 1.3381616074109965, 1.3417705308501606, 1.3345468409240235, 1.3418842093296908, 1.3353048554729905, 1.3367006553605225, 1.32890419657115, 1.336284002874866, 1.3278495354392246, 1.3307575309853437, 1.3287342847462262, 1.3301256014842484, 1.3241760435420606, 1.3209036406878678, 1.3213160711261425, 1.324103384078158, 1.3243861542010964, 1.3208046214461489, 1.3167263408237646, 1.318134988023064, 1.3191821608259335, 1.3205156110834704, 1.3138571494527007, 1.3131060038363058, 1.3198986781546669, 1.3143283016354863, 1.3155934632525454, 1.3095369402075725, 1.308257323113944, 1.305536186934115, 1.3107751373245418, 1.3078121732304913], 'acc': [0.21626319770751684, 0.7957559398702052, 0.8614518448110029, 0.8668692792935111, 0.8680845958799991, 0.8689269152753082, 0.8690749160099379, 0.8691581720121798, 0.869187623842709, 0.8691976794117283, 0.8692045600308098, 0.8692066023086384, 0.869205927651168, 0.869194227570809, 0.8691876474570419, 0.8691240638258922, 0.8690687499915977, 0.8691178306761977, 0.8694657113181159, 0.8703901687522418, 0.8716742631348889, 0.8734438281677949, 0.8754385240463616, 0.877898080042844, 0.8803507696118271, 0.8832352034774892, 0.8862900362898211, 0.8885776292089554, 0.8908209635936987, 0.8927811831551893, 0.8951576200551051, 0.8969974833944065, 0.8984291119361305, 0.9003149739161194, 0.9015954511460252, 0.9032012596855693, 0.9049321045437966, 0.906629618788032, 0.9083211699018411, 0.9096771387528059, 0.9111356430713763, 0.9129076197769271, 0.9139139691781786, 0.9153200132391323, 0.9164902781484121, 0.9176380788634134, 0.9186191943906649, 0.9198658149872576, 0.9203020708970489, 0.9214896545696056, 0.9219919953797625, 0.9227046444739178, 0.9238177031617599, 0.924436602894456, 0.9249176022540817, 0.9257397843229993, 0.9264338971655195, 0.9265647735672924, 0.9272775356884932, 0.9278792395673553, 0.9279689129800357, 0.9284624032071498, 0.9288954818687711, 0.9292975639892447, 0.9297219589884074, 0.9300381801182166, 0.930192507632509, 0.9305862618798072, 0.9310745432447187, 0.9311219980336578, 0.9315712082034387, 0.9317948855208321, 0.9321559742969288, 0.9324767412720146, 0.9323755001571337, 0.9327734351870594, 0.9329262286660206, 0.9331054371722934, 0.9334830463978282, 0.9337238711108357, 0.9337670386860268, 0.933722247628318, 0.9338151922753626, 0.9342179410554596, 0.9342997861616363, 0.9345301568037011, 0.9347130083982623, 0.934945717927967, 0.9347073037040149, 0.9350352793478061, 0.935463381303122, 0.9355817877742995, 0.9356478534614463, 0.9359205138115656, 0.9361675051404534, 0.9363175182414234, 0.9364298090370308, 0.9363473617940917, 0.9365593403066448, 0.9368100829130885, 0.936919962886397, 0.9368277857913571, 0.93711295935927, 0.9372391104928004, 0.9376066172998582, 0.9376108538605745, 0.9376725598017742, 0.9378545552015258, 0.9380120758677531, 0.9383058007199663, 0.9383394220373132, 0.93844599026992, 0.9383029955325615, 0.9386353061818332, 0.9387598983212337, 0.9388954063382985, 0.9388475591377506, 0.939159036027413, 0.9391816029871317, 0.9392610608163955, 0.939255844898621, 0.9391590576078107, 0.9393384747360859, 0.939454333329904, 0.9395230865234903, 0.939484157703827, 0.9395726311176941, 0.9396090303309418, 0.9398536088725425, 0.9400119677384234, 0.939826519670389, 0.9400716795842267, 0.9403296052111075, 0.9402232237902822, 0.9402329313265512, 0.9402988087080659, 0.9403292786898079, 0.9403496035404326, 0.9403895963955091, 0.9404314918074048, 0.9403436492237255, 0.9404968809870045, 0.9404706562532955, 0.9407318225825295, 0.9407995548555335, 0.9408414728245754, 0.9408914528303071, 0.9408668428113609, 0.9411138582487016, 0.94128759585986, 0.9410470759498607, 0.9411817716338718, 0.9413067854528646, 0.9412375707053623, 0.9412726765196517, 0.9412975143968013, 0.9415272187929243, 0.9414885199458378, 0.9415602175652096, 0.9417480678946292, 0.9415453621847956, 0.9417137492822273, 0.941717875700179, 0.9417858144003738, 0.9418617954184284, 0.9418244668691402, 0.941895141717853, 0.9419932063774541, 0.9418497490519753, 0.9421656024302043, 0.9421898377798922, 0.9420335699833035, 0.9421269290015627, 0.942110407506429, 0.9422826462566956, 0.9422644299534169, 0.9419703560227577, 0.942295481686604, 0.942456503398896, 0.9420825063504303, 0.9425022884189267, 0.9424017674894904, 0.9425843386308108, 0.9424274463356633, 0.9427720324506368, 0.9426934809291296, 0.9425908718146822, 0.9426437520282173, 0.9426179145121129, 0.9428655082765333, 0.9427713129661528, 0.9426285315276985, 0.9429093512905938, 0.942760628980089, 0.9428186993252364, 0.9427241350796863, 0.9429850739299251, 0.9429431803106503, 0.9430708330054951, 0.9429335639912061, 0.9430992895866925, 0.9430750048709831, 0.9431261903912846, 0.9432052309995261, 0.9432879309905203, 0.9432235611687052, 0.943080403073022, 0.9433100387175373, 0.9433784621148434, 0.9433913902669185, 0.94335466769244, 0.943181134819272, 0.9434817636750763, 0.9435076925458995, 0.9434662836007537, 0.9434730256479417, 0.9434868378838708, 0.9436874590284083, 0.9436078883099929, 0.9435580903701937, 0.943609045377834, 0.9437731943632427], 'mDice': [0.01916392585210587, 0.020894762764555402, 0.02053046089397567, 0.02094731874855895, 0.02174469928076002, 0.023056545030704142, 0.025981313403726704, 0.02943086165145502, 0.03364341768448882, 0.03826968430042129, 0.04238953617483475, 0.04688748918259698, 0.0519946055540304, 0.05922155201733859, 0.06791141594745577, 0.07736944979915321, 0.08716736543016533, 0.0977283933212331, 0.11095683030167916, 0.12786664395909653, 0.14492226876239453, 0.16116986263572813, 0.17650076249660682, 0.19409828215222702, 0.2120095947148691, 0.231114048129427, 0.24921319771185302, 0.2656791579155695, 0.28069606405566694, 0.29332633598337565, 0.3086324641303288, 0.3206225670278015, 0.33115830230924326, 0.342868581095343, 0.35286297599014943, 0.36317163049680595, 0.3739756110409953, 0.3838342124323682, 0.39251746319727054, 0.3989030558386818, 0.4068147488726985, 0.41557978881630386, 0.4220840649864957, 0.429462496579808, 0.4365221303920882, 0.44276685887778544, 0.4485976672802262, 0.45469063325081804, 0.45882915998897916, 0.4653251821224414, 0.46915463707638727, 0.47410770379120337, 0.48033812104885765, 0.48453843718851053, 0.4888554540171383, 0.49327205652121475, 0.4980695547914146, 0.5008301544515802, 0.5057540008712417, 0.509807788650034, 0.511511756961507, 0.5154838048644165, 0.5190990851958271, 0.522035444026419, 0.5252427067029493, 0.5289029756308475, 0.5305200009210872, 0.5327713050570174, 0.5361773372845589, 0.5368647038971088, 0.5413702990430983, 0.5432039185804003, 0.5454690308843891, 0.5486332314630821, 0.549564197501489, 0.5526777202393109, 0.5526789128263263, 0.555812760564158, 0.5592922366076819, 0.5612033761625509, 0.5620973710267329, 0.5626280412798974, 0.5643286847737292, 0.5669391152140307, 0.5688945615201965, 0.571202413790346, 0.5719632527514278, 0.5739613200902525, 0.5728475310472894, 0.5746776447696954, 0.5784207207517218, 0.5806509188794897, 0.5804951892598691, 0.583419711276794, 0.58489110430978, 0.5869189117463632, 0.5875735931458996, 0.5867633253410646, 0.5894777497758933, 0.5912081318851241, 0.5926790226854891, 0.5908176397252272, 0.5935861792250913, 0.5946883293321557, 0.59697888852177, 0.5973355006638441, 0.5970979124359342, 0.5999315594670767, 0.6009485462117752, 0.6027974995754117, 0.6035055426335965, 0.6039587120479395, 0.6041982720554139, 0.6055682210166583, 0.6066819219798832, 0.608192265332492, 0.6078196532560356, 0.6099728369427849, 0.6101634073560813, 0.6109835655904814, 0.6111949288203855, 0.6098484803376824, 0.6124654403766134, 0.6141334001283, 0.6141577418861257, 0.6145671585839402, 0.6141047711567266, 0.6157652115532262, 0.61709826628505, 0.6171341227018835, 0.6161814907600546, 0.619090002603974, 0.620985762011644, 0.6205552283545921, 0.6206151213355439, 0.621652712166045, 0.6226976422737714, 0.6227368224165861, 0.6237987439274352, 0.6243551387156231, 0.6235925887668434, 0.6255075270692484, 0.6254021415412598, 0.6270882198064571, 0.6283175719825431, 0.6270927339361333, 0.6283817326652538, 0.6282362106197116, 0.6297987101049076, 0.6313886456529093, 0.6300771167702544, 0.6309503703097146, 0.6321024441732844, 0.6309366068422668, 0.6322860262723518, 0.6311910655748093, 0.6341542409683023, 0.6349924661438888, 0.6353271279993236, 0.6360438828273433, 0.6346148888178131, 0.6343929575246631, 0.6354325665705048, 0.6372043045242328, 0.6369994657908834, 0.636947426006287, 0.6374841467916379, 0.6384310638925256, 0.6380960499594522, 0.6390153001401382, 0.6411016395332405, 0.6394036148976608, 0.6409303241895761, 0.6405891865026475, 0.6409411702699369, 0.6423194068967893, 0.640296408430055, 0.6418934878251308, 0.6433488931388369, 0.6417626977449487, 0.6432779139467625, 0.6432427840355115, 0.6445364148395701, 0.642950669060283, 0.6464657955186225, 0.6464002014056828, 0.6449092402494043, 0.6446266917334785, 0.6464220502024007, 0.6469536754973297, 0.6470074762561682, 0.6457254363664524, 0.6472044819256738, 0.6462440854302945, 0.647115596438824, 0.6472034856766112, 0.6486604670832928, 0.6468745944172242, 0.6496027581191187, 0.6486480465646836, 0.6490822785020633, 0.6489541724694816, 0.6498830085640035, 0.6513678012658158, 0.6511042343069414, 0.6503775659291436, 0.6501740865540087, 0.6510461741889261, 0.6521001813819419, 0.6518401525281219, 0.6512995929370525, 0.6512144912592313, 0.6527429237654381, 0.653221567035158, 0.6512734253258978, 0.6526116512633942, 0.652659548370048, 0.6537860324439319, 0.6537315798375748, 0.6545427813335998, 0.6539173359491243, 0.6543849542326198]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.66s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.35s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.07s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:54,  1.46s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:18,  1.55s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:13,  1.54s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:45,  1.66s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:24,  1.59s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<07:54,  1.70s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:17,  1.79s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:31,  1.85s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:08,  1.77s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:25,  1.84s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:43,  1.91s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:49,  1.94s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:51,  1.95s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:57,  1.98s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<09:07,  2.03s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:08,  2.04s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:58,  2.01s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<09:04,  2.04s/it]predicting train subjects:   7%|▋         | 19/285 [00:36<09:03,  2.04s/it]predicting train subjects:   7%|▋         | 20/285 [00:38<09:07,  2.07s/it]predicting train subjects:   7%|▋         | 21/285 [00:40<09:02,  2.06s/it]predicting train subjects:   8%|▊         | 22/285 [00:42<09:09,  2.09s/it]predicting train subjects:   8%|▊         | 23/285 [00:44<09:05,  2.08s/it]predicting train subjects:   8%|▊         | 24/285 [00:46<09:00,  2.07s/it]predicting train subjects:   9%|▉         | 25/285 [00:48<09:02,  2.09s/it]predicting train subjects:   9%|▉         | 26/285 [00:50<08:53,  2.06s/it]predicting train subjects:   9%|▉         | 27/285 [00:52<08:48,  2.05s/it]predicting train subjects:  10%|▉         | 28/285 [00:54<08:40,  2.02s/it]predicting train subjects:  10%|█         | 29/285 [00:56<08:30,  1.99s/it]predicting train subjects:  11%|█         | 30/285 [00:58<08:24,  1.98s/it]predicting train subjects:  11%|█         | 31/285 [01:00<08:22,  1.98s/it]predicting train subjects:  11%|█         | 32/285 [01:02<08:18,  1.97s/it]predicting train subjects:  12%|█▏        | 33/285 [01:04<08:15,  1.96s/it]predicting train subjects:  12%|█▏        | 34/285 [01:06<08:08,  1.95s/it]predicting train subjects:  12%|█▏        | 35/285 [01:08<08:07,  1.95s/it]predicting train subjects:  13%|█▎        | 36/285 [01:10<08:06,  1.96s/it]predicting train subjects:  13%|█▎        | 37/285 [01:12<08:03,  1.95s/it]predicting train subjects:  13%|█▎        | 38/285 [01:14<08:02,  1.95s/it]predicting train subjects:  14%|█▎        | 39/285 [01:16<08:06,  1.98s/it]predicting train subjects:  14%|█▍        | 40/285 [01:18<08:01,  1.97s/it]predicting train subjects:  14%|█▍        | 41/285 [01:19<07:55,  1.95s/it]predicting train subjects:  15%|█▍        | 42/285 [01:21<07:51,  1.94s/it]predicting train subjects:  15%|█▌        | 43/285 [01:23<07:46,  1.93s/it]predicting train subjects:  15%|█▌        | 44/285 [01:25<07:40,  1.91s/it]predicting train subjects:  16%|█▌        | 45/285 [01:27<07:43,  1.93s/it]predicting train subjects:  16%|█▌        | 46/285 [01:29<07:21,  1.85s/it]predicting train subjects:  16%|█▋        | 47/285 [01:30<07:00,  1.77s/it]predicting train subjects:  17%|█▋        | 48/285 [01:32<06:49,  1.73s/it]predicting train subjects:  17%|█▋        | 49/285 [01:34<06:45,  1.72s/it]predicting train subjects:  18%|█▊        | 50/285 [01:35<06:35,  1.68s/it]predicting train subjects:  18%|█▊        | 51/285 [01:37<06:29,  1.66s/it]predicting train subjects:  18%|█▊        | 52/285 [01:39<06:24,  1.65s/it]predicting train subjects:  19%|█▊        | 53/285 [01:40<06:22,  1.65s/it]predicting train subjects:  19%|█▉        | 54/285 [01:42<06:18,  1.64s/it]predicting train subjects:  19%|█▉        | 55/285 [01:43<06:17,  1.64s/it]predicting train subjects:  20%|█▉        | 56/285 [01:45<06:20,  1.66s/it]predicting train subjects:  20%|██        | 57/285 [01:47<06:18,  1.66s/it]predicting train subjects:  20%|██        | 58/285 [01:48<06:18,  1.67s/it]predicting train subjects:  21%|██        | 59/285 [01:50<06:12,  1.65s/it]predicting train subjects:  21%|██        | 60/285 [01:52<06:11,  1.65s/it]predicting train subjects:  21%|██▏       | 61/285 [01:53<06:13,  1.67s/it]predicting train subjects:  22%|██▏       | 62/285 [01:55<06:14,  1.68s/it]predicting train subjects:  22%|██▏       | 63/285 [01:57<06:13,  1.68s/it]predicting train subjects:  22%|██▏       | 64/285 [01:59<06:14,  1.70s/it]predicting train subjects:  23%|██▎       | 65/285 [02:01<06:32,  1.78s/it]predicting train subjects:  23%|██▎       | 66/285 [02:02<06:36,  1.81s/it]predicting train subjects:  24%|██▎       | 67/285 [02:04<06:27,  1.78s/it]predicting train subjects:  24%|██▍       | 68/285 [02:06<06:20,  1.75s/it]predicting train subjects:  24%|██▍       | 69/285 [02:08<06:15,  1.74s/it]predicting train subjects:  25%|██▍       | 70/285 [02:09<06:12,  1.73s/it]predicting train subjects:  25%|██▍       | 71/285 [02:11<06:09,  1.73s/it]predicting train subjects:  25%|██▌       | 72/285 [02:13<06:04,  1.71s/it]predicting train subjects:  26%|██▌       | 73/285 [02:14<06:01,  1.70s/it]predicting train subjects:  26%|██▌       | 74/285 [02:16<05:57,  1.69s/it]predicting train subjects:  26%|██▋       | 75/285 [02:18<05:53,  1.68s/it]predicting train subjects:  27%|██▋       | 76/285 [02:19<05:53,  1.69s/it]predicting train subjects:  27%|██▋       | 77/285 [02:21<05:52,  1.69s/it]predicting train subjects:  27%|██▋       | 78/285 [02:23<05:55,  1.72s/it]predicting train subjects:  28%|██▊       | 79/285 [02:25<05:51,  1.71s/it]predicting train subjects:  28%|██▊       | 80/285 [02:26<05:48,  1.70s/it]predicting train subjects:  28%|██▊       | 81/285 [02:28<05:50,  1.72s/it]predicting train subjects:  29%|██▉       | 82/285 [02:30<05:49,  1.72s/it]predicting train subjects:  29%|██▉       | 83/285 [02:31<05:48,  1.73s/it]predicting train subjects:  29%|██▉       | 84/285 [02:33<05:45,  1.72s/it]predicting train subjects:  30%|██▉       | 85/285 [02:35<05:56,  1.78s/it]predicting train subjects:  30%|███       | 86/285 [02:37<06:02,  1.82s/it]predicting train subjects:  31%|███       | 87/285 [02:39<06:10,  1.87s/it]predicting train subjects:  31%|███       | 88/285 [02:41<06:09,  1.87s/it]predicting train subjects:  31%|███       | 89/285 [02:43<06:07,  1.87s/it]predicting train subjects:  32%|███▏      | 90/285 [02:45<06:07,  1.88s/it]predicting train subjects:  32%|███▏      | 91/285 [02:46<06:04,  1.88s/it]predicting train subjects:  32%|███▏      | 92/285 [02:48<06:06,  1.90s/it]predicting train subjects:  33%|███▎      | 93/285 [02:50<06:12,  1.94s/it]predicting train subjects:  33%|███▎      | 94/285 [02:52<06:07,  1.92s/it]predicting train subjects:  33%|███▎      | 95/285 [02:54<06:06,  1.93s/it]predicting train subjects:  34%|███▎      | 96/285 [02:56<06:02,  1.92s/it]predicting train subjects:  34%|███▍      | 97/285 [02:58<06:00,  1.92s/it]predicting train subjects:  34%|███▍      | 98/285 [03:00<06:01,  1.93s/it]predicting train subjects:  35%|███▍      | 99/285 [03:02<06:00,  1.94s/it]predicting train subjects:  35%|███▌      | 100/285 [03:04<05:58,  1.94s/it]predicting train subjects:  35%|███▌      | 101/285 [03:06<05:58,  1.95s/it]predicting train subjects:  36%|███▌      | 102/285 [03:08<05:58,  1.96s/it]predicting train subjects:  36%|███▌      | 103/285 [03:10<05:55,  1.95s/it]predicting train subjects:  36%|███▋      | 104/285 [03:12<05:47,  1.92s/it]predicting train subjects:  37%|███▋      | 105/285 [03:14<05:41,  1.90s/it]predicting train subjects:  37%|███▋      | 106/285 [03:15<05:38,  1.89s/it]predicting train subjects:  38%|███▊      | 107/285 [03:17<05:35,  1.89s/it]predicting train subjects:  38%|███▊      | 108/285 [03:19<05:30,  1.87s/it]predicting train subjects:  38%|███▊      | 109/285 [03:21<05:26,  1.86s/it]predicting train subjects:  39%|███▊      | 110/285 [03:23<05:20,  1.83s/it]predicting train subjects:  39%|███▉      | 111/285 [03:25<05:17,  1.82s/it]predicting train subjects:  39%|███▉      | 112/285 [03:26<05:18,  1.84s/it]predicting train subjects:  40%|███▉      | 113/285 [03:28<05:20,  1.86s/it]predicting train subjects:  40%|████      | 114/285 [03:30<05:19,  1.87s/it]predicting train subjects:  40%|████      | 115/285 [03:32<05:16,  1.86s/it]predicting train subjects:  41%|████      | 116/285 [03:34<05:13,  1.85s/it]predicting train subjects:  41%|████      | 117/285 [03:36<05:13,  1.86s/it]predicting train subjects:  41%|████▏     | 118/285 [03:38<05:13,  1.87s/it]predicting train subjects:  42%|████▏     | 119/285 [03:40<05:12,  1.88s/it]predicting train subjects:  42%|████▏     | 120/285 [03:41<05:10,  1.88s/it]predicting train subjects:  42%|████▏     | 121/285 [03:43<04:58,  1.82s/it]predicting train subjects:  43%|████▎     | 122/285 [03:45<04:43,  1.74s/it]predicting train subjects:  43%|████▎     | 123/285 [03:46<04:32,  1.68s/it]predicting train subjects:  44%|████▎     | 124/285 [03:48<04:33,  1.70s/it]predicting train subjects:  44%|████▍     | 125/285 [03:50<04:31,  1.70s/it]predicting train subjects:  44%|████▍     | 126/285 [03:51<04:31,  1.71s/it]predicting train subjects:  45%|████▍     | 127/285 [03:53<04:29,  1.71s/it]predicting train subjects:  45%|████▍     | 128/285 [03:55<04:31,  1.73s/it]predicting train subjects:  45%|████▌     | 129/285 [03:57<04:31,  1.74s/it]predicting train subjects:  46%|████▌     | 130/285 [03:58<04:29,  1.74s/it]predicting train subjects:  46%|████▌     | 131/285 [04:00<04:24,  1.72s/it]predicting train subjects:  46%|████▋     | 132/285 [04:02<04:21,  1.71s/it]predicting train subjects:  47%|████▋     | 133/285 [04:03<04:19,  1.70s/it]predicting train subjects:  47%|████▋     | 134/285 [04:05<04:17,  1.70s/it]predicting train subjects:  47%|████▋     | 135/285 [04:07<04:17,  1.72s/it]predicting train subjects:  48%|████▊     | 136/285 [04:09<04:13,  1.70s/it]predicting train subjects:  48%|████▊     | 137/285 [04:10<04:13,  1.71s/it]predicting train subjects:  48%|████▊     | 138/285 [04:12<04:09,  1.70s/it]predicting train subjects:  49%|████▉     | 139/285 [04:14<04:07,  1.70s/it]predicting train subjects:  49%|████▉     | 140/285 [04:15<04:07,  1.71s/it]predicting train subjects:  49%|████▉     | 141/285 [04:17<04:05,  1.70s/it]predicting train subjects:  50%|████▉     | 142/285 [04:19<03:57,  1.66s/it]predicting train subjects:  50%|█████     | 143/285 [04:20<03:50,  1.63s/it]predicting train subjects:  51%|█████     | 144/285 [04:22<03:45,  1.60s/it]predicting train subjects:  51%|█████     | 145/285 [04:23<03:40,  1.58s/it]predicting train subjects:  51%|█████     | 146/285 [04:25<03:38,  1.57s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:26<03:37,  1.58s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:28<03:34,  1.57s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:29<03:31,  1.55s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:31<03:28,  1.54s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:33<03:26,  1.54s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:34<03:23,  1.53s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:36<03:24,  1.55s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:37<03:25,  1.57s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:39<03:24,  1.57s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:40<03:22,  1.57s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:42<03:19,  1.56s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:43<03:15,  1.54s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:45<03:12,  1.53s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:46<03:10,  1.53s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:48<03:06,  1.50s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:49<03:03,  1.49s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:51<03:02,  1.50s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:52<03:00,  1.49s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:54<02:59,  1.49s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:55<02:57,  1.49s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:57<02:54,  1.48s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:58<02:52,  1.48s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:00<02:50,  1.47s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:01<02:49,  1.47s/it]predicting train subjects:  60%|██████    | 171/285 [05:03<02:46,  1.46s/it]predicting train subjects:  60%|██████    | 172/285 [05:04<02:47,  1.49s/it]predicting train subjects:  61%|██████    | 173/285 [05:06<02:46,  1.49s/it]predicting train subjects:  61%|██████    | 174/285 [05:07<02:46,  1.50s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:09<02:43,  1.48s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:10<02:40,  1.47s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:11<02:36,  1.45s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:13<02:34,  1.44s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:14<02:33,  1.45s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:16<02:32,  1.45s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:17<02:31,  1.46s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:19<02:30,  1.46s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:20<02:31,  1.49s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:22<02:29,  1.48s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:23<02:26,  1.47s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:25<02:24,  1.46s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:26<02:26,  1.49s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:28<02:22,  1.47s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:29<02:21,  1.47s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:31<02:18,  1.46s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:32<02:16,  1.45s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:33<02:14,  1.45s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:35<02:11,  1.43s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:36<02:08,  1.41s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:38<02:08,  1.43s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:39<02:14,  1.51s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:41<02:18,  1.57s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:43<02:22,  1.64s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:45<02:22,  1.66s/it]predicting train subjects:  70%|███████   | 200/285 [05:46<02:21,  1.67s/it]predicting train subjects:  71%|███████   | 201/285 [05:48<02:21,  1.68s/it]predicting train subjects:  71%|███████   | 202/285 [05:50<02:21,  1.71s/it]predicting train subjects:  71%|███████   | 203/285 [05:51<02:21,  1.73s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:53<02:19,  1.72s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:55<02:16,  1.71s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:57<02:15,  1.71s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:58<02:17,  1.76s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:00<02:14,  1.75s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:02<02:11,  1.73s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:04<02:10,  1.74s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:05<02:07,  1.73s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:07<02:05,  1.72s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:09<02:03,  1.72s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:10<02:02,  1.72s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:12<01:56,  1.66s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:14<01:51,  1.62s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:15<01:48,  1.59s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:17<01:44,  1.56s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:18<01:42,  1.56s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:20<01:41,  1.55s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:21<01:38,  1.54s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:23<01:36,  1.53s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:24<01:35,  1.53s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:26<01:33,  1.53s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:27<01:33,  1.55s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:29<01:30,  1.53s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:30<01:28,  1.52s/it]predicting train subjects:  80%|████████  | 228/285 [06:32<01:26,  1.52s/it]predicting train subjects:  80%|████████  | 229/285 [06:33<01:24,  1.51s/it]predicting train subjects:  81%|████████  | 230/285 [06:35<01:22,  1.51s/it]predicting train subjects:  81%|████████  | 231/285 [06:36<01:21,  1.51s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:38<01:25,  1.62s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:40<01:28,  1.69s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:42<01:29,  1.75s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:44<01:30,  1.81s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:46<01:30,  1.85s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:48<01:30,  1.88s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:50<01:28,  1.89s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:52<01:28,  1.91s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:54<01:25,  1.89s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:55<01:23,  1.90s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:57<01:22,  1.93s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:59<01:20,  1.92s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:01<01:18,  1.92s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:03<01:16,  1.92s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:05<01:14,  1.91s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:07<01:11,  1.89s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:09<01:10,  1.90s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:11<01:08,  1.90s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:12<01:02,  1.79s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:14<00:56,  1.67s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:15<00:52,  1.59s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:16<00:49,  1.54s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:18<00:46,  1.50s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:19<00:44,  1.49s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:21<00:43,  1.49s/it]predicting train subjects:  90%|█████████ | 257/285 [07:22<00:41,  1.47s/it]predicting train subjects:  91%|█████████ | 258/285 [07:24<00:39,  1.45s/it]predicting train subjects:  91%|█████████ | 259/285 [07:25<00:36,  1.41s/it]predicting train subjects:  91%|█████████ | 260/285 [07:26<00:34,  1.39s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:28<00:33,  1.38s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:29<00:31,  1.39s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:31<00:30,  1.40s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:32<00:29,  1.41s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:33<00:28,  1.42s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:35<00:26,  1.41s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:36<00:25,  1.41s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:38<00:26,  1.56s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:40<00:26,  1.66s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:42<00:25,  1.73s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:44<00:24,  1.78s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:46<00:23,  1.81s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:48<00:21,  1.82s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:49<00:20,  1.86s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:52<00:19,  1.92s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:53<00:17,  1.91s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:55<00:15,  1.92s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:57<00:13,  1.91s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:59<00:11,  1.91s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:01<00:09,  1.93s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:03<00:07,  1.93s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:05<00:05,  1.93s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:07<00:03,  1.92s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:09<00:01,  1.92s/it]predicting train subjects: 100%|██████████| 285/285 [08:11<00:00,  1.91s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:24,  1.35s/it]Loading train:   1%|          | 2/285 [00:02<06:37,  1.41s/it]Loading train:   1%|          | 3/285 [00:04<06:26,  1.37s/it]Loading train:   1%|▏         | 4/285 [00:05<06:40,  1.43s/it]Loading train:   2%|▏         | 5/285 [00:07<06:30,  1.39s/it]Loading train:   2%|▏         | 6/285 [00:08<06:52,  1.48s/it]Loading train:   2%|▏         | 7/285 [00:10<07:30,  1.62s/it]Loading train:   3%|▎         | 8/285 [00:12<07:23,  1.60s/it]Loading train:   3%|▎         | 9/285 [00:13<07:03,  1.53s/it]Loading train:   4%|▎         | 10/285 [00:14<06:34,  1.43s/it]Loading train:   4%|▍         | 11/285 [00:15<06:12,  1.36s/it]Loading train:   4%|▍         | 12/285 [00:17<05:59,  1.32s/it]Loading train:   5%|▍         | 13/285 [00:18<05:52,  1.29s/it]Loading train:   5%|▍         | 14/285 [00:19<05:47,  1.28s/it]Loading train:   5%|▌         | 15/285 [00:20<05:43,  1.27s/it]Loading train:   6%|▌         | 16/285 [00:22<05:46,  1.29s/it]Loading train:   6%|▌         | 17/285 [00:23<05:34,  1.25s/it]Loading train:   6%|▋         | 18/285 [00:24<05:29,  1.23s/it]Loading train:   7%|▋         | 19/285 [00:25<05:30,  1.24s/it]Loading train:   7%|▋         | 20/285 [00:27<05:26,  1.23s/it]Loading train:   7%|▋         | 21/285 [00:28<05:32,  1.26s/it]Loading train:   8%|▊         | 22/285 [00:29<05:26,  1.24s/it]Loading train:   8%|▊         | 23/285 [00:30<05:17,  1.21s/it]Loading train:   8%|▊         | 24/285 [00:32<05:20,  1.23s/it]Loading train:   9%|▉         | 25/285 [00:33<05:13,  1.21s/it]Loading train:   9%|▉         | 26/285 [00:34<05:11,  1.20s/it]Loading train:   9%|▉         | 27/285 [00:35<05:15,  1.22s/it]Loading train:  10%|▉         | 28/285 [00:36<05:00,  1.17s/it]Loading train:  10%|█         | 29/285 [00:37<04:47,  1.12s/it]Loading train:  11%|█         | 30/285 [00:38<04:42,  1.11s/it]Loading train:  11%|█         | 31/285 [00:39<04:40,  1.11s/it]Loading train:  11%|█         | 32/285 [00:40<04:37,  1.10s/it]Loading train:  12%|█▏        | 33/285 [00:42<04:34,  1.09s/it]Loading train:  12%|█▏        | 34/285 [00:43<04:28,  1.07s/it]Loading train:  12%|█▏        | 35/285 [00:44<04:26,  1.07s/it]Loading train:  13%|█▎        | 36/285 [00:45<04:25,  1.07s/it]Loading train:  13%|█▎        | 37/285 [00:46<04:26,  1.07s/it]Loading train:  13%|█▎        | 38/285 [00:47<04:27,  1.08s/it]Loading train:  14%|█▎        | 39/285 [00:48<04:30,  1.10s/it]Loading train:  14%|█▍        | 40/285 [00:49<04:31,  1.11s/it]Loading train:  14%|█▍        | 41/285 [00:50<04:23,  1.08s/it]Loading train:  15%|█▍        | 42/285 [00:51<04:21,  1.08s/it]Loading train:  15%|█▌        | 43/285 [00:52<04:18,  1.07s/it]Loading train:  15%|█▌        | 44/285 [00:53<04:13,  1.05s/it]Loading train:  16%|█▌        | 45/285 [00:54<04:07,  1.03s/it]Loading train:  16%|█▌        | 46/285 [00:55<04:02,  1.01s/it]Loading train:  16%|█▋        | 47/285 [00:56<03:58,  1.00s/it]Loading train:  17%|█▋        | 48/285 [00:57<04:00,  1.01s/it]Loading train:  17%|█▋        | 49/285 [00:58<04:07,  1.05s/it]Loading train:  18%|█▊        | 50/285 [00:59<04:02,  1.03s/it]Loading train:  18%|█▊        | 51/285 [01:00<03:58,  1.02s/it]Loading train:  18%|█▊        | 52/285 [01:01<03:57,  1.02s/it]Loading train:  19%|█▊        | 53/285 [01:02<03:51,  1.00it/s]Loading train:  19%|█▉        | 54/285 [01:03<03:42,  1.04it/s]Loading train:  19%|█▉        | 55/285 [01:04<03:39,  1.05it/s]Loading train:  20%|█▉        | 56/285 [01:05<03:36,  1.06it/s]Loading train:  20%|██        | 57/285 [01:06<03:37,  1.05it/s]Loading train:  20%|██        | 58/285 [01:07<03:39,  1.04it/s]Loading train:  21%|██        | 59/285 [01:08<03:33,  1.06it/s]Loading train:  21%|██        | 60/285 [01:09<03:38,  1.03it/s]Loading train:  21%|██▏       | 61/285 [01:10<03:37,  1.03it/s]Loading train:  22%|██▏       | 62/285 [01:11<03:33,  1.05it/s]Loading train:  22%|██▏       | 63/285 [01:12<03:35,  1.03it/s]Loading train:  22%|██▏       | 64/285 [01:13<04:08,  1.12s/it]Loading train:  23%|██▎       | 65/285 [01:15<04:39,  1.27s/it]Loading train:  23%|██▎       | 66/285 [01:16<04:51,  1.33s/it]Loading train:  24%|██▎       | 67/285 [01:17<04:29,  1.23s/it]Loading train:  24%|██▍       | 68/285 [01:18<04:09,  1.15s/it]Loading train:  24%|██▍       | 69/285 [01:19<03:56,  1.10s/it]Loading train:  25%|██▍       | 70/285 [01:20<03:45,  1.05s/it]Loading train:  25%|██▍       | 71/285 [01:21<03:46,  1.06s/it]Loading train:  25%|██▌       | 72/285 [01:22<03:40,  1.04s/it]Loading train:  26%|██▌       | 73/285 [01:23<03:41,  1.04s/it]Loading train:  26%|██▌       | 74/285 [01:24<03:34,  1.02s/it]Loading train:  26%|██▋       | 75/285 [01:25<03:30,  1.00s/it]Loading train:  27%|██▋       | 76/285 [01:26<03:30,  1.01s/it]Loading train:  27%|██▋       | 77/285 [01:27<03:32,  1.02s/it]Loading train:  27%|██▋       | 78/285 [01:28<03:32,  1.03s/it]Loading train:  28%|██▊       | 79/285 [01:30<03:32,  1.03s/it]Loading train:  28%|██▊       | 80/285 [01:30<03:25,  1.00s/it]Loading train:  28%|██▊       | 81/285 [01:31<03:24,  1.00s/it]Loading train:  29%|██▉       | 82/285 [01:32<03:24,  1.01s/it]Loading train:  29%|██▉       | 83/285 [01:33<03:20,  1.01it/s]Loading train:  29%|██▉       | 84/285 [01:34<03:14,  1.03it/s]Loading train:  30%|██▉       | 85/285 [01:35<03:22,  1.01s/it]Loading train:  30%|███       | 86/285 [01:37<03:26,  1.04s/it]Loading train:  31%|███       | 87/285 [01:38<03:25,  1.04s/it]Loading train:  31%|███       | 88/285 [01:39<03:30,  1.07s/it]Loading train:  31%|███       | 89/285 [01:40<03:30,  1.07s/it]Loading train:  32%|███▏      | 90/285 [01:41<03:30,  1.08s/it]Loading train:  32%|███▏      | 91/285 [01:42<03:32,  1.09s/it]Loading train:  32%|███▏      | 92/285 [01:43<03:28,  1.08s/it]Loading train:  33%|███▎      | 93/285 [01:44<03:29,  1.09s/it]Loading train:  33%|███▎      | 94/285 [01:45<03:29,  1.10s/it]Loading train:  33%|███▎      | 95/285 [01:46<03:26,  1.09s/it]Loading train:  34%|███▎      | 96/285 [01:47<03:23,  1.08s/it]Loading train:  34%|███▍      | 97/285 [01:48<03:21,  1.07s/it]Loading train:  34%|███▍      | 98/285 [01:50<03:19,  1.07s/it]Loading train:  35%|███▍      | 99/285 [01:50<03:12,  1.03s/it]Loading train:  35%|███▌      | 100/285 [01:52<03:17,  1.07s/it]Loading train:  35%|███▌      | 101/285 [01:53<03:21,  1.10s/it]Loading train:  36%|███▌      | 102/285 [01:54<03:24,  1.12s/it]Loading train:  36%|███▌      | 103/285 [01:55<03:14,  1.07s/it]Loading train:  36%|███▋      | 104/285 [01:56<03:30,  1.16s/it]Loading train:  37%|███▋      | 105/285 [01:57<03:23,  1.13s/it]Loading train:  37%|███▋      | 106/285 [01:58<03:12,  1.08s/it]Loading train:  38%|███▊      | 107/285 [01:59<03:10,  1.07s/it]Loading train:  38%|███▊      | 108/285 [02:00<03:12,  1.09s/it]Loading train:  38%|███▊      | 109/285 [02:02<03:08,  1.07s/it]Loading train:  39%|███▊      | 110/285 [02:03<03:04,  1.05s/it]Loading train:  39%|███▉      | 111/285 [02:04<03:06,  1.07s/it]Loading train:  39%|███▉      | 112/285 [02:05<03:08,  1.09s/it]Loading train:  40%|███▉      | 113/285 [02:06<03:07,  1.09s/it]Loading train:  40%|████      | 114/285 [02:07<03:03,  1.08s/it]Loading train:  40%|████      | 115/285 [02:08<02:58,  1.05s/it]Loading train:  41%|████      | 116/285 [02:09<02:58,  1.05s/it]Loading train:  41%|████      | 117/285 [02:10<03:00,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:11<02:56,  1.06s/it]Loading train:  42%|████▏     | 119/285 [02:12<02:56,  1.06s/it]Loading train:  42%|████▏     | 120/285 [02:13<02:56,  1.07s/it]Loading train:  42%|████▏     | 121/285 [02:15<03:15,  1.19s/it]Loading train:  43%|████▎     | 122/285 [02:16<03:16,  1.21s/it]Loading train:  43%|████▎     | 123/285 [02:17<03:24,  1.26s/it]Loading train:  44%|████▎     | 124/285 [02:18<03:10,  1.18s/it]Loading train:  44%|████▍     | 125/285 [02:19<02:58,  1.12s/it]Loading train:  44%|████▍     | 126/285 [02:20<02:47,  1.06s/it]Loading train:  45%|████▍     | 127/285 [02:21<02:41,  1.02s/it]Loading train:  45%|████▍     | 128/285 [02:22<02:34,  1.01it/s]Loading train:  45%|████▌     | 129/285 [02:23<02:34,  1.01it/s]Loading train:  46%|████▌     | 130/285 [02:24<02:31,  1.02it/s]Loading train:  46%|████▌     | 131/285 [02:25<02:26,  1.05it/s]Loading train:  46%|████▋     | 132/285 [02:26<02:21,  1.08it/s]Loading train:  47%|████▋     | 133/285 [02:27<02:19,  1.09it/s]Loading train:  47%|████▋     | 134/285 [02:28<02:18,  1.09it/s]Loading train:  47%|████▋     | 135/285 [02:28<02:14,  1.11it/s]Loading train:  48%|████▊     | 136/285 [02:29<02:14,  1.10it/s]Loading train:  48%|████▊     | 137/285 [02:30<02:14,  1.10it/s]Loading train:  48%|████▊     | 138/285 [02:31<02:14,  1.09it/s]Loading train:  49%|████▉     | 139/285 [02:32<02:20,  1.04it/s]Loading train:  49%|████▉     | 140/285 [02:33<02:16,  1.06it/s]Loading train:  49%|████▉     | 141/285 [02:34<02:15,  1.06it/s]Loading train:  50%|████▉     | 142/285 [02:35<02:15,  1.06it/s]Loading train:  50%|█████     | 143/285 [02:36<02:10,  1.09it/s]Loading train:  51%|█████     | 144/285 [02:37<02:09,  1.09it/s]Loading train:  51%|█████     | 145/285 [02:38<02:09,  1.08it/s]Loading train:  51%|█████     | 146/285 [02:39<02:08,  1.09it/s]Loading train:  52%|█████▏    | 147/285 [02:40<02:05,  1.10it/s]Loading train:  52%|█████▏    | 148/285 [02:41<02:06,  1.08it/s]Loading train:  52%|█████▏    | 149/285 [02:41<02:05,  1.09it/s]Loading train:  53%|█████▎    | 150/285 [02:42<02:02,  1.10it/s]Loading train:  53%|█████▎    | 151/285 [02:43<02:04,  1.08it/s]Loading train:  53%|█████▎    | 152/285 [02:44<02:05,  1.06it/s]Loading train:  54%|█████▎    | 153/285 [02:45<02:04,  1.06it/s]Loading train:  54%|█████▍    | 154/285 [02:46<02:00,  1.09it/s]Loading train:  54%|█████▍    | 155/285 [02:47<01:59,  1.09it/s]Loading train:  55%|█████▍    | 156/285 [02:48<01:59,  1.08it/s]Loading train:  55%|█████▌    | 157/285 [02:49<01:59,  1.08it/s]Loading train:  55%|█████▌    | 158/285 [02:50<01:55,  1.10it/s]Loading train:  56%|█████▌    | 159/285 [02:51<01:53,  1.11it/s]Loading train:  56%|█████▌    | 160/285 [02:52<01:51,  1.12it/s]Loading train:  56%|█████▋    | 161/285 [02:53<01:53,  1.09it/s]Loading train:  57%|█████▋    | 162/285 [02:53<01:53,  1.08it/s]Loading train:  57%|█████▋    | 163/285 [02:54<01:52,  1.08it/s]Loading train:  58%|█████▊    | 164/285 [02:55<01:51,  1.09it/s]Loading train:  58%|█████▊    | 165/285 [02:56<01:51,  1.08it/s]Loading train:  58%|█████▊    | 166/285 [02:57<01:47,  1.11it/s]Loading train:  59%|█████▊    | 167/285 [02:58<01:45,  1.12it/s]Loading train:  59%|█████▉    | 168/285 [02:59<01:42,  1.15it/s]Loading train:  59%|█████▉    | 169/285 [03:00<01:41,  1.14it/s]Loading train:  60%|█████▉    | 170/285 [03:00<01:39,  1.15it/s]Loading train:  60%|██████    | 171/285 [03:01<01:41,  1.12it/s]Loading train:  60%|██████    | 172/285 [03:02<01:43,  1.09it/s]Loading train:  61%|██████    | 173/285 [03:03<01:40,  1.12it/s]Loading train:  61%|██████    | 174/285 [03:04<01:41,  1.10it/s]Loading train:  61%|██████▏   | 175/285 [03:05<01:37,  1.12it/s]Loading train:  62%|██████▏   | 176/285 [03:06<01:38,  1.11it/s]Loading train:  62%|██████▏   | 177/285 [03:07<01:35,  1.13it/s]Loading train:  62%|██████▏   | 178/285 [03:08<01:36,  1.11it/s]Loading train:  63%|██████▎   | 179/285 [03:09<01:34,  1.12it/s]Loading train:  63%|██████▎   | 180/285 [03:09<01:30,  1.16it/s]Loading train:  64%|██████▎   | 181/285 [03:10<01:27,  1.18it/s]Loading train:  64%|██████▍   | 182/285 [03:11<01:28,  1.16it/s]Loading train:  64%|██████▍   | 183/285 [03:12<01:29,  1.14it/s]Loading train:  65%|██████▍   | 184/285 [03:13<01:32,  1.09it/s]Loading train:  65%|██████▍   | 185/285 [03:14<01:29,  1.12it/s]Loading train:  65%|██████▌   | 186/285 [03:15<01:29,  1.10it/s]Loading train:  66%|██████▌   | 187/285 [03:16<01:26,  1.13it/s]Loading train:  66%|██████▌   | 188/285 [03:17<01:26,  1.13it/s]Loading train:  66%|██████▋   | 189/285 [03:17<01:24,  1.14it/s]Loading train:  67%|██████▋   | 190/285 [03:18<01:21,  1.17it/s]Loading train:  67%|██████▋   | 191/285 [03:19<01:21,  1.15it/s]Loading train:  67%|██████▋   | 192/285 [03:20<01:20,  1.15it/s]Loading train:  68%|██████▊   | 193/285 [03:21<01:20,  1.14it/s]Loading train:  68%|██████▊   | 194/285 [03:22<01:19,  1.15it/s]Loading train:  68%|██████▊   | 195/285 [03:23<01:19,  1.14it/s]Loading train:  69%|██████▉   | 196/285 [03:24<01:19,  1.12it/s]Loading train:  69%|██████▉   | 197/285 [03:25<01:19,  1.10it/s]Loading train:  69%|██████▉   | 198/285 [03:25<01:19,  1.09it/s]Loading train:  70%|██████▉   | 199/285 [03:26<01:18,  1.09it/s]Loading train:  70%|███████   | 200/285 [03:27<01:18,  1.09it/s]Loading train:  71%|███████   | 201/285 [03:28<01:17,  1.08it/s]Loading train:  71%|███████   | 202/285 [03:29<01:17,  1.07it/s]Loading train:  71%|███████   | 203/285 [03:30<01:15,  1.09it/s]Loading train:  72%|███████▏  | 204/285 [03:31<01:13,  1.10it/s]Loading train:  72%|███████▏  | 205/285 [03:32<01:14,  1.08it/s]Loading train:  72%|███████▏  | 206/285 [03:33<01:13,  1.07it/s]Loading train:  73%|███████▎  | 207/285 [03:34<01:12,  1.07it/s]Loading train:  73%|███████▎  | 208/285 [03:35<01:12,  1.07it/s]Loading train:  73%|███████▎  | 209/285 [03:36<01:12,  1.05it/s]Loading train:  74%|███████▎  | 210/285 [03:37<01:11,  1.05it/s]Loading train:  74%|███████▍  | 211/285 [03:38<01:09,  1.06it/s]Loading train:  74%|███████▍  | 212/285 [03:39<01:10,  1.03it/s]Loading train:  75%|███████▍  | 213/285 [03:40<01:12,  1.00s/it]Loading train:  75%|███████▌  | 214/285 [03:41<01:09,  1.02it/s]Loading train:  75%|███████▌  | 215/285 [03:41<01:05,  1.07it/s]Loading train:  76%|███████▌  | 216/285 [03:42<01:01,  1.12it/s]Loading train:  76%|███████▌  | 217/285 [03:43<00:59,  1.14it/s]Loading train:  76%|███████▋  | 218/285 [03:44<00:58,  1.15it/s]Loading train:  77%|███████▋  | 219/285 [03:45<00:58,  1.13it/s]Loading train:  77%|███████▋  | 220/285 [03:46<00:56,  1.14it/s]Loading train:  78%|███████▊  | 221/285 [03:47<00:54,  1.17it/s]Loading train:  78%|███████▊  | 222/285 [03:47<00:52,  1.21it/s]Loading train:  78%|███████▊  | 223/285 [03:48<00:50,  1.23it/s]Loading train:  79%|███████▊  | 224/285 [03:49<00:49,  1.23it/s]Loading train:  79%|███████▉  | 225/285 [03:50<00:47,  1.25it/s]Loading train:  79%|███████▉  | 226/285 [03:51<00:47,  1.24it/s]Loading train:  80%|███████▉  | 227/285 [03:51<00:49,  1.18it/s]Loading train:  80%|████████  | 228/285 [03:52<00:48,  1.19it/s]Loading train:  80%|████████  | 229/285 [03:53<00:48,  1.16it/s]Loading train:  81%|████████  | 230/285 [03:54<00:50,  1.09it/s]Loading train:  81%|████████  | 231/285 [03:55<00:49,  1.10it/s]Loading train:  81%|████████▏ | 232/285 [03:56<00:51,  1.04it/s]Loading train:  82%|████████▏ | 233/285 [03:57<00:51,  1.01it/s]Loading train:  82%|████████▏ | 234/285 [03:58<00:52,  1.03s/it]Loading train:  82%|████████▏ | 235/285 [03:59<00:51,  1.03s/it]Loading train:  83%|████████▎ | 236/285 [04:00<00:50,  1.02s/it]Loading train:  83%|████████▎ | 237/285 [04:01<00:49,  1.04s/it]Loading train:  84%|████████▎ | 238/285 [04:02<00:48,  1.03s/it]Loading train:  84%|████████▍ | 239/285 [04:03<00:46,  1.02s/it]Loading train:  84%|████████▍ | 240/285 [04:05<00:46,  1.04s/it]Loading train:  85%|████████▍ | 241/285 [04:06<00:45,  1.03s/it]Loading train:  85%|████████▍ | 242/285 [04:07<00:44,  1.04s/it]Loading train:  85%|████████▌ | 243/285 [04:08<00:44,  1.06s/it]Loading train:  86%|████████▌ | 244/285 [04:09<00:43,  1.06s/it]Loading train:  86%|████████▌ | 245/285 [04:10<00:42,  1.06s/it]Loading train:  86%|████████▋ | 246/285 [04:11<00:40,  1.05s/it]Loading train:  87%|████████▋ | 247/285 [04:12<00:39,  1.04s/it]Loading train:  87%|████████▋ | 248/285 [04:13<00:39,  1.07s/it]Loading train:  87%|████████▋ | 249/285 [04:14<00:38,  1.06s/it]Loading train:  88%|████████▊ | 250/285 [04:15<00:34,  1.01it/s]Loading train:  88%|████████▊ | 251/285 [04:16<00:31,  1.07it/s]Loading train:  88%|████████▊ | 252/285 [04:17<00:29,  1.11it/s]Loading train:  89%|████████▉ | 253/285 [04:17<00:28,  1.11it/s]Loading train:  89%|████████▉ | 254/285 [04:18<00:27,  1.14it/s]Loading train:  89%|████████▉ | 255/285 [04:19<00:26,  1.14it/s]Loading train:  90%|████████▉ | 256/285 [04:20<00:25,  1.15it/s]Loading train:  90%|█████████ | 257/285 [04:21<00:24,  1.16it/s]Loading train:  91%|█████████ | 258/285 [04:22<00:24,  1.11it/s]Loading train:  91%|█████████ | 259/285 [04:23<00:23,  1.10it/s]Loading train:  91%|█████████ | 260/285 [04:24<00:22,  1.13it/s]Loading train:  92%|█████████▏| 261/285 [04:24<00:20,  1.16it/s]Loading train:  92%|█████████▏| 262/285 [04:25<00:19,  1.16it/s]Loading train:  92%|█████████▏| 263/285 [04:26<00:19,  1.15it/s]Loading train:  93%|█████████▎| 264/285 [04:27<00:18,  1.16it/s]Loading train:  93%|█████████▎| 265/285 [04:28<00:17,  1.15it/s]Loading train:  93%|█████████▎| 266/285 [04:29<00:16,  1.15it/s]Loading train:  94%|█████████▎| 267/285 [04:30<00:15,  1.15it/s]Loading train:  94%|█████████▍| 268/285 [04:31<00:15,  1.07it/s]Loading train:  94%|█████████▍| 269/285 [04:32<00:16,  1.05s/it]Loading train:  95%|█████████▍| 270/285 [04:33<00:15,  1.05s/it]Loading train:  95%|█████████▌| 271/285 [04:34<00:14,  1.06s/it]Loading train:  95%|█████████▌| 272/285 [04:35<00:13,  1.05s/it]Loading train:  96%|█████████▌| 273/285 [04:36<00:12,  1.08s/it]Loading train:  96%|█████████▌| 274/285 [04:37<00:11,  1.08s/it]Loading train:  96%|█████████▋| 275/285 [04:39<00:10,  1.10s/it]Loading train:  97%|█████████▋| 276/285 [04:40<00:09,  1.09s/it]Loading train:  97%|█████████▋| 277/285 [04:41<00:09,  1.15s/it]Loading train:  98%|█████████▊| 278/285 [04:42<00:08,  1.20s/it]Loading train:  98%|█████████▊| 279/285 [04:44<00:07,  1.26s/it]Loading train:  98%|█████████▊| 280/285 [04:45<00:06,  1.31s/it]Loading train:  99%|█████████▊| 281/285 [04:47<00:05,  1.41s/it]Loading train:  99%|█████████▉| 282/285 [04:48<00:04,  1.43s/it]Loading train:  99%|█████████▉| 283/285 [04:50<00:02,  1.45s/it]Loading train: 100%|█████████▉| 284/285 [04:51<00:01,  1.47s/it]Loading train: 100%|██████████| 285/285 [04:53<00:00,  1.43s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/285 [00:00<00:15, 18.15it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:15, 17.85it/s]concatenating: train:   5%|▌         | 15/285 [00:00<00:11, 23.83it/s]concatenating: train:   9%|▉         | 26/285 [00:00<00:08, 30.86it/s]concatenating: train:  13%|█▎        | 37/285 [00:00<00:06, 39.26it/s]concatenating: train:  16%|█▌        | 45/285 [00:00<00:05, 41.36it/s]concatenating: train:  18%|█▊        | 52/285 [00:00<00:05, 46.14it/s]concatenating: train:  24%|██▎       | 67/285 [00:00<00:03, 58.07it/s]concatenating: train:  33%|███▎      | 94/285 [00:01<00:02, 75.85it/s]concatenating: train:  41%|████▏     | 118/285 [00:01<00:01, 95.40it/s]concatenating: train:  48%|████▊     | 136/285 [00:01<00:01, 99.16it/s]concatenating: train:  53%|█████▎    | 152/285 [00:01<00:02, 64.37it/s]concatenating: train:  58%|█████▊    | 164/285 [00:01<00:01, 72.25it/s]concatenating: train:  69%|██████▉   | 196/285 [00:01<00:00, 94.03it/s]concatenating: train:  79%|███████▊  | 224/285 [00:02<00:00, 115.03it/s]concatenating: train:  86%|████████▌ | 244/285 [00:02<00:00, 80.01it/s] concatenating: train:  91%|█████████ | 259/285 [00:02<00:00, 74.16it/s]concatenating: train: 100%|██████████| 285/285 [00:02<00:00, 100.45it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.69s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.70s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 22.75it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:14,  1.32s/it]Loading train:   1%|          | 2/285 [00:02<06:42,  1.42s/it]Loading train:   1%|          | 3/285 [00:04<06:40,  1.42s/it]Loading train:   1%|▏         | 4/285 [00:06<07:09,  1.53s/it]Loading train:   2%|▏         | 5/285 [00:07<06:45,  1.45s/it]Loading train:   2%|▏         | 6/285 [00:09<06:53,  1.48s/it]Loading train:   2%|▏         | 7/285 [00:10<07:11,  1.55s/it]Loading train:   3%|▎         | 8/285 [00:12<07:16,  1.57s/it]Loading train:   3%|▎         | 9/285 [00:13<07:09,  1.56s/it]Loading train:   4%|▎         | 10/285 [00:15<06:38,  1.45s/it]Loading train:   4%|▍         | 11/285 [00:16<06:13,  1.36s/it]Loading train:   4%|▍         | 12/285 [00:17<05:42,  1.26s/it]Loading train:   5%|▍         | 13/285 [00:18<05:41,  1.25s/it]Loading train:   5%|▍         | 14/285 [00:19<05:49,  1.29s/it]Loading train:   5%|▌         | 15/285 [00:21<05:46,  1.28s/it]Loading train:   6%|▌         | 16/285 [00:22<05:47,  1.29s/it]Loading train:   6%|▌         | 17/285 [00:23<05:40,  1.27s/it]Loading train:   6%|▋         | 18/285 [00:24<05:41,  1.28s/it]Loading train:   7%|▋         | 19/285 [00:26<05:28,  1.23s/it]Loading train:   7%|▋         | 20/285 [00:27<05:25,  1.23s/it]Loading train:   7%|▋         | 21/285 [00:28<05:20,  1.21s/it]Loading train:   8%|▊         | 22/285 [00:29<05:25,  1.24s/it]Loading train:   8%|▊         | 23/285 [00:31<05:41,  1.30s/it]Loading train:   8%|▊         | 24/285 [00:32<05:23,  1.24s/it]Loading train:   9%|▉         | 25/285 [00:33<05:26,  1.26s/it]Loading train:   9%|▉         | 26/285 [00:34<05:26,  1.26s/it]Loading train:   9%|▉         | 27/285 [00:36<05:30,  1.28s/it]Loading train:  10%|▉         | 28/285 [00:37<05:26,  1.27s/it]Loading train:  10%|█         | 29/285 [00:38<05:21,  1.26s/it]Loading train:  11%|█         | 30/285 [00:39<05:20,  1.26s/it]Loading train:  11%|█         | 31/285 [00:41<05:06,  1.21s/it]Loading train:  11%|█         | 32/285 [00:42<05:14,  1.24s/it]Loading train:  12%|█▏        | 33/285 [00:43<05:01,  1.20s/it]Loading train:  12%|█▏        | 34/285 [00:44<04:54,  1.17s/it]Loading train:  12%|█▏        | 35/285 [00:45<05:10,  1.24s/it]Loading train:  13%|█▎        | 36/285 [00:46<04:46,  1.15s/it]Loading train:  13%|█▎        | 37/285 [00:48<04:56,  1.19s/it]Loading train:  13%|█▎        | 38/285 [00:49<04:50,  1.17s/it]Loading train:  14%|█▎        | 39/285 [00:50<04:43,  1.15s/it]Loading train:  14%|█▍        | 40/285 [00:51<04:27,  1.09s/it]Loading train:  14%|█▍        | 41/285 [00:52<04:15,  1.05s/it]Loading train:  15%|█▍        | 42/285 [00:53<04:32,  1.12s/it]Loading train:  15%|█▌        | 43/285 [00:54<04:49,  1.19s/it]Loading train:  15%|█▌        | 44/285 [00:56<04:39,  1.16s/it]Loading train:  16%|█▌        | 45/285 [00:57<04:27,  1.11s/it]Loading train:  16%|█▌        | 46/285 [00:57<04:12,  1.06s/it]Loading train:  16%|█▋        | 47/285 [00:58<04:04,  1.03s/it]Loading train:  17%|█▋        | 48/285 [00:59<04:01,  1.02s/it]Loading train:  17%|█▋        | 49/285 [01:00<03:58,  1.01s/it]Loading train:  18%|█▊        | 50/285 [01:02<04:18,  1.10s/it]Loading train:  18%|█▊        | 51/285 [01:03<04:06,  1.05s/it]Loading train:  18%|█▊        | 52/285 [01:04<03:59,  1.03s/it]Loading train:  19%|█▊        | 53/285 [01:05<03:52,  1.00s/it]Loading train:  19%|█▉        | 54/285 [01:06<04:08,  1.08s/it]Loading train:  19%|█▉        | 55/285 [01:07<03:50,  1.00s/it]Loading train:  20%|█▉        | 56/285 [01:08<03:55,  1.03s/it]Loading train:  20%|██        | 57/285 [01:09<03:54,  1.03s/it]Loading train:  20%|██        | 58/285 [01:10<03:49,  1.01s/it]Loading train:  21%|██        | 59/285 [01:11<03:57,  1.05s/it]Loading train:  21%|██        | 60/285 [01:12<03:51,  1.03s/it]Loading train:  21%|██▏       | 61/285 [01:13<04:08,  1.11s/it]Loading train:  22%|██▏       | 62/285 [01:14<04:06,  1.11s/it]Loading train:  22%|██▏       | 63/285 [01:15<04:04,  1.10s/it]Loading train:  22%|██▏       | 64/285 [01:17<04:29,  1.22s/it]Loading train:  23%|██▎       | 65/285 [01:19<04:58,  1.36s/it]Loading train:  23%|██▎       | 66/285 [01:20<05:08,  1.41s/it]Loading train:  24%|██▎       | 67/285 [01:21<04:45,  1.31s/it]Loading train:  24%|██▍       | 68/285 [01:22<04:27,  1.23s/it]Loading train:  24%|██▍       | 69/285 [01:23<04:19,  1.20s/it]Loading train:  25%|██▍       | 70/285 [01:25<04:24,  1.23s/it]Loading train:  25%|██▍       | 71/285 [01:26<04:20,  1.22s/it]Loading train:  25%|██▌       | 72/285 [01:27<04:02,  1.14s/it]Loading train:  26%|██▌       | 73/285 [01:28<03:50,  1.09s/it]Loading train:  26%|██▌       | 74/285 [01:29<03:50,  1.09s/it]Loading train:  26%|██▋       | 75/285 [01:30<03:41,  1.06s/it]Loading train:  27%|██▋       | 76/285 [01:31<03:44,  1.07s/it]Loading train:  27%|██▋       | 77/285 [01:32<03:39,  1.06s/it]Loading train:  27%|██▋       | 78/285 [01:33<03:31,  1.02s/it]Loading train:  28%|██▊       | 79/285 [01:34<03:25,  1.00it/s]Loading train:  28%|██▊       | 80/285 [01:35<03:24,  1.00it/s]Loading train:  28%|██▊       | 81/285 [01:36<03:24,  1.00s/it]Loading train:  29%|██▉       | 82/285 [01:37<03:28,  1.03s/it]Loading train:  29%|██▉       | 83/285 [01:38<03:30,  1.04s/it]Loading train:  29%|██▉       | 84/285 [01:39<03:37,  1.08s/it]Loading train:  30%|██▉       | 85/285 [01:40<03:48,  1.14s/it]Loading train:  30%|███       | 86/285 [01:42<03:44,  1.13s/it]Loading train:  31%|███       | 87/285 [01:43<03:43,  1.13s/it]Loading train:  31%|███       | 88/285 [01:44<03:45,  1.15s/it]Loading train:  31%|███       | 89/285 [01:45<03:40,  1.12s/it]Loading train:  32%|███▏      | 90/285 [01:46<03:31,  1.09s/it]Loading train:  32%|███▏      | 91/285 [01:47<03:25,  1.06s/it]Loading train:  32%|███▏      | 92/285 [01:48<03:25,  1.06s/it]Loading train:  33%|███▎      | 93/285 [01:49<03:35,  1.12s/it]Loading train:  33%|███▎      | 94/285 [01:50<03:35,  1.13s/it]Loading train:  33%|███▎      | 95/285 [01:52<03:37,  1.14s/it]Loading train:  34%|███▎      | 96/285 [01:53<03:34,  1.14s/it]Loading train:  34%|███▍      | 97/285 [01:54<03:38,  1.16s/it]Loading train:  34%|███▍      | 98/285 [01:55<03:42,  1.19s/it]Loading train:  35%|███▍      | 99/285 [01:56<03:42,  1.19s/it]Loading train:  35%|███▌      | 100/285 [01:58<03:41,  1.20s/it]Loading train:  35%|███▌      | 101/285 [01:59<03:50,  1.25s/it]Loading train:  36%|███▌      | 102/285 [02:00<03:44,  1.23s/it]Loading train:  36%|███▌      | 103/285 [02:02<03:50,  1.27s/it]Loading train:  36%|███▋      | 104/285 [02:03<03:41,  1.22s/it]Loading train:  37%|███▋      | 105/285 [02:04<03:34,  1.19s/it]Loading train:  37%|███▋      | 106/285 [02:05<03:29,  1.17s/it]Loading train:  38%|███▊      | 107/285 [02:06<03:38,  1.23s/it]Loading train:  38%|███▊      | 108/285 [02:07<03:31,  1.20s/it]Loading train:  38%|███▊      | 109/285 [02:08<03:23,  1.16s/it]Loading train:  39%|███▊      | 110/285 [02:10<03:22,  1.16s/it]Loading train:  39%|███▉      | 111/285 [02:10<03:07,  1.08s/it]Loading train:  39%|███▉      | 112/285 [02:11<02:59,  1.04s/it]Loading train:  40%|███▉      | 113/285 [02:12<02:58,  1.04s/it]Loading train:  40%|████      | 114/285 [02:14<02:58,  1.05s/it]Loading train:  40%|████      | 115/285 [02:15<03:07,  1.10s/it]Loading train:  41%|████      | 116/285 [02:16<03:05,  1.10s/it]Loading train:  41%|████      | 117/285 [02:17<03:07,  1.12s/it]Loading train:  41%|████▏     | 118/285 [02:18<03:12,  1.15s/it]Loading train:  42%|████▏     | 119/285 [02:19<03:09,  1.14s/it]Loading train:  42%|████▏     | 120/285 [02:20<03:04,  1.12s/it]Loading train:  42%|████▏     | 121/285 [02:22<03:16,  1.20s/it]Loading train:  43%|████▎     | 122/285 [02:23<03:15,  1.20s/it]Loading train:  43%|████▎     | 123/285 [02:24<03:16,  1.22s/it]Loading train:  44%|████▎     | 124/285 [02:25<03:03,  1.14s/it]Loading train:  44%|████▍     | 125/285 [02:26<02:51,  1.07s/it]Loading train:  44%|████▍     | 126/285 [02:27<02:37,  1.01it/s]Loading train:  45%|████▍     | 127/285 [02:28<02:33,  1.03it/s]Loading train:  45%|████▍     | 128/285 [02:29<02:37,  1.01s/it]Loading train:  45%|████▌     | 129/285 [02:30<02:40,  1.03s/it]Loading train:  46%|████▌     | 130/285 [02:31<02:32,  1.02it/s]Loading train:  46%|████▌     | 131/285 [02:32<02:23,  1.07it/s]Loading train:  46%|████▋     | 132/285 [02:33<02:20,  1.09it/s]Loading train:  47%|████▋     | 133/285 [02:34<02:26,  1.04it/s]Loading train:  47%|████▋     | 134/285 [02:35<02:32,  1.01s/it]Loading train:  47%|████▋     | 135/285 [02:36<02:32,  1.02s/it]Loading train:  48%|████▊     | 136/285 [02:37<02:28,  1.00it/s]Loading train:  48%|████▊     | 137/285 [02:38<02:33,  1.04s/it]Loading train:  48%|████▊     | 138/285 [02:39<02:27,  1.00s/it]Loading train:  49%|████▉     | 139/285 [02:40<02:29,  1.02s/it]Loading train:  49%|████▉     | 140/285 [02:41<02:33,  1.06s/it]Loading train:  49%|████▉     | 141/285 [02:42<02:36,  1.09s/it]Loading train:  50%|████▉     | 142/285 [02:43<02:30,  1.05s/it]Loading train:  50%|█████     | 143/285 [02:44<02:26,  1.03s/it]Loading train:  51%|█████     | 144/285 [02:45<02:18,  1.02it/s]Loading train:  51%|█████     | 145/285 [02:46<02:11,  1.06it/s]Loading train:  51%|█████     | 146/285 [02:47<02:18,  1.00it/s]Loading train:  52%|█████▏    | 147/285 [02:48<02:20,  1.02s/it]Loading train:  52%|█████▏    | 148/285 [02:49<02:25,  1.06s/it]Loading train:  52%|█████▏    | 149/285 [02:50<02:22,  1.05s/it]Loading train:  53%|█████▎    | 150/285 [02:52<02:30,  1.11s/it]Loading train:  53%|█████▎    | 151/285 [02:53<02:25,  1.09s/it]Loading train:  53%|█████▎    | 152/285 [02:54<02:29,  1.12s/it]Loading train:  54%|█████▎    | 153/285 [02:55<02:23,  1.09s/it]Loading train:  54%|█████▍    | 154/285 [02:56<02:20,  1.07s/it]Loading train:  54%|█████▍    | 155/285 [02:57<02:26,  1.13s/it]Loading train:  55%|█████▍    | 156/285 [02:58<02:27,  1.15s/it]Loading train:  55%|█████▌    | 157/285 [02:59<02:21,  1.11s/it]Loading train:  55%|█████▌    | 158/285 [03:01<02:26,  1.15s/it]Loading train:  56%|█████▌    | 159/285 [03:02<02:25,  1.15s/it]Loading train:  56%|█████▌    | 160/285 [03:03<02:18,  1.10s/it]Loading train:  56%|█████▋    | 161/285 [03:04<02:18,  1.12s/it]Loading train:  57%|█████▋    | 162/285 [03:05<02:19,  1.13s/it]Loading train:  57%|█████▋    | 163/285 [03:06<02:13,  1.09s/it]Loading train:  58%|█████▊    | 164/285 [03:07<02:09,  1.07s/it]Loading train:  58%|█████▊    | 165/285 [03:08<02:09,  1.08s/it]Loading train:  58%|█████▊    | 166/285 [03:09<02:08,  1.08s/it]Loading train:  59%|█████▊    | 167/285 [03:10<02:10,  1.11s/it]Loading train:  59%|█████▉    | 168/285 [03:11<02:10,  1.12s/it]Loading train:  59%|█████▉    | 169/285 [03:12<02:02,  1.05s/it]Loading train:  60%|█████▉    | 170/285 [03:13<01:59,  1.04s/it]Loading train:  60%|██████    | 171/285 [03:14<01:51,  1.03it/s]Loading train:  60%|██████    | 172/285 [03:15<01:49,  1.03it/s]Loading train:  61%|██████    | 173/285 [03:16<01:47,  1.04it/s]Loading train:  61%|██████    | 174/285 [03:17<01:50,  1.00it/s]Loading train:  61%|██████▏   | 175/285 [03:18<01:47,  1.02it/s]Loading train:  62%|██████▏   | 176/285 [03:19<01:53,  1.04s/it]Loading train:  62%|██████▏   | 177/285 [03:21<01:57,  1.08s/it]Loading train:  62%|██████▏   | 178/285 [03:22<01:55,  1.08s/it]Loading train:  63%|██████▎   | 179/285 [03:23<01:53,  1.07s/it]Loading train:  63%|██████▎   | 180/285 [03:23<01:41,  1.04it/s]Loading train:  64%|██████▎   | 181/285 [03:24<01:39,  1.05it/s]Loading train:  64%|██████▍   | 182/285 [03:25<01:38,  1.05it/s]Loading train:  64%|██████▍   | 183/285 [03:26<01:30,  1.13it/s]Loading train:  65%|██████▍   | 184/285 [03:27<01:27,  1.16it/s]Loading train:  65%|██████▍   | 185/285 [03:28<01:25,  1.16it/s]Loading train:  65%|██████▌   | 186/285 [03:29<01:30,  1.09it/s]Loading train:  66%|██████▌   | 187/285 [03:29<01:26,  1.13it/s]Loading train:  66%|██████▌   | 188/285 [03:30<01:23,  1.17it/s]Loading train:  66%|██████▋   | 189/285 [03:31<01:21,  1.18it/s]Loading train:  67%|██████▋   | 190/285 [03:32<01:19,  1.20it/s]Loading train:  67%|██████▋   | 191/285 [03:33<01:18,  1.20it/s]Loading train:  67%|██████▋   | 192/285 [03:33<01:15,  1.23it/s]Loading train:  68%|██████▊   | 193/285 [03:34<01:17,  1.19it/s]Loading train:  68%|██████▊   | 194/285 [03:35<01:23,  1.09it/s]Loading train:  68%|██████▊   | 195/285 [03:36<01:19,  1.14it/s]Loading train:  69%|██████▉   | 196/285 [03:38<01:27,  1.02it/s]Loading train:  69%|██████▉   | 197/285 [03:39<01:31,  1.04s/it]Loading train:  69%|██████▉   | 198/285 [03:40<01:29,  1.03s/it]Loading train:  70%|██████▉   | 199/285 [03:41<01:24,  1.02it/s]Loading train:  70%|███████   | 200/285 [03:41<01:19,  1.07it/s]Loading train:  71%|███████   | 201/285 [03:42<01:23,  1.01it/s]Loading train:  71%|███████   | 202/285 [03:44<01:25,  1.03s/it]Loading train:  71%|███████   | 203/285 [03:45<01:21,  1.00it/s]Loading train:  72%|███████▏  | 204/285 [03:46<01:24,  1.04s/it]Loading train:  72%|███████▏  | 205/285 [03:47<01:21,  1.02s/it]Loading train:  72%|███████▏  | 206/285 [03:48<01:19,  1.00s/it]Loading train:  73%|███████▎  | 207/285 [03:49<01:19,  1.01s/it]Loading train:  73%|███████▎  | 208/285 [03:50<01:20,  1.05s/it]Loading train:  73%|███████▎  | 209/285 [03:51<01:20,  1.05s/it]Loading train:  74%|███████▎  | 210/285 [03:52<01:16,  1.02s/it]Loading train:  74%|███████▍  | 211/285 [03:53<01:17,  1.05s/it]Loading train:  74%|███████▍  | 212/285 [03:54<01:15,  1.04s/it]Loading train:  75%|███████▍  | 213/285 [03:55<01:14,  1.04s/it]Loading train:  75%|███████▌  | 214/285 [03:56<01:15,  1.07s/it]Loading train:  75%|███████▌  | 215/285 [03:57<01:07,  1.03it/s]Loading train:  76%|███████▌  | 216/285 [03:58<01:08,  1.01it/s]Loading train:  76%|███████▌  | 217/285 [03:59<01:07,  1.01it/s]Loading train:  76%|███████▋  | 218/285 [04:00<01:02,  1.06it/s]Loading train:  77%|███████▋  | 219/285 [04:01<01:00,  1.09it/s]Loading train:  77%|███████▋  | 220/285 [04:01<00:59,  1.09it/s]Loading train:  78%|███████▊  | 221/285 [04:03<01:04,  1.00s/it]Loading train:  78%|███████▊  | 222/285 [04:04<01:06,  1.05s/it]Loading train:  78%|███████▊  | 223/285 [04:05<01:06,  1.08s/it]Loading train:  79%|███████▊  | 224/285 [04:06<01:08,  1.13s/it]Loading train:  79%|███████▉  | 225/285 [04:08<01:12,  1.22s/it]Loading train:  79%|███████▉  | 226/285 [04:09<01:13,  1.25s/it]Loading train:  80%|███████▉  | 227/285 [04:10<01:11,  1.23s/it]Loading train:  80%|████████  | 228/285 [04:11<01:08,  1.19s/it]Loading train:  80%|████████  | 229/285 [04:12<01:03,  1.13s/it]Loading train:  81%|████████  | 230/285 [04:14<01:03,  1.16s/it]Loading train:  81%|████████  | 231/285 [04:15<01:02,  1.15s/it]Loading train:  81%|████████▏ | 232/285 [04:16<01:04,  1.21s/it]Loading train:  82%|████████▏ | 233/285 [04:17<01:03,  1.23s/it]Loading train:  82%|████████▏ | 234/285 [04:19<01:05,  1.29s/it]Loading train:  82%|████████▏ | 235/285 [04:20<01:09,  1.38s/it]Loading train:  83%|████████▎ | 236/285 [04:22<01:05,  1.34s/it]Loading train:  83%|████████▎ | 237/285 [04:22<00:59,  1.23s/it]Loading train:  84%|████████▎ | 238/285 [04:23<00:54,  1.16s/it]Loading train:  84%|████████▍ | 239/285 [04:24<00:49,  1.08s/it]Loading train:  84%|████████▍ | 240/285 [04:26<00:49,  1.10s/it]Loading train:  85%|████████▍ | 241/285 [04:27<00:50,  1.15s/it]Loading train:  85%|████████▍ | 242/285 [04:28<00:48,  1.13s/it]Loading train:  85%|████████▌ | 243/285 [04:29<00:47,  1.13s/it]Loading train:  86%|████████▌ | 244/285 [04:30<00:48,  1.17s/it]Loading train:  86%|████████▌ | 245/285 [04:31<00:46,  1.17s/it]Loading train:  86%|████████▋ | 246/285 [04:33<00:46,  1.19s/it]Loading train:  87%|████████▋ | 247/285 [04:34<00:45,  1.21s/it]Loading train:  87%|████████▋ | 248/285 [04:35<00:43,  1.19s/it]Loading train:  87%|████████▋ | 249/285 [04:36<00:42,  1.18s/it]Loading train:  88%|████████▊ | 250/285 [04:37<00:41,  1.20s/it]Loading train:  88%|████████▊ | 251/285 [04:38<00:37,  1.11s/it]Loading train:  88%|████████▊ | 252/285 [04:39<00:34,  1.05s/it]Loading train:  89%|████████▉ | 253/285 [04:40<00:33,  1.04s/it]Loading train:  89%|████████▉ | 254/285 [04:42<00:33,  1.09s/it]Loading train:  89%|████████▉ | 255/285 [04:42<00:31,  1.05s/it]Loading train:  90%|████████▉ | 256/285 [04:43<00:29,  1.03s/it]Loading train:  90%|█████████ | 257/285 [04:45<00:28,  1.04s/it]Loading train:  91%|█████████ | 258/285 [04:45<00:26,  1.00it/s]Loading train:  91%|█████████ | 259/285 [04:46<00:23,  1.10it/s]Loading train:  91%|█████████ | 260/285 [04:47<00:21,  1.15it/s]Loading train:  92%|█████████▏| 261/285 [04:48<00:20,  1.16it/s]Loading train:  92%|█████████▏| 262/285 [04:49<00:22,  1.04it/s]Loading train:  92%|█████████▏| 263/285 [04:50<00:20,  1.09it/s]Loading train:  93%|█████████▎| 264/285 [04:51<00:19,  1.09it/s]Loading train:  93%|█████████▎| 265/285 [04:52<00:18,  1.07it/s]Loading train:  93%|█████████▎| 266/285 [04:52<00:17,  1.10it/s]Loading train:  94%|█████████▎| 267/285 [04:53<00:16,  1.10it/s]Loading train:  94%|█████████▍| 268/285 [04:55<00:17,  1.03s/it]Loading train:  94%|█████████▍| 269/285 [04:56<00:18,  1.16s/it]Loading train:  95%|█████████▍| 270/285 [04:57<00:17,  1.16s/it]Loading train:  95%|█████████▌| 271/285 [04:58<00:15,  1.14s/it]Loading train:  95%|█████████▌| 272/285 [05:00<00:14,  1.14s/it]Loading train:  96%|█████████▌| 273/285 [05:01<00:13,  1.14s/it]Loading train:  96%|█████████▌| 274/285 [05:02<00:11,  1.08s/it]Loading train:  96%|█████████▋| 275/285 [05:03<00:10,  1.07s/it]Loading train:  97%|█████████▋| 276/285 [05:04<00:09,  1.02s/it]Loading train:  97%|█████████▋| 277/285 [05:05<00:08,  1.05s/it]Loading train:  98%|█████████▊| 278/285 [05:06<00:07,  1.07s/it]Loading train:  98%|█████████▊| 279/285 [05:07<00:06,  1.07s/it]Loading train:  98%|█████████▊| 280/285 [05:08<00:05,  1.06s/it]Loading train:  99%|█████████▊| 281/285 [05:09<00:04,  1.10s/it]Loading train:  99%|█████████▉| 282/285 [05:10<00:03,  1.14s/it]Loading train:  99%|█████████▉| 283/285 [05:12<00:02,  1.14s/it]Loading train: 100%|█████████▉| 284/285 [05:13<00:01,  1.11s/it]Loading train: 100%|██████████| 285/285 [05:14<00:00,  1.11s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:12, 23.01it/s]concatenating: train:   2%|▏         | 7/285 [00:00<00:10, 25.56it/s]concatenating: train:   4%|▎         | 10/285 [00:00<00:10, 26.19it/s]concatenating: train:  13%|█▎        | 38/285 [00:00<00:06, 35.93it/s]concatenating: train:  17%|█▋        | 48/285 [00:00<00:05, 40.53it/s]concatenating: train:  20%|██        | 58/285 [00:00<00:04, 48.71it/s]concatenating: train:  24%|██▎       | 67/285 [00:00<00:04, 51.81it/s]concatenating: train:  26%|██▋       | 75/285 [00:01<00:03, 54.78it/s]concatenating: train:  38%|███▊      | 108/285 [00:01<00:02, 72.96it/s]concatenating: train:  50%|████▉     | 142/285 [00:01<00:01, 95.33it/s]concatenating: train:  59%|█████▊    | 167/285 [00:01<00:01, 113.58it/s]concatenating: train:  66%|██████▌   | 188/285 [00:01<00:01, 92.17it/s] concatenating: train:  72%|███████▏  | 205/285 [00:01<00:00, 95.83it/s]concatenating: train:  85%|████████▍ | 241/285 [00:01<00:00, 122.68it/s]concatenating: train:  96%|█████████▋| 275/285 [00:02<00:00, 151.41it/s]concatenating: train: 100%|██████████| 285/285 [00:02<00:00, 138.59it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.55s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.37s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 63.78it/s]2019-07-05 20:50:33.698200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 20:50:33.698320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 20:50:33.698346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 20:50:33.698356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 20:50:33.698810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:15,  2.58it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:12,  3.05it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:12,  2.92it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:08,  3.73it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:09,  3.35it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:02<00:07,  3.63it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:08,  3.28it/s]loading the weights for Unet:  40%|████      | 16/40 [00:03<00:05,  4.15it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:03<00:06,  3.46it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:05,  3.81it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:06,  3.27it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:04<00:04,  4.00it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:05<00:03,  4.34it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:05<00:03,  3.61it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:05<00:02,  4.00it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:06<00:03,  3.45it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:06<00:01,  4.18it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:06<00:01,  4.44it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:07<00:01,  3.48it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:07<00:00,  3.91it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:08<00:00,  3.43it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:08<00:00,  4.95it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 40)   400         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 40)   160         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 40)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 40)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 40)   14440       dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 40)   160         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 40)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 40)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 40)   14440       dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 40)   160         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 40)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 40)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   7220        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 40)   7240        dropout_15[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 52, 80, 40)   160         conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 52, 80, 40)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 52, 80, 40)   0           activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 52, 80, 13)   533         dropout_16[0][0]                 
==================================================================================================
Total params: 228,553
Trainable params: 84,753
Non-trainable params: 143,800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 23s - loss: 143.8151 - acc: 0.6136 - mDice: 0.0166 - val_loss: 46.3846 - val_acc: 0.9047 - val_mDice: 0.0147

Epoch 00001: val_mDice improved from -inf to 0.01474, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 38.9777 - acc: 0.8682 - mDice: 0.0165 - val_loss: 17.0067 - val_acc: 0.9047 - val_mDice: 0.0152

Epoch 00002: val_mDice improved from 0.01474 to 0.01517, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 14s - loss: 18.0299 - acc: 0.8692 - mDice: 0.0173 - val_loss: 10.0900 - val_acc: 0.9047 - val_mDice: 0.0165

Epoch 00003: val_mDice improved from 0.01517 to 0.01648, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 14s - loss: 12.0259 - acc: 0.8692 - mDice: 0.0194 - val_loss: 8.1971 - val_acc: 0.9047 - val_mDice: 0.0215

Epoch 00004: val_mDice improved from 0.01648 to 0.02153, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 13s - loss: 9.4764 - acc: 0.8692 - mDice: 0.0229 - val_loss: 6.8330 - val_acc: 0.9047 - val_mDice: 0.0265

Epoch 00005: val_mDice improved from 0.02153 to 0.02655, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 13s - loss: 8.0881 - acc: 0.8692 - mDice: 0.0277 - val_loss: 6.6222 - val_acc: 0.9047 - val_mDice: 0.0293

Epoch 00006: val_mDice improved from 0.02655 to 0.02934, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 14s - loss: 7.2120 - acc: 0.8692 - mDice: 0.0336 - val_loss: 5.5258 - val_acc: 0.9047 - val_mDice: 0.0340

Epoch 00007: val_mDice improved from 0.02934 to 0.03401, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 14s - loss: 6.5958 - acc: 0.8692 - mDice: 0.0404 - val_loss: 5.2536 - val_acc: 0.9047 - val_mDice: 0.0452

Epoch 00008: val_mDice improved from 0.03401 to 0.04517, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 13s - loss: 6.1070 - acc: 0.8692 - mDice: 0.0495 - val_loss: 5.0714 - val_acc: 0.9047 - val_mDice: 0.0520

Epoch 00009: val_mDice improved from 0.04517 to 0.05198, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 13s - loss: 5.6069 - acc: 0.8693 - mDice: 0.0653 - val_loss: 4.6641 - val_acc: 0.9047 - val_mDice: 0.0725

Epoch 00010: val_mDice improved from 0.05198 to 0.07246, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 5.2054 - acc: 0.8701 - mDice: 0.0817 - val_loss: 4.5683 - val_acc: 0.9047 - val_mDice: 0.0833

Epoch 00011: val_mDice improved from 0.07246 to 0.08335, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 14s - loss: 4.8922 - acc: 0.8715 - mDice: 0.0969 - val_loss: 4.1274 - val_acc: 0.9054 - val_mDice: 0.1142

Epoch 00012: val_mDice improved from 0.08335 to 0.11423, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 14s - loss: 4.5898 - acc: 0.8731 - mDice: 0.1150 - val_loss: 4.0693 - val_acc: 0.9088 - val_mDice: 0.1262

Epoch 00013: val_mDice improved from 0.11423 to 0.12617, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 4.2982 - acc: 0.8752 - mDice: 0.1365 - val_loss: 3.6952 - val_acc: 0.9097 - val_mDice: 0.1619

Epoch 00014: val_mDice improved from 0.12617 to 0.16193, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 13s - loss: 4.0447 - acc: 0.8795 - mDice: 0.1582 - val_loss: 3.6321 - val_acc: 0.9136 - val_mDice: 0.1780

Epoch 00015: val_mDice improved from 0.16193 to 0.17796, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 13s - loss: 3.7966 - acc: 0.8844 - mDice: 0.1818 - val_loss: 3.3820 - val_acc: 0.9171 - val_mDice: 0.2109

Epoch 00016: val_mDice improved from 0.17796 to 0.21091, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 3.5755 - acc: 0.8878 - mDice: 0.2069 - val_loss: 3.3110 - val_acc: 0.9196 - val_mDice: 0.2313

Epoch 00017: val_mDice improved from 0.21091 to 0.23132, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 3.3745 - acc: 0.8914 - mDice: 0.2329 - val_loss: 3.1551 - val_acc: 0.9220 - val_mDice: 0.2588

Epoch 00018: val_mDice improved from 0.23132 to 0.25880, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 13s - loss: 3.2025 - acc: 0.8953 - mDice: 0.2578 - val_loss: 3.1438 - val_acc: 0.9222 - val_mDice: 0.2766

Epoch 00019: val_mDice improved from 0.25880 to 0.27665, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 13s - loss: 3.0626 - acc: 0.8986 - mDice: 0.2802 - val_loss: 3.1019 - val_acc: 0.9286 - val_mDice: 0.2959

Epoch 00020: val_mDice improved from 0.27665 to 0.29586, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 14s - loss: 2.9365 - acc: 0.9015 - mDice: 0.3006 - val_loss: 2.9596 - val_acc: 0.9227 - val_mDice: 0.3229

Epoch 00021: val_mDice improved from 0.29586 to 0.32292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 14s - loss: 2.8264 - acc: 0.9044 - mDice: 0.3209 - val_loss: 2.9133 - val_acc: 0.9222 - val_mDice: 0.3424

Epoch 00022: val_mDice improved from 0.32292 to 0.34239, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 13s - loss: 2.7262 - acc: 0.9067 - mDice: 0.3386 - val_loss: 2.8731 - val_acc: 0.9242 - val_mDice: 0.3528

Epoch 00023: val_mDice improved from 0.34239 to 0.35281, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 13s - loss: 2.6262 - acc: 0.9092 - mDice: 0.3570 - val_loss: 3.0185 - val_acc: 0.9167 - val_mDice: 0.3527

Epoch 00024: val_mDice did not improve from 0.35281
Epoch 25/300
 - 13s - loss: 2.5420 - acc: 0.9125 - mDice: 0.3748 - val_loss: 2.8915 - val_acc: 0.9257 - val_mDice: 0.3575

Epoch 00025: val_mDice improved from 0.35281 to 0.35754, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 13s - loss: 2.4672 - acc: 0.9152 - mDice: 0.3891 - val_loss: 2.7917 - val_acc: 0.9271 - val_mDice: 0.3829

Epoch 00026: val_mDice improved from 0.35754 to 0.38291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 14s - loss: 2.4035 - acc: 0.9177 - mDice: 0.4026 - val_loss: 2.8028 - val_acc: 0.9365 - val_mDice: 0.3892

Epoch 00027: val_mDice improved from 0.38291 to 0.38917, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 13s - loss: 2.3398 - acc: 0.9199 - mDice: 0.4151 - val_loss: 2.7424 - val_acc: 0.9334 - val_mDice: 0.3990

Epoch 00028: val_mDice improved from 0.38917 to 0.39897, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 13s - loss: 2.2874 - acc: 0.9219 - mDice: 0.4255 - val_loss: 2.6727 - val_acc: 0.9325 - val_mDice: 0.4066

Epoch 00029: val_mDice improved from 0.39897 to 0.40662, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 13s - loss: 2.2435 - acc: 0.9233 - mDice: 0.4348 - val_loss: 2.7752 - val_acc: 0.9379 - val_mDice: 0.4118

Epoch 00030: val_mDice improved from 0.40662 to 0.41185, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 14s - loss: 2.1956 - acc: 0.9247 - mDice: 0.4453 - val_loss: 2.9166 - val_acc: 0.9276 - val_mDice: 0.3961

Epoch 00031: val_mDice did not improve from 0.41185
Epoch 32/300
 - 14s - loss: 2.1510 - acc: 0.9255 - mDice: 0.4544 - val_loss: 2.8442 - val_acc: 0.9388 - val_mDice: 0.4161

Epoch 00032: val_mDice improved from 0.41185 to 0.41610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 13s - loss: 2.1251 - acc: 0.9262 - mDice: 0.4605 - val_loss: 2.8951 - val_acc: 0.9384 - val_mDice: 0.4153

Epoch 00033: val_mDice did not improve from 0.41610
Epoch 34/300
 - 14s - loss: 2.0898 - acc: 0.9270 - mDice: 0.4675 - val_loss: 2.8704 - val_acc: 0.9375 - val_mDice: 0.4208

Epoch 00034: val_mDice improved from 0.41610 to 0.42079, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 14s - loss: 2.0517 - acc: 0.9280 - mDice: 0.4766 - val_loss: 2.9394 - val_acc: 0.9380 - val_mDice: 0.4189

Epoch 00035: val_mDice did not improve from 0.42079
Epoch 36/300
 - 13s - loss: 2.0333 - acc: 0.9284 - mDice: 0.4809 - val_loss: 2.8393 - val_acc: 0.9365 - val_mDice: 0.4223

Epoch 00036: val_mDice improved from 0.42079 to 0.42226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 14s - loss: 1.9966 - acc: 0.9292 - mDice: 0.4889 - val_loss: 2.9491 - val_acc: 0.9380 - val_mDice: 0.4337

Epoch 00037: val_mDice improved from 0.42226 to 0.43368, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 14s - loss: 1.9735 - acc: 0.9297 - mDice: 0.4945 - val_loss: 2.9071 - val_acc: 0.9299 - val_mDice: 0.4261

Epoch 00038: val_mDice did not improve from 0.43368
Epoch 39/300
 - 13s - loss: 1.9476 - acc: 0.9302 - mDice: 0.5005 - val_loss: 2.8841 - val_acc: 0.9388 - val_mDice: 0.4346

Epoch 00039: val_mDice improved from 0.43368 to 0.43456, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 14s - loss: 1.9238 - acc: 0.9307 - mDice: 0.5060 - val_loss: 2.8190 - val_acc: 0.9397 - val_mDice: 0.4429

Epoch 00040: val_mDice improved from 0.43456 to 0.44289, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 14s - loss: 1.8933 - acc: 0.9314 - mDice: 0.5127 - val_loss: 2.9730 - val_acc: 0.9254 - val_mDice: 0.4242

Epoch 00041: val_mDice did not improve from 0.44289
Epoch 42/300
 - 13s - loss: 1.8789 - acc: 0.9316 - mDice: 0.5163 - val_loss: 3.0029 - val_acc: 0.9340 - val_mDice: 0.4319

Epoch 00042: val_mDice did not improve from 0.44289
Epoch 43/300
 - 14s - loss: 1.8629 - acc: 0.9320 - mDice: 0.5201 - val_loss: 2.8937 - val_acc: 0.9294 - val_mDice: 0.4354

Epoch 00043: val_mDice did not improve from 0.44289
Epoch 44/300
 - 14s - loss: 1.8482 - acc: 0.9324 - mDice: 0.5242 - val_loss: 2.9608 - val_acc: 0.9373 - val_mDice: 0.4383

Epoch 00044: val_mDice did not improve from 0.44289
Epoch 45/300
 - 13s - loss: 1.8206 - acc: 0.9330 - mDice: 0.5295 - val_loss: 2.8447 - val_acc: 0.9364 - val_mDice: 0.4487

Epoch 00045: val_mDice improved from 0.44289 to 0.44871, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 14s - loss: 1.8012 - acc: 0.9335 - mDice: 0.5343 - val_loss: 2.8377 - val_acc: 0.9309 - val_mDice: 0.4393

Epoch 00046: val_mDice did not improve from 0.44871
Epoch 47/300
 - 13s - loss: 1.7906 - acc: 0.9335 - mDice: 0.5372 - val_loss: 3.0141 - val_acc: 0.9332 - val_mDice: 0.4369

Epoch 00047: val_mDice did not improve from 0.44871
Epoch 48/300
 - 13s - loss: 1.7741 - acc: 0.9340 - mDice: 0.5414 - val_loss: 2.9270 - val_acc: 0.9261 - val_mDice: 0.4430

Epoch 00048: val_mDice did not improve from 0.44871
Epoch 49/300
 - 14s - loss: 1.7593 - acc: 0.9341 - mDice: 0.5447 - val_loss: 2.7812 - val_acc: 0.9374 - val_mDice: 0.4632

Epoch 00049: val_mDice improved from 0.44871 to 0.46319, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 14s - loss: 1.7397 - acc: 0.9347 - mDice: 0.5495 - val_loss: 2.9410 - val_acc: 0.9289 - val_mDice: 0.4391

Epoch 00050: val_mDice did not improve from 0.46319
Epoch 51/300
 - 14s - loss: 1.7345 - acc: 0.9347 - mDice: 0.5510 - val_loss: 3.1432 - val_acc: 0.9276 - val_mDice: 0.4314

Epoch 00051: val_mDice did not improve from 0.46319
Epoch 52/300
 - 14s - loss: 1.7228 - acc: 0.9351 - mDice: 0.5535 - val_loss: 2.9718 - val_acc: 0.9312 - val_mDice: 0.4457

Epoch 00052: val_mDice did not improve from 0.46319
Epoch 53/300
 - 14s - loss: 1.6996 - acc: 0.9355 - mDice: 0.5587 - val_loss: 2.8855 - val_acc: 0.9378 - val_mDice: 0.4564

Epoch 00053: val_mDice did not improve from 0.46319
Epoch 54/300
 - 13s - loss: 1.6968 - acc: 0.9357 - mDice: 0.5604 - val_loss: 3.0341 - val_acc: 0.9299 - val_mDice: 0.4427

Epoch 00054: val_mDice did not improve from 0.46319
Epoch 55/300
 - 14s - loss: 1.6746 - acc: 0.9360 - mDice: 0.5646 - val_loss: 2.9872 - val_acc: 0.9350 - val_mDice: 0.4521

Epoch 00055: val_mDice did not improve from 0.46319
Epoch 56/300
 - 14s - loss: 1.6685 - acc: 0.9362 - mDice: 0.5675 - val_loss: 3.0380 - val_acc: 0.9408 - val_mDice: 0.4589

Epoch 00056: val_mDice did not improve from 0.46319
Epoch 57/300
 - 13s - loss: 1.6591 - acc: 0.9367 - mDice: 0.5687 - val_loss: 2.9191 - val_acc: 0.9405 - val_mDice: 0.4651

Epoch 00057: val_mDice improved from 0.46319 to 0.46508, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 14s - loss: 1.6453 - acc: 0.9367 - mDice: 0.5722 - val_loss: 3.1062 - val_acc: 0.9323 - val_mDice: 0.4474

Epoch 00058: val_mDice did not improve from 0.46508
Epoch 59/300
 - 14s - loss: 1.6352 - acc: 0.9369 - mDice: 0.5743 - val_loss: 2.8672 - val_acc: 0.9329 - val_mDice: 0.4600

Epoch 00059: val_mDice did not improve from 0.46508
Epoch 60/300
 - 13s - loss: 1.6323 - acc: 0.9371 - mDice: 0.5760 - val_loss: 2.9094 - val_acc: 0.9390 - val_mDice: 0.4642

Epoch 00060: val_mDice did not improve from 0.46508
Epoch 61/300
 - 13s - loss: 1.6152 - acc: 0.9374 - mDice: 0.5791 - val_loss: 2.9573 - val_acc: 0.9411 - val_mDice: 0.4680

Epoch 00061: val_mDice improved from 0.46508 to 0.46796, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 13s - loss: 1.6069 - acc: 0.9377 - mDice: 0.5816 - val_loss: 3.0476 - val_acc: 0.9337 - val_mDice: 0.4548

Epoch 00062: val_mDice did not improve from 0.46796
Epoch 63/300
 - 14s - loss: 1.6030 - acc: 0.9379 - mDice: 0.5826 - val_loss: 3.0453 - val_acc: 0.9362 - val_mDice: 0.4575

Epoch 00063: val_mDice did not improve from 0.46796
Epoch 64/300
 - 13s - loss: 1.5880 - acc: 0.9381 - mDice: 0.5861 - val_loss: 2.9481 - val_acc: 0.9310 - val_mDice: 0.4543

Epoch 00064: val_mDice did not improve from 0.46796
Epoch 65/300
 - 13s - loss: 1.5845 - acc: 0.9382 - mDice: 0.5870 - val_loss: 3.0721 - val_acc: 0.9210 - val_mDice: 0.4382

Epoch 00065: val_mDice did not improve from 0.46796
Epoch 66/300
 - 13s - loss: 1.5719 - acc: 0.9385 - mDice: 0.5898 - val_loss: 2.9206 - val_acc: 0.9330 - val_mDice: 0.4684

Epoch 00066: val_mDice improved from 0.46796 to 0.46835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 13s - loss: 1.5746 - acc: 0.9383 - mDice: 0.5897 - val_loss: 3.0458 - val_acc: 0.9373 - val_mDice: 0.4667

Epoch 00067: val_mDice did not improve from 0.46835
Epoch 68/300
 - 14s - loss: 1.5628 - acc: 0.9386 - mDice: 0.5917 - val_loss: 2.8776 - val_acc: 0.9356 - val_mDice: 0.4760

Epoch 00068: val_mDice improved from 0.46835 to 0.47604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 14s - loss: 1.5600 - acc: 0.9388 - mDice: 0.5924 - val_loss: 2.9487 - val_acc: 0.9312 - val_mDice: 0.4618

Epoch 00069: val_mDice did not improve from 0.47604
Epoch 70/300
 - 13s - loss: 1.5523 - acc: 0.9390 - mDice: 0.5942 - val_loss: 3.0749 - val_acc: 0.9398 - val_mDice: 0.4629

Epoch 00070: val_mDice did not improve from 0.47604
Epoch 71/300
 - 13s - loss: 1.5373 - acc: 0.9395 - mDice: 0.5985 - val_loss: 3.2530 - val_acc: 0.9397 - val_mDice: 0.4553

Epoch 00071: val_mDice did not improve from 0.47604
Epoch 72/300
 - 13s - loss: 1.5364 - acc: 0.9398 - mDice: 0.5981 - val_loss: 3.1093 - val_acc: 0.9382 - val_mDice: 0.4638

Epoch 00072: val_mDice did not improve from 0.47604
Epoch 73/300
 - 13s - loss: 1.5205 - acc: 0.9400 - mDice: 0.6015 - val_loss: 3.0348 - val_acc: 0.9257 - val_mDice: 0.4530

Epoch 00073: val_mDice did not improve from 0.47604
Epoch 74/300
 - 14s - loss: 1.5246 - acc: 0.9399 - mDice: 0.6012 - val_loss: 3.0462 - val_acc: 0.9396 - val_mDice: 0.4679

Epoch 00074: val_mDice did not improve from 0.47604
Epoch 75/300
 - 14s - loss: 1.5066 - acc: 0.9404 - mDice: 0.6053 - val_loss: 2.9371 - val_acc: 0.9365 - val_mDice: 0.4685

Epoch 00075: val_mDice did not improve from 0.47604
Epoch 76/300
 - 13s - loss: 1.5136 - acc: 0.9402 - mDice: 0.6040 - val_loss: 3.1105 - val_acc: 0.9374 - val_mDice: 0.4581

Epoch 00076: val_mDice did not improve from 0.47604
Epoch 77/300
 - 13s - loss: 1.5146 - acc: 0.9401 - mDice: 0.6033 - val_loss: 2.9588 - val_acc: 0.9356 - val_mDice: 0.4636

Epoch 00077: val_mDice did not improve from 0.47604
Epoch 78/300
 - 13s - loss: 1.4986 - acc: 0.9408 - mDice: 0.6074 - val_loss: 3.2108 - val_acc: 0.9427 - val_mDice: 0.4641

Epoch 00078: val_mDice did not improve from 0.47604
Epoch 79/300
 - 13s - loss: 1.4963 - acc: 0.9406 - mDice: 0.6075 - val_loss: 2.9860 - val_acc: 0.9389 - val_mDice: 0.4695

Epoch 00079: val_mDice did not improve from 0.47604
Epoch 80/300
 - 14s - loss: 1.4868 - acc: 0.9411 - mDice: 0.6101 - val_loss: 3.1123 - val_acc: 0.9401 - val_mDice: 0.4720

Epoch 00080: val_mDice did not improve from 0.47604
Epoch 81/300
 - 14s - loss: 1.4798 - acc: 0.9413 - mDice: 0.6119 - val_loss: 3.0556 - val_acc: 0.9400 - val_mDice: 0.4683

Epoch 00081: val_mDice did not improve from 0.47604
Epoch 82/300
 - 13s - loss: 1.4805 - acc: 0.9413 - mDice: 0.6117 - val_loss: 3.1344 - val_acc: 0.9361 - val_mDice: 0.4579

Epoch 00082: val_mDice did not improve from 0.47604
Epoch 83/300
 - 14s - loss: 1.4763 - acc: 0.9415 - mDice: 0.6130 - val_loss: 3.0003 - val_acc: 0.9404 - val_mDice: 0.4750

Epoch 00083: val_mDice did not improve from 0.47604
Epoch 84/300
 - 14s - loss: 1.4670 - acc: 0.9416 - mDice: 0.6150 - val_loss: 3.1048 - val_acc: 0.9336 - val_mDice: 0.4627

Epoch 00084: val_mDice did not improve from 0.47604
Epoch 85/300
 - 14s - loss: 1.4583 - acc: 0.9419 - mDice: 0.6171 - val_loss: 2.9815 - val_acc: 0.9390 - val_mDice: 0.4802

Epoch 00085: val_mDice improved from 0.47604 to 0.48025, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 86/300
 - 14s - loss: 1.4666 - acc: 0.9416 - mDice: 0.6156 - val_loss: 3.0255 - val_acc: 0.9383 - val_mDice: 0.4682

Epoch 00086: val_mDice did not improve from 0.48025
Epoch 87/300
 - 13s - loss: 1.4605 - acc: 0.9417 - mDice: 0.6165 - val_loss: 3.3336 - val_acc: 0.9414 - val_mDice: 0.4632

Epoch 00087: val_mDice did not improve from 0.48025
Epoch 88/300
 - 14s - loss: 1.4497 - acc: 0.9420 - mDice: 0.6195 - val_loss: 3.0607 - val_acc: 0.9381 - val_mDice: 0.4701

Epoch 00088: val_mDice did not improve from 0.48025
Epoch 89/300
 - 14s - loss: 1.4448 - acc: 0.9421 - mDice: 0.6200 - val_loss: 3.2861 - val_acc: 0.9384 - val_mDice: 0.4641

Epoch 00089: val_mDice did not improve from 0.48025
Epoch 90/300
 - 13s - loss: 1.4400 - acc: 0.9422 - mDice: 0.6218 - val_loss: 3.2624 - val_acc: 0.9378 - val_mDice: 0.4623

Epoch 00090: val_mDice did not improve from 0.48025
Epoch 91/300
 - 14s - loss: 1.4363 - acc: 0.9423 - mDice: 0.6228 - val_loss: 3.0600 - val_acc: 0.9278 - val_mDice: 0.4650

Epoch 00091: val_mDice did not improve from 0.48025
Epoch 92/300
 - 14s - loss: 1.4387 - acc: 0.9423 - mDice: 0.6222 - val_loss: 2.9067 - val_acc: 0.9366 - val_mDice: 0.4811

Epoch 00092: val_mDice improved from 0.48025 to 0.48115, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 93/300
 - 13s - loss: 1.4291 - acc: 0.9425 - mDice: 0.6238 - val_loss: 3.0089 - val_acc: 0.9338 - val_mDice: 0.4663

Epoch 00093: val_mDice did not improve from 0.48115
Epoch 94/300
 - 13s - loss: 1.4265 - acc: 0.9427 - mDice: 0.6246 - val_loss: 3.2378 - val_acc: 0.9362 - val_mDice: 0.4596

Epoch 00094: val_mDice did not improve from 0.48115
Epoch 95/300
 - 14s - loss: 1.4289 - acc: 0.9427 - mDice: 0.6244 - val_loss: 3.3965 - val_acc: 0.9389 - val_mDice: 0.4599

Epoch 00095: val_mDice did not improve from 0.48115
Epoch 96/300
 - 14s - loss: 1.4210 - acc: 0.9429 - mDice: 0.6265 - val_loss: 3.1296 - val_acc: 0.9370 - val_mDice: 0.4714

Epoch 00096: val_mDice did not improve from 0.48115
Epoch 97/300
 - 13s - loss: 1.4135 - acc: 0.9431 - mDice: 0.6276 - val_loss: 3.0580 - val_acc: 0.9411 - val_mDice: 0.4784

Epoch 00097: val_mDice did not improve from 0.48115
Epoch 98/300
 - 13s - loss: 1.4129 - acc: 0.9431 - mDice: 0.6282 - val_loss: 3.0189 - val_acc: 0.9364 - val_mDice: 0.4737

Epoch 00098: val_mDice did not improve from 0.48115
Epoch 99/300
 - 13s - loss: 1.4066 - acc: 0.9434 - mDice: 0.6293 - val_loss: 3.1862 - val_acc: 0.9386 - val_mDice: 0.4678

Epoch 00099: val_mDice did not improve from 0.48115
Epoch 100/300
 - 14s - loss: 1.4050 - acc: 0.9432 - mDice: 0.6295 - val_loss: 3.2173 - val_acc: 0.9410 - val_mDice: 0.4695

Epoch 00100: val_mDice did not improve from 0.48115
Epoch 101/300
 - 14s - loss: 1.4002 - acc: 0.9434 - mDice: 0.6308 - val_loss: 3.2288 - val_acc: 0.9381 - val_mDice: 0.4680

Epoch 00101: val_mDice did not improve from 0.48115
Epoch 102/300
 - 13s - loss: 1.3974 - acc: 0.9434 - mDice: 0.6324 - val_loss: 3.1533 - val_acc: 0.9330 - val_mDice: 0.4644

Epoch 00102: val_mDice did not improve from 0.48115
Epoch 103/300
 - 13s - loss: 1.3926 - acc: 0.9435 - mDice: 0.6330 - val_loss: 3.0880 - val_acc: 0.9320 - val_mDice: 0.4677

Epoch 00103: val_mDice did not improve from 0.48115
Epoch 104/300
 - 13s - loss: 1.3948 - acc: 0.9435 - mDice: 0.6328 - val_loss: 3.1506 - val_acc: 0.9303 - val_mDice: 0.4685

Epoch 00104: val_mDice did not improve from 0.48115
Epoch 105/300
 - 13s - loss: 1.3934 - acc: 0.9433 - mDice: 0.6332 - val_loss: 2.8323 - val_acc: 0.9353 - val_mDice: 0.4817

Epoch 00105: val_mDice improved from 0.48115 to 0.48170, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 14s - loss: 1.3872 - acc: 0.9436 - mDice: 0.6343 - val_loss: 3.0411 - val_acc: 0.9329 - val_mDice: 0.4649

Epoch 00106: val_mDice did not improve from 0.48170
Epoch 107/300
 - 13s - loss: 1.3829 - acc: 0.9438 - mDice: 0.6350 - val_loss: 3.3005 - val_acc: 0.9350 - val_mDice: 0.4614

Epoch 00107: val_mDice did not improve from 0.48170
Epoch 108/300
 - 13s - loss: 1.3851 - acc: 0.9436 - mDice: 0.6354 - val_loss: 3.0449 - val_acc: 0.9343 - val_mDice: 0.4724

Epoch 00108: val_mDice did not improve from 0.48170
Epoch 109/300
 - 13s - loss: 1.3779 - acc: 0.9439 - mDice: 0.6368 - val_loss: 3.0833 - val_acc: 0.9308 - val_mDice: 0.4718

Epoch 00109: val_mDice did not improve from 0.48170
Epoch 110/300
 - 13s - loss: 1.3789 - acc: 0.9440 - mDice: 0.6363 - val_loss: 2.9013 - val_acc: 0.9379 - val_mDice: 0.4790

Epoch 00110: val_mDice did not improve from 0.48170
Epoch 111/300
 - 14s - loss: 1.3737 - acc: 0.9438 - mDice: 0.6376 - val_loss: 3.2983 - val_acc: 0.9285 - val_mDice: 0.4521

Epoch 00111: val_mDice did not improve from 0.48170
Epoch 112/300
 - 14s - loss: 1.3698 - acc: 0.9440 - mDice: 0.6387 - val_loss: 3.3818 - val_acc: 0.9323 - val_mDice: 0.4532

Epoch 00112: val_mDice did not improve from 0.48170
Epoch 113/300
 - 13s - loss: 1.3690 - acc: 0.9442 - mDice: 0.6390 - val_loss: 3.2239 - val_acc: 0.9352 - val_mDice: 0.4672

Epoch 00113: val_mDice did not improve from 0.48170
Epoch 114/300
 - 13s - loss: 1.3612 - acc: 0.9443 - mDice: 0.6404 - val_loss: 3.1343 - val_acc: 0.9383 - val_mDice: 0.4723

Epoch 00114: val_mDice did not improve from 0.48170
Epoch 115/300
 - 13s - loss: 1.3616 - acc: 0.9443 - mDice: 0.6404 - val_loss: 3.4589 - val_acc: 0.9390 - val_mDice: 0.4560

Epoch 00115: val_mDice did not improve from 0.48170
Epoch 116/300
 - 14s - loss: 1.3603 - acc: 0.9443 - mDice: 0.6409 - val_loss: 3.2539 - val_acc: 0.9334 - val_mDice: 0.4589

Epoch 00116: val_mDice did not improve from 0.48170
Epoch 117/300
 - 14s - loss: 1.3531 - acc: 0.9446 - mDice: 0.6424 - val_loss: 3.1288 - val_acc: 0.9375 - val_mDice: 0.4764

Epoch 00117: val_mDice did not improve from 0.48170
Epoch 118/300
 - 14s - loss: 1.3483 - acc: 0.9447 - mDice: 0.6434 - val_loss: 3.0331 - val_acc: 0.9381 - val_mDice: 0.4804

Epoch 00118: val_mDice did not improve from 0.48170
Epoch 119/300
 - 13s - loss: 1.3530 - acc: 0.9445 - mDice: 0.6425 - val_loss: 3.2812 - val_acc: 0.9369 - val_mDice: 0.4662

Epoch 00119: val_mDice did not improve from 0.48170
Epoch 120/300
 - 13s - loss: 1.3469 - acc: 0.9446 - mDice: 0.6439 - val_loss: 3.2069 - val_acc: 0.9375 - val_mDice: 0.4775

Epoch 00120: val_mDice did not improve from 0.48170
Epoch 121/300
 - 14s - loss: 1.3513 - acc: 0.9446 - mDice: 0.6434 - val_loss: 3.5846 - val_acc: 0.9374 - val_mDice: 0.4521

Epoch 00121: val_mDice did not improve from 0.48170
Epoch 122/300
 - 14s - loss: 1.3420 - acc: 0.9447 - mDice: 0.6456 - val_loss: 3.3565 - val_acc: 0.9372 - val_mDice: 0.4673

Epoch 00122: val_mDice did not improve from 0.48170
Epoch 123/300
 - 13s - loss: 1.3438 - acc: 0.9448 - mDice: 0.6450 - val_loss: 3.1954 - val_acc: 0.9334 - val_mDice: 0.4676

Epoch 00123: val_mDice did not improve from 0.48170
Epoch 124/300
 - 13s - loss: 1.3360 - acc: 0.9449 - mDice: 0.6467 - val_loss: 3.2910 - val_acc: 0.9386 - val_mDice: 0.4693

Epoch 00124: val_mDice did not improve from 0.48170
Epoch 125/300
 - 13s - loss: 1.3348 - acc: 0.9450 - mDice: 0.6471 - val_loss: 2.9938 - val_acc: 0.9390 - val_mDice: 0.4744

Epoch 00125: val_mDice did not improve from 0.48170
Epoch 126/300
 - 14s - loss: 1.3348 - acc: 0.9449 - mDice: 0.6472 - val_loss: 3.5971 - val_acc: 0.9389 - val_mDice: 0.4552

Epoch 00126: val_mDice did not improve from 0.48170
Epoch 127/300
 - 14s - loss: 1.3346 - acc: 0.9450 - mDice: 0.6469 - val_loss: 3.0313 - val_acc: 0.9393 - val_mDice: 0.4831

Epoch 00127: val_mDice improved from 0.48170 to 0.48305, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 128/300
 - 13s - loss: 1.3274 - acc: 0.9451 - mDice: 0.6488 - val_loss: 3.3721 - val_acc: 0.9367 - val_mDice: 0.4607

Epoch 00128: val_mDice did not improve from 0.48305
Epoch 129/300
 - 13s - loss: 1.3299 - acc: 0.9451 - mDice: 0.6490 - val_loss: 3.3291 - val_acc: 0.9400 - val_mDice: 0.4729

Epoch 00129: val_mDice did not improve from 0.48305
Epoch 130/300
 - 13s - loss: 1.3267 - acc: 0.9452 - mDice: 0.6493 - val_loss: 3.3250 - val_acc: 0.9412 - val_mDice: 0.4749

Epoch 00130: val_mDice did not improve from 0.48305
Epoch 131/300
 - 13s - loss: 1.3225 - acc: 0.9453 - mDice: 0.6504 - val_loss: 3.2699 - val_acc: 0.9383 - val_mDice: 0.4769

Epoch 00131: val_mDice did not improve from 0.48305
Epoch 132/300
 - 14s - loss: 1.3321 - acc: 0.9449 - mDice: 0.6483 - val_loss: 3.3983 - val_acc: 0.9359 - val_mDice: 0.4679

Epoch 00132: val_mDice did not improve from 0.48305
Epoch 133/300
 - 14s - loss: 1.3173 - acc: 0.9453 - mDice: 0.6509 - val_loss: 3.1954 - val_acc: 0.9375 - val_mDice: 0.4728

Epoch 00133: val_mDice did not improve from 0.48305
Epoch 134/300
 - 13s - loss: 1.3146 - acc: 0.9454 - mDice: 0.6522 - val_loss: 3.2238 - val_acc: 0.9356 - val_mDice: 0.4704

Epoch 00134: val_mDice did not improve from 0.48305
Epoch 135/300
 - 13s - loss: 1.3154 - acc: 0.9454 - mDice: 0.6524 - val_loss: 3.2804 - val_acc: 0.9346 - val_mDice: 0.4667

Epoch 00135: val_mDice did not improve from 0.48305
Epoch 136/300
 - 13s - loss: 1.3169 - acc: 0.9454 - mDice: 0.6518 - val_loss: 4.2809 - val_acc: 0.9350 - val_mDice: 0.4120

Epoch 00136: val_mDice did not improve from 0.48305
Epoch 137/300
 - 13s - loss: 1.3126 - acc: 0.9455 - mDice: 0.6524 - val_loss: 3.1077 - val_acc: 0.9390 - val_mDice: 0.4850

Epoch 00137: val_mDice improved from 0.48305 to 0.48497, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 13s - loss: 1.3084 - acc: 0.9455 - mDice: 0.6539 - val_loss: 3.3414 - val_acc: 0.9410 - val_mDice: 0.4698

Epoch 00138: val_mDice did not improve from 0.48497
Epoch 139/300
 - 14s - loss: 1.3079 - acc: 0.9455 - mDice: 0.6539 - val_loss: 3.4118 - val_acc: 0.9425 - val_mDice: 0.4726

Epoch 00139: val_mDice did not improve from 0.48497
Epoch 140/300
 - 14s - loss: 1.3083 - acc: 0.9456 - mDice: 0.6538 - val_loss: 3.0870 - val_acc: 0.9390 - val_mDice: 0.4869

Epoch 00140: val_mDice improved from 0.48497 to 0.48690, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 141/300
 - 13s - loss: 1.3051 - acc: 0.9456 - mDice: 0.6548 - val_loss: 3.0965 - val_acc: 0.9392 - val_mDice: 0.4801

Epoch 00141: val_mDice did not improve from 0.48690
Epoch 142/300
 - 13s - loss: 1.3076 - acc: 0.9456 - mDice: 0.6539 - val_loss: 3.1788 - val_acc: 0.9338 - val_mDice: 0.4726

Epoch 00142: val_mDice did not improve from 0.48690
Epoch 143/300
 - 13s - loss: 1.2983 - acc: 0.9458 - mDice: 0.6559 - val_loss: 3.4016 - val_acc: 0.9354 - val_mDice: 0.4575

Epoch 00143: val_mDice did not improve from 0.48690
Epoch 144/300
 - 13s - loss: 1.3020 - acc: 0.9457 - mDice: 0.6552 - val_loss: 3.3413 - val_acc: 0.9388 - val_mDice: 0.4756

Epoch 00144: val_mDice did not improve from 0.48690
Epoch 145/300
 - 13s - loss: 1.3001 - acc: 0.9457 - mDice: 0.6562 - val_loss: 3.2874 - val_acc: 0.9336 - val_mDice: 0.4621

Epoch 00145: val_mDice did not improve from 0.48690
Epoch 146/300
 - 13s - loss: 1.2909 - acc: 0.9459 - mDice: 0.6581 - val_loss: 3.5539 - val_acc: 0.9356 - val_mDice: 0.4567

Epoch 00146: val_mDice did not improve from 0.48690
Epoch 147/300
 - 14s - loss: 1.2866 - acc: 0.9461 - mDice: 0.6592 - val_loss: 3.5566 - val_acc: 0.9316 - val_mDice: 0.4548

Epoch 00147: val_mDice did not improve from 0.48690
Epoch 148/300
 - 14s - loss: 1.2926 - acc: 0.9459 - mDice: 0.6575 - val_loss: 3.7988 - val_acc: 0.9245 - val_mDice: 0.4314

Epoch 00148: val_mDice did not improve from 0.48690
Epoch 149/300
 - 13s - loss: 1.2849 - acc: 0.9460 - mDice: 0.6592 - val_loss: 3.3103 - val_acc: 0.9355 - val_mDice: 0.4696

Epoch 00149: val_mDice did not improve from 0.48690
Epoch 150/300
 - 13s - loss: 1.2867 - acc: 0.9461 - mDice: 0.6589 - val_loss: 3.1508 - val_acc: 0.9395 - val_mDice: 0.4800

Epoch 00150: val_mDice did not improve from 0.48690
Epoch 151/300
 - 13s - loss: 1.2831 - acc: 0.9461 - mDice: 0.6600 - val_loss: 3.2787 - val_acc: 0.9339 - val_mDice: 0.4674

Epoch 00151: val_mDice did not improve from 0.48690
Epoch 152/300
 - 13s - loss: 1.2889 - acc: 0.9461 - mDice: 0.6583 - val_loss: 3.0899 - val_acc: 0.9384 - val_mDice: 0.4828

Epoch 00152: val_mDice did not improve from 0.48690
Epoch 153/300
 - 13s - loss: 1.2883 - acc: 0.9460 - mDice: 0.6589 - val_loss: 3.2488 - val_acc: 0.9342 - val_mDice: 0.4724

Epoch 00153: val_mDice did not improve from 0.48690
Epoch 154/300
 - 14s - loss: 1.2844 - acc: 0.9462 - mDice: 0.6596 - val_loss: 3.4106 - val_acc: 0.9388 - val_mDice: 0.4664

Epoch 00154: val_mDice did not improve from 0.48690
Epoch 155/300
 - 14s - loss: 1.2827 - acc: 0.9461 - mDice: 0.6601 - val_loss: 3.2260 - val_acc: 0.9363 - val_mDice: 0.4751

Epoch 00155: val_mDice did not improve from 0.48690
Epoch 156/300
 - 13s - loss: 1.2795 - acc: 0.9463 - mDice: 0.6609 - val_loss: 3.3864 - val_acc: 0.9373 - val_mDice: 0.4770

Epoch 00156: val_mDice did not improve from 0.48690
Epoch 157/300
 - 13s - loss: 1.2802 - acc: 0.9462 - mDice: 0.6612 - val_loss: 3.2610 - val_acc: 0.9382 - val_mDice: 0.4711

Epoch 00157: val_mDice did not improve from 0.48690
Epoch 158/300
 - 13s - loss: 1.2747 - acc: 0.9462 - mDice: 0.6621 - val_loss: 3.2146 - val_acc: 0.9384 - val_mDice: 0.4781

Epoch 00158: val_mDice did not improve from 0.48690
Epoch 159/300
 - 13s - loss: 1.2792 - acc: 0.9462 - mDice: 0.6615 - val_loss: 3.1877 - val_acc: 0.9381 - val_mDice: 0.4757

Epoch 00159: val_mDice did not improve from 0.48690
Epoch 160/300
 - 13s - loss: 1.2730 - acc: 0.9463 - mDice: 0.6623 - val_loss: 3.2576 - val_acc: 0.9369 - val_mDice: 0.4757

Epoch 00160: val_mDice did not improve from 0.48690
Epoch 161/300
 - 13s - loss: 1.2693 - acc: 0.9465 - mDice: 0.6635 - val_loss: 3.1887 - val_acc: 0.9365 - val_mDice: 0.4765

Epoch 00161: val_mDice did not improve from 0.48690
Epoch 162/300
 - 13s - loss: 1.2740 - acc: 0.9463 - mDice: 0.6623 - val_loss: 3.2293 - val_acc: 0.9346 - val_mDice: 0.4715

Epoch 00162: val_mDice did not improve from 0.48690
Epoch 163/300
 - 14s - loss: 1.2710 - acc: 0.9466 - mDice: 0.6638 - val_loss: 3.4678 - val_acc: 0.9275 - val_mDice: 0.4538

Epoch 00163: val_mDice did not improve from 0.48690
Epoch 164/300
 - 13s - loss: 1.2787 - acc: 0.9461 - mDice: 0.6611 - val_loss: 3.3969 - val_acc: 0.9347 - val_mDice: 0.4714

Epoch 00164: val_mDice did not improve from 0.48690
Epoch 165/300
 - 13s - loss: 1.2669 - acc: 0.9465 - mDice: 0.6647 - val_loss: 3.7811 - val_acc: 0.9392 - val_mDice: 0.4519

Epoch 00165: val_mDice did not improve from 0.48690
Epoch 166/300
 - 13s - loss: 1.2653 - acc: 0.9465 - mDice: 0.6643 - val_loss: 3.5434 - val_acc: 0.9316 - val_mDice: 0.4551

Epoch 00166: val_mDice did not improve from 0.48690
Epoch 167/300
 - 13s - loss: 1.2640 - acc: 0.9466 - mDice: 0.6649 - val_loss: 3.1558 - val_acc: 0.9362 - val_mDice: 0.4797

Epoch 00167: val_mDice did not improve from 0.48690
Epoch 168/300
 - 13s - loss: 1.2619 - acc: 0.9466 - mDice: 0.6661 - val_loss: 3.1761 - val_acc: 0.9389 - val_mDice: 0.4785

Epoch 00168: val_mDice did not improve from 0.48690
Epoch 169/300
 - 13s - loss: 1.2551 - acc: 0.9467 - mDice: 0.6669 - val_loss: 3.2840 - val_acc: 0.9352 - val_mDice: 0.4678

Epoch 00169: val_mDice did not improve from 0.48690
Epoch 170/300
 - 14s - loss: 1.2545 - acc: 0.9467 - mDice: 0.6669 - val_loss: 3.2742 - val_acc: 0.9389 - val_mDice: 0.4740

Epoch 00170: val_mDice did not improve from 0.48690
Restoring model weights from the end of the best epoch
Epoch 00170: early stopping
{'val_loss': [46.38457943144299, 17.006650354181016, 10.089989616757347, 8.19706683144683, 6.8329942637965795, 6.622182837554386, 5.525811319904668, 5.253612087241241, 5.071368029252405, 4.664145707019737, 4.5683067334549765, 4.127383757027841, 4.069257451221347, 3.6951635105624083, 3.632117462832303, 3.382031867635392, 3.311032898546684, 3.1551090144064453, 3.1438318638219718, 3.1018743876013017, 2.9595872534527663, 2.9133301518325294, 2.873117194156207, 3.0185251016879366, 2.8915473145565818, 2.791694208358725, 2.8028094124137644, 2.7423865214167606, 2.672731094594513, 2.775156434625387, 2.9165640605524894, 2.8442460174805353, 2.8951040666461703, 2.8704297574059594, 2.9394189531338357, 2.8393256215910827, 2.949095175424147, 2.907125085015737, 2.8840824641908207, 2.819033030864029, 2.9730477192926976, 3.0029253015472066, 2.8936719559133053, 2.960773916648967, 2.844665803458719, 2.8377049500122666, 3.014134632090905, 2.9269674820825458, 2.7812095901795795, 2.9410303879440542, 3.1431699757952067, 2.971768487910075, 2.88553701483068, 3.0340664510038637, 2.987191027429487, 3.0379665936565114, 2.919060528455746, 3.106236102547319, 2.8672380292610753, 2.9094062932279137, 2.9573200232837173, 3.047594889155811, 3.045289416797459, 2.948078711073668, 3.0720681269608794, 2.9205967011373666, 3.0458082032522986, 2.8775994066769877, 2.9486537368169854, 3.074927455390848, 3.2530119452359423, 3.1093309270439757, 3.034825654478655, 3.0462027154419395, 2.9370966330614117, 3.110525966156274, 2.958784382790327, 3.210830952617384, 2.986032000521109, 3.1123214726824138, 3.0556219249875065, 3.1344148474745452, 3.0003028353454457, 3.1048117275128053, 2.9814720899531886, 3.0255207801237702, 3.333616431735988, 3.0606614238405156, 3.286061697373433, 3.2623553284044777, 3.0599879859281436, 2.906749886992787, 3.008937714643599, 3.237750406842679, 3.3965122699072317, 3.1295868563465774, 3.057991456018672, 3.018882704011741, 3.18621620400587, 3.217313638223069, 3.2288137940867316, 3.1533195340917226, 3.0880005135245265, 3.1505641472925032, 2.8323288719568933, 3.0411490719055845, 3.300530532951511, 3.0449159009204734, 3.083276392448516, 2.9013235107330337, 3.2983226847080958, 3.381771983756196, 3.223884697515695, 3.1343074432529865, 3.4588938861020972, 3.253905855651413, 3.128776041968238, 3.033128186173382, 3.2811539434783517, 3.2068846583632484, 3.5846009876002514, 3.3564810143961084, 3.1953665612797653, 3.2910309336369945, 2.993829809528376, 3.5970844680088616, 3.0312580498866737, 3.372145321264508, 3.3290606723388745, 3.325003854420391, 3.2698813006281853, 3.3982674707436846, 3.195361352037816, 3.2237591788261417, 3.280390418090281, 4.280919716604764, 3.1077001784440306, 3.341408164107374, 3.411837429872581, 3.0869964008618678, 3.0965029377756372, 3.1788346699572037, 3.4015554100319387, 3.341309068901908, 3.287411342153237, 3.5539186349848197, 3.5566262261764634, 3.79883192018384, 3.310288754824017, 3.1507679877714034, 3.278652508964851, 3.089870376867198, 3.24884706190122, 3.410569358129232, 3.2260477949111235, 3.386413536123222, 3.261024508226131, 3.2145901894019473, 3.187674995112632, 3.257577690428921, 3.188741200692242, 3.22929051797837, 3.467771736284097, 3.3969049731801664, 3.7810914516448975, 3.543428668796661, 3.1557749338181957, 3.176136182532424, 3.283970831095108, 3.2741590393707156], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047207009224665, 0.9054395726748875, 0.9087774668421064, 0.9097000899769011, 0.9135897727239699, 0.9171428765569415, 0.9195902092116219, 0.9219757176580883, 0.9221954998515901, 0.9286080797513326, 0.9226991477466765, 0.922193223521823, 0.9242170112473624, 0.9166850107056754, 0.9257028159641084, 0.9271336793899536, 0.9365132649739584, 0.9333516416095552, 0.9325000047683716, 0.9378594301995777, 0.9275892802647182, 0.9387774666150411, 0.9384066008386158, 0.9374587876456124, 0.9380105222974505, 0.936547611440931, 0.9379555838448661, 0.9299450516700745, 0.9387591481208801, 0.9397207044419789, 0.9253548809460231, 0.9340384574163527, 0.9293956018629528, 0.9372710443678356, 0.936391952491942, 0.930934074379149, 0.9331936637560526, 0.926080564657847, 0.9373832316625685, 0.928914813768296, 0.9275892972946167, 0.9311767504328773, 0.9378204884983244, 0.9299038705371675, 0.9350320327849615, 0.9408218576794579, 0.9405128104346139, 0.9322985609372457, 0.932939574832008, 0.9390384498096648, 0.9411332380203974, 0.9337065276645479, 0.9361790276709057, 0.9309615123839605, 0.9210393797783625, 0.9330128402937026, 0.9372916391917637, 0.9356341816130138, 0.9312385320663452, 0.9397916538374764, 0.9397069442839849, 0.9381776395298186, 0.9256685205868312, 0.9395833441189357, 0.9365109886441912, 0.9374038264864967, 0.9356410191172645, 0.9427083418482826, 0.9388850586754935, 0.9400847270375207, 0.9399564975783938, 0.936112662156423, 0.9404349639302209, 0.9335576778366452, 0.9390269886879694, 0.9383333524068197, 0.9413896572022211, 0.9381364442053295, 0.9383631093161446, 0.9378456955864316, 0.9277930373237246, 0.9366002792403811, 0.9338416003045582, 0.9361904973075503, 0.9389217297236124, 0.937044396286919, 0.9410714336803981, 0.9363759387107122, 0.9385714474178496, 0.9410393947646731, 0.9380906479699271, 0.9329716109094166, 0.9319665517125811, 0.9303388141450428, 0.9352953547523135, 0.9329372871489752, 0.9350091673078991, 0.9343429463250297, 0.9308081723394848, 0.9379280919120425, 0.9285210739998591, 0.9323351439975557, 0.9351785778999329, 0.9383195780572438, 0.9389995336532593, 0.9333951615151905, 0.9374542151178632, 0.9380769417399452, 0.936923086643219, 0.9375068460191999, 0.9373603690238226, 0.9372298575582958, 0.9334340805099124, 0.9385577156430199, 0.9389904084659758, 0.9388530424662999, 0.9392536396072024, 0.9366895471300397, 0.9399862488110861, 0.9412385282062349, 0.9383058377674648, 0.9359409582047236, 0.9374885559082031, 0.9355677763621012, 0.9346222565287635, 0.9350457929429554, 0.9390453582718259, 0.941000433195205, 0.9425137128148761, 0.9389720644269671, 0.9392284750938416, 0.9337820439111619, 0.9353686145373753, 0.938782050496056, 0.9335691645031884, 0.9356227091380528, 0.9316094091960362, 0.9245032213983082, 0.9354899128278097, 0.9394620202836537, 0.9339491668201628, 0.9384363293647766, 0.9341506134896052, 0.9387843183108738, 0.9362980865296864, 0.9372504410289583, 0.9382005504199437, 0.9383745392163595, 0.9380975252106076, 0.9369299723988488, 0.9364949521564302, 0.9346153679348174, 0.9275228892053876, 0.9347046471777416, 0.9391826873733884, 0.9316369011288598, 0.9362408178193229, 0.9389285842577616, 0.9352472538039798, 0.9389446150688898], 'val_mDice': [0.014744778651566733, 0.015170773003427755, 0.01648449350059742, 0.021533890294709375, 0.026545712713240868, 0.02933884379897444, 0.03401151904836297, 0.04517054156444612, 0.05198295514232346, 0.07246245257556438, 0.08334938019868873, 0.11422698563408284, 0.12617341016552278, 0.16192884814171565, 0.17796244587571847, 0.21090929698021638, 0.2313206752850896, 0.2588042693123931, 0.27664561321338016, 0.29586496266225976, 0.32292126296531587, 0.342386004115854, 0.3528122731617519, 0.35269723442338763, 0.3575381469868478, 0.3829091741215615, 0.38916866587741034, 0.3989678110395159, 0.40662006643556414, 0.4118450028555734, 0.39612802401894615, 0.4161022440308616, 0.4152668728714898, 0.42078608380896704, 0.41893273024331956, 0.42226042846838635, 0.4336764612013385, 0.426071279105686, 0.4345640431912172, 0.4428945534995624, 0.42418040796404793, 0.4318603211570354, 0.43537895204055876, 0.43826922951709657, 0.4487147263827778, 0.43925720222649123, 0.4368618730278242, 0.4430484094080471, 0.463194318293106, 0.4391139807800452, 0.4314002185350373, 0.4456822487215201, 0.4563991603042398, 0.44265542995362056, 0.45208599134570077, 0.4588715900622663, 0.46508080547764186, 0.4474320700835614, 0.4599755218341237, 0.46421876833552406, 0.46796342357993126, 0.45484210178256035, 0.45748924570424215, 0.4543148417557989, 0.43821144778104054, 0.46835174553451087, 0.4666984716341609, 0.4760360496029967, 0.4617976253586156, 0.46285680273459073, 0.4553130293885867, 0.46375915266218637, 0.4530316846711295, 0.4679039532230014, 0.46851242635221707, 0.4580839757053625, 0.4636303640547253, 0.4641280960114229, 0.4694672977285726, 0.47196451610042933, 0.46827826436076847, 0.4579315504857472, 0.4749768465047791, 0.46274136361621676, 0.4802478090638206, 0.46820949567925363, 0.46324961313179563, 0.4700695978743689, 0.4640879082892622, 0.462341580362547, 0.4650335657809462, 0.48114575871399473, 0.46626907480614527, 0.4596213073957534, 0.45985751244283857, 0.47143171993749483, 0.47839410461130594, 0.47372843236440704, 0.46777484104746864, 0.4695338209470113, 0.4679855207602183, 0.4643678299727894, 0.46769887457291287, 0.46851738045612973, 0.4816968893366201, 0.464922281957808, 0.4614469071938878, 0.47243419555681093, 0.4717654614221482, 0.4790458916908219, 0.45206610255298163, 0.4532416697059359, 0.46723303021419615, 0.472333030686492, 0.4559520176124005, 0.45894403312177884, 0.4763605215010189, 0.48036010847205207, 0.46618651988960447, 0.47747229997600826, 0.4521410124642508, 0.4673054145560378, 0.4676291612642152, 0.46925474348522367, 0.4743892520311333, 0.45518590775983675, 0.48305303550192286, 0.4607297925367242, 0.472870124237878, 0.47493496998434975, 0.47685629608375685, 0.4679494709486053, 0.47283830493688583, 0.47042644804432276, 0.4667316111070769, 0.41196668964056743, 0.484967440366745, 0.46983480151920093, 0.47259372224410373, 0.4868994876742363, 0.48013584989876973, 0.4726268546212287, 0.4574663074953215, 0.47560786899356616, 0.4621434460083644, 0.4567344951487723, 0.4548092922639279, 0.4313802348361129, 0.46955642317022595, 0.4800333684044225, 0.4674496781967935, 0.4827987753919193, 0.47238500096968244, 0.4663964109051795, 0.4750915650455725, 0.47699453344657305, 0.471051146586736, 0.4781275297559443, 0.4757269164990811, 0.4757487937098458, 0.4765480031215009, 0.47154107760815395, 0.4537596053310803, 0.4713635813622248, 0.45191922411322594, 0.4551065683010079, 0.47965928486415316, 0.47845662633577984, 0.46783545045625596, 0.4739718252704257], 'loss': [143.8150839856164, 38.97774208132095, 18.02992439692894, 12.025908291351366, 9.476439995873243, 8.088103369662637, 7.212030384474828, 6.595834128631892, 6.106992565553635, 5.6069390162903145, 5.20538550598074, 4.892168663773942, 4.589792416278213, 4.298222853633189, 4.044715602388266, 3.7965841823766886, 3.5754593816455307, 3.3744630013354278, 3.202494202860753, 3.0626428937185746, 2.936474349426584, 2.8264065136311025, 2.726164128386448, 2.6261719282086653, 2.5420199145189666, 2.4672052472548285, 2.4034759560967998, 2.3397615479209506, 2.287359962671139, 2.2434786492651635, 2.1956119142057626, 2.150966352273762, 2.1250666851295956, 2.0898298456279534, 2.051651662545602, 2.033252140587181, 1.996624832165271, 1.9734649560987916, 1.9476252197414918, 1.9238329523671953, 1.8932897570920584, 1.8788996473175954, 1.8628534181328988, 1.848235884245039, 1.8206409557444387, 1.8011540227113765, 1.7906038426975994, 1.7741195336917888, 1.7592811501276662, 1.7396559369387092, 1.7344641171831006, 1.7228308302004351, 1.6995839767196814, 1.6968206230422447, 1.6745942148004902, 1.6685442852564853, 1.6590823093682372, 1.6453341195319966, 1.6352093394192884, 1.632340037204499, 1.6151713245241817, 1.6069439213837138, 1.6029782175718992, 1.5879573647126035, 1.5844947627359718, 1.5718582576700597, 1.5745868315603868, 1.5627941426363023, 1.56004120707397, 1.5523175704173553, 1.5372627133552752, 1.5364148802947557, 1.5205284898074716, 1.524627511501772, 1.5065735469600794, 1.5136230094643808, 1.5146496421872064, 1.4986191377580935, 1.4962587277967851, 1.4868064833349268, 1.4797819847772868, 1.4805463213255945, 1.4762623143812033, 1.4669732844781296, 1.4583102233703595, 1.4666115570004008, 1.4604844861759578, 1.4497440812305975, 1.4448252809008628, 1.439987650031595, 1.4362634509566297, 1.4387037158127303, 1.4290646921354198, 1.4265079644113832, 1.428885963201201, 1.4210310313475023, 1.4134527035064726, 1.4129440872016983, 1.4065704068536355, 1.4049873178534271, 1.400183381454228, 1.3974115373496723, 1.392612631849087, 1.3947826434082669, 1.393362540763905, 1.3871706973754856, 1.382941184761852, 1.3851105385900395, 1.3778753828538137, 1.3788925559621383, 1.3736955326762255, 1.3697501737211444, 1.3690223315903418, 1.3611905751992264, 1.3615817363399612, 1.3603331757713335, 1.3530818746801008, 1.3483109227007286, 1.3529676036704177, 1.3469241035106394, 1.3512724833334941, 1.341982318000433, 1.3437863535703618, 1.3359726603283504, 1.3347746194362733, 1.3348119032849506, 1.3346105511992428, 1.3273614245686476, 1.3298573154968312, 1.3267457672389964, 1.3225062725308177, 1.3320891324646882, 1.3173058611224653, 1.3145780009710424, 1.3154498383055218, 1.3169461828057698, 1.312572769514738, 1.308393702287308, 1.3078507896744724, 1.3082693856484586, 1.3051041792140936, 1.3076160658249305, 1.2982589834132174, 1.301956205163959, 1.3000905429763052, 1.2909208108102377, 1.2866174953853047, 1.2926341157348396, 1.284903523192048, 1.2866621453200826, 1.2831448287616374, 1.288898777497488, 1.2883235273366722, 1.2844002520920845, 1.282658617586581, 1.279490203199668, 1.280230892217065, 1.2746501353427304, 1.2791631474806575, 1.2729707861856756, 1.2693407120032647, 1.2739696123592192, 1.2710361573101814, 1.27874132281258, 1.266862116063149, 1.2653016797926744, 1.2640151444119434, 1.2618883391761448, 1.2551488563093762, 1.2544982255091206], 'acc': [0.613572047855644, 0.8682305124055588, 0.8692040512827014, 0.8692371390884911, 0.869239316962003, 0.86923989466856, 0.8692394352290174, 0.8692362817330743, 0.8692225906369496, 0.8693003126231374, 0.8701290484795755, 0.8715423681912037, 0.8730652890201707, 0.875173884548616, 0.879527431888481, 0.8843822987502036, 0.887766013720924, 0.8914456052749815, 0.8952989442421564, 0.8985638319354354, 0.9014840124199012, 0.9043806571939306, 0.906740454090384, 0.9092077233208979, 0.9124987969996958, 0.9152046868468793, 0.91773201301421, 0.9198631053488449, 0.9218747921593966, 0.9232945060003739, 0.9247029833603346, 0.9254625062159267, 0.926193557046203, 0.9269748867672717, 0.9280165581659979, 0.92837856929151, 0.9292116648019658, 0.9297432757674189, 0.930188218959384, 0.9307447406674295, 0.9313988793280076, 0.9315584169225655, 0.9319594509113909, 0.932423699079094, 0.9330457222583507, 0.9334627693619185, 0.9335114076368745, 0.9339789915618029, 0.9341459010799703, 0.9347115033781089, 0.9347147015172069, 0.935131654450722, 0.9354991116964084, 0.9356667607741156, 0.9359923270205116, 0.9362130368033976, 0.9366506590810106, 0.9367118134925148, 0.936922398701968, 0.9370624953229142, 0.9374054597594822, 0.937662319110174, 0.9378725811767284, 0.9380768608306905, 0.9382166828588873, 0.9384618623881342, 0.9383296936560808, 0.9385874763903922, 0.9387970876551235, 0.9390263746004011, 0.939486335450936, 0.9397550809583338, 0.9400179475884872, 0.9399439095186778, 0.9403719395393348, 0.9401826954701513, 0.9400738322806078, 0.9407626885496309, 0.9406189998956184, 0.9410917298720709, 0.9412736477869335, 0.9413180219108989, 0.941492626093384, 0.9415997906367352, 0.941875511634228, 0.9415995832098135, 0.9417119908590227, 0.942013896302219, 0.9420913581055714, 0.9421833530312584, 0.9423386717614858, 0.9422862832776517, 0.9425342916890747, 0.9427111626866468, 0.9426630541426565, 0.9428510014804452, 0.9431133958584131, 0.9431029482569933, 0.9433783705762662, 0.9432163052483516, 0.9433902338196001, 0.9434032087406947, 0.9435376093547101, 0.9435068329382155, 0.9433470416188539, 0.9435644141736462, 0.9438346446721062, 0.9436435944454413, 0.9438890339966475, 0.9440001631509323, 0.9437794292481023, 0.9440161506059153, 0.9441536532354787, 0.9443338375480413, 0.9443497812102152, 0.9443354593528496, 0.9445788806197684, 0.9446756878707229, 0.9445375631097794, 0.9446477695584228, 0.9445736183581472, 0.9447405493647831, 0.9447740798100401, 0.9449017315970833, 0.944985610649998, 0.9448674153748426, 0.9449649215411056, 0.9450802701687523, 0.94514795748834, 0.9451627393138415, 0.9452888415180145, 0.9449417989033827, 0.9452514160867599, 0.9454041220950142, 0.9454244450955623, 0.9454091251965132, 0.9454571848686016, 0.9455156924064066, 0.945520743782267, 0.9455506816313909, 0.9456010838797816, 0.9455744571048721, 0.9457593439791459, 0.9457191867545227, 0.9457313035043283, 0.9459193479653979, 0.9461314398037668, 0.9459348954928273, 0.946044407967361, 0.9460633138781088, 0.9461447391662612, 0.9461175364429422, 0.9460484811352792, 0.9461916388862276, 0.9460949448346861, 0.946257867782314, 0.946207649909579, 0.9462354809120485, 0.9462255873224146, 0.946330249252084, 0.9464853867437423, 0.9462920633014145, 0.9465559237100312, 0.9460999941766113, 0.9465472566293158, 0.946538084178893, 0.9466091494972085, 0.9466043991092912, 0.9467060309810539, 0.9467431273399715], 'mDice': [0.016595333057281735, 0.016505079116605485, 0.01728200455271074, 0.019444359771308352, 0.02291765793474258, 0.027670494493162424, 0.033625457857428244, 0.040431511184947214, 0.04947677637835171, 0.06529062281247622, 0.08169775449726333, 0.09686642823730426, 0.11504353670479131, 0.1364535300320735, 0.15815357227867363, 0.18177042925360118, 0.2068998895044016, 0.2329074342930641, 0.2578064096566912, 0.28018032218625327, 0.3006413282859939, 0.32089366077664316, 0.3385919558707749, 0.3570012290261259, 0.37482638526381107, 0.389103199541856, 0.4025634023031794, 0.41511695936439996, 0.4254620592730202, 0.43479156484947695, 0.44532804252302943, 0.45443998280921977, 0.4604991328884049, 0.4674591162380787, 0.47657869007285447, 0.4809472044814591, 0.4888658281325926, 0.49454779053368686, 0.5004923010644643, 0.5059905847488213, 0.5127170638631758, 0.5163234410222518, 0.5201477633061289, 0.5241685283305858, 0.5295492221055657, 0.5343324067115968, 0.537167814923822, 0.5413910422178967, 0.5447415859914824, 0.5495314590844066, 0.5509733735124615, 0.5535459089122895, 0.5587412370568138, 0.560448924838313, 0.5646263165857283, 0.5674704445656357, 0.5686884420511463, 0.5722499478063258, 0.5742992988689019, 0.5760069347333421, 0.5790949112789374, 0.5816280928240912, 0.5825502631842159, 0.5860830674815872, 0.5869573215333304, 0.589805409493234, 0.5896889712278706, 0.5916884145870911, 0.5923766715231947, 0.5941806711717587, 0.5985045332611699, 0.5980778009141827, 0.6015401866042087, 0.6011864115604896, 0.6053209056945258, 0.6039935263774747, 0.6033062671153375, 0.607405704513827, 0.6075012781083619, 0.610068075156336, 0.6119199498140538, 0.6117181742101684, 0.6130042546788737, 0.6150248550440228, 0.6170962745338501, 0.6155772174923825, 0.6165136509096092, 0.6194931470821514, 0.6199508091377022, 0.6217823442476021, 0.6228106675728085, 0.6221785968108883, 0.6238266941162118, 0.6245584662465753, 0.6244337461990774, 0.6264561425290495, 0.6276198416080736, 0.6282282731931564, 0.6293010385895548, 0.6295391566403596, 0.6307794889067095, 0.6324491555552412, 0.63303419871208, 0.6328074923594931, 0.6332324534361593, 0.6343435751051946, 0.634971316894493, 0.6353725794654649, 0.636833587179116, 0.6363008480345971, 0.6375751945355137, 0.6387017680795661, 0.6389967415956535, 0.6403904961142348, 0.6404196645268039, 0.6408758020731946, 0.6424235133667867, 0.6433641096519234, 0.6424568555868497, 0.643946508669959, 0.6434353496703381, 0.6455852916435123, 0.6450220959619081, 0.6467009585373016, 0.6471195925814994, 0.6471653431395425, 0.6468593941986021, 0.6488050687788813, 0.6489615373719927, 0.6493478087025707, 0.6503563216938227, 0.6482662395725872, 0.6508949814859512, 0.6521729069544857, 0.6523895964402786, 0.6518068886088203, 0.6523722307471059, 0.6539001546890629, 0.6538809804022829, 0.6537720233069833, 0.654845023467772, 0.653881381053053, 0.6559206244801656, 0.6552305471375427, 0.6561741758592561, 0.6580641007340664, 0.6591819679895669, 0.6574797219386191, 0.6592288621707943, 0.6589467711524052, 0.6600128541981896, 0.6582911067186396, 0.6588601955380172, 0.6595708055718684, 0.660119894625526, 0.6609028881630825, 0.6611531257675359, 0.6620935638538785, 0.6615032539079018, 0.6622881528980497, 0.663463884082998, 0.662349522470026, 0.6637651360124573, 0.6611218851426675, 0.6646615475364474, 0.6643380694176068, 0.6648931203836279, 0.666062882773008, 0.6668755853477185, 0.666903803303496]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.64s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.41s/it]predicting test subjects: 100%|██████████| 3/3 [00:06<00:00,  2.17s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:03,  1.49s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:26,  1.58s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:32,  1.60s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:12,  1.75s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:53,  1.69s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:24,  1.81s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:41,  1.88s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:42,  1.89s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:23,  1.82s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<08:53,  1.94s/it]predicting train subjects:   4%|▍         | 11/285 [00:20<09:10,  2.01s/it]predicting train subjects:   4%|▍         | 12/285 [00:22<09:09,  2.01s/it]predicting train subjects:   5%|▍         | 13/285 [00:24<09:18,  2.05s/it]predicting train subjects:   5%|▍         | 14/285 [00:27<09:26,  2.09s/it]predicting train subjects:   5%|▌         | 15/285 [00:29<09:36,  2.13s/it]predicting train subjects:   6%|▌         | 16/285 [00:31<09:31,  2.12s/it]predicting train subjects:   6%|▌         | 17/285 [00:33<09:32,  2.13s/it]predicting train subjects:   6%|▋         | 18/285 [00:35<09:34,  2.15s/it]predicting train subjects:   7%|▋         | 19/285 [00:38<09:45,  2.20s/it]predicting train subjects:   7%|▋         | 20/285 [00:40<09:49,  2.23s/it]predicting train subjects:   7%|▋         | 21/285 [00:42<09:39,  2.19s/it]predicting train subjects:   8%|▊         | 22/285 [00:44<09:37,  2.20s/it]predicting train subjects:   8%|▊         | 23/285 [00:46<09:44,  2.23s/it]predicting train subjects:   8%|▊         | 24/285 [00:49<09:37,  2.21s/it]predicting train subjects:   9%|▉         | 25/285 [00:51<09:21,  2.16s/it]predicting train subjects:   9%|▉         | 26/285 [00:53<09:22,  2.17s/it]predicting train subjects:   9%|▉         | 27/285 [00:55<09:16,  2.16s/it]predicting train subjects:  10%|▉         | 28/285 [00:57<09:14,  2.16s/it]predicting train subjects:  10%|█         | 29/285 [00:59<08:58,  2.10s/it]predicting train subjects:  11%|█         | 30/285 [01:01<08:57,  2.11s/it]predicting train subjects:  11%|█         | 31/285 [01:03<08:43,  2.06s/it]predicting train subjects:  11%|█         | 32/285 [01:05<08:29,  2.01s/it]predicting train subjects:  12%|█▏        | 33/285 [01:07<08:14,  1.96s/it]predicting train subjects:  12%|█▏        | 34/285 [01:09<08:19,  1.99s/it]predicting train subjects:  12%|█▏        | 35/285 [01:11<08:14,  1.98s/it]predicting train subjects:  13%|█▎        | 36/285 [01:13<08:19,  2.01s/it]predicting train subjects:  13%|█▎        | 37/285 [01:15<08:24,  2.04s/it]predicting train subjects:  13%|█▎        | 38/285 [01:17<08:19,  2.02s/it]predicting train subjects:  14%|█▎        | 39/285 [01:19<08:31,  2.08s/it]predicting train subjects:  14%|█▍        | 40/285 [01:21<08:27,  2.07s/it]predicting train subjects:  14%|█▍        | 41/285 [01:23<08:30,  2.09s/it]predicting train subjects:  15%|█▍        | 42/285 [01:25<08:21,  2.06s/it]predicting train subjects:  15%|█▌        | 43/285 [01:27<08:09,  2.02s/it]predicting train subjects:  15%|█▌        | 44/285 [01:29<08:08,  2.03s/it]predicting train subjects:  16%|█▌        | 45/285 [01:31<08:05,  2.02s/it]predicting train subjects:  16%|█▌        | 46/285 [01:33<07:40,  1.93s/it]predicting train subjects:  16%|█▋        | 47/285 [01:35<07:17,  1.84s/it]predicting train subjects:  17%|█▋        | 48/285 [01:36<07:04,  1.79s/it]predicting train subjects:  17%|█▋        | 49/285 [01:38<06:55,  1.76s/it]predicting train subjects:  18%|█▊        | 50/285 [01:40<06:50,  1.74s/it]predicting train subjects:  18%|█▊        | 51/285 [01:42<06:44,  1.73s/it]predicting train subjects:  18%|█▊        | 52/285 [01:43<06:49,  1.76s/it]predicting train subjects:  19%|█▊        | 53/285 [01:45<06:47,  1.76s/it]predicting train subjects:  19%|█▉        | 54/285 [01:47<06:46,  1.76s/it]predicting train subjects:  19%|█▉        | 55/285 [01:49<06:41,  1.75s/it]predicting train subjects:  20%|█▉        | 56/285 [01:50<06:40,  1.75s/it]predicting train subjects:  20%|██        | 57/285 [01:52<06:36,  1.74s/it]predicting train subjects:  20%|██        | 58/285 [01:54<06:46,  1.79s/it]predicting train subjects:  21%|██        | 59/285 [01:56<06:41,  1.78s/it]predicting train subjects:  21%|██        | 60/285 [01:58<06:37,  1.77s/it]predicting train subjects:  21%|██▏       | 61/285 [01:59<06:40,  1.79s/it]predicting train subjects:  22%|██▏       | 62/285 [02:01<06:32,  1.76s/it]predicting train subjects:  22%|██▏       | 63/285 [02:03<06:34,  1.78s/it]predicting train subjects:  22%|██▏       | 64/285 [02:05<07:03,  1.92s/it]predicting train subjects:  23%|██▎       | 65/285 [02:08<07:40,  2.09s/it]predicting train subjects:  23%|██▎       | 66/285 [02:10<07:42,  2.11s/it]predicting train subjects:  24%|██▎       | 67/285 [02:12<07:35,  2.09s/it]predicting train subjects:  24%|██▍       | 68/285 [02:14<07:35,  2.10s/it]predicting train subjects:  24%|██▍       | 69/285 [02:16<07:42,  2.14s/it]predicting train subjects:  25%|██▍       | 70/285 [02:18<07:34,  2.12s/it]predicting train subjects:  25%|██▍       | 71/285 [02:20<07:19,  2.05s/it]predicting train subjects:  25%|██▌       | 72/285 [02:22<07:17,  2.05s/it]predicting train subjects:  26%|██▌       | 73/285 [02:24<07:19,  2.07s/it]predicting train subjects:  26%|██▌       | 74/285 [02:26<07:15,  2.06s/it]predicting train subjects:  26%|██▋       | 75/285 [02:28<07:11,  2.06s/it]predicting train subjects:  27%|██▋       | 76/285 [02:30<07:09,  2.05s/it]predicting train subjects:  27%|██▋       | 77/285 [02:33<07:16,  2.10s/it]predicting train subjects:  27%|██▋       | 78/285 [02:35<07:14,  2.10s/it]predicting train subjects:  28%|██▊       | 79/285 [02:37<07:01,  2.05s/it]predicting train subjects:  28%|██▊       | 80/285 [02:39<06:59,  2.04s/it]predicting train subjects:  28%|██▊       | 81/285 [02:41<06:55,  2.04s/it]predicting train subjects:  29%|██▉       | 82/285 [02:43<06:52,  2.03s/it]predicting train subjects:  29%|██▉       | 83/285 [02:45<06:40,  1.98s/it]predicting train subjects:  29%|██▉       | 84/285 [02:47<06:40,  1.99s/it]predicting train subjects:  30%|██▉       | 85/285 [02:49<07:00,  2.10s/it]predicting train subjects:  30%|███       | 86/285 [02:52<07:25,  2.24s/it]predicting train subjects:  31%|███       | 87/285 [02:54<07:27,  2.26s/it]predicting train subjects:  31%|███       | 88/285 [02:56<07:42,  2.35s/it]predicting train subjects:  31%|███       | 89/285 [02:59<07:25,  2.27s/it]predicting train subjects:  32%|███▏      | 90/285 [03:01<07:16,  2.24s/it]predicting train subjects:  32%|███▏      | 91/285 [03:03<07:18,  2.26s/it]predicting train subjects:  32%|███▏      | 92/285 [03:05<07:11,  2.24s/it]predicting train subjects:  33%|███▎      | 93/285 [03:08<07:30,  2.35s/it]predicting train subjects:  33%|███▎      | 94/285 [03:10<07:36,  2.39s/it]predicting train subjects:  33%|███▎      | 95/285 [03:13<07:40,  2.42s/it]predicting train subjects:  34%|███▎      | 96/285 [03:15<07:21,  2.34s/it]predicting train subjects:  34%|███▍      | 97/285 [03:17<07:14,  2.31s/it]predicting train subjects:  34%|███▍      | 98/285 [03:20<07:16,  2.33s/it]predicting train subjects:  35%|███▍      | 99/285 [03:22<06:59,  2.26s/it]predicting train subjects:  35%|███▌      | 100/285 [03:24<07:02,  2.28s/it]predicting train subjects:  35%|███▌      | 101/285 [03:26<07:03,  2.30s/it]predicting train subjects:  36%|███▌      | 102/285 [03:29<07:07,  2.33s/it]predicting train subjects:  36%|███▌      | 103/285 [03:31<07:04,  2.33s/it]predicting train subjects:  36%|███▋      | 104/285 [03:33<06:55,  2.30s/it]predicting train subjects:  37%|███▋      | 105/285 [03:36<06:52,  2.29s/it]predicting train subjects:  37%|███▋      | 106/285 [03:38<06:54,  2.32s/it]predicting train subjects:  38%|███▊      | 107/285 [03:40<06:40,  2.25s/it]predicting train subjects:  38%|███▊      | 108/285 [03:42<06:41,  2.27s/it]predicting train subjects:  38%|███▊      | 109/285 [03:45<06:44,  2.30s/it]predicting train subjects:  39%|███▊      | 110/285 [03:47<06:39,  2.28s/it]predicting train subjects:  39%|███▉      | 111/285 [03:49<06:46,  2.34s/it]predicting train subjects:  39%|███▉      | 112/285 [03:52<06:46,  2.35s/it]predicting train subjects:  40%|███▉      | 113/285 [03:54<06:32,  2.28s/it]predicting train subjects:  40%|████      | 114/285 [03:56<06:19,  2.22s/it]predicting train subjects:  40%|████      | 115/285 [03:58<06:23,  2.26s/it]predicting train subjects:  41%|████      | 116/285 [04:01<06:29,  2.30s/it]predicting train subjects:  41%|████      | 117/285 [04:03<06:16,  2.24s/it]predicting train subjects:  41%|████▏     | 118/285 [04:05<06:17,  2.26s/it]predicting train subjects:  42%|████▏     | 119/285 [04:08<06:25,  2.32s/it]predicting train subjects:  42%|████▏     | 120/285 [04:10<06:18,  2.29s/it]predicting train subjects:  42%|████▏     | 121/285 [04:12<06:04,  2.23s/it]predicting train subjects:  43%|████▎     | 122/285 [04:14<05:43,  2.11s/it]predicting train subjects:  43%|████▎     | 123/285 [04:16<05:30,  2.04s/it]predicting train subjects:  44%|████▎     | 124/285 [04:18<05:29,  2.04s/it]predicting train subjects:  44%|████▍     | 125/285 [04:20<05:27,  2.05s/it]predicting train subjects:  44%|████▍     | 126/285 [04:22<05:22,  2.03s/it]predicting train subjects:  45%|████▍     | 127/285 [04:24<05:22,  2.04s/it]predicting train subjects:  45%|████▍     | 128/285 [04:26<05:31,  2.11s/it]predicting train subjects:  45%|████▌     | 129/285 [04:28<05:18,  2.04s/it]predicting train subjects:  46%|████▌     | 130/285 [04:30<05:19,  2.06s/it]predicting train subjects:  46%|████▌     | 131/285 [04:32<05:16,  2.05s/it]predicting train subjects:  46%|████▋     | 132/285 [04:34<05:19,  2.09s/it]predicting train subjects:  47%|████▋     | 133/285 [04:36<05:14,  2.07s/it]predicting train subjects:  47%|████▋     | 134/285 [04:38<05:17,  2.10s/it]predicting train subjects:  47%|████▋     | 135/285 [04:40<05:04,  2.03s/it]predicting train subjects:  48%|████▊     | 136/285 [04:42<05:04,  2.04s/it]predicting train subjects:  48%|████▊     | 137/285 [04:45<05:07,  2.08s/it]predicting train subjects:  48%|████▊     | 138/285 [04:46<04:52,  1.99s/it]predicting train subjects:  49%|████▉     | 139/285 [04:48<04:49,  1.98s/it]predicting train subjects:  49%|████▉     | 140/285 [04:50<04:50,  2.00s/it]predicting train subjects:  49%|████▉     | 141/285 [04:52<04:38,  1.93s/it]predicting train subjects:  50%|████▉     | 142/285 [04:54<04:37,  1.94s/it]predicting train subjects:  50%|█████     | 143/285 [04:56<04:29,  1.90s/it]predicting train subjects:  51%|█████     | 144/285 [04:58<04:30,  1.92s/it]predicting train subjects:  51%|█████     | 145/285 [05:00<04:23,  1.88s/it]predicting train subjects:  51%|█████     | 146/285 [05:01<04:20,  1.88s/it]predicting train subjects:  52%|█████▏    | 147/285 [05:03<04:12,  1.83s/it]predicting train subjects:  52%|█████▏    | 148/285 [05:05<04:07,  1.80s/it]predicting train subjects:  52%|█████▏    | 149/285 [05:07<04:06,  1.82s/it]predicting train subjects:  53%|█████▎    | 150/285 [05:09<04:10,  1.86s/it]predicting train subjects:  53%|█████▎    | 151/285 [05:11<04:13,  1.89s/it]predicting train subjects:  53%|█████▎    | 152/285 [05:12<04:01,  1.81s/it]predicting train subjects:  54%|█████▎    | 153/285 [05:14<04:07,  1.88s/it]predicting train subjects:  54%|█████▍    | 154/285 [05:16<04:11,  1.92s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:18<04:04,  1.88s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:20<04:06,  1.91s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:22<03:58,  1.86s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:24<03:53,  1.84s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:26<03:51,  1.84s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:27<03:48,  1.83s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:29<03:44,  1.81s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:31<03:42,  1.81s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:33<03:40,  1.80s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:34<03:32,  1.76s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:36<03:29,  1.75s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:38<03:25,  1.73s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:39<03:22,  1.71s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:41<03:22,  1.73s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:43<03:24,  1.76s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:45<03:27,  1.80s/it]predicting train subjects:  60%|██████    | 171/285 [05:47<03:21,  1.77s/it]predicting train subjects:  60%|██████    | 172/285 [05:48<03:18,  1.76s/it]predicting train subjects:  61%|██████    | 173/285 [05:50<03:09,  1.69s/it]predicting train subjects:  61%|██████    | 174/285 [05:52<03:07,  1.69s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:53<03:11,  1.74s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:55<03:17,  1.81s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:57<03:16,  1.82s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:59<03:15,  1.83s/it]predicting train subjects:  63%|██████▎   | 179/285 [06:01<03:11,  1.81s/it]predicting train subjects:  63%|██████▎   | 180/285 [06:03<03:06,  1.78s/it]predicting train subjects:  64%|██████▎   | 181/285 [06:04<03:01,  1.74s/it]predicting train subjects:  64%|██████▍   | 182/285 [06:06<03:01,  1.76s/it]predicting train subjects:  64%|██████▍   | 183/285 [06:08<02:58,  1.75s/it]predicting train subjects:  65%|██████▍   | 184/285 [06:10<02:59,  1.78s/it]predicting train subjects:  65%|██████▍   | 185/285 [06:11<02:55,  1.75s/it]predicting train subjects:  65%|██████▌   | 186/285 [06:13<02:47,  1.69s/it]predicting train subjects:  66%|██████▌   | 187/285 [06:15<02:47,  1.71s/it]predicting train subjects:  66%|██████▌   | 188/285 [06:16<02:46,  1.72s/it]predicting train subjects:  66%|██████▋   | 189/285 [06:18<02:51,  1.79s/it]predicting train subjects:  67%|██████▋   | 190/285 [06:20<02:50,  1.80s/it]predicting train subjects:  67%|██████▋   | 191/285 [06:22<02:48,  1.79s/it]predicting train subjects:  67%|██████▋   | 192/285 [06:24<02:49,  1.83s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:26<02:45,  1.80s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:27<02:37,  1.73s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:29<02:33,  1.71s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:31<02:39,  1.80s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:33<02:43,  1.86s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:35<02:50,  1.96s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:37<02:49,  1.97s/it]predicting train subjects:  70%|███████   | 200/285 [06:39<02:49,  1.99s/it]predicting train subjects:  71%|███████   | 201/285 [06:41<02:49,  2.01s/it]predicting train subjects:  71%|███████   | 202/285 [06:43<02:42,  1.96s/it]predicting train subjects:  71%|███████   | 203/285 [06:45<02:43,  2.00s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:47<02:40,  1.98s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:49<02:40,  2.01s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:51<02:44,  2.08s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:53<02:43,  2.09s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:55<02:38,  2.06s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:57<02:33,  2.02s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:59<02:28,  1.98s/it]predicting train subjects:  74%|███████▍  | 211/285 [07:01<02:23,  1.94s/it]predicting train subjects:  74%|███████▍  | 212/285 [07:03<02:18,  1.90s/it]predicting train subjects:  75%|███████▍  | 213/285 [07:05<02:15,  1.88s/it]predicting train subjects:  75%|███████▌  | 214/285 [07:06<02:06,  1.78s/it]predicting train subjects:  75%|███████▌  | 215/285 [07:08<02:00,  1.71s/it]predicting train subjects:  76%|███████▌  | 216/285 [07:09<01:56,  1.69s/it]predicting train subjects:  76%|███████▌  | 217/285 [07:11<01:57,  1.73s/it]predicting train subjects:  76%|███████▋  | 218/285 [07:13<01:51,  1.66s/it]predicting train subjects:  77%|███████▋  | 219/285 [07:15<01:53,  1.72s/it]predicting train subjects:  77%|███████▋  | 220/285 [07:16<01:51,  1.72s/it]predicting train subjects:  78%|███████▊  | 221/285 [07:18<01:48,  1.70s/it]predicting train subjects:  78%|███████▊  | 222/285 [07:20<01:46,  1.69s/it]predicting train subjects:  78%|███████▊  | 223/285 [07:21<01:47,  1.73s/it]predicting train subjects:  79%|███████▊  | 224/285 [07:23<01:43,  1.70s/it]predicting train subjects:  79%|███████▉  | 225/285 [07:25<01:43,  1.73s/it]predicting train subjects:  79%|███████▉  | 226/285 [07:27<01:42,  1.75s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:29<01:44,  1.80s/it]predicting train subjects:  80%|████████  | 228/285 [07:30<01:42,  1.80s/it]predicting train subjects:  80%|████████  | 229/285 [07:32<01:38,  1.76s/it]predicting train subjects:  81%|████████  | 230/285 [07:34<01:38,  1.79s/it]predicting train subjects:  81%|████████  | 231/285 [07:35<01:32,  1.71s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:38<01:38,  1.85s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:40<01:40,  1.94s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:42<01:43,  2.03s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:44<01:43,  2.07s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:46<01:44,  2.13s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:49<01:44,  2.17s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:51<01:40,  2.14s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:53<01:37,  2.12s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:55<01:33,  2.08s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:57<01:33,  2.12s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:59<01:31,  2.13s/it]predicting train subjects:  85%|████████▌ | 243/285 [08:01<01:30,  2.16s/it]predicting train subjects:  86%|████████▌ | 244/285 [08:04<01:28,  2.15s/it]predicting train subjects:  86%|████████▌ | 245/285 [08:06<01:25,  2.15s/it]predicting train subjects:  86%|████████▋ | 246/285 [08:08<01:23,  2.14s/it]predicting train subjects:  87%|████████▋ | 247/285 [08:10<01:21,  2.13s/it]predicting train subjects:  87%|████████▋ | 248/285 [08:12<01:21,  2.19s/it]predicting train subjects:  87%|████████▋ | 249/285 [08:14<01:17,  2.15s/it]predicting train subjects:  88%|████████▊ | 250/285 [08:16<01:09,  1.99s/it]predicting train subjects:  88%|████████▊ | 251/285 [08:18<01:03,  1.87s/it]predicting train subjects:  88%|████████▊ | 252/285 [08:19<01:00,  1.82s/it]predicting train subjects:  89%|████████▉ | 253/285 [08:21<00:57,  1.81s/it]predicting train subjects:  89%|████████▉ | 254/285 [08:23<00:54,  1.75s/it]predicting train subjects:  89%|████████▉ | 255/285 [08:24<00:50,  1.69s/it]predicting train subjects:  90%|████████▉ | 256/285 [08:26<00:48,  1.68s/it]predicting train subjects:  90%|█████████ | 257/285 [08:28<00:47,  1.71s/it]predicting train subjects:  91%|█████████ | 258/285 [08:29<00:46,  1.71s/it]predicting train subjects:  91%|█████████ | 259/285 [08:31<00:43,  1.67s/it]predicting train subjects:  91%|█████████ | 260/285 [08:33<00:42,  1.71s/it]predicting train subjects:  92%|█████████▏| 261/285 [08:34<00:40,  1.70s/it]predicting train subjects:  92%|█████████▏| 262/285 [08:36<00:39,  1.72s/it]predicting train subjects:  92%|█████████▏| 263/285 [08:38<00:36,  1.68s/it]predicting train subjects:  93%|█████████▎| 264/285 [08:39<00:35,  1.70s/it]predicting train subjects:  93%|█████████▎| 265/285 [08:41<00:33,  1.68s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:43<00:32,  1.70s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:44<00:30,  1.68s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:47<00:30,  1.79s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:49<00:30,  1.91s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:51<00:29,  1.99s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:53<00:28,  2.03s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:55<00:26,  2.05s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:57<00:25,  2.09s/it]predicting train subjects:  96%|█████████▌| 274/285 [09:00<00:23,  2.13s/it]predicting train subjects:  96%|█████████▋| 275/285 [09:02<00:20,  2.10s/it]predicting train subjects:  97%|█████████▋| 276/285 [09:04<00:18,  2.10s/it]predicting train subjects:  97%|█████████▋| 277/285 [09:06<00:17,  2.16s/it]predicting train subjects:  98%|█████████▊| 278/285 [09:08<00:15,  2.18s/it]predicting train subjects:  98%|█████████▊| 279/285 [09:10<00:12,  2.15s/it]predicting train subjects:  98%|█████████▊| 280/285 [09:12<00:10,  2.13s/it]predicting train subjects:  99%|█████████▊| 281/285 [09:15<00:08,  2.15s/it]predicting train subjects:  99%|█████████▉| 282/285 [09:17<00:06,  2.19s/it]predicting train subjects:  99%|█████████▉| 283/285 [09:19<00:04,  2.17s/it]predicting train subjects: 100%|█████████▉| 284/285 [09:21<00:02,  2.18s/it]predicting train subjects: 100%|██████████| 285/285 [09:23<00:00,  2.13s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:53,  1.67s/it]Loading train:   1%|          | 2/285 [00:03<08:02,  1.70s/it]Loading train:   1%|          | 3/285 [00:04<07:35,  1.62s/it]Loading train:   1%|▏         | 4/285 [00:06<07:56,  1.70s/it]Loading train:   2%|▏         | 5/285 [00:08<07:49,  1.68s/it]Loading train:   2%|▏         | 6/285 [00:10<07:55,  1.70s/it]Loading train:   2%|▏         | 7/285 [00:12<08:25,  1.82s/it]Loading train:   3%|▎         | 8/285 [00:13<08:16,  1.79s/it]Loading train:   3%|▎         | 9/285 [00:15<08:12,  1.78s/it]Loading train:   4%|▎         | 10/285 [00:17<08:03,  1.76s/it]Loading train:   4%|▍         | 11/285 [00:19<08:00,  1.75s/it]Loading train:   4%|▍         | 12/285 [00:20<07:42,  1.69s/it]Loading train:   5%|▍         | 13/285 [00:22<07:31,  1.66s/it]Loading train:   5%|▍         | 14/285 [00:24<07:41,  1.70s/it]Loading train:   5%|▌         | 15/285 [00:25<07:42,  1.71s/it]Loading train:   6%|▌         | 16/285 [00:27<07:19,  1.63s/it]Loading train:   6%|▌         | 17/285 [00:29<07:35,  1.70s/it]Loading train:   6%|▋         | 18/285 [00:30<07:33,  1.70s/it]Loading train:   7%|▋         | 19/285 [00:32<07:18,  1.65s/it]Loading train:   7%|▋         | 20/285 [00:34<07:16,  1.65s/it]Loading train:   7%|▋         | 21/285 [00:35<07:02,  1.60s/it]Loading train:   8%|▊         | 22/285 [00:36<06:45,  1.54s/it]Loading train:   8%|▊         | 23/285 [00:38<06:47,  1.55s/it]Loading train:   8%|▊         | 24/285 [00:39<06:33,  1.51s/it]Loading train:   9%|▉         | 25/285 [00:41<06:52,  1.59s/it]Loading train:   9%|▉         | 26/285 [00:43<06:47,  1.57s/it]Loading train:   9%|▉         | 27/285 [00:44<06:39,  1.55s/it]Loading train:  10%|▉         | 28/285 [00:46<06:55,  1.61s/it]Loading train:  10%|█         | 29/285 [00:47<06:25,  1.50s/it]Loading train:  11%|█         | 30/285 [00:48<06:05,  1.43s/it]Loading train:  11%|█         | 31/285 [00:50<05:42,  1.35s/it]Loading train:  11%|█         | 32/285 [00:51<05:57,  1.41s/it]Loading train:  12%|█▏        | 33/285 [00:52<05:45,  1.37s/it]Loading train:  12%|█▏        | 34/285 [00:54<05:39,  1.35s/it]Loading train:  12%|█▏        | 35/285 [00:55<05:22,  1.29s/it]Loading train:  13%|█▎        | 36/285 [00:56<05:39,  1.36s/it]Loading train:  13%|█▎        | 37/285 [00:58<05:32,  1.34s/it]Loading train:  13%|█▎        | 38/285 [00:59<05:18,  1.29s/it]Loading train:  14%|█▎        | 39/285 [01:00<05:27,  1.33s/it]Loading train:  14%|█▍        | 40/285 [01:02<05:29,  1.35s/it]Loading train:  14%|█▍        | 41/285 [01:03<05:39,  1.39s/it]Loading train:  15%|█▍        | 42/285 [01:05<05:57,  1.47s/it]Loading train:  15%|█▌        | 43/285 [01:07<06:13,  1.54s/it]Loading train:  15%|█▌        | 44/285 [01:08<06:23,  1.59s/it]Loading train:  16%|█▌        | 45/285 [01:10<06:24,  1.60s/it]Loading train:  16%|█▌        | 46/285 [01:11<05:57,  1.50s/it]Loading train:  16%|█▋        | 47/285 [01:13<05:55,  1.49s/it]Loading train:  17%|█▋        | 48/285 [01:14<05:44,  1.45s/it]Loading train:  17%|█▋        | 49/285 [01:15<05:23,  1.37s/it]Loading train:  18%|█▊        | 50/285 [01:17<05:26,  1.39s/it]Loading train:  18%|█▊        | 51/285 [01:18<05:17,  1.36s/it]Loading train:  18%|█▊        | 52/285 [01:19<05:04,  1.30s/it]Loading train:  19%|█▊        | 53/285 [01:20<05:00,  1.30s/it]Loading train:  19%|█▉        | 54/285 [01:21<04:45,  1.24s/it]Loading train:  19%|█▉        | 55/285 [01:23<04:43,  1.23s/it]Loading train:  20%|█▉        | 56/285 [01:24<04:30,  1.18s/it]Loading train:  20%|██        | 57/285 [01:25<04:47,  1.26s/it]Loading train:  20%|██        | 58/285 [01:27<04:49,  1.28s/it]Loading train:  21%|██        | 59/285 [01:28<04:44,  1.26s/it]Loading train:  21%|██        | 60/285 [01:29<04:48,  1.28s/it]Loading train:  21%|██▏       | 61/285 [01:30<04:57,  1.33s/it]Loading train:  22%|██▏       | 62/285 [01:32<04:49,  1.30s/it]Loading train:  22%|██▏       | 63/285 [01:33<04:46,  1.29s/it]Loading train:  22%|██▏       | 64/285 [01:35<05:11,  1.41s/it]Loading train:  23%|██▎       | 65/285 [01:37<05:38,  1.54s/it]Loading train:  23%|██▎       | 66/285 [01:39<06:05,  1.67s/it]Loading train:  24%|██▎       | 67/285 [01:40<05:43,  1.57s/it]Loading train:  24%|██▍       | 68/285 [01:41<05:17,  1.46s/it]Loading train:  24%|██▍       | 69/285 [01:42<05:01,  1.40s/it]Loading train:  25%|██▍       | 70/285 [01:43<04:46,  1.33s/it]Loading train:  25%|██▍       | 71/285 [01:45<04:37,  1.30s/it]Loading train:  25%|██▌       | 72/285 [01:46<04:56,  1.39s/it]Loading train:  26%|██▌       | 73/285 [01:48<04:42,  1.33s/it]Loading train:  26%|██▌       | 74/285 [01:49<04:42,  1.34s/it]Loading train:  26%|██▋       | 75/285 [01:50<04:43,  1.35s/it]Loading train:  27%|██▋       | 76/285 [01:52<04:46,  1.37s/it]Loading train:  27%|██▋       | 77/285 [01:53<04:43,  1.36s/it]Loading train:  27%|██▋       | 78/285 [01:55<05:02,  1.46s/it]Loading train:  28%|██▊       | 79/285 [01:56<04:32,  1.32s/it]Loading train:  28%|██▊       | 80/285 [01:57<04:47,  1.40s/it]Loading train:  28%|██▊       | 81/285 [01:58<04:32,  1.33s/it]Loading train:  29%|██▉       | 82/285 [02:00<04:36,  1.36s/it]Loading train:  29%|██▉       | 83/285 [02:01<04:27,  1.33s/it]Loading train:  29%|██▉       | 84/285 [02:02<04:22,  1.30s/it]Loading train:  30%|██▉       | 85/285 [02:04<04:23,  1.32s/it]Loading train:  30%|███       | 86/285 [02:05<04:42,  1.42s/it]Loading train:  31%|███       | 87/285 [02:07<04:30,  1.37s/it]Loading train:  31%|███       | 88/285 [02:08<04:25,  1.35s/it]Loading train:  31%|███       | 89/285 [02:09<04:25,  1.36s/it]Loading train:  32%|███▏      | 90/285 [02:11<04:24,  1.35s/it]Loading train:  32%|███▏      | 91/285 [02:12<04:11,  1.29s/it]Loading train:  32%|███▏      | 92/285 [02:13<04:03,  1.26s/it]Loading train:  33%|███▎      | 93/285 [02:14<04:02,  1.26s/it]Loading train:  33%|███▎      | 94/285 [02:15<03:53,  1.22s/it]Loading train:  33%|███▎      | 95/285 [02:17<03:56,  1.24s/it]Loading train:  34%|███▎      | 96/285 [02:18<03:49,  1.22s/it]Loading train:  34%|███▍      | 97/285 [02:19<04:12,  1.34s/it]Loading train:  34%|███▍      | 98/285 [02:21<04:00,  1.29s/it]Loading train:  35%|███▍      | 99/285 [02:22<03:58,  1.28s/it]Loading train:  35%|███▌      | 100/285 [02:23<04:00,  1.30s/it]Loading train:  35%|███▌      | 101/285 [02:25<03:58,  1.30s/it]Loading train:  36%|███▌      | 102/285 [02:26<04:16,  1.40s/it]Loading train:  36%|███▌      | 103/285 [02:27<04:07,  1.36s/it]Loading train:  36%|███▋      | 104/285 [02:29<03:55,  1.30s/it]Loading train:  37%|███▋      | 105/285 [02:30<03:55,  1.31s/it]Loading train:  37%|███▋      | 106/285 [02:31<03:48,  1.28s/it]Loading train:  38%|███▊      | 107/285 [02:33<04:01,  1.36s/it]Loading train:  38%|███▊      | 108/285 [02:34<04:01,  1.36s/it]Loading train:  38%|███▊      | 109/285 [02:35<03:59,  1.36s/it]Loading train:  39%|███▊      | 110/285 [02:37<03:54,  1.34s/it]Loading train:  39%|███▉      | 111/285 [02:38<03:42,  1.28s/it]Loading train:  39%|███▉      | 112/285 [02:39<03:46,  1.31s/it]Loading train:  40%|███▉      | 113/285 [02:41<03:51,  1.35s/it]Loading train:  40%|████      | 114/285 [02:42<03:59,  1.40s/it]Loading train:  40%|████      | 115/285 [02:44<03:54,  1.38s/it]Loading train:  41%|████      | 116/285 [02:45<04:07,  1.47s/it]Loading train:  41%|████      | 117/285 [02:47<04:01,  1.44s/it]Loading train:  41%|████▏     | 118/285 [02:48<03:59,  1.43s/it]Loading train:  42%|████▏     | 119/285 [02:49<03:47,  1.37s/it]Loading train:  42%|████▏     | 120/285 [02:51<03:56,  1.43s/it]Loading train:  42%|████▏     | 121/285 [02:52<04:03,  1.48s/it]Loading train:  43%|████▎     | 122/285 [02:54<04:07,  1.52s/it]Loading train:  43%|████▎     | 123/285 [02:56<04:09,  1.54s/it]Loading train:  44%|████▎     | 124/285 [02:57<03:51,  1.44s/it]Loading train:  44%|████▍     | 125/285 [02:58<03:50,  1.44s/it]Loading train:  44%|████▍     | 126/285 [02:59<03:38,  1.37s/it]Loading train:  45%|████▍     | 127/285 [03:01<03:40,  1.40s/it]Loading train:  45%|████▍     | 128/285 [03:02<03:32,  1.35s/it]Loading train:  45%|████▌     | 129/285 [03:03<03:19,  1.28s/it]Loading train:  46%|████▌     | 130/285 [03:05<03:26,  1.33s/it]Loading train:  46%|████▌     | 131/285 [03:06<03:22,  1.31s/it]Loading train:  46%|████▋     | 132/285 [03:07<03:21,  1.32s/it]Loading train:  47%|████▋     | 133/285 [03:09<03:29,  1.38s/it]Loading train:  47%|████▋     | 134/285 [03:10<03:16,  1.30s/it]Loading train:  47%|████▋     | 135/285 [03:11<03:15,  1.30s/it]Loading train:  48%|████▊     | 136/285 [03:13<03:20,  1.35s/it]Loading train:  48%|████▊     | 137/285 [03:14<03:14,  1.31s/it]Loading train:  48%|████▊     | 138/285 [03:15<03:17,  1.35s/it]Loading train:  49%|████▉     | 139/285 [03:17<03:17,  1.36s/it]Loading train:  49%|████▉     | 140/285 [03:18<03:04,  1.28s/it]Loading train:  49%|████▉     | 141/285 [03:19<02:54,  1.21s/it]Loading train:  50%|████▉     | 142/285 [03:20<02:56,  1.23s/it]Loading train:  50%|█████     | 143/285 [03:21<02:53,  1.22s/it]Loading train:  51%|█████     | 144/285 [03:23<03:00,  1.28s/it]Loading train:  51%|█████     | 145/285 [03:24<02:56,  1.26s/it]Loading train:  51%|█████     | 146/285 [03:25<02:54,  1.26s/it]Loading train:  52%|█████▏    | 147/285 [03:27<03:08,  1.37s/it]Loading train:  52%|█████▏    | 148/285 [03:28<03:05,  1.35s/it]Loading train:  52%|█████▏    | 149/285 [03:29<02:55,  1.29s/it]Loading train:  53%|█████▎    | 150/285 [03:30<02:49,  1.25s/it]Loading train:  53%|█████▎    | 151/285 [03:32<02:54,  1.30s/it]Loading train:  53%|█████▎    | 152/285 [03:33<03:01,  1.37s/it]Loading train:  54%|█████▎    | 153/285 [03:35<03:08,  1.43s/it]Loading train:  54%|█████▍    | 154/285 [03:36<03:03,  1.40s/it]Loading train:  54%|█████▍    | 155/285 [03:37<02:51,  1.32s/it]Loading train:  55%|█████▍    | 156/285 [03:39<02:40,  1.25s/it]Loading train:  55%|█████▌    | 157/285 [03:40<02:35,  1.22s/it]Loading train:  55%|█████▌    | 158/285 [03:41<02:27,  1.16s/it]Loading train:  56%|█████▌    | 159/285 [03:42<02:29,  1.19s/it]Loading train:  56%|█████▌    | 160/285 [03:43<02:33,  1.23s/it]Loading train:  56%|█████▋    | 161/285 [03:44<02:23,  1.16s/it]Loading train:  57%|█████▋    | 162/285 [03:45<02:18,  1.13s/it]Loading train:  57%|█████▋    | 163/285 [03:46<02:17,  1.13s/it]Loading train:  58%|█████▊    | 164/285 [03:48<02:13,  1.11s/it]Loading train:  58%|█████▊    | 165/285 [03:49<02:12,  1.10s/it]Loading train:  58%|█████▊    | 166/285 [03:50<02:24,  1.21s/it]Loading train:  59%|█████▊    | 167/285 [03:51<02:30,  1.27s/it]Loading train:  59%|█████▉    | 168/285 [03:53<02:35,  1.33s/it]Loading train:  59%|█████▉    | 169/285 [03:54<02:35,  1.34s/it]Loading train:  60%|█████▉    | 170/285 [03:55<02:27,  1.29s/it]Loading train:  60%|██████    | 171/285 [03:57<02:34,  1.35s/it]Loading train:  60%|██████    | 172/285 [03:58<02:27,  1.31s/it]Loading train:  61%|██████    | 173/285 [03:59<02:24,  1.29s/it]Loading train:  61%|██████    | 174/285 [04:01<02:42,  1.46s/it]Loading train:  61%|██████▏   | 175/285 [04:03<02:46,  1.52s/it]Loading train:  62%|██████▏   | 176/285 [04:04<02:23,  1.32s/it]Loading train:  62%|██████▏   | 177/285 [04:05<02:08,  1.19s/it]Loading train:  62%|██████▏   | 178/285 [04:06<01:58,  1.11s/it]Loading train:  63%|██████▎   | 179/285 [04:06<01:49,  1.04s/it]Loading train:  63%|██████▎   | 180/285 [04:07<01:44,  1.01it/s]Loading train:  64%|██████▎   | 181/285 [04:08<01:40,  1.03it/s]Loading train:  64%|██████▍   | 182/285 [04:09<01:37,  1.05it/s]Loading train:  64%|██████▍   | 183/285 [04:10<01:36,  1.06it/s]Loading train:  65%|██████▍   | 184/285 [04:11<01:32,  1.09it/s]Loading train:  65%|██████▍   | 185/285 [04:12<01:36,  1.04it/s]Loading train:  65%|██████▌   | 186/285 [04:13<01:38,  1.01it/s]Loading train:  66%|██████▌   | 187/285 [04:14<01:33,  1.05it/s]Loading train:  66%|██████▌   | 188/285 [04:15<01:34,  1.03it/s]Loading train:  66%|██████▋   | 189/285 [04:16<01:31,  1.05it/s]Loading train:  67%|██████▋   | 190/285 [04:17<01:42,  1.08s/it]Loading train:  67%|██████▋   | 191/285 [04:18<01:36,  1.03s/it]Loading train:  67%|██████▋   | 192/285 [04:19<01:38,  1.06s/it]Loading train:  68%|██████▊   | 193/285 [04:20<01:33,  1.02s/it]Loading train:  68%|██████▊   | 194/285 [04:21<01:35,  1.05s/it]Loading train:  68%|██████▊   | 195/285 [04:22<01:34,  1.05s/it]Loading train:  69%|██████▉   | 196/285 [04:24<01:44,  1.17s/it]Loading train:  69%|██████▉   | 197/285 [04:25<01:51,  1.27s/it]Loading train:  69%|██████▉   | 198/285 [04:26<01:44,  1.20s/it]Loading train:  70%|██████▉   | 199/285 [04:28<01:44,  1.22s/it]Loading train:  70%|███████   | 200/285 [04:29<01:38,  1.16s/it]Loading train:  71%|███████   | 201/285 [04:30<01:41,  1.20s/it]Loading train:  71%|███████   | 202/285 [04:31<01:36,  1.16s/it]Loading train:  71%|███████   | 203/285 [04:32<01:34,  1.16s/it]Loading train:  72%|███████▏  | 204/285 [04:34<01:42,  1.26s/it]Loading train:  72%|███████▏  | 205/285 [04:35<01:35,  1.19s/it]Loading train:  72%|███████▏  | 206/285 [04:36<01:35,  1.21s/it]Loading train:  73%|███████▎  | 207/285 [04:37<01:31,  1.17s/it]Loading train:  73%|███████▎  | 208/285 [04:38<01:35,  1.24s/it]Loading train:  73%|███████▎  | 209/285 [04:40<01:35,  1.26s/it]Loading train:  74%|███████▎  | 210/285 [04:41<01:31,  1.22s/it]Loading train:  74%|███████▍  | 211/285 [04:42<01:31,  1.23s/it]Loading train:  74%|███████▍  | 212/285 [04:43<01:31,  1.25s/it]Loading train:  75%|███████▍  | 213/285 [04:44<01:23,  1.16s/it]Loading train:  75%|███████▌  | 214/285 [04:45<01:18,  1.11s/it]Loading train:  75%|███████▌  | 215/285 [04:46<01:12,  1.04s/it]Loading train:  76%|███████▌  | 216/285 [04:47<01:09,  1.01s/it]Loading train:  76%|███████▌  | 217/285 [04:48<01:05,  1.04it/s]Loading train:  76%|███████▋  | 218/285 [04:49<01:01,  1.09it/s]Loading train:  77%|███████▋  | 219/285 [04:50<01:01,  1.08it/s]Loading train:  77%|███████▋  | 220/285 [04:51<01:02,  1.04it/s]Loading train:  78%|███████▊  | 221/285 [04:52<01:01,  1.04it/s]Loading train:  78%|███████▊  | 222/285 [04:53<00:58,  1.08it/s]Loading train:  78%|███████▊  | 223/285 [04:54<00:57,  1.07it/s]Loading train:  79%|███████▊  | 224/285 [04:54<00:55,  1.10it/s]Loading train:  79%|███████▉  | 225/285 [04:55<00:54,  1.11it/s]Loading train:  79%|███████▉  | 226/285 [04:56<00:53,  1.10it/s]Loading train:  80%|███████▉  | 227/285 [04:57<00:51,  1.13it/s]Loading train:  80%|████████  | 228/285 [04:58<00:50,  1.12it/s]Loading train:  80%|████████  | 229/285 [04:59<00:49,  1.13it/s]Loading train:  81%|████████  | 230/285 [05:00<00:47,  1.15it/s]Loading train:  81%|████████  | 231/285 [05:01<00:45,  1.19it/s]Loading train:  81%|████████▏ | 232/285 [05:02<00:48,  1.09it/s]Loading train:  82%|████████▏ | 233/285 [05:03<00:48,  1.07it/s]Loading train:  82%|████████▏ | 234/285 [05:04<00:49,  1.03it/s]Loading train:  82%|████████▏ | 235/285 [05:05<00:51,  1.02s/it]Loading train:  83%|████████▎ | 236/285 [05:06<00:51,  1.05s/it]Loading train:  83%|████████▎ | 237/285 [05:07<00:50,  1.06s/it]Loading train:  84%|████████▎ | 238/285 [05:08<00:50,  1.07s/it]Loading train:  84%|████████▍ | 239/285 [05:09<00:49,  1.08s/it]Loading train:  84%|████████▍ | 240/285 [05:10<00:48,  1.07s/it]Loading train:  85%|████████▍ | 241/285 [05:11<00:46,  1.06s/it]Loading train:  85%|████████▍ | 242/285 [05:12<00:45,  1.07s/it]Loading train:  85%|████████▌ | 243/285 [05:13<00:44,  1.06s/it]Loading train:  86%|████████▌ | 244/285 [05:15<00:44,  1.08s/it]Loading train:  86%|████████▌ | 245/285 [05:16<00:42,  1.06s/it]Loading train:  86%|████████▋ | 246/285 [05:17<00:41,  1.07s/it]Loading train:  87%|████████▋ | 247/285 [05:18<00:41,  1.09s/it]Loading train:  87%|████████▋ | 248/285 [05:19<00:40,  1.08s/it]Loading train:  87%|████████▋ | 249/285 [05:20<00:38,  1.06s/it]Loading train:  88%|████████▊ | 250/285 [05:21<00:35,  1.03s/it]Loading train:  88%|████████▊ | 251/285 [05:22<00:33,  1.01it/s]Loading train:  88%|████████▊ | 252/285 [05:23<00:32,  1.03it/s]Loading train:  89%|████████▉ | 253/285 [05:24<00:31,  1.03it/s]Loading train:  89%|████████▉ | 254/285 [05:25<00:29,  1.04it/s]Loading train:  89%|████████▉ | 255/285 [05:25<00:28,  1.06it/s]Loading train:  90%|████████▉ | 256/285 [05:26<00:26,  1.08it/s]Loading train:  90%|█████████ | 257/285 [05:27<00:26,  1.05it/s]Loading train:  91%|█████████ | 258/285 [05:28<00:25,  1.06it/s]Loading train:  91%|█████████ | 259/285 [05:29<00:24,  1.07it/s]Loading train:  91%|█████████ | 260/285 [05:30<00:23,  1.07it/s]Loading train:  92%|█████████▏| 261/285 [05:31<00:22,  1.06it/s]Loading train:  92%|█████████▏| 262/285 [05:32<00:21,  1.05it/s]Loading train:  92%|█████████▏| 263/285 [05:33<00:20,  1.06it/s]Loading train:  93%|█████████▎| 264/285 [05:34<00:19,  1.10it/s]Loading train:  93%|█████████▎| 265/285 [05:35<00:18,  1.06it/s]Loading train:  93%|█████████▎| 266/285 [05:36<00:17,  1.07it/s]Loading train:  94%|█████████▎| 267/285 [05:37<00:18,  1.02s/it]Loading train:  94%|█████████▍| 268/285 [05:38<00:17,  1.05s/it]Loading train:  94%|█████████▍| 269/285 [05:39<00:17,  1.08s/it]Loading train:  95%|█████████▍| 270/285 [05:40<00:16,  1.09s/it]Loading train:  95%|█████████▌| 271/285 [05:41<00:15,  1.11s/it]Loading train:  95%|█████████▌| 272/285 [05:43<00:14,  1.11s/it]Loading train:  96%|█████████▌| 273/285 [05:44<00:13,  1.10s/it]Loading train:  96%|█████████▌| 274/285 [05:45<00:12,  1.10s/it]Loading train:  96%|█████████▋| 275/285 [05:46<00:11,  1.10s/it]Loading train:  97%|█████████▋| 276/285 [05:47<00:09,  1.09s/it]Loading train:  97%|█████████▋| 277/285 [05:48<00:08,  1.12s/it]Loading train:  98%|█████████▊| 278/285 [05:49<00:07,  1.12s/it]Loading train:  98%|█████████▊| 279/285 [05:50<00:06,  1.13s/it]Loading train:  98%|█████████▊| 280/285 [05:52<00:05,  1.13s/it]Loading train:  99%|█████████▊| 281/285 [05:53<00:04,  1.12s/it]Loading train:  99%|█████████▉| 282/285 [05:54<00:03,  1.13s/it]Loading train:  99%|█████████▉| 283/285 [05:55<00:02,  1.10s/it]Loading train: 100%|█████████▉| 284/285 [05:56<00:01,  1.07s/it]Loading train: 100%|██████████| 285/285 [05:57<00:00,  1.09s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:10, 27.13it/s]concatenating: train:   5%|▌         | 15/285 [00:00<00:07, 35.30it/s]concatenating: train:  15%|█▍        | 42/285 [00:00<00:05, 47.55it/s]concatenating: train:  25%|██▍       | 70/285 [00:00<00:03, 63.30it/s]concatenating: train:  34%|███▍      | 97/285 [00:00<00:02, 82.14it/s]concatenating: train:  44%|████▍     | 125/285 [00:00<00:01, 104.08it/s]concatenating: train:  54%|█████▎    | 153/285 [00:00<00:01, 127.24it/s]concatenating: train:  64%|██████▍   | 183/285 [00:00<00:00, 153.61it/s]concatenating: train:  75%|███████▌  | 214/285 [00:00<00:00, 180.67it/s]concatenating: train:  87%|████████▋ | 247/285 [00:01<00:00, 208.17it/s]concatenating: train:  97%|█████████▋| 276/285 [00:01<00:00, 226.65it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 246.17it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.49s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.43s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 33.13it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:04,  1.28s/it]Loading train:   1%|          | 2/285 [00:02<06:21,  1.35s/it]Loading train:   1%|          | 3/285 [00:04<06:09,  1.31s/it]Loading train:   1%|▏         | 4/285 [00:05<06:39,  1.42s/it]Loading train:   2%|▏         | 5/285 [00:06<06:15,  1.34s/it]Loading train:   2%|▏         | 6/285 [00:08<06:32,  1.41s/it]Loading train:   2%|▏         | 7/285 [00:10<06:51,  1.48s/it]Loading train:   3%|▎         | 8/285 [00:11<06:58,  1.51s/it]Loading train:   3%|▎         | 9/285 [00:12<06:37,  1.44s/it]Loading train:   4%|▎         | 10/285 [00:13<06:02,  1.32s/it]Loading train:   4%|▍         | 11/285 [00:14<05:39,  1.24s/it]Loading train:   4%|▍         | 12/285 [00:16<05:21,  1.18s/it]Loading train:   5%|▍         | 13/285 [00:17<05:12,  1.15s/it]Loading train:   5%|▍         | 14/285 [00:18<05:03,  1.12s/it]Loading train:   5%|▌         | 15/285 [00:19<04:59,  1.11s/it]Loading train:   6%|▌         | 16/285 [00:20<04:48,  1.07s/it]Loading train:   6%|▌         | 17/285 [00:21<04:47,  1.07s/it]Loading train:   6%|▋         | 18/285 [00:22<04:52,  1.09s/it]Loading train:   7%|▋         | 19/285 [00:23<04:41,  1.06s/it]Loading train:   7%|▋         | 20/285 [00:24<04:56,  1.12s/it]Loading train:   7%|▋         | 21/285 [00:25<04:47,  1.09s/it]Loading train:   8%|▊         | 22/285 [00:26<04:43,  1.08s/it]Loading train:   8%|▊         | 23/285 [00:27<04:36,  1.06s/it]Loading train:   8%|▊         | 24/285 [00:28<04:30,  1.04s/it]Loading train:   9%|▉         | 25/285 [00:29<04:30,  1.04s/it]Loading train:   9%|▉         | 26/285 [00:30<04:36,  1.07s/it]Loading train:   9%|▉         | 27/285 [00:32<04:36,  1.07s/it]Loading train:  10%|▉         | 28/285 [00:33<04:32,  1.06s/it]Loading train:  10%|█         | 29/285 [00:34<04:29,  1.05s/it]Loading train:  11%|█         | 30/285 [00:35<04:22,  1.03s/it]Loading train:  11%|█         | 31/285 [00:36<04:19,  1.02s/it]Loading train:  11%|█         | 32/285 [00:37<04:22,  1.04s/it]Loading train:  12%|█▏        | 33/285 [00:38<04:21,  1.04s/it]Loading train:  12%|█▏        | 34/285 [00:39<04:15,  1.02s/it]Loading train:  12%|█▏        | 35/285 [00:40<04:21,  1.04s/it]Loading train:  13%|█▎        | 36/285 [00:41<04:16,  1.03s/it]Loading train:  13%|█▎        | 37/285 [00:42<04:03,  1.02it/s]Loading train:  13%|█▎        | 38/285 [00:43<04:04,  1.01it/s]Loading train:  14%|█▎        | 39/285 [00:44<04:02,  1.01it/s]Loading train:  14%|█▍        | 40/285 [00:45<04:01,  1.01it/s]Loading train:  14%|█▍        | 41/285 [00:46<03:57,  1.03it/s]Loading train:  15%|█▍        | 42/285 [00:47<04:01,  1.01it/s]Loading train:  15%|█▌        | 43/285 [00:48<03:59,  1.01it/s]Loading train:  15%|█▌        | 44/285 [00:49<03:56,  1.02it/s]Loading train:  16%|█▌        | 45/285 [00:49<03:54,  1.02it/s]Loading train:  16%|█▌        | 46/285 [00:50<03:50,  1.03it/s]Loading train:  16%|█▋        | 47/285 [00:51<03:38,  1.09it/s]Loading train:  17%|█▋        | 48/285 [00:52<03:28,  1.14it/s]Loading train:  17%|█▋        | 49/285 [00:53<03:30,  1.12it/s]Loading train:  18%|█▊        | 50/285 [00:54<03:26,  1.14it/s]Loading train:  18%|█▊        | 51/285 [00:55<03:27,  1.13it/s]Loading train:  18%|█▊        | 52/285 [00:56<03:23,  1.15it/s]Loading train:  19%|█▊        | 53/285 [00:56<03:21,  1.15it/s]Loading train:  19%|█▉        | 54/285 [00:57<03:12,  1.20it/s]Loading train:  19%|█▉        | 55/285 [00:58<03:16,  1.17it/s]Loading train:  20%|█▉        | 56/285 [00:59<03:18,  1.15it/s]Loading train:  20%|██        | 57/285 [01:00<03:23,  1.12it/s]Loading train:  20%|██        | 58/285 [01:01<03:24,  1.11it/s]Loading train:  21%|██        | 59/285 [01:02<03:24,  1.11it/s]Loading train:  21%|██        | 60/285 [01:03<03:18,  1.13it/s]Loading train:  21%|██▏       | 61/285 [01:03<03:14,  1.15it/s]Loading train:  22%|██▏       | 62/285 [01:04<03:08,  1.19it/s]Loading train:  22%|██▏       | 63/285 [01:05<03:02,  1.22it/s]Loading train:  22%|██▏       | 64/285 [01:06<03:33,  1.03it/s]Loading train:  23%|██▎       | 65/285 [01:08<04:14,  1.16s/it]Loading train:  23%|██▎       | 66/285 [01:09<04:20,  1.19s/it]Loading train:  24%|██▎       | 67/285 [01:10<03:57,  1.09s/it]Loading train:  24%|██▍       | 68/285 [01:11<03:33,  1.01it/s]Loading train:  24%|██▍       | 69/285 [01:12<03:23,  1.06it/s]Loading train:  25%|██▍       | 70/285 [01:12<03:12,  1.12it/s]Loading train:  25%|██▍       | 71/285 [01:13<03:02,  1.17it/s]Loading train:  25%|██▌       | 72/285 [01:14<02:57,  1.20it/s]Loading train:  26%|██▌       | 73/285 [01:15<02:58,  1.19it/s]Loading train:  26%|██▌       | 74/285 [01:16<02:55,  1.20it/s]Loading train:  26%|██▋       | 75/285 [01:17<03:03,  1.14it/s]Loading train:  27%|██▋       | 76/285 [01:17<03:03,  1.14it/s]Loading train:  27%|██▋       | 77/285 [01:18<02:59,  1.16it/s]Loading train:  27%|██▋       | 78/285 [01:19<02:55,  1.18it/s]Loading train:  28%|██▊       | 79/285 [01:20<02:57,  1.16it/s]Loading train:  28%|██▊       | 80/285 [01:21<02:56,  1.16it/s]Loading train:  28%|██▊       | 81/285 [01:22<02:55,  1.16it/s]Loading train:  29%|██▉       | 82/285 [01:23<02:57,  1.15it/s]Loading train:  29%|██▉       | 83/285 [01:23<02:57,  1.14it/s]Loading train:  29%|██▉       | 84/285 [01:24<03:00,  1.11it/s]Loading train:  30%|██▉       | 85/285 [01:25<03:09,  1.05it/s]Loading train:  30%|███       | 86/285 [01:27<03:12,  1.03it/s]Loading train:  31%|███       | 87/285 [01:28<03:18,  1.00s/it]Loading train:  31%|███       | 88/285 [01:29<03:18,  1.01s/it]Loading train:  31%|███       | 89/285 [01:30<03:17,  1.01s/it]Loading train:  32%|███▏      | 90/285 [01:31<03:17,  1.01s/it]Loading train:  32%|███▏      | 91/285 [01:32<03:15,  1.01s/it]Loading train:  32%|███▏      | 92/285 [01:33<03:14,  1.01s/it]Loading train:  33%|███▎      | 93/285 [01:34<03:10,  1.01it/s]Loading train:  33%|███▎      | 94/285 [01:35<03:10,  1.00it/s]Loading train:  33%|███▎      | 95/285 [01:36<03:10,  1.00s/it]Loading train:  34%|███▎      | 96/285 [01:37<03:10,  1.01s/it]Loading train:  34%|███▍      | 97/285 [01:38<03:11,  1.02s/it]Loading train:  34%|███▍      | 98/285 [01:39<03:06,  1.00it/s]Loading train:  35%|███▍      | 99/285 [01:40<03:02,  1.02it/s]Loading train:  35%|███▌      | 100/285 [01:41<02:59,  1.03it/s]Loading train:  35%|███▌      | 101/285 [01:41<02:56,  1.04it/s]Loading train:  36%|███▌      | 102/285 [01:42<02:53,  1.05it/s]Loading train:  36%|███▌      | 103/285 [01:43<02:52,  1.05it/s]Loading train:  36%|███▋      | 104/285 [01:44<02:46,  1.09it/s]Loading train:  37%|███▋      | 105/285 [01:45<02:39,  1.13it/s]Loading train:  37%|███▋      | 106/285 [01:46<02:33,  1.17it/s]Loading train:  38%|███▊      | 107/285 [01:47<02:32,  1.17it/s]Loading train:  38%|███▊      | 108/285 [01:48<02:32,  1.16it/s]Loading train:  38%|███▊      | 109/285 [01:48<02:35,  1.13it/s]Loading train:  39%|███▊      | 110/285 [01:49<02:35,  1.12it/s]Loading train:  39%|███▉      | 111/285 [01:50<02:30,  1.15it/s]Loading train:  39%|███▉      | 112/285 [01:51<02:27,  1.18it/s]Loading train:  40%|███▉      | 113/285 [01:52<02:28,  1.16it/s]Loading train:  40%|████      | 114/285 [01:53<02:35,  1.10it/s]Loading train:  40%|████      | 115/285 [01:54<02:37,  1.08it/s]Loading train:  41%|████      | 116/285 [01:55<02:43,  1.03it/s]Loading train:  41%|████      | 117/285 [01:56<02:42,  1.04it/s]Loading train:  41%|████▏     | 118/285 [01:57<02:39,  1.05it/s]Loading train:  42%|████▏     | 119/285 [01:58<02:41,  1.03it/s]Loading train:  42%|████▏     | 120/285 [01:59<02:43,  1.01it/s]Loading train:  42%|████▏     | 121/285 [02:00<02:59,  1.09s/it]Loading train:  43%|████▎     | 122/285 [02:01<03:02,  1.12s/it]Loading train:  43%|████▎     | 123/285 [02:03<03:07,  1.15s/it]Loading train:  44%|████▎     | 124/285 [02:04<02:54,  1.08s/it]Loading train:  44%|████▍     | 125/285 [02:04<02:41,  1.01s/it]Loading train:  44%|████▍     | 126/285 [02:05<02:31,  1.05it/s]Loading train:  45%|████▍     | 127/285 [02:06<02:25,  1.08it/s]Loading train:  45%|████▍     | 128/285 [02:07<02:19,  1.13it/s]Loading train:  45%|████▌     | 129/285 [02:08<02:20,  1.11it/s]Loading train:  46%|████▌     | 130/285 [02:09<02:15,  1.14it/s]Loading train:  46%|████▌     | 131/285 [02:10<02:17,  1.12it/s]Loading train:  46%|████▋     | 132/285 [02:10<02:11,  1.16it/s]Loading train:  47%|████▋     | 133/285 [02:11<02:07,  1.19it/s]Loading train:  47%|████▋     | 134/285 [02:12<02:05,  1.20it/s]Loading train:  47%|████▋     | 135/285 [02:13<02:03,  1.22it/s]Loading train:  48%|████▊     | 136/285 [02:13<02:01,  1.23it/s]Loading train:  48%|████▊     | 137/285 [02:14<02:05,  1.18it/s]Loading train:  48%|████▊     | 138/285 [02:15<02:07,  1.15it/s]Loading train:  49%|████▉     | 139/285 [02:16<02:07,  1.14it/s]Loading train:  49%|████▉     | 140/285 [02:17<02:06,  1.15it/s]Loading train:  49%|████▉     | 141/285 [02:18<02:07,  1.13it/s]Loading train:  50%|████▉     | 142/285 [02:19<02:05,  1.14it/s]Loading train:  50%|█████     | 143/285 [02:20<02:04,  1.14it/s]Loading train:  51%|█████     | 144/285 [02:21<02:02,  1.15it/s]Loading train:  51%|█████     | 145/285 [02:21<01:55,  1.21it/s]Loading train:  51%|█████     | 146/285 [02:22<01:50,  1.25it/s]Loading train:  52%|█████▏    | 147/285 [02:23<01:45,  1.31it/s]Loading train:  52%|█████▏    | 148/285 [02:23<01:41,  1.35it/s]Loading train:  52%|█████▏    | 149/285 [02:24<01:43,  1.31it/s]Loading train:  53%|█████▎    | 150/285 [02:25<01:44,  1.29it/s]Loading train:  53%|█████▎    | 151/285 [02:26<01:51,  1.20it/s]Loading train:  53%|█████▎    | 152/285 [02:27<01:47,  1.24it/s]Loading train:  54%|█████▎    | 153/285 [02:27<01:44,  1.27it/s]Loading train:  54%|█████▍    | 154/285 [02:28<01:40,  1.30it/s]Loading train:  54%|█████▍    | 155/285 [02:29<01:38,  1.32it/s]Loading train:  55%|█████▍    | 156/285 [02:30<01:37,  1.33it/s]Loading train:  55%|█████▌    | 157/285 [02:31<01:39,  1.29it/s]Loading train:  55%|█████▌    | 158/285 [02:31<01:35,  1.32it/s]Loading train:  56%|█████▌    | 159/285 [02:32<01:34,  1.34it/s]Loading train:  56%|█████▌    | 160/285 [02:33<01:35,  1.31it/s]Loading train:  56%|█████▋    | 161/285 [02:33<01:33,  1.33it/s]Loading train:  57%|█████▋    | 162/285 [02:34<01:37,  1.26it/s]Loading train:  57%|█████▋    | 163/285 [02:35<01:39,  1.22it/s]Loading train:  58%|█████▊    | 164/285 [02:36<01:39,  1.21it/s]Loading train:  58%|█████▊    | 165/285 [02:37<01:39,  1.21it/s]Loading train:  58%|█████▊    | 166/285 [02:38<01:39,  1.19it/s]Loading train:  59%|█████▊    | 167/285 [02:39<01:41,  1.16it/s]Loading train:  59%|█████▉    | 168/285 [02:39<01:38,  1.19it/s]Loading train:  59%|█████▉    | 169/285 [02:40<01:35,  1.22it/s]Loading train:  60%|█████▉    | 170/285 [02:41<01:29,  1.29it/s]Loading train:  60%|██████    | 171/285 [02:42<01:27,  1.31it/s]Loading train:  60%|██████    | 172/285 [02:42<01:25,  1.32it/s]Loading train:  61%|██████    | 173/285 [02:43<01:21,  1.37it/s]Loading train:  61%|██████    | 174/285 [02:44<01:19,  1.39it/s]Loading train:  61%|██████▏   | 175/285 [02:45<01:20,  1.36it/s]Loading train:  62%|██████▏   | 176/285 [02:45<01:21,  1.34it/s]Loading train:  62%|██████▏   | 177/285 [02:46<01:27,  1.23it/s]Loading train:  62%|██████▏   | 178/285 [02:47<01:26,  1.24it/s]Loading train:  63%|██████▎   | 179/285 [02:48<01:25,  1.23it/s]Loading train:  63%|██████▎   | 180/285 [02:49<01:24,  1.24it/s]Loading train:  64%|██████▎   | 181/285 [02:49<01:19,  1.30it/s]Loading train:  64%|██████▍   | 182/285 [02:50<01:19,  1.30it/s]Loading train:  64%|██████▍   | 183/285 [02:51<01:17,  1.32it/s]Loading train:  65%|██████▍   | 184/285 [02:52<01:15,  1.35it/s]Loading train:  65%|██████▍   | 185/285 [02:52<01:16,  1.30it/s]Loading train:  65%|██████▌   | 186/285 [02:53<01:16,  1.30it/s]Loading train:  66%|██████▌   | 187/285 [02:54<01:12,  1.35it/s]Loading train:  66%|██████▌   | 188/285 [02:55<01:10,  1.37it/s]Loading train:  66%|██████▋   | 189/285 [02:55<01:11,  1.34it/s]Loading train:  67%|██████▋   | 190/285 [02:56<01:11,  1.33it/s]Loading train:  67%|██████▋   | 191/285 [02:57<01:08,  1.37it/s]Loading train:  67%|██████▋   | 192/285 [02:58<01:10,  1.31it/s]Loading train:  68%|██████▊   | 193/285 [02:58<01:09,  1.32it/s]Loading train:  68%|██████▊   | 194/285 [02:59<01:07,  1.35it/s]Loading train:  68%|██████▊   | 195/285 [03:00<01:08,  1.31it/s]Loading train:  69%|██████▉   | 196/285 [03:01<01:10,  1.26it/s]Loading train:  69%|██████▉   | 197/285 [03:02<01:10,  1.24it/s]Loading train:  69%|██████▉   | 198/285 [03:02<01:08,  1.27it/s]Loading train:  70%|██████▉   | 199/285 [03:03<01:09,  1.24it/s]Loading train:  70%|███████   | 200/285 [03:04<01:09,  1.22it/s]Loading train:  71%|███████   | 201/285 [03:05<01:07,  1.25it/s]Loading train:  71%|███████   | 202/285 [03:06<01:07,  1.22it/s]Loading train:  71%|███████   | 203/285 [03:06<01:05,  1.25it/s]Loading train:  72%|███████▏  | 204/285 [03:07<01:05,  1.24it/s]Loading train:  72%|███████▏  | 205/285 [03:08<01:05,  1.22it/s]Loading train:  72%|███████▏  | 206/285 [03:09<01:04,  1.23it/s]Loading train:  73%|███████▎  | 207/285 [03:10<01:06,  1.18it/s]Loading train:  73%|███████▎  | 208/285 [03:11<01:04,  1.20it/s]Loading train:  73%|███████▎  | 209/285 [03:11<01:01,  1.24it/s]Loading train:  74%|███████▎  | 210/285 [03:12<01:01,  1.21it/s]Loading train:  74%|███████▍  | 211/285 [03:13<01:02,  1.19it/s]Loading train:  74%|███████▍  | 212/285 [03:14<01:03,  1.14it/s]Loading train:  75%|███████▍  | 213/285 [03:15<01:02,  1.15it/s]Loading train:  75%|███████▌  | 214/285 [03:16<01:02,  1.14it/s]Loading train:  75%|███████▌  | 215/285 [03:16<00:57,  1.22it/s]Loading train:  76%|███████▌  | 216/285 [03:17<00:53,  1.29it/s]Loading train:  76%|███████▌  | 217/285 [03:18<00:50,  1.34it/s]Loading train:  76%|███████▋  | 218/285 [03:19<00:50,  1.34it/s]Loading train:  77%|███████▋  | 219/285 [03:19<00:49,  1.35it/s]Loading train:  77%|███████▋  | 220/285 [03:20<00:52,  1.25it/s]Loading train:  78%|███████▊  | 221/285 [03:21<00:54,  1.18it/s]Loading train:  78%|███████▊  | 222/285 [03:22<00:53,  1.18it/s]Loading train:  78%|███████▊  | 223/285 [03:23<00:52,  1.17it/s]Loading train:  79%|███████▊  | 224/285 [03:24<00:51,  1.20it/s]Loading train:  79%|███████▉  | 225/285 [03:25<00:50,  1.18it/s]Loading train:  79%|███████▉  | 226/285 [03:25<00:49,  1.19it/s]Loading train:  80%|███████▉  | 227/285 [03:26<00:48,  1.20it/s]Loading train:  80%|████████  | 228/285 [03:27<00:47,  1.20it/s]Loading train:  80%|████████  | 229/285 [03:28<00:46,  1.20it/s]Loading train:  81%|████████  | 230/285 [03:29<00:45,  1.20it/s]Loading train:  81%|████████  | 231/285 [03:30<00:44,  1.22it/s]Loading train:  81%|████████▏ | 232/285 [03:31<00:47,  1.11it/s]Loading train:  82%|████████▏ | 233/285 [03:32<00:49,  1.04it/s]Loading train:  82%|████████▏ | 234/285 [03:33<00:49,  1.02it/s]Loading train:  82%|████████▏ | 235/285 [03:34<00:49,  1.02it/s]Loading train:  83%|████████▎ | 236/285 [03:35<00:49,  1.01s/it]Loading train:  83%|████████▎ | 237/285 [03:36<00:48,  1.01s/it]Loading train:  84%|████████▎ | 238/285 [03:37<00:46,  1.01it/s]Loading train:  84%|████████▍ | 239/285 [03:38<00:45,  1.01it/s]Loading train:  84%|████████▍ | 240/285 [03:39<00:45,  1.01s/it]Loading train:  85%|████████▍ | 241/285 [03:40<00:44,  1.02s/it]Loading train:  85%|████████▍ | 242/285 [03:41<00:43,  1.01s/it]Loading train:  85%|████████▌ | 243/285 [03:42<00:42,  1.01s/it]Loading train:  86%|████████▌ | 244/285 [03:43<00:41,  1.01s/it]Loading train:  86%|████████▌ | 245/285 [03:44<00:39,  1.01it/s]Loading train:  86%|████████▋ | 246/285 [03:45<00:40,  1.04s/it]Loading train:  87%|████████▋ | 247/285 [03:46<00:38,  1.02s/it]Loading train:  87%|████████▋ | 248/285 [03:47<00:38,  1.03s/it]Loading train:  87%|████████▋ | 249/285 [03:48<00:35,  1.01it/s]Loading train:  88%|████████▊ | 250/285 [03:49<00:31,  1.10it/s]Loading train:  88%|████████▊ | 251/285 [03:49<00:29,  1.17it/s]Loading train:  88%|████████▊ | 252/285 [03:50<00:26,  1.24it/s]Loading train:  89%|████████▉ | 253/285 [03:51<00:24,  1.30it/s]Loading train:  89%|████████▉ | 254/285 [03:52<00:25,  1.22it/s]Loading train:  89%|████████▉ | 255/285 [03:52<00:24,  1.24it/s]Loading train:  90%|████████▉ | 256/285 [03:53<00:22,  1.26it/s]Loading train:  90%|█████████ | 257/285 [03:54<00:21,  1.28it/s]Loading train:  91%|█████████ | 258/285 [03:55<00:21,  1.24it/s]Loading train:  91%|█████████ | 259/285 [03:56<00:20,  1.28it/s]Loading train:  91%|█████████ | 260/285 [03:56<00:19,  1.29it/s]Loading train:  92%|█████████▏| 261/285 [03:57<00:19,  1.25it/s]Loading train:  92%|█████████▏| 262/285 [03:58<00:19,  1.20it/s]Loading train:  92%|█████████▏| 263/285 [03:59<00:18,  1.17it/s]Loading train:  93%|█████████▎| 264/285 [04:00<00:17,  1.20it/s]Loading train:  93%|█████████▎| 265/285 [04:01<00:16,  1.22it/s]Loading train:  93%|█████████▎| 266/285 [04:01<00:15,  1.19it/s]Loading train:  94%|█████████▎| 267/285 [04:02<00:14,  1.23it/s]Loading train:  94%|█████████▍| 268/285 [04:03<00:15,  1.09it/s]Loading train:  94%|█████████▍| 269/285 [04:04<00:14,  1.07it/s]Loading train:  95%|█████████▍| 270/285 [04:05<00:14,  1.04it/s]Loading train:  95%|█████████▌| 271/285 [04:06<00:14,  1.02s/it]Loading train:  95%|█████████▌| 272/285 [04:07<00:12,  1.01it/s]Loading train:  96%|█████████▌| 273/285 [04:08<00:11,  1.03it/s]Loading train:  96%|█████████▌| 274/285 [04:09<00:10,  1.03it/s]Loading train:  96%|█████████▋| 275/285 [04:10<00:09,  1.04it/s]Loading train:  97%|█████████▋| 276/285 [04:11<00:08,  1.03it/s]Loading train:  97%|█████████▋| 277/285 [04:12<00:07,  1.01it/s]Loading train:  98%|█████████▊| 278/285 [04:13<00:06,  1.04it/s]Loading train:  98%|█████████▊| 279/285 [04:14<00:05,  1.04it/s]Loading train:  98%|█████████▊| 280/285 [04:15<00:04,  1.01it/s]Loading train:  99%|█████████▊| 281/285 [04:16<00:03,  1.04it/s]Loading train:  99%|█████████▉| 282/285 [04:17<00:02,  1.05it/s]Loading train:  99%|█████████▉| 283/285 [04:18<00:01,  1.04it/s]Loading train: 100%|█████████▉| 284/285 [04:19<00:00,  1.01it/s]Loading train: 100%|██████████| 285/285 [04:20<00:00,  1.02it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 13/285 [00:00<00:02, 127.59it/s]concatenating: train:  15%|█▍        | 42/285 [00:00<00:01, 153.21it/s]concatenating: train:  25%|██▍       | 71/285 [00:00<00:01, 177.97it/s]concatenating: train:  33%|███▎      | 93/285 [00:00<00:01, 188.30it/s]concatenating: train:  44%|████▍     | 125/285 [00:00<00:00, 214.67it/s]concatenating: train:  55%|█████▌    | 157/285 [00:00<00:00, 237.80it/s]concatenating: train:  67%|██████▋   | 192/285 [00:00<00:00, 261.58it/s]concatenating: train:  80%|███████▉  | 227/285 [00:00<00:00, 283.01it/s]concatenating: train:  90%|█████████ | 257/285 [00:00<00:00, 286.46it/s]concatenating: train: 100%|██████████| 285/285 [00:00<00:00, 288.30it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.30s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 568.51it/s]2019-07-05 21:50:16.640271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 21:50:16.640387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 21:50:16.640418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 21:50:16.640429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 21:50:16.640899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:14,  2.68it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:11,  3.15it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:11,  3.03it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:08,  3.91it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:09,  3.41it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:02<00:07,  3.84it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:08,  3.41it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:05,  4.27it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:03<00:06,  3.56it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:05,  3.99it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:05,  3.48it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:04<00:04,  4.16it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:04<00:03,  4.38it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:05<00:03,  3.61it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:05<00:03,  3.96it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:06<00:03,  3.39it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:06<00:01,  4.06it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:06<00:01,  4.28it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:07<00:01,  3.53it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:07<00:00,  3.88it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:08<00:00,  3.30it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:08<00:00,  4.97it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 50)   500         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 50)   200         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 50)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 50)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 50)   22550       dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 50)   200         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 50)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 50)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 50)   22550       dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 50)   200         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 50)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 50)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   9020        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 50)   9050        dropout_15[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 52, 80, 50)   200         conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 52, 80, 50)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 52, 80, 50)   0           activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 52, 80, 13)   663         dropout_16[0][0]                 
==================================================================================================
Total params: 248,773
Trainable params: 104,893
Non-trainable params: 143,880
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 24s - loss: 135.4347 - acc: 0.5701 - mDice: 0.0172 - val_loss: 36.3267 - val_acc: 0.9047 - val_mDice: 0.0173

Epoch 00001: val_mDice improved from -inf to 0.01730, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 15s - loss: 31.9202 - acc: 0.8663 - mDice: 0.0157 - val_loss: 15.1470 - val_acc: 0.9047 - val_mDice: 0.0131

Epoch 00002: val_mDice did not improve from 0.01730
Epoch 3/300
 - 15s - loss: 15.0271 - acc: 0.8692 - mDice: 0.0164 - val_loss: 9.8197 - val_acc: 0.9047 - val_mDice: 0.0097

Epoch 00003: val_mDice did not improve from 0.01730
Epoch 4/300
 - 15s - loss: 10.3442 - acc: 0.8692 - mDice: 0.0201 - val_loss: 8.0021 - val_acc: 0.9047 - val_mDice: 0.0130

Epoch 00004: val_mDice did not improve from 0.01730
Epoch 5/300
 - 14s - loss: 8.2039 - acc: 0.8692 - mDice: 0.0308 - val_loss: 7.1989 - val_acc: 0.9047 - val_mDice: 0.0119

Epoch 00005: val_mDice did not improve from 0.01730
Epoch 6/300
 - 14s - loss: 6.9813 - acc: 0.8691 - mDice: 0.0429 - val_loss: 6.5933 - val_acc: 0.9047 - val_mDice: 0.0178

Epoch 00006: val_mDice improved from 0.01730 to 0.01780, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 14s - loss: 6.1050 - acc: 0.8695 - mDice: 0.0596 - val_loss: 6.7846 - val_acc: 0.9047 - val_mDice: 0.0184

Epoch 00007: val_mDice improved from 0.01780 to 0.01838, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 15s - loss: 5.4375 - acc: 0.8706 - mDice: 0.0820 - val_loss: 4.8515 - val_acc: 0.9043 - val_mDice: 0.0809

Epoch 00008: val_mDice improved from 0.01838 to 0.08093, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 15s - loss: 4.9168 - acc: 0.8733 - mDice: 0.1071 - val_loss: 5.2212 - val_acc: 0.9045 - val_mDice: 0.0766

Epoch 00009: val_mDice did not improve from 0.08093
Epoch 10/300
 - 14s - loss: 4.4810 - acc: 0.8795 - mDice: 0.1355 - val_loss: 4.3918 - val_acc: 0.9132 - val_mDice: 0.1362

Epoch 00010: val_mDice improved from 0.08093 to 0.13620, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 14s - loss: 4.0615 - acc: 0.8891 - mDice: 0.1718 - val_loss: 3.6644 - val_acc: 0.9253 - val_mDice: 0.1904

Epoch 00011: val_mDice improved from 0.13620 to 0.19038, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 14s - loss: 3.7265 - acc: 0.8947 - mDice: 0.2050 - val_loss: 3.5560 - val_acc: 0.9280 - val_mDice: 0.2190

Epoch 00012: val_mDice improved from 0.19038 to 0.21901, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 14s - loss: 3.4549 - acc: 0.8989 - mDice: 0.2366 - val_loss: 3.1784 - val_acc: 0.9300 - val_mDice: 0.2595

Epoch 00013: val_mDice improved from 0.21901 to 0.25949, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 3.2345 - acc: 0.9021 - mDice: 0.2663 - val_loss: 4.2278 - val_acc: 0.9286 - val_mDice: 0.2218

Epoch 00014: val_mDice did not improve from 0.25949
Epoch 15/300
 - 15s - loss: 3.0605 - acc: 0.9047 - mDice: 0.2916 - val_loss: 3.5760 - val_acc: 0.9296 - val_mDice: 0.2659

Epoch 00015: val_mDice improved from 0.25949 to 0.26591, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 15s - loss: 2.9134 - acc: 0.9071 - mDice: 0.3151 - val_loss: 2.9588 - val_acc: 0.9268 - val_mDice: 0.3342

Epoch 00016: val_mDice improved from 0.26591 to 0.33416, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 2.7919 - acc: 0.9093 - mDice: 0.3347 - val_loss: 3.4094 - val_acc: 0.9352 - val_mDice: 0.3140

Epoch 00017: val_mDice did not improve from 0.33416
Epoch 18/300
 - 14s - loss: 2.6804 - acc: 0.9116 - mDice: 0.3547 - val_loss: 3.1528 - val_acc: 0.9357 - val_mDice: 0.3300

Epoch 00018: val_mDice did not improve from 0.33416
Epoch 19/300
 - 14s - loss: 2.5805 - acc: 0.9139 - mDice: 0.3719 - val_loss: 2.9783 - val_acc: 0.9339 - val_mDice: 0.3620

Epoch 00019: val_mDice improved from 0.33416 to 0.36200, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 2.5239 - acc: 0.9148 - mDice: 0.3843 - val_loss: 2.9266 - val_acc: 0.9359 - val_mDice: 0.3677

Epoch 00020: val_mDice improved from 0.36200 to 0.36768, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 15s - loss: 2.4339 - acc: 0.9170 - mDice: 0.3992 - val_loss: 3.0355 - val_acc: 0.9357 - val_mDice: 0.3605

Epoch 00021: val_mDice did not improve from 0.36768
Epoch 22/300
 - 15s - loss: 2.3741 - acc: 0.9184 - mDice: 0.4114 - val_loss: 2.9062 - val_acc: 0.9345 - val_mDice: 0.3823

Epoch 00022: val_mDice improved from 0.36768 to 0.38226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 14s - loss: 2.3329 - acc: 0.9193 - mDice: 0.4203 - val_loss: 3.4959 - val_acc: 0.9350 - val_mDice: 0.3553

Epoch 00023: val_mDice did not improve from 0.38226
Epoch 24/300
 - 14s - loss: 2.2710 - acc: 0.9207 - mDice: 0.4324 - val_loss: 2.8186 - val_acc: 0.9358 - val_mDice: 0.4003

Epoch 00024: val_mDice improved from 0.38226 to 0.40033, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 15s - loss: 2.2310 - acc: 0.9218 - mDice: 0.4408 - val_loss: 2.8412 - val_acc: 0.9334 - val_mDice: 0.4078

Epoch 00025: val_mDice improved from 0.40033 to 0.40780, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 15s - loss: 2.1907 - acc: 0.9225 - mDice: 0.4493 - val_loss: 2.9862 - val_acc: 0.9390 - val_mDice: 0.3937

Epoch 00026: val_mDice did not improve from 0.40780
Epoch 27/300
 - 15s - loss: 2.1372 - acc: 0.9239 - mDice: 0.4603 - val_loss: 2.9528 - val_acc: 0.9405 - val_mDice: 0.4108

Epoch 00027: val_mDice improved from 0.40780 to 0.41076, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 15s - loss: 2.1135 - acc: 0.9245 - mDice: 0.4656 - val_loss: 2.9511 - val_acc: 0.9404 - val_mDice: 0.4077

Epoch 00028: val_mDice did not improve from 0.41076
Epoch 29/300
 - 15s - loss: 2.0858 - acc: 0.9251 - mDice: 0.4720 - val_loss: 2.7219 - val_acc: 0.9364 - val_mDice: 0.4259

Epoch 00029: val_mDice improved from 0.41076 to 0.42594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 14s - loss: 2.0410 - acc: 0.9261 - mDice: 0.4807 - val_loss: 2.7812 - val_acc: 0.9341 - val_mDice: 0.4285

Epoch 00030: val_mDice improved from 0.42594 to 0.42845, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 14s - loss: 2.0085 - acc: 0.9271 - mDice: 0.4878 - val_loss: 2.6987 - val_acc: 0.9292 - val_mDice: 0.4334

Epoch 00031: val_mDice improved from 0.42845 to 0.43341, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 15s - loss: 1.9922 - acc: 0.9274 - mDice: 0.4922 - val_loss: 2.8687 - val_acc: 0.9277 - val_mDice: 0.4259

Epoch 00032: val_mDice did not improve from 0.43341
Epoch 33/300
 - 15s - loss: 1.9631 - acc: 0.9282 - mDice: 0.4985 - val_loss: 2.8847 - val_acc: 0.9419 - val_mDice: 0.4338

Epoch 00033: val_mDice improved from 0.43341 to 0.43382, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 15s - loss: 1.9415 - acc: 0.9290 - mDice: 0.5044 - val_loss: 3.2185 - val_acc: 0.9393 - val_mDice: 0.4048

Epoch 00034: val_mDice did not improve from 0.43382
Epoch 35/300
 - 15s - loss: 1.9273 - acc: 0.9292 - mDice: 0.5069 - val_loss: 2.7580 - val_acc: 0.9402 - val_mDice: 0.4431

Epoch 00035: val_mDice improved from 0.43382 to 0.44315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 15s - loss: 1.8899 - acc: 0.9303 - mDice: 0.5155 - val_loss: 2.8228 - val_acc: 0.9392 - val_mDice: 0.4358

Epoch 00036: val_mDice did not improve from 0.44315
Epoch 37/300
 - 15s - loss: 1.8783 - acc: 0.9307 - mDice: 0.5186 - val_loss: 2.6561 - val_acc: 0.9385 - val_mDice: 0.4507

Epoch 00037: val_mDice improved from 0.44315 to 0.45067, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 15s - loss: 1.8669 - acc: 0.9307 - mDice: 0.5212 - val_loss: 2.9706 - val_acc: 0.9386 - val_mDice: 0.4348

Epoch 00038: val_mDice did not improve from 0.45067
Epoch 39/300
 - 14s - loss: 1.8313 - acc: 0.9320 - mDice: 0.5282 - val_loss: 2.8371 - val_acc: 0.9333 - val_mDice: 0.4436

Epoch 00039: val_mDice did not improve from 0.45067
Epoch 40/300
 - 15s - loss: 1.8229 - acc: 0.9322 - mDice: 0.5315 - val_loss: 3.0901 - val_acc: 0.9412 - val_mDice: 0.4375

Epoch 00040: val_mDice did not improve from 0.45067
Epoch 41/300
 - 15s - loss: 1.8065 - acc: 0.9325 - mDice: 0.5336 - val_loss: 2.8124 - val_acc: 0.9428 - val_mDice: 0.4484

Epoch 00041: val_mDice did not improve from 0.45067
Epoch 42/300
 - 15s - loss: 1.7970 - acc: 0.9328 - mDice: 0.5374 - val_loss: 3.5975 - val_acc: 0.9392 - val_mDice: 0.4047

Epoch 00042: val_mDice did not improve from 0.45067
Epoch 43/300
 - 15s - loss: 1.7710 - acc: 0.9336 - mDice: 0.5424 - val_loss: 3.0859 - val_acc: 0.9423 - val_mDice: 0.4372

Epoch 00043: val_mDice did not improve from 0.45067
Epoch 44/300
 - 15s - loss: 1.7576 - acc: 0.9338 - mDice: 0.5456 - val_loss: 3.2918 - val_acc: 0.9410 - val_mDice: 0.4176

Epoch 00044: val_mDice did not improve from 0.45067
Epoch 45/300
 - 15s - loss: 1.7442 - acc: 0.9341 - mDice: 0.5493 - val_loss: 2.8692 - val_acc: 0.9381 - val_mDice: 0.4472

Epoch 00045: val_mDice did not improve from 0.45067
Epoch 46/300
 - 14s - loss: 1.7306 - acc: 0.9346 - mDice: 0.5528 - val_loss: 3.2973 - val_acc: 0.9412 - val_mDice: 0.4308

Epoch 00046: val_mDice did not improve from 0.45067
Epoch 47/300
 - 15s - loss: 1.7117 - acc: 0.9350 - mDice: 0.5564 - val_loss: 2.8980 - val_acc: 0.9381 - val_mDice: 0.4527

Epoch 00047: val_mDice improved from 0.45067 to 0.45269, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 15s - loss: 1.7109 - acc: 0.9352 - mDice: 0.5574 - val_loss: 2.8112 - val_acc: 0.9417 - val_mDice: 0.4588

Epoch 00048: val_mDice improved from 0.45269 to 0.45880, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 15s - loss: 1.6888 - acc: 0.9355 - mDice: 0.5623 - val_loss: 2.9426 - val_acc: 0.9392 - val_mDice: 0.4550

Epoch 00049: val_mDice did not improve from 0.45880
Epoch 50/300
 - 15s - loss: 1.6873 - acc: 0.9356 - mDice: 0.5634 - val_loss: 2.7617 - val_acc: 0.9443 - val_mDice: 0.4656

Epoch 00050: val_mDice improved from 0.45880 to 0.46558, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 15s - loss: 1.6640 - acc: 0.9362 - mDice: 0.5679 - val_loss: 2.9875 - val_acc: 0.9434 - val_mDice: 0.4462

Epoch 00051: val_mDice did not improve from 0.46558
Epoch 52/300
 - 14s - loss: 1.6601 - acc: 0.9363 - mDice: 0.5686 - val_loss: 2.8399 - val_acc: 0.9429 - val_mDice: 0.4619

Epoch 00052: val_mDice did not improve from 0.46558
Epoch 53/300
 - 14s - loss: 1.6558 - acc: 0.9365 - mDice: 0.5704 - val_loss: 2.8316 - val_acc: 0.9393 - val_mDice: 0.4640

Epoch 00053: val_mDice did not improve from 0.46558
Epoch 54/300
 - 15s - loss: 1.6328 - acc: 0.9369 - mDice: 0.5748 - val_loss: 2.8858 - val_acc: 0.9397 - val_mDice: 0.4635

Epoch 00054: val_mDice did not improve from 0.46558
Epoch 55/300
 - 15s - loss: 1.6299 - acc: 0.9369 - mDice: 0.5767 - val_loss: 3.0780 - val_acc: 0.9438 - val_mDice: 0.4554

Epoch 00055: val_mDice did not improve from 0.46558
Epoch 56/300
 - 14s - loss: 1.6174 - acc: 0.9374 - mDice: 0.5793 - val_loss: 3.1277 - val_acc: 0.9383 - val_mDice: 0.4501

Epoch 00056: val_mDice did not improve from 0.46558
Epoch 57/300
 - 14s - loss: 1.6053 - acc: 0.9376 - mDice: 0.5822 - val_loss: 2.8572 - val_acc: 0.9421 - val_mDice: 0.4661

Epoch 00057: val_mDice improved from 0.46558 to 0.46607, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 14s - loss: 1.6001 - acc: 0.9378 - mDice: 0.5831 - val_loss: 2.9378 - val_acc: 0.9392 - val_mDice: 0.4652

Epoch 00058: val_mDice did not improve from 0.46607
Epoch 59/300
 - 14s - loss: 1.5980 - acc: 0.9379 - mDice: 0.5845 - val_loss: 2.9532 - val_acc: 0.9417 - val_mDice: 0.4560

Epoch 00059: val_mDice did not improve from 0.46607
Epoch 60/300
 - 15s - loss: 1.5815 - acc: 0.9382 - mDice: 0.5876 - val_loss: 2.9059 - val_acc: 0.9433 - val_mDice: 0.4745

Epoch 00060: val_mDice improved from 0.46607 to 0.47447, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 15s - loss: 1.5706 - acc: 0.9385 - mDice: 0.5902 - val_loss: 2.9724 - val_acc: 0.9445 - val_mDice: 0.4655

Epoch 00061: val_mDice did not improve from 0.47447
Epoch 62/300
 - 15s - loss: 1.5599 - acc: 0.9388 - mDice: 0.5925 - val_loss: 3.1248 - val_acc: 0.9311 - val_mDice: 0.4520

Epoch 00062: val_mDice did not improve from 0.47447
Epoch 63/300
 - 15s - loss: 1.5564 - acc: 0.9388 - mDice: 0.5946 - val_loss: 3.8652 - val_acc: 0.9397 - val_mDice: 0.4213

Epoch 00063: val_mDice did not improve from 0.47447
Epoch 64/300
 - 15s - loss: 1.5524 - acc: 0.9390 - mDice: 0.5949 - val_loss: 2.9101 - val_acc: 0.9437 - val_mDice: 0.4742

Epoch 00064: val_mDice did not improve from 0.47447
Epoch 65/300
 - 14s - loss: 1.5392 - acc: 0.9394 - mDice: 0.5981 - val_loss: 2.8218 - val_acc: 0.9414 - val_mDice: 0.4768

Epoch 00065: val_mDice improved from 0.47447 to 0.47677, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 15s - loss: 1.5286 - acc: 0.9396 - mDice: 0.6004 - val_loss: 3.0226 - val_acc: 0.9400 - val_mDice: 0.4607

Epoch 00066: val_mDice did not improve from 0.47677
Epoch 67/300
 - 15s - loss: 1.5265 - acc: 0.9397 - mDice: 0.6015 - val_loss: 2.9766 - val_acc: 0.9415 - val_mDice: 0.4652

Epoch 00067: val_mDice did not improve from 0.47677
Epoch 68/300
 - 15s - loss: 1.5180 - acc: 0.9398 - mDice: 0.6029 - val_loss: 3.2150 - val_acc: 0.9424 - val_mDice: 0.4552

Epoch 00068: val_mDice did not improve from 0.47677
Epoch 69/300
 - 15s - loss: 1.5113 - acc: 0.9402 - mDice: 0.6041 - val_loss: 2.9762 - val_acc: 0.9398 - val_mDice: 0.4684

Epoch 00069: val_mDice did not improve from 0.47677
Epoch 70/300
 - 14s - loss: 1.5176 - acc: 0.9399 - mDice: 0.6039 - val_loss: 2.8765 - val_acc: 0.9404 - val_mDice: 0.4709

Epoch 00070: val_mDice did not improve from 0.47677
Epoch 71/300
 - 14s - loss: 1.4962 - acc: 0.9405 - mDice: 0.6078 - val_loss: 3.3144 - val_acc: 0.9358 - val_mDice: 0.4498

Epoch 00071: val_mDice did not improve from 0.47677
Epoch 72/300
 - 15s - loss: 1.4958 - acc: 0.9404 - mDice: 0.6084 - val_loss: 2.8977 - val_acc: 0.9417 - val_mDice: 0.4766

Epoch 00072: val_mDice did not improve from 0.47677
Epoch 73/300
 - 15s - loss: 1.5033 - acc: 0.9405 - mDice: 0.6071 - val_loss: 3.2815 - val_acc: 0.9448 - val_mDice: 0.4570

Epoch 00073: val_mDice did not improve from 0.47677
Epoch 74/300
 - 15s - loss: 1.4834 - acc: 0.9407 - mDice: 0.6117 - val_loss: 2.9826 - val_acc: 0.9456 - val_mDice: 0.4737

Epoch 00074: val_mDice did not improve from 0.47677
Epoch 75/300
 - 15s - loss: 1.4806 - acc: 0.9410 - mDice: 0.6115 - val_loss: 3.1680 - val_acc: 0.9405 - val_mDice: 0.4636

Epoch 00075: val_mDice did not improve from 0.47677
Epoch 76/300
 - 15s - loss: 1.4693 - acc: 0.9413 - mDice: 0.6147 - val_loss: 3.1592 - val_acc: 0.9447 - val_mDice: 0.4588

Epoch 00076: val_mDice did not improve from 0.47677
Epoch 77/300
 - 14s - loss: 1.4680 - acc: 0.9414 - mDice: 0.6153 - val_loss: 3.4337 - val_acc: 0.9438 - val_mDice: 0.4501

Epoch 00077: val_mDice did not improve from 0.47677
Epoch 78/300
 - 15s - loss: 1.4551 - acc: 0.9416 - mDice: 0.6186 - val_loss: 2.9443 - val_acc: 0.9432 - val_mDice: 0.4694

Epoch 00078: val_mDice did not improve from 0.47677
Epoch 79/300
 - 15s - loss: 1.4527 - acc: 0.9416 - mDice: 0.6194 - val_loss: 3.0573 - val_acc: 0.9426 - val_mDice: 0.4681

Epoch 00079: val_mDice did not improve from 0.47677
Epoch 80/300
 - 15s - loss: 1.4465 - acc: 0.9419 - mDice: 0.6205 - val_loss: 3.0917 - val_acc: 0.9457 - val_mDice: 0.4735

Epoch 00080: val_mDice did not improve from 0.47677
Epoch 81/300
 - 14s - loss: 1.4381 - acc: 0.9420 - mDice: 0.6224 - val_loss: 3.0759 - val_acc: 0.9446 - val_mDice: 0.4744

Epoch 00081: val_mDice did not improve from 0.47677
Epoch 82/300
 - 15s - loss: 1.4360 - acc: 0.9422 - mDice: 0.6232 - val_loss: 3.0116 - val_acc: 0.9385 - val_mDice: 0.4692

Epoch 00082: val_mDice did not improve from 0.47677
Epoch 83/300
 - 15s - loss: 1.4405 - acc: 0.9419 - mDice: 0.6224 - val_loss: 3.0973 - val_acc: 0.9457 - val_mDice: 0.4759

Epoch 00083: val_mDice did not improve from 0.47677
Epoch 84/300
 - 15s - loss: 1.4295 - acc: 0.9423 - mDice: 0.6247 - val_loss: 2.9402 - val_acc: 0.9433 - val_mDice: 0.4785

Epoch 00084: val_mDice improved from 0.47677 to 0.47853, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 85/300
 - 14s - loss: 1.4216 - acc: 0.9425 - mDice: 0.6265 - val_loss: 3.1544 - val_acc: 0.9412 - val_mDice: 0.4690

Epoch 00085: val_mDice did not improve from 0.47853
Epoch 86/300
 - 15s - loss: 1.4225 - acc: 0.9426 - mDice: 0.6264 - val_loss: 2.9886 - val_acc: 0.9401 - val_mDice: 0.4726

Epoch 00086: val_mDice did not improve from 0.47853
Epoch 87/300
 - 14s - loss: 1.4226 - acc: 0.9424 - mDice: 0.6265 - val_loss: 3.3479 - val_acc: 0.9450 - val_mDice: 0.4659

Epoch 00087: val_mDice did not improve from 0.47853
Epoch 88/300
 - 15s - loss: 1.4101 - acc: 0.9426 - mDice: 0.6289 - val_loss: 3.2348 - val_acc: 0.9439 - val_mDice: 0.4560

Epoch 00088: val_mDice did not improve from 0.47853
Epoch 89/300
 - 15s - loss: 1.4041 - acc: 0.9430 - mDice: 0.6305 - val_loss: 3.1148 - val_acc: 0.9453 - val_mDice: 0.4704

Epoch 00089: val_mDice did not improve from 0.47853
Epoch 90/300
 - 15s - loss: 1.3966 - acc: 0.9433 - mDice: 0.6324 - val_loss: 3.0771 - val_acc: 0.9430 - val_mDice: 0.4755

Epoch 00090: val_mDice did not improve from 0.47853
Epoch 91/300
 - 15s - loss: 1.3924 - acc: 0.9435 - mDice: 0.6333 - val_loss: 3.0732 - val_acc: 0.9447 - val_mDice: 0.4806

Epoch 00091: val_mDice improved from 0.47853 to 0.48059, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 92/300
 - 15s - loss: 1.4019 - acc: 0.9433 - mDice: 0.6313 - val_loss: 3.2202 - val_acc: 0.9413 - val_mDice: 0.4652

Epoch 00092: val_mDice did not improve from 0.48059
Epoch 93/300
 - 14s - loss: 1.3904 - acc: 0.9435 - mDice: 0.6338 - val_loss: 3.2382 - val_acc: 0.9412 - val_mDice: 0.4692

Epoch 00093: val_mDice did not improve from 0.48059
Epoch 94/300
 - 14s - loss: 1.3868 - acc: 0.9437 - mDice: 0.6348 - val_loss: 3.0884 - val_acc: 0.9449 - val_mDice: 0.4779

Epoch 00094: val_mDice did not improve from 0.48059
Epoch 95/300
 - 14s - loss: 1.3760 - acc: 0.9439 - mDice: 0.6374 - val_loss: 3.0304 - val_acc: 0.9403 - val_mDice: 0.4782

Epoch 00095: val_mDice did not improve from 0.48059
Epoch 96/300
 - 15s - loss: 1.3800 - acc: 0.9439 - mDice: 0.6366 - val_loss: 3.4686 - val_acc: 0.9443 - val_mDice: 0.4649

Epoch 00096: val_mDice did not improve from 0.48059
Epoch 97/300
 - 14s - loss: 1.3720 - acc: 0.9439 - mDice: 0.6386 - val_loss: 3.3451 - val_acc: 0.9442 - val_mDice: 0.4599

Epoch 00097: val_mDice did not improve from 0.48059
Epoch 98/300
 - 14s - loss: 1.3731 - acc: 0.9440 - mDice: 0.6381 - val_loss: 2.9180 - val_acc: 0.9430 - val_mDice: 0.4887

Epoch 00098: val_mDice improved from 0.48059 to 0.48873, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 15s - loss: 1.3649 - acc: 0.9442 - mDice: 0.6404 - val_loss: 2.8672 - val_acc: 0.9436 - val_mDice: 0.4862

Epoch 00099: val_mDice did not improve from 0.48873
Epoch 100/300
 - 15s - loss: 1.3696 - acc: 0.9441 - mDice: 0.6392 - val_loss: 3.2601 - val_acc: 0.9465 - val_mDice: 0.4662

Epoch 00100: val_mDice did not improve from 0.48873
Epoch 101/300
 - 15s - loss: 1.3624 - acc: 0.9442 - mDice: 0.6405 - val_loss: 3.0845 - val_acc: 0.9430 - val_mDice: 0.4794

Epoch 00101: val_mDice did not improve from 0.48873
Epoch 102/300
 - 14s - loss: 1.3640 - acc: 0.9443 - mDice: 0.6406 - val_loss: 3.2520 - val_acc: 0.9448 - val_mDice: 0.4733

Epoch 00102: val_mDice did not improve from 0.48873
Epoch 103/300
 - 15s - loss: 1.3518 - acc: 0.9447 - mDice: 0.6432 - val_loss: 3.2701 - val_acc: 0.9459 - val_mDice: 0.4694

Epoch 00103: val_mDice did not improve from 0.48873
Epoch 104/300
 - 15s - loss: 1.3524 - acc: 0.9446 - mDice: 0.6435 - val_loss: 3.1177 - val_acc: 0.9447 - val_mDice: 0.4749

Epoch 00104: val_mDice did not improve from 0.48873
Epoch 105/300
 - 15s - loss: 1.3480 - acc: 0.9448 - mDice: 0.6446 - val_loss: 3.3218 - val_acc: 0.9448 - val_mDice: 0.4701

Epoch 00105: val_mDice did not improve from 0.48873
Epoch 106/300
 - 15s - loss: 1.3437 - acc: 0.9448 - mDice: 0.6449 - val_loss: 3.2418 - val_acc: 0.9334 - val_mDice: 0.4617

Epoch 00106: val_mDice did not improve from 0.48873
Epoch 107/300
 - 15s - loss: 1.3428 - acc: 0.9450 - mDice: 0.6456 - val_loss: 3.3220 - val_acc: 0.9421 - val_mDice: 0.4744

Epoch 00107: val_mDice did not improve from 0.48873
Epoch 108/300
 - 15s - loss: 1.3346 - acc: 0.9453 - mDice: 0.6480 - val_loss: 3.0936 - val_acc: 0.9399 - val_mDice: 0.4797

Epoch 00108: val_mDice did not improve from 0.48873
Epoch 109/300
 - 14s - loss: 1.3398 - acc: 0.9449 - mDice: 0.6467 - val_loss: 3.1667 - val_acc: 0.9383 - val_mDice: 0.4760

Epoch 00109: val_mDice did not improve from 0.48873
Epoch 110/300
 - 14s - loss: 1.3404 - acc: 0.9450 - mDice: 0.6458 - val_loss: 3.0127 - val_acc: 0.9410 - val_mDice: 0.4762

Epoch 00110: val_mDice did not improve from 0.48873
Epoch 111/300
 - 15s - loss: 1.3264 - acc: 0.9455 - mDice: 0.6500 - val_loss: 2.9991 - val_acc: 0.9420 - val_mDice: 0.4818

Epoch 00111: val_mDice did not improve from 0.48873
Epoch 112/300
 - 15s - loss: 1.3283 - acc: 0.9454 - mDice: 0.6488 - val_loss: 3.0788 - val_acc: 0.9404 - val_mDice: 0.4821

Epoch 00112: val_mDice did not improve from 0.48873
Epoch 113/300
 - 14s - loss: 1.3185 - acc: 0.9457 - mDice: 0.6513 - val_loss: 3.2788 - val_acc: 0.9431 - val_mDice: 0.4742

Epoch 00113: val_mDice did not improve from 0.48873
Epoch 114/300
 - 15s - loss: 1.3281 - acc: 0.9455 - mDice: 0.6487 - val_loss: 3.0666 - val_acc: 0.9438 - val_mDice: 0.4837

Epoch 00114: val_mDice did not improve from 0.48873
Epoch 115/300
 - 15s - loss: 1.3160 - acc: 0.9457 - mDice: 0.6523 - val_loss: 3.0801 - val_acc: 0.9393 - val_mDice: 0.4856

Epoch 00115: val_mDice did not improve from 0.48873
Epoch 116/300
 - 14s - loss: 1.3187 - acc: 0.9458 - mDice: 0.6513 - val_loss: 3.2357 - val_acc: 0.9430 - val_mDice: 0.4727

Epoch 00116: val_mDice did not improve from 0.48873
Epoch 117/300
 - 14s - loss: 1.3154 - acc: 0.9457 - mDice: 0.6518 - val_loss: 3.2720 - val_acc: 0.9450 - val_mDice: 0.4719

Epoch 00117: val_mDice did not improve from 0.48873
Epoch 118/300
 - 15s - loss: 1.3051 - acc: 0.9463 - mDice: 0.6549 - val_loss: 3.3462 - val_acc: 0.9429 - val_mDice: 0.4662

Epoch 00118: val_mDice did not improve from 0.48873
Epoch 119/300
 - 14s - loss: 1.3177 - acc: 0.9457 - mDice: 0.6519 - val_loss: 3.5022 - val_acc: 0.9423 - val_mDice: 0.4671

Epoch 00119: val_mDice did not improve from 0.48873
Epoch 120/300
 - 14s - loss: 1.3073 - acc: 0.9459 - mDice: 0.6543 - val_loss: 3.1913 - val_acc: 0.9407 - val_mDice: 0.4709

Epoch 00120: val_mDice did not improve from 0.48873
Epoch 121/300
 - 14s - loss: 1.3034 - acc: 0.9460 - mDice: 0.6552 - val_loss: 3.1075 - val_acc: 0.9440 - val_mDice: 0.4806

Epoch 00121: val_mDice did not improve from 0.48873
Epoch 122/300
 - 15s - loss: 1.2981 - acc: 0.9461 - mDice: 0.6563 - val_loss: 3.1673 - val_acc: 0.9460 - val_mDice: 0.4811

Epoch 00122: val_mDice did not improve from 0.48873
Epoch 123/300
 - 15s - loss: 1.2964 - acc: 0.9463 - mDice: 0.6570 - val_loss: 3.3133 - val_acc: 0.9405 - val_mDice: 0.4662

Epoch 00123: val_mDice did not improve from 0.48873
Epoch 124/300
 - 14s - loss: 1.2960 - acc: 0.9462 - mDice: 0.6575 - val_loss: 3.1485 - val_acc: 0.9436 - val_mDice: 0.4782

Epoch 00124: val_mDice did not improve from 0.48873
Epoch 125/300
 - 15s - loss: 1.2955 - acc: 0.9462 - mDice: 0.6571 - val_loss: 3.3770 - val_acc: 0.9446 - val_mDice: 0.4817

Epoch 00125: val_mDice did not improve from 0.48873
Epoch 126/300
 - 15s - loss: 1.2837 - acc: 0.9465 - mDice: 0.6599 - val_loss: 3.2767 - val_acc: 0.9429 - val_mDice: 0.4779

Epoch 00126: val_mDice did not improve from 0.48873
Epoch 127/300
 - 15s - loss: 1.2892 - acc: 0.9465 - mDice: 0.6586 - val_loss: 3.0784 - val_acc: 0.9410 - val_mDice: 0.4767

Epoch 00127: val_mDice did not improve from 0.48873
Epoch 128/300
 - 14s - loss: 1.2900 - acc: 0.9464 - mDice: 0.6582 - val_loss: 3.1982 - val_acc: 0.9441 - val_mDice: 0.4774

Epoch 00128: val_mDice did not improve from 0.48873
Restoring model weights from the end of the best epoch
Epoch 00128: early stopping
{'val_loss': [36.32667326359522, 15.147006860801152, 9.819716172558921, 8.002071488471259, 7.198858329937572, 6.593302952391761, 6.784561028970139, 4.8515428847500255, 5.2212272340520505, 4.3917543564346575, 3.664393248479991, 3.556009165055695, 3.1783843217861083, 4.227800901801813, 3.5759733472729014, 2.958808679488443, 3.4093961390622316, 3.1527800089014426, 2.978270883583242, 2.9265556127010357, 3.035452714899466, 2.9062372897529887, 3.495934738467137, 2.8186050180700564, 2.8412117831231583, 2.9861963115011654, 2.9528107910550068, 2.9510910176184204, 2.7218670307525565, 2.7811887448077046, 2.6986785097313777, 2.8686580676585436, 2.8847228823052276, 3.2185152013830485, 2.757952719860311, 2.8227565332227167, 2.656073149471056, 2.97061708387697, 2.8370543212319412, 3.090063182991885, 2.8124192093188563, 3.597539385470251, 3.0859242090955377, 3.291846191333163, 2.869234472069712, 3.297274387086786, 2.8980093864901435, 2.811224627973778, 2.9426078954711556, 2.7617446011420164, 2.987539562662797, 2.8399433347706995, 2.831571985634842, 2.8857540723734667, 3.0779951498178497, 3.12765330098392, 2.8571755003095385, 2.9378335698773816, 2.9532086137859594, 2.9059156002920297, 2.9723524065200415, 3.1248440537158224, 3.865226966328919, 2.9101251018277945, 2.8217835733667016, 3.022612027424787, 2.9765667502901385, 3.214973367262809, 2.9761953139677644, 2.876529724558904, 3.3143515376640216, 2.8976592342812744, 3.2814822665726147, 2.9825721811946657, 3.1679899777684892, 3.1592093481283103, 3.4336963454926654, 2.9443341799612557, 3.057320193964101, 3.0916771765221798, 3.075880251514415, 3.0115961462170597, 3.0972993445493993, 2.940171923976214, 3.1543552456867125, 2.9886046428942965, 3.347924278672075, 3.2348175896109925, 3.1147889118153778, 3.0771474692349634, 3.073210700415075, 3.220222389547243, 3.2381899508958063, 3.08843466987656, 3.03036937614282, 3.46855646216621, 3.345074564085475, 2.918014864703374, 2.8672317812396657, 3.260103766185542, 3.0844803718140437, 3.2519893496785137, 3.270055973237114, 3.1177478377219465, 3.3217617854520323, 3.24177664088174, 3.321967463629941, 3.0936182488110804, 3.1666914495595155, 3.012695667333901, 2.9990854690827073, 3.078826967316369, 3.278792887438266, 3.0666266191041185, 3.080096822542449, 3.2356914877448055, 3.271985063462385, 3.3461540894405473, 3.5021762503754523, 3.191336562679637, 3.1075143132004, 3.1673280884999606, 3.313302055178654, 3.148472488325621, 3.3770229913560406, 3.2767373253369616, 3.078350464414273, 3.198243271692523], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047229715756008, 0.9043200441769191, 0.9044711561430068, 0.9132326216924758, 0.9253319672175816, 0.9279899512018476, 0.9299862413179307, 0.928550830909184, 0.9295695963360014, 0.9268131880533128, 0.9352060159047445, 0.935682251339867, 0.9338667392730713, 0.9358836894943601, 0.9357234608559382, 0.9345283848898751, 0.935043508098239, 0.9358218823160444, 0.9333631084078834, 0.9389537828309196, 0.940537983462924, 0.9403594136238098, 0.9364148435138521, 0.9340636616661435, 0.9292445126033965, 0.9276511243411473, 0.9418932965823582, 0.9392605282011486, 0.9402472518739247, 0.9391620641662961, 0.9384913245836893, 0.9386149020422072, 0.9333402060327076, 0.9411904613176981, 0.9428136604172843, 0.939152941817329, 0.9423076879410517, 0.9410096179871332, 0.9380700417927333, 0.9412499893279302, 0.9380860612505958, 0.9417124362218947, 0.9391712319283259, 0.9443223618325733, 0.9433813918204534, 0.9429441264697483, 0.9393200363431659, 0.9397230091549101, 0.9437751656486875, 0.9382806959606352, 0.9420787408238366, 0.9391689641135079, 0.9416804143360683, 0.9433379371960958, 0.9444803169795445, 0.9311171968777975, 0.9396634584381467, 0.9437133896918524, 0.9414056709834507, 0.9400434976532346, 0.9415040952818734, 0.9424152885164533, 0.9398237438428969, 0.9404487098966327, 0.9358013073603312, 0.9417010233515785, 0.9448397471791222, 0.9455975152197338, 0.9405494559378851, 0.9447115517797924, 0.9437568670227414, 0.9431845233553932, 0.9426190342221942, 0.9457074346996489, 0.9445901825314477, 0.9385370981125605, 0.9456616441408793, 0.9433195732888722, 0.9412041902542114, 0.9401350503876096, 0.9449679369018191, 0.9439491970198495, 0.9452999319349017, 0.942978492804936, 0.9446840371404376, 0.9412683191753569, 0.9411675816490537, 0.9449290235837301, 0.9403273718697684, 0.9442628423372904, 0.9441895343008495, 0.9430151241166251, 0.943596635546003, 0.9465498952638536, 0.9430402857916695, 0.9447985206331525, 0.9459180207479567, 0.9447366935866219, 0.9447618978364127, 0.9333951444852919, 0.9420856067112514, 0.9398649164608547, 0.93832190263839, 0.941025645959945, 0.9419688667569842, 0.9403502515384129, 0.943122699147179, 0.943772900672186, 0.9392834362529573, 0.9430174174762908, 0.9449519004140582, 0.9429258221671695, 0.9422504362605867, 0.9406868190992446, 0.9439972497168041, 0.9460050151461646, 0.9405242460114616, 0.9436126322973342, 0.9446199507940383, 0.9428960425513131, 0.9409958720207214, 0.9441163284437997], 'val_mDice': [0.017297234474903063, 0.01309470705954092, 0.009723234982673256, 0.012992691154414345, 0.011942524740117647, 0.01780157244675571, 0.01838107131853966, 0.08092887974565938, 0.07658963912122306, 0.13620045213472276, 0.1903802700163353, 0.2190130017697811, 0.2594874577508086, 0.22184760853027305, 0.265913575754634, 0.33416266845805304, 0.3139816053389084, 0.3299805855467206, 0.3620033922294776, 0.3676774147010985, 0.36045926179559457, 0.382262952980541, 0.355276729734171, 0.40032843216544106, 0.40780010464645566, 0.3936670200810546, 0.41076250055006575, 0.4076556127873205, 0.42593888619116377, 0.42845371507463004, 0.43340703135445, 0.42588500554362935, 0.4338212234988099, 0.4047568449307056, 0.44314964913896154, 0.43575878707425936, 0.4506738476810001, 0.4347786360553333, 0.44361595738501775, 0.43745928452838034, 0.4483761769675073, 0.4047466789682706, 0.4371685726302011, 0.41759971901774406, 0.447216995770023, 0.4307618902197906, 0.4526863557597001, 0.45880496679317384, 0.45500188338614644, 0.46557902367342086, 0.4461841122025535, 0.46190365510327475, 0.46396697489988237, 0.4635028111792746, 0.4554315675936994, 0.450060527239527, 0.46606966764444396, 0.4652282503389177, 0.45602536840098246, 0.47446965976130395, 0.4654872559365772, 0.45196504252297537, 0.42132606809692724, 0.4741926257099424, 0.47677310715828625, 0.4606921419146515, 0.4652465046161697, 0.45520060332048506, 0.46838079889615375, 0.47091816791466307, 0.44980123745543615, 0.47657081413836705, 0.45697142219259623, 0.47369439438695, 0.46358904313473476, 0.4587823510879562, 0.45013234906253363, 0.46936332966600147, 0.46814841706128346, 0.47353317872399375, 0.4743640392663933, 0.46916289006670314, 0.4758970061583178, 0.47852878804717747, 0.46899788986359325, 0.472557905174437, 0.46592376115066664, 0.45603933628825916, 0.470367645578725, 0.47550545632839203, 0.4805928133428097, 0.46521792170547305, 0.469177176377603, 0.47790679725862684, 0.4781731620785736, 0.46494943950147855, 0.45985043403648196, 0.48872948437929153, 0.4861708620474452, 0.46617053502372335, 0.4793707032998403, 0.47334069616737817, 0.4693606602294104, 0.4748731968658311, 0.470133819040798, 0.4617212585040501, 0.47437344597918646, 0.47971926877895993, 0.4760490290465809, 0.4761953304211299, 0.4817628917239961, 0.4821050461559069, 0.47422995861797107, 0.48369939128557843, 0.4856262632778713, 0.47267601301982287, 0.47189667430661975, 0.4661822201950209, 0.46714685129977407, 0.47086704948118757, 0.4805678902637391, 0.48111657053232193, 0.46618333884647917, 0.4782106981036209, 0.48173110524103757, 0.47786584141708555, 0.4767012170382908, 0.4774155474844433], 'loss': [135.4347341635656, 31.920152597811633, 15.02706551850589, 10.344235920506174, 8.20392596186161, 6.981343443278336, 6.105046042822357, 5.4374846612324665, 4.916830599089682, 4.481009411035026, 4.061509135235061, 3.7265398896266806, 3.4548609981147087, 3.234491846523281, 3.0605086253423415, 2.913369941141464, 2.7919074373942063, 2.6804009935220177, 2.580522803205825, 2.523937390239937, 2.4339295794376867, 2.3741318359755588, 2.332878017623185, 2.271011950539007, 2.2310118953317994, 2.1907420977935734, 2.1372245517290924, 2.113502836305557, 2.0857933176168246, 2.0410057788688554, 2.0084845109185387, 1.992166608083632, 1.9630723304777953, 1.9415287940470451, 1.9272589159076192, 1.8898674718258537, 1.8783404645097952, 1.8668610532549919, 1.831331749263011, 1.8229490242092647, 1.8064695457422093, 1.7970383440986266, 1.7710384634709988, 1.7576089056098487, 1.744187008629007, 1.7305992212694505, 1.7116511469749627, 1.7108610571522416, 1.6887653471303394, 1.6873046382902581, 1.663966460726223, 1.660073571911156, 1.6557641855802108, 1.6328029550751304, 1.6298803350519577, 1.6173974067828825, 1.6053359501389886, 1.6000858370867541, 1.5980133779342633, 1.5814905589041004, 1.570618489814995, 1.5599288464465857, 1.5564407937577724, 1.5524118044759259, 1.5392220273329524, 1.5286454589743363, 1.5265144334722858, 1.5179610187800054, 1.5113016123346035, 1.5175700141488644, 1.4962127839114732, 1.4958088764317905, 1.5032830648691593, 1.4834083807774079, 1.4805812760771, 1.4692747519382603, 1.467986930145196, 1.4551247824863223, 1.4527203156259998, 1.44650968789181, 1.4380661025083155, 1.436013850169258, 1.440452083103547, 1.4294567561595455, 1.421570582428442, 1.422477278770085, 1.4225675352786762, 1.410063762413828, 1.404133688720959, 1.396640355218829, 1.3923569820003976, 1.4018868946353433, 1.3904006532191402, 1.3868367868695204, 1.3759866903599043, 1.3799563455793102, 1.3720454182587125, 1.3731159624024718, 1.3648608066866985, 1.369561174057434, 1.362410001058856, 1.364039773469443, 1.351761989024004, 1.3523641086530198, 1.3480035760096096, 1.343720150127194, 1.3428217414626502, 1.3345717569755318, 1.3397989449000942, 1.3404118115556753, 1.3263551024244267, 1.328263143526007, 1.3184507057251718, 1.3280692626130541, 1.316016260003777, 1.3187121483133735, 1.3153992858102195, 1.3050827862831675, 1.317695777517811, 1.3072525989073926, 1.3033594887266275, 1.2980826013874682, 1.2963686311591998, 1.2959674129900685, 1.2955123027303626, 1.2837491517860304, 1.2892048935773448, 1.289966544136046], 'acc': [0.5700621071054524, 0.8662800227536249, 0.8691939034396705, 0.8692352422657605, 0.8692340151937752, 0.8691338865265626, 0.8695003517749155, 0.8705849337835222, 0.8732812286158713, 0.8794753135962824, 0.8890840960463646, 0.8946957114185468, 0.8988926656356041, 0.9020850925930943, 0.9046546897608718, 0.907050983292624, 0.9093139846591056, 0.9115794740225139, 0.9138668840483524, 0.9147803385248069, 0.917042977954235, 0.9183670187332001, 0.9193269771396114, 0.920746624170987, 0.9218016842759733, 0.9224546441274363, 0.9239121055083627, 0.9244927697060356, 0.9251292318260829, 0.9260619873712811, 0.9270509380767495, 0.927356088106311, 0.9282247514905833, 0.9289559906035093, 0.9292330785089085, 0.9303052594994037, 0.9306849546576824, 0.9307493732824522, 0.9319750457364331, 0.9321917994922817, 0.9324820463382051, 0.9328411180021126, 0.9336447635780807, 0.9337503127171994, 0.934088362649109, 0.9346313043460696, 0.934966812031288, 0.9352002443319483, 0.9355285644692126, 0.9355653376162845, 0.9362077096741807, 0.9363272734315312, 0.9364795349617328, 0.9368981369570866, 0.9369102047545523, 0.9374475616100599, 0.9375822359090231, 0.9377529426234201, 0.9379348197446661, 0.9382326904119084, 0.9384824852160183, 0.9387808690899389, 0.9388438967098407, 0.9390177544840549, 0.9393993044533848, 0.9396456838780247, 0.9397208586993051, 0.9398055308605633, 0.9401664995885157, 0.9398639412521052, 0.9405229551060847, 0.9403898523495977, 0.9404508630296778, 0.9407429213534896, 0.9410239247451151, 0.9412800903055556, 0.9413579693184392, 0.9415947849152518, 0.941645229611986, 0.9419158778945352, 0.9420411722816804, 0.9421516269278798, 0.9418911354169924, 0.9422705237383279, 0.9424522608742678, 0.94256552762555, 0.9424350434170538, 0.942642871954778, 0.9429963372542906, 0.9433462297454468, 0.9434962456272773, 0.9433002833665531, 0.9435222912827553, 0.9436921398028809, 0.9439350474264573, 0.9438525618949745, 0.943895960132212, 0.9439545391335386, 0.944238647057276, 0.9441298546532756, 0.9442368150906502, 0.9442966226348304, 0.9447234007281239, 0.9446299952855799, 0.9448007950553872, 0.944825265640603, 0.9450037122255395, 0.9452702572792969, 0.944905672019126, 0.9449720577350673, 0.9455054526455903, 0.9453683880015745, 0.9457021382058635, 0.9454955824864308, 0.9457327689259457, 0.9457784616374915, 0.9456516652868413, 0.9462535294099237, 0.9457494275106133, 0.9459164066652078, 0.9460041551937458, 0.9461342455427473, 0.9462970716543733, 0.9462353871212039, 0.9462143281233755, 0.9465499221072023, 0.946510323272497, 0.9464010202128555], 'mDice': [0.017225521761534048, 0.01572455518006566, 0.01640925023877853, 0.020149507701141264, 0.030815775612312083, 0.04288558792075096, 0.05957695110376938, 0.08200310854805395, 0.10713504630291602, 0.13553931307983416, 0.17178268414346243, 0.20502375109084933, 0.2366101003828503, 0.26626879362350303, 0.2916344091201394, 0.3151076835992595, 0.3347150704431837, 0.35470417936528054, 0.3718582302624949, 0.38425482825343954, 0.39923373072138635, 0.41136416076395765, 0.42027829020083557, 0.4323600996820734, 0.44082855495134254, 0.44932555556527126, 0.46028643166235195, 0.4656365841414534, 0.47195479585781064, 0.4806895999348138, 0.4877747707234106, 0.4922191953300533, 0.4985484452015039, 0.5044256492297222, 0.5068904196731107, 0.5155289014203484, 0.5185637728704156, 0.5212245209267173, 0.5282449932233525, 0.531477401429848, 0.5336249807286819, 0.5373840885445496, 0.5424261319285669, 0.5455611371594126, 0.5492713431850527, 0.5528120902638595, 0.5564058678031129, 0.5574077007994386, 0.5623379699614367, 0.5633541535889917, 0.5678927332812107, 0.5685720889237843, 0.5703539878664298, 0.574799034736601, 0.5766639403463076, 0.5792824493705686, 0.5822065164984502, 0.5831312061689115, 0.5845224364589399, 0.5875615407121142, 0.5902277476093042, 0.5925041616411137, 0.5945784894285208, 0.594851702779204, 0.5980849698119847, 0.6004401581673947, 0.6014675716915319, 0.6029279414964407, 0.6041448397376253, 0.6038754417047304, 0.607774828805017, 0.6083548185865337, 0.6070590421646483, 0.6116703649189561, 0.6115435201560506, 0.6147303708330017, 0.6153303573695548, 0.61862489059984, 0.619371445624383, 0.6205034841570938, 0.6224393401620291, 0.6231924064108373, 0.6223705832223246, 0.6247260124898955, 0.6264941556067142, 0.6263988440221092, 0.6264719745201345, 0.6288555385130137, 0.630493822988573, 0.6323998814054783, 0.6333429149655632, 0.631313733737869, 0.633767632923766, 0.6347581852096019, 0.637371303087396, 0.6365944622614169, 0.6386167531370175, 0.63812826836709, 0.6403602028895096, 0.6392412966952885, 0.6405079784433759, 0.6405949476644531, 0.6432285756303645, 0.6435232741859852, 0.6446383635111115, 0.6449362295818899, 0.645571454195979, 0.6480405212184104, 0.6466534235860333, 0.6457685504962604, 0.649959514210949, 0.6487659584793347, 0.6512858548880497, 0.6486831747407694, 0.6523284223788915, 0.6512908219532721, 0.6518252392092811, 0.6549234997106926, 0.6519371519687205, 0.6543086823043691, 0.6551736693219549, 0.6563140190656599, 0.6570346888742948, 0.6575211583821098, 0.6571144993097906, 0.6598573830607127, 0.6586375011973972, 0.6581867972573081]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.63s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.31s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.06s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:11,  1.52s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:31,  1.60s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:36,  1.62s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<08:05,  1.73s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:44,  1.66s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:13,  1.77s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:36,  1.86s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:50,  1.92s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:39,  1.88s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<08:58,  1.96s/it]predicting train subjects:   4%|▍         | 11/285 [00:20<09:01,  1.98s/it]predicting train subjects:   4%|▍         | 12/285 [00:22<09:05,  2.00s/it]predicting train subjects:   5%|▍         | 13/285 [00:24<09:09,  2.02s/it]predicting train subjects:   5%|▍         | 14/285 [00:26<09:13,  2.04s/it]predicting train subjects:   5%|▌         | 15/285 [00:28<09:18,  2.07s/it]predicting train subjects:   6%|▌         | 16/285 [00:30<09:15,  2.07s/it]predicting train subjects:   6%|▌         | 17/285 [00:32<09:08,  2.05s/it]predicting train subjects:   6%|▋         | 18/285 [00:35<09:14,  2.08s/it]predicting train subjects:   7%|▋         | 19/285 [00:37<09:24,  2.12s/it]predicting train subjects:   7%|▋         | 20/285 [00:39<09:26,  2.14s/it]predicting train subjects:   7%|▋         | 21/285 [00:41<09:22,  2.13s/it]predicting train subjects:   8%|▊         | 22/285 [00:43<09:14,  2.11s/it]predicting train subjects:   8%|▊         | 23/285 [00:45<09:24,  2.16s/it]predicting train subjects:   8%|▊         | 24/285 [00:47<09:12,  2.12s/it]predicting train subjects:   9%|▉         | 25/285 [00:50<09:06,  2.10s/it]predicting train subjects:   9%|▉         | 26/285 [00:52<09:10,  2.13s/it]predicting train subjects:   9%|▉         | 27/285 [00:54<09:05,  2.12s/it]predicting train subjects:  10%|▉         | 28/285 [00:56<08:56,  2.09s/it]predicting train subjects:  10%|█         | 29/285 [00:58<08:52,  2.08s/it]predicting train subjects:  11%|█         | 30/285 [01:00<08:46,  2.06s/it]predicting train subjects:  11%|█         | 31/285 [01:02<08:39,  2.04s/it]predicting train subjects:  11%|█         | 32/285 [01:04<08:31,  2.02s/it]predicting train subjects:  12%|█▏        | 33/285 [01:06<08:33,  2.04s/it]predicting train subjects:  12%|█▏        | 34/285 [01:08<08:34,  2.05s/it]predicting train subjects:  12%|█▏        | 35/285 [01:10<08:35,  2.06s/it]predicting train subjects:  13%|█▎        | 36/285 [01:12<08:31,  2.06s/it]predicting train subjects:  13%|█▎        | 37/285 [01:14<08:31,  2.06s/it]predicting train subjects:  13%|█▎        | 38/285 [01:16<08:28,  2.06s/it]predicting train subjects:  14%|█▎        | 39/285 [01:18<08:28,  2.07s/it]predicting train subjects:  14%|█▍        | 40/285 [01:20<08:28,  2.08s/it]predicting train subjects:  14%|█▍        | 41/285 [01:23<08:28,  2.08s/it]predicting train subjects:  15%|█▍        | 42/285 [01:25<08:27,  2.09s/it]predicting train subjects:  15%|█▌        | 43/285 [01:27<08:26,  2.09s/it]predicting train subjects:  15%|█▌        | 44/285 [01:29<08:15,  2.06s/it]predicting train subjects:  16%|█▌        | 45/285 [01:31<08:09,  2.04s/it]predicting train subjects:  16%|█▌        | 46/285 [01:33<07:53,  1.98s/it]predicting train subjects:  16%|█▋        | 47/285 [01:34<07:38,  1.93s/it]predicting train subjects:  17%|█▋        | 48/285 [01:36<07:22,  1.87s/it]predicting train subjects:  17%|█▋        | 49/285 [01:38<07:13,  1.84s/it]predicting train subjects:  18%|█▊        | 50/285 [01:40<07:06,  1.82s/it]predicting train subjects:  18%|█▊        | 51/285 [01:41<07:04,  1.82s/it]predicting train subjects:  18%|█▊        | 52/285 [01:43<07:12,  1.86s/it]predicting train subjects:  19%|█▊        | 53/285 [01:45<07:04,  1.83s/it]predicting train subjects:  19%|█▉        | 54/285 [01:47<06:56,  1.81s/it]predicting train subjects:  19%|█▉        | 55/285 [01:49<06:56,  1.81s/it]predicting train subjects:  20%|█▉        | 56/285 [01:51<06:48,  1.79s/it]predicting train subjects:  20%|██        | 57/285 [01:52<06:49,  1.80s/it]predicting train subjects:  20%|██        | 58/285 [01:54<06:40,  1.77s/it]predicting train subjects:  21%|██        | 59/285 [01:56<06:33,  1.74s/it]predicting train subjects:  21%|██        | 60/285 [01:58<06:38,  1.77s/it]predicting train subjects:  21%|██▏       | 61/285 [01:59<06:34,  1.76s/it]predicting train subjects:  22%|██▏       | 62/285 [02:01<06:33,  1.77s/it]predicting train subjects:  22%|██▏       | 63/285 [02:03<06:32,  1.77s/it]predicting train subjects:  22%|██▏       | 64/285 [02:05<06:33,  1.78s/it]predicting train subjects:  23%|██▎       | 65/285 [02:07<06:47,  1.85s/it]predicting train subjects:  23%|██▎       | 66/285 [02:09<06:55,  1.90s/it]predicting train subjects:  24%|██▎       | 67/285 [02:10<06:46,  1.86s/it]predicting train subjects:  24%|██▍       | 68/285 [02:12<06:48,  1.88s/it]predicting train subjects:  24%|██▍       | 69/285 [02:14<06:42,  1.86s/it]predicting train subjects:  25%|██▍       | 70/285 [02:16<06:39,  1.86s/it]predicting train subjects:  25%|██▍       | 71/285 [02:18<06:39,  1.87s/it]predicting train subjects:  25%|██▌       | 72/285 [02:20<06:39,  1.87s/it]predicting train subjects:  26%|██▌       | 73/285 [02:22<06:29,  1.84s/it]predicting train subjects:  26%|██▌       | 74/285 [02:23<06:26,  1.83s/it]predicting train subjects:  26%|██▋       | 75/285 [02:25<06:24,  1.83s/it]predicting train subjects:  27%|██▋       | 76/285 [02:27<06:20,  1.82s/it]predicting train subjects:  27%|██▋       | 77/285 [02:29<06:15,  1.81s/it]predicting train subjects:  27%|██▋       | 78/285 [02:31<06:16,  1.82s/it]predicting train subjects:  28%|██▊       | 79/285 [02:32<06:16,  1.83s/it]predicting train subjects:  28%|██▊       | 80/285 [02:34<06:14,  1.83s/it]predicting train subjects:  28%|██▊       | 81/285 [02:36<06:19,  1.86s/it]predicting train subjects:  29%|██▉       | 82/285 [02:38<06:10,  1.83s/it]predicting train subjects:  29%|██▉       | 83/285 [02:40<06:05,  1.81s/it]predicting train subjects:  29%|██▉       | 84/285 [02:42<06:03,  1.81s/it]predicting train subjects:  30%|██▉       | 85/285 [02:44<06:13,  1.87s/it]predicting train subjects:  30%|███       | 86/285 [02:46<06:24,  1.93s/it]predicting train subjects:  31%|███       | 87/285 [02:48<06:25,  1.95s/it]predicting train subjects:  31%|███       | 88/285 [02:50<06:30,  1.98s/it]predicting train subjects:  31%|███       | 89/285 [02:52<06:33,  2.01s/it]predicting train subjects:  32%|███▏      | 90/285 [02:54<06:38,  2.04s/it]predicting train subjects:  32%|███▏      | 91/285 [02:56<06:38,  2.05s/it]predicting train subjects:  32%|███▏      | 92/285 [02:58<06:36,  2.05s/it]predicting train subjects:  33%|███▎      | 93/285 [03:00<06:33,  2.05s/it]predicting train subjects:  33%|███▎      | 94/285 [03:02<06:25,  2.02s/it]predicting train subjects:  33%|███▎      | 95/285 [03:04<06:23,  2.02s/it]predicting train subjects:  34%|███▎      | 96/285 [03:06<06:19,  2.01s/it]predicting train subjects:  34%|███▍      | 97/285 [03:08<06:16,  2.00s/it]predicting train subjects:  34%|███▍      | 98/285 [03:10<06:12,  1.99s/it]predicting train subjects:  35%|███▍      | 99/285 [03:12<06:06,  1.97s/it]predicting train subjects:  35%|███▌      | 100/285 [03:14<06:05,  1.98s/it]predicting train subjects:  35%|███▌      | 101/285 [03:16<06:02,  1.97s/it]predicting train subjects:  36%|███▌      | 102/285 [03:18<06:07,  2.01s/it]predicting train subjects:  36%|███▌      | 103/285 [03:20<06:03,  2.00s/it]predicting train subjects:  36%|███▋      | 104/285 [03:22<05:59,  1.99s/it]predicting train subjects:  37%|███▋      | 105/285 [03:24<05:50,  1.95s/it]predicting train subjects:  37%|███▋      | 106/285 [03:26<05:43,  1.92s/it]predicting train subjects:  38%|███▊      | 107/285 [03:27<05:40,  1.91s/it]predicting train subjects:  38%|███▊      | 108/285 [03:29<05:40,  1.92s/it]predicting train subjects:  38%|███▊      | 109/285 [03:31<05:36,  1.91s/it]predicting train subjects:  39%|███▊      | 110/285 [03:33<05:29,  1.88s/it]predicting train subjects:  39%|███▉      | 111/285 [03:35<05:25,  1.87s/it]predicting train subjects:  39%|███▉      | 112/285 [03:37<05:25,  1.88s/it]predicting train subjects:  40%|███▉      | 113/285 [03:39<05:23,  1.88s/it]predicting train subjects:  40%|████      | 114/285 [03:41<05:20,  1.88s/it]predicting train subjects:  40%|████      | 115/285 [03:42<05:16,  1.86s/it]predicting train subjects:  41%|████      | 116/285 [03:44<05:15,  1.87s/it]predicting train subjects:  41%|████      | 117/285 [03:46<05:16,  1.88s/it]predicting train subjects:  41%|████▏     | 118/285 [03:48<05:16,  1.90s/it]predicting train subjects:  42%|████▏     | 119/285 [03:50<05:17,  1.91s/it]predicting train subjects:  42%|████▏     | 120/285 [03:52<05:17,  1.93s/it]predicting train subjects:  42%|████▏     | 121/285 [03:54<05:04,  1.86s/it]predicting train subjects:  43%|████▎     | 122/285 [03:55<04:51,  1.79s/it]predicting train subjects:  43%|████▎     | 123/285 [03:57<04:37,  1.71s/it]predicting train subjects:  44%|████▎     | 124/285 [03:59<04:36,  1.72s/it]predicting train subjects:  44%|████▍     | 125/285 [04:00<04:35,  1.72s/it]predicting train subjects:  44%|████▍     | 126/285 [04:02<04:37,  1.74s/it]predicting train subjects:  45%|████▍     | 127/285 [04:04<04:39,  1.77s/it]predicting train subjects:  45%|████▍     | 128/285 [04:06<04:33,  1.74s/it]predicting train subjects:  45%|████▌     | 129/285 [04:07<04:29,  1.73s/it]predicting train subjects:  46%|████▌     | 130/285 [04:09<04:28,  1.73s/it]predicting train subjects:  46%|████▌     | 131/285 [04:11<04:23,  1.71s/it]predicting train subjects:  46%|████▋     | 132/285 [04:13<04:21,  1.71s/it]predicting train subjects:  47%|████▋     | 133/285 [04:14<04:18,  1.70s/it]predicting train subjects:  47%|████▋     | 134/285 [04:16<04:16,  1.70s/it]predicting train subjects:  47%|████▋     | 135/285 [04:18<04:15,  1.70s/it]predicting train subjects:  48%|████▊     | 136/285 [04:19<04:14,  1.70s/it]predicting train subjects:  48%|████▊     | 137/285 [04:21<04:12,  1.71s/it]predicting train subjects:  48%|████▊     | 138/285 [04:23<04:12,  1.72s/it]predicting train subjects:  49%|████▉     | 139/285 [04:25<04:12,  1.73s/it]predicting train subjects:  49%|████▉     | 140/285 [04:26<04:09,  1.72s/it]predicting train subjects:  49%|████▉     | 141/285 [04:28<04:05,  1.71s/it]predicting train subjects:  50%|████▉     | 142/285 [04:29<03:57,  1.66s/it]predicting train subjects:  50%|█████     | 143/285 [04:31<03:52,  1.64s/it]predicting train subjects:  51%|█████     | 144/285 [04:33<03:46,  1.61s/it]predicting train subjects:  51%|█████     | 145/285 [04:34<03:39,  1.57s/it]predicting train subjects:  51%|█████     | 146/285 [04:36<03:35,  1.55s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:37<03:35,  1.56s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:39<03:33,  1.56s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:40<03:30,  1.55s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:42<03:29,  1.55s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:43<03:27,  1.55s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:45<03:24,  1.53s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:46<03:21,  1.52s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:48<03:21,  1.54s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:49<03:21,  1.55s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:51<03:19,  1.54s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:53<03:15,  1.53s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:54<03:10,  1.50s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:56<03:12,  1.53s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:57<03:08,  1.51s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:59<03:07,  1.52s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:00<03:04,  1.50s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:01<03:01,  1.49s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:03<02:59,  1.48s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:04<02:57,  1.48s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:06<02:56,  1.48s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:07<02:54,  1.48s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:09<02:54,  1.49s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:10<02:52,  1.49s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:12<02:50,  1.48s/it]predicting train subjects:  60%|██████    | 171/285 [05:13<02:52,  1.51s/it]predicting train subjects:  60%|██████    | 172/285 [05:15<02:52,  1.53s/it]predicting train subjects:  61%|██████    | 173/285 [05:17<02:52,  1.54s/it]predicting train subjects:  61%|██████    | 174/285 [05:18<02:51,  1.54s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:20<02:49,  1.54s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:21<02:50,  1.57s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:23<02:48,  1.56s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:24<02:47,  1.56s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:26<02:43,  1.54s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:27<02:38,  1.51s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:29<02:35,  1.50s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:30<02:33,  1.49s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:32<02:32,  1.49s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:33<02:32,  1.51s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:35<02:30,  1.50s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:36<02:27,  1.49s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:38<02:25,  1.49s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:39<02:23,  1.48s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:41<02:21,  1.48s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:42<02:21,  1.49s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:44<02:17,  1.47s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:45<02:18,  1.49s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:47<02:16,  1.48s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:48<02:14,  1.48s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:50<02:12,  1.48s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:51<02:19,  1.57s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:53<02:24,  1.64s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:55<02:26,  1.68s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:57<02:25,  1.70s/it]predicting train subjects:  70%|███████   | 200/285 [05:58<02:26,  1.72s/it]predicting train subjects:  71%|███████   | 201/285 [06:00<02:26,  1.74s/it]predicting train subjects:  71%|███████   | 202/285 [06:02<02:26,  1.77s/it]predicting train subjects:  71%|███████   | 203/285 [06:04<02:25,  1.77s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:06<02:24,  1.78s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:07<02:21,  1.77s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:09<02:19,  1.77s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:11<02:18,  1.77s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:13<02:15,  1.77s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:14<02:13,  1.76s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:16<02:13,  1.77s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:18<02:11,  1.77s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:20<02:08,  1.76s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:21<02:06,  1.75s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:23<02:03,  1.73s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:25<01:59,  1.70s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:26<01:54,  1.67s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:28<01:51,  1.64s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:30<01:48,  1.62s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:31<01:45,  1.60s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:33<01:44,  1.60s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:34<01:41,  1.58s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:36<01:39,  1.58s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:37<01:38,  1.58s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:39<01:36,  1.58s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:40<01:34,  1.57s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:42<01:34,  1.61s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:44<01:32,  1.60s/it]predicting train subjects:  80%|████████  | 228/285 [06:45<01:32,  1.62s/it]predicting train subjects:  80%|████████  | 229/285 [06:47<01:29,  1.60s/it]predicting train subjects:  81%|████████  | 230/285 [06:49<01:29,  1.64s/it]predicting train subjects:  81%|████████  | 231/285 [06:50<01:28,  1.64s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:52<01:33,  1.76s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:54<01:34,  1.82s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:56<01:35,  1.87s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:58<01:35,  1.90s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:00<01:35,  1.94s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:02<01:34,  1.97s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:04<01:32,  1.98s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:06<01:31,  1.98s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:08<01:29,  1.98s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:10<01:27,  1.99s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:12<01:25,  1.99s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:14<01:22,  1.97s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:16<01:20,  1.96s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:18<01:19,  1.99s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:20<01:17,  1.98s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:22<01:15,  1.99s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:24<01:13,  1.99s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:26<01:11,  2.00s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:28<01:04,  1.84s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:29<00:58,  1.73s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:31<00:54,  1.65s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:32<00:51,  1.62s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:34<00:48,  1.57s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:35<00:46,  1.55s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:37<00:44,  1.54s/it]predicting train subjects:  90%|█████████ | 257/285 [07:38<00:43,  1.56s/it]predicting train subjects:  91%|█████████ | 258/285 [07:40<00:43,  1.60s/it]predicting train subjects:  91%|█████████ | 259/285 [07:42<00:41,  1.58s/it]predicting train subjects:  91%|█████████ | 260/285 [07:43<00:39,  1.57s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:45<00:36,  1.53s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:46<00:34,  1.52s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:47<00:33,  1.51s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:49<00:31,  1.50s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:50<00:29,  1.49s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:52<00:28,  1.49s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:53<00:26,  1.48s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:55<00:28,  1.66s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:57<00:27,  1.75s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:59<00:27,  1.83s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:01<00:26,  1.88s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:03<00:24,  1.90s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:05<00:22,  1.92s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:07<00:21,  1.92s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:09<00:19,  1.96s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:11<00:18,  2.01s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:13<00:16,  2.02s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:15<00:14,  2.00s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:17<00:11,  1.99s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:19<00:09,  1.99s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:21<00:07,  1.97s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:23<00:05,  1.98s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:25<00:03,  1.97s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:27<00:01,  1.99s/it]predicting train subjects: 100%|██████████| 285/285 [08:29<00:00,  1.98s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<08:23,  1.77s/it]Loading train:   1%|          | 2/285 [00:03<08:28,  1.80s/it]Loading train:   1%|          | 3/285 [00:05<08:16,  1.76s/it]Loading train:   1%|▏         | 4/285 [00:07<08:18,  1.77s/it]Loading train:   2%|▏         | 5/285 [00:09<08:56,  1.91s/it]Loading train:   2%|▏         | 6/285 [00:11<09:17,  2.00s/it]Loading train:   2%|▏         | 7/285 [00:13<09:47,  2.11s/it]Loading train:   3%|▎         | 8/285 [00:16<09:58,  2.16s/it]Loading train:   3%|▎         | 9/285 [00:17<09:23,  2.04s/it]Loading train:   4%|▎         | 10/285 [00:19<08:39,  1.89s/it]Loading train:   4%|▍         | 11/285 [00:21<08:14,  1.80s/it]Loading train:   4%|▍         | 12/285 [00:23<08:21,  1.84s/it]Loading train:   5%|▍         | 13/285 [00:25<08:49,  1.95s/it]Loading train:   5%|▍         | 14/285 [00:27<08:57,  1.98s/it]Loading train:   5%|▌         | 15/285 [00:29<08:45,  1.95s/it]Loading train:   6%|▌         | 16/285 [00:31<08:37,  1.92s/it]Loading train:   6%|▌         | 17/285 [00:32<08:18,  1.86s/it]Loading train:   6%|▋         | 18/285 [00:34<08:14,  1.85s/it]Loading train:   7%|▋         | 19/285 [00:36<08:25,  1.90s/it]Loading train:   7%|▋         | 20/285 [00:38<08:18,  1.88s/it]Loading train:   7%|▋         | 21/285 [00:40<07:59,  1.82s/it]Loading train:   8%|▊         | 22/285 [00:41<07:29,  1.71s/it]Loading train:   8%|▊         | 23/285 [00:43<07:36,  1.74s/it]Loading train:   8%|▊         | 24/285 [00:45<07:41,  1.77s/it]Loading train:   9%|▉         | 25/285 [00:47<08:05,  1.87s/it]Loading train:   9%|▉         | 26/285 [00:49<07:57,  1.84s/it]Loading train:   9%|▉         | 27/285 [00:50<07:59,  1.86s/it]Loading train:  10%|▉         | 28/285 [00:52<07:25,  1.73s/it]Loading train:  10%|█         | 29/285 [00:53<07:01,  1.64s/it]Loading train:  11%|█         | 30/285 [00:55<07:05,  1.67s/it]Loading train:  11%|█         | 31/285 [00:57<07:01,  1.66s/it]Loading train:  11%|█         | 32/285 [00:58<07:01,  1.66s/it]Loading train:  12%|█▏        | 33/285 [01:00<06:57,  1.66s/it]Loading train:  12%|█▏        | 34/285 [01:02<06:53,  1.65s/it]Loading train:  12%|█▏        | 35/285 [01:03<06:29,  1.56s/it]Loading train:  13%|█▎        | 36/285 [01:05<06:52,  1.66s/it]Loading train:  13%|█▎        | 37/285 [01:06<06:39,  1.61s/it]Loading train:  13%|█▎        | 38/285 [01:08<06:17,  1.53s/it]Loading train:  14%|█▎        | 39/285 [01:09<06:18,  1.54s/it]Loading train:  14%|█▍        | 40/285 [01:11<06:15,  1.53s/it]Loading train:  14%|█▍        | 41/285 [01:12<05:57,  1.47s/it]Loading train:  15%|█▍        | 42/285 [01:14<06:06,  1.51s/it]Loading train:  15%|█▌        | 43/285 [01:15<06:16,  1.55s/it]Loading train:  15%|█▌        | 44/285 [01:17<05:52,  1.46s/it]Loading train:  16%|█▌        | 45/285 [01:18<06:13,  1.56s/it]Loading train:  16%|█▌        | 46/285 [01:20<06:26,  1.62s/it]Loading train:  16%|█▋        | 47/285 [01:22<06:05,  1.54s/it]Loading train:  17%|█▋        | 48/285 [01:23<06:01,  1.53s/it]Loading train:  17%|█▋        | 49/285 [01:25<06:38,  1.69s/it]Loading train:  18%|█▊        | 50/285 [01:26<06:09,  1.57s/it]Loading train:  18%|█▊        | 51/285 [01:28<06:13,  1.60s/it]Loading train:  18%|█▊        | 52/285 [01:30<06:22,  1.64s/it]Loading train:  19%|█▊        | 53/285 [01:32<06:29,  1.68s/it]Loading train:  19%|█▉        | 54/285 [01:33<06:00,  1.56s/it]Loading train:  19%|█▉        | 55/285 [01:34<05:59,  1.56s/it]Loading train:  20%|█▉        | 56/285 [01:36<06:14,  1.64s/it]Loading train:  20%|██        | 57/285 [01:38<06:31,  1.72s/it]Loading train:  20%|██        | 58/285 [01:39<05:49,  1.54s/it]Loading train:  21%|██        | 59/285 [01:41<05:44,  1.52s/it]Loading train:  21%|██        | 60/285 [01:42<05:39,  1.51s/it]Loading train:  21%|██▏       | 61/285 [01:44<05:42,  1.53s/it]Loading train:  22%|██▏       | 62/285 [01:45<05:53,  1.59s/it]Loading train:  22%|██▏       | 63/285 [01:47<05:50,  1.58s/it]Loading train:  22%|██▏       | 64/285 [01:49<06:08,  1.67s/it]Loading train:  23%|██▎       | 65/285 [01:51<06:32,  1.78s/it]Loading train:  23%|██▎       | 66/285 [01:53<06:58,  1.91s/it]Loading train:  24%|██▎       | 67/285 [01:55<06:20,  1.74s/it]Loading train:  24%|██▍       | 68/285 [01:56<06:27,  1.79s/it]Loading train:  24%|██▍       | 69/285 [01:58<06:33,  1.82s/it]Loading train:  25%|██▍       | 70/285 [02:00<06:52,  1.92s/it]Loading train:  25%|██▍       | 71/285 [02:02<06:56,  1.94s/it]Loading train:  25%|██▌       | 72/285 [02:04<06:12,  1.75s/it]Loading train:  26%|██▌       | 73/285 [02:05<05:55,  1.68s/it]Loading train:  26%|██▌       | 74/285 [02:07<06:04,  1.73s/it]Loading train:  26%|██▋       | 75/285 [02:09<06:00,  1.72s/it]Loading train:  27%|██▋       | 76/285 [02:11<05:56,  1.71s/it]Loading train:  27%|██▋       | 77/285 [02:12<05:56,  1.71s/it]Loading train:  27%|██▋       | 78/285 [02:14<05:54,  1.71s/it]Loading train:  28%|██▊       | 79/285 [02:16<05:56,  1.73s/it]Loading train:  28%|██▊       | 80/285 [02:17<05:36,  1.64s/it]Loading train:  28%|██▊       | 81/285 [02:19<05:29,  1.62s/it]Loading train:  29%|██▉       | 82/285 [02:20<05:23,  1.60s/it]Loading train:  29%|██▉       | 83/285 [02:22<05:22,  1.60s/it]Loading train:  29%|██▉       | 84/285 [02:23<05:17,  1.58s/it]Loading train:  30%|██▉       | 85/285 [02:25<05:24,  1.62s/it]Loading train:  30%|███       | 86/285 [02:26<05:04,  1.53s/it]Loading train:  31%|███       | 87/285 [02:28<05:10,  1.57s/it]Loading train:  31%|███       | 88/285 [02:30<05:30,  1.68s/it]Loading train:  31%|███       | 89/285 [02:32<05:38,  1.73s/it]Loading train:  32%|███▏      | 90/285 [02:34<05:33,  1.71s/it]Loading train:  32%|███▏      | 91/285 [02:35<05:24,  1.67s/it]Loading train:  32%|███▏      | 92/285 [02:37<05:34,  1.74s/it]Loading train:  33%|███▎      | 93/285 [02:39<05:25,  1.69s/it]Loading train:  33%|███▎      | 94/285 [02:40<05:29,  1.73s/it]Loading train:  33%|███▎      | 95/285 [02:42<05:15,  1.66s/it]Loading train:  34%|███▎      | 96/285 [02:44<05:32,  1.76s/it]Loading train:  34%|███▍      | 97/285 [02:46<05:34,  1.78s/it]Loading train:  34%|███▍      | 98/285 [02:47<05:30,  1.77s/it]Loading train:  35%|███▍      | 99/285 [02:49<04:54,  1.59s/it]Loading train:  35%|███▌      | 100/285 [02:50<04:36,  1.49s/it]Loading train:  35%|███▌      | 101/285 [02:52<04:59,  1.63s/it]Loading train:  36%|███▌      | 102/285 [02:54<05:08,  1.69s/it]Loading train:  36%|███▌      | 103/285 [02:56<05:33,  1.83s/it]Loading train:  36%|███▋      | 104/285 [02:57<05:11,  1.72s/it]Loading train:  37%|███▋      | 105/285 [02:58<04:39,  1.55s/it]Loading train:  37%|███▋      | 106/285 [03:00<04:15,  1.43s/it]Loading train:  38%|███▊      | 107/285 [03:01<03:58,  1.34s/it]Loading train:  38%|███▊      | 108/285 [03:02<03:46,  1.28s/it]Loading train:  38%|███▊      | 109/285 [03:03<03:38,  1.24s/it]Loading train:  39%|███▊      | 110/285 [03:04<03:36,  1.24s/it]Loading train:  39%|███▉      | 111/285 [03:05<03:28,  1.20s/it]Loading train:  39%|███▉      | 112/285 [03:06<03:23,  1.18s/it]Loading train:  40%|███▉      | 113/285 [03:08<03:16,  1.14s/it]Loading train:  40%|████      | 114/285 [03:09<03:15,  1.14s/it]Loading train:  40%|████      | 115/285 [03:10<03:14,  1.15s/it]Loading train:  41%|████      | 116/285 [03:11<03:14,  1.15s/it]Loading train:  41%|████      | 117/285 [03:12<03:11,  1.14s/it]Loading train:  41%|████▏     | 118/285 [03:13<03:10,  1.14s/it]Loading train:  42%|████▏     | 119/285 [03:14<03:08,  1.13s/it]Loading train:  42%|████▏     | 120/285 [03:15<03:05,  1.12s/it]Loading train:  42%|████▏     | 121/285 [03:17<03:16,  1.20s/it]Loading train:  43%|████▎     | 122/285 [03:18<03:21,  1.23s/it]Loading train:  43%|████▎     | 123/285 [03:20<03:24,  1.27s/it]Loading train:  44%|████▎     | 124/285 [03:21<03:09,  1.18s/it]Loading train:  44%|████▍     | 125/285 [03:21<02:58,  1.12s/it]Loading train:  44%|████▍     | 126/285 [03:22<02:50,  1.07s/it]Loading train:  45%|████▍     | 127/285 [03:23<02:45,  1.05s/it]Loading train:  45%|████▍     | 128/285 [03:24<02:41,  1.03s/it]Loading train:  45%|████▌     | 129/285 [03:25<02:37,  1.01s/it]Loading train:  46%|████▌     | 130/285 [03:26<02:35,  1.00s/it]Loading train:  46%|████▌     | 131/285 [03:27<02:28,  1.04it/s]Loading train:  46%|████▋     | 132/285 [03:28<02:25,  1.05it/s]Loading train:  47%|████▋     | 133/285 [03:29<02:25,  1.04it/s]Loading train:  47%|████▋     | 134/285 [03:30<02:26,  1.03it/s]Loading train:  47%|████▋     | 135/285 [03:31<02:25,  1.03it/s]Loading train:  48%|████▊     | 136/285 [03:32<02:22,  1.04it/s]Loading train:  48%|████▊     | 137/285 [03:33<02:23,  1.03it/s]Loading train:  48%|████▊     | 138/285 [03:34<02:19,  1.05it/s]Loading train:  49%|████▉     | 139/285 [03:35<02:16,  1.07it/s]Loading train:  49%|████▉     | 140/285 [03:36<02:18,  1.05it/s]Loading train:  49%|████▉     | 141/285 [03:37<02:18,  1.04it/s]Loading train:  50%|████▉     | 142/285 [03:38<02:21,  1.01it/s]Loading train:  50%|█████     | 143/285 [03:39<02:19,  1.02it/s]Loading train:  51%|█████     | 144/285 [03:40<02:16,  1.03it/s]Loading train:  51%|█████     | 145/285 [03:41<02:16,  1.03it/s]Loading train:  51%|█████     | 146/285 [03:42<02:15,  1.03it/s]Loading train:  52%|█████▏    | 147/285 [03:43<02:14,  1.03it/s]Loading train:  52%|█████▏    | 148/285 [03:44<02:14,  1.02it/s]Loading train:  52%|█████▏    | 149/285 [03:45<02:14,  1.01it/s]Loading train:  53%|█████▎    | 150/285 [03:46<02:06,  1.07it/s]Loading train:  53%|█████▎    | 151/285 [03:46<02:05,  1.06it/s]Loading train:  53%|█████▎    | 152/285 [03:47<02:07,  1.04it/s]Loading train:  54%|█████▎    | 153/285 [03:49<02:11,  1.01it/s]Loading train:  54%|█████▍    | 154/285 [03:50<02:13,  1.02s/it]Loading train:  54%|█████▍    | 155/285 [03:51<02:12,  1.02s/it]Loading train:  55%|█████▍    | 156/285 [03:52<02:05,  1.03it/s]Loading train:  55%|█████▌    | 157/285 [03:52<02:01,  1.05it/s]Loading train:  55%|█████▌    | 158/285 [03:53<02:01,  1.04it/s]Loading train:  56%|█████▌    | 159/285 [03:54<01:58,  1.06it/s]Loading train:  56%|█████▌    | 160/285 [03:55<02:03,  1.01it/s]Loading train:  56%|█████▋    | 161/285 [03:56<01:58,  1.05it/s]Loading train:  57%|█████▋    | 162/285 [03:57<01:55,  1.07it/s]Loading train:  57%|█████▋    | 163/285 [03:58<01:51,  1.09it/s]Loading train:  58%|█████▊    | 164/285 [03:59<01:50,  1.09it/s]Loading train:  58%|█████▊    | 165/285 [04:00<01:49,  1.09it/s]Loading train:  58%|█████▊    | 166/285 [04:01<01:51,  1.07it/s]Loading train:  59%|█████▊    | 167/285 [04:02<01:51,  1.06it/s]Loading train:  59%|█████▉    | 168/285 [04:03<01:49,  1.06it/s]Loading train:  59%|█████▉    | 169/285 [04:04<01:51,  1.04it/s]Loading train:  60%|█████▉    | 170/285 [04:05<01:57,  1.02s/it]Loading train:  60%|██████    | 171/285 [04:06<02:03,  1.09s/it]Loading train:  60%|██████    | 172/285 [04:07<02:03,  1.10s/it]Loading train:  61%|██████    | 173/285 [04:08<02:05,  1.12s/it]Loading train:  61%|██████    | 174/285 [04:10<02:15,  1.22s/it]Loading train:  61%|██████▏   | 175/285 [04:11<02:08,  1.17s/it]Loading train:  62%|██████▏   | 176/285 [04:12<02:03,  1.13s/it]Loading train:  62%|██████▏   | 177/285 [04:13<01:56,  1.08s/it]Loading train:  62%|██████▏   | 178/285 [04:14<01:52,  1.05s/it]Loading train:  63%|██████▎   | 179/285 [04:15<01:43,  1.02it/s]Loading train:  63%|██████▎   | 180/285 [04:16<01:39,  1.05it/s]Loading train:  64%|██████▎   | 181/285 [04:17<01:37,  1.06it/s]Loading train:  64%|██████▍   | 182/285 [04:17<01:36,  1.06it/s]Loading train:  64%|██████▍   | 183/285 [04:18<01:33,  1.09it/s]Loading train:  65%|██████▍   | 184/285 [04:19<01:32,  1.09it/s]Loading train:  65%|██████▍   | 185/285 [04:20<01:30,  1.10it/s]Loading train:  65%|██████▌   | 186/285 [04:21<01:30,  1.10it/s]Loading train:  66%|██████▌   | 187/285 [04:22<01:27,  1.12it/s]Loading train:  66%|██████▌   | 188/285 [04:23<01:27,  1.11it/s]Loading train:  66%|██████▋   | 189/285 [04:24<01:23,  1.14it/s]Loading train:  67%|██████▋   | 190/285 [04:25<01:23,  1.14it/s]Loading train:  67%|██████▋   | 191/285 [04:25<01:23,  1.13it/s]Loading train:  67%|██████▋   | 192/285 [04:26<01:24,  1.11it/s]Loading train:  68%|██████▊   | 193/285 [04:27<01:23,  1.10it/s]Loading train:  68%|██████▊   | 194/285 [04:28<01:22,  1.10it/s]Loading train:  68%|██████▊   | 195/285 [04:29<01:21,  1.11it/s]Loading train:  69%|██████▉   | 196/285 [04:30<01:24,  1.05it/s]Loading train:  69%|██████▉   | 197/285 [04:31<01:22,  1.06it/s]Loading train:  69%|██████▉   | 198/285 [04:32<01:19,  1.10it/s]Loading train:  70%|██████▉   | 199/285 [04:33<01:19,  1.08it/s]Loading train:  70%|███████   | 200/285 [04:34<01:19,  1.07it/s]Loading train:  71%|███████   | 201/285 [04:35<01:18,  1.07it/s]Loading train:  71%|███████   | 202/285 [04:36<01:16,  1.09it/s]Loading train:  71%|███████   | 203/285 [04:37<01:14,  1.10it/s]Loading train:  72%|███████▏  | 204/285 [04:38<01:16,  1.06it/s]Loading train:  72%|███████▏  | 205/285 [04:39<01:17,  1.03it/s]Loading train:  72%|███████▏  | 206/285 [04:40<01:16,  1.03it/s]Loading train:  73%|███████▎  | 207/285 [04:41<01:16,  1.01it/s]Loading train:  73%|███████▎  | 208/285 [04:41<01:13,  1.04it/s]Loading train:  73%|███████▎  | 209/285 [04:42<01:12,  1.06it/s]Loading train:  74%|███████▎  | 210/285 [04:43<01:08,  1.09it/s]Loading train:  74%|███████▍  | 211/285 [04:44<01:07,  1.10it/s]Loading train:  74%|███████▍  | 212/285 [04:45<01:07,  1.09it/s]Loading train:  75%|███████▍  | 213/285 [04:46<01:06,  1.08it/s]Loading train:  75%|███████▌  | 214/285 [04:47<01:06,  1.07it/s]Loading train:  75%|███████▌  | 215/285 [04:48<01:04,  1.08it/s]Loading train:  76%|███████▌  | 216/285 [04:49<01:03,  1.08it/s]Loading train:  76%|███████▌  | 217/285 [04:50<01:02,  1.08it/s]Loading train:  76%|███████▋  | 218/285 [04:51<01:00,  1.10it/s]Loading train:  77%|███████▋  | 219/285 [04:52<00:59,  1.10it/s]Loading train:  77%|███████▋  | 220/285 [04:52<00:58,  1.11it/s]Loading train:  78%|███████▊  | 221/285 [04:53<00:57,  1.11it/s]Loading train:  78%|███████▊  | 222/285 [04:54<00:56,  1.12it/s]Loading train:  78%|███████▊  | 223/285 [04:55<00:55,  1.11it/s]Loading train:  79%|███████▊  | 224/285 [04:56<00:53,  1.14it/s]Loading train:  79%|███████▉  | 225/285 [04:57<00:50,  1.19it/s]Loading train:  79%|███████▉  | 226/285 [04:58<00:49,  1.18it/s]Loading train:  80%|███████▉  | 227/285 [04:58<00:49,  1.18it/s]Loading train:  80%|████████  | 228/285 [04:59<00:49,  1.15it/s]Loading train:  80%|████████  | 229/285 [05:00<00:49,  1.14it/s]Loading train:  81%|████████  | 230/285 [05:01<00:49,  1.11it/s]Loading train:  81%|████████  | 231/285 [05:02<00:48,  1.11it/s]Loading train:  81%|████████▏ | 232/285 [05:03<00:50,  1.04it/s]Loading train:  82%|████████▏ | 233/285 [05:04<00:51,  1.02it/s]Loading train:  82%|████████▏ | 234/285 [05:05<00:51,  1.02s/it]Loading train:  82%|████████▏ | 235/285 [05:06<00:52,  1.04s/it]Loading train:  83%|████████▎ | 236/285 [05:07<00:51,  1.05s/it]Loading train:  83%|████████▎ | 237/285 [05:09<00:50,  1.05s/it]Loading train:  84%|████████▎ | 238/285 [05:10<00:49,  1.06s/it]Loading train:  84%|████████▍ | 239/285 [05:11<00:48,  1.06s/it]Loading train:  84%|████████▍ | 240/285 [05:12<00:47,  1.06s/it]Loading train:  85%|████████▍ | 241/285 [05:13<00:46,  1.07s/it]Loading train:  85%|████████▍ | 242/285 [05:14<00:44,  1.04s/it]Loading train:  85%|████████▌ | 243/285 [05:15<00:41,  1.00it/s]Loading train:  86%|████████▌ | 244/285 [05:16<00:41,  1.02s/it]Loading train:  86%|████████▌ | 245/285 [05:17<00:41,  1.04s/it]Loading train:  86%|████████▋ | 246/285 [05:18<00:41,  1.07s/it]Loading train:  87%|████████▋ | 247/285 [05:19<00:40,  1.06s/it]Loading train:  87%|████████▋ | 248/285 [05:20<00:39,  1.07s/it]Loading train:  87%|████████▋ | 249/285 [05:21<00:38,  1.07s/it]Loading train:  88%|████████▊ | 250/285 [05:22<00:36,  1.03s/it]Loading train:  88%|████████▊ | 251/285 [05:23<00:34,  1.00s/it]Loading train:  88%|████████▊ | 252/285 [05:24<00:31,  1.04it/s]Loading train:  89%|████████▉ | 253/285 [05:25<00:30,  1.05it/s]Loading train:  89%|████████▉ | 254/285 [05:26<00:28,  1.09it/s]Loading train:  89%|████████▉ | 255/285 [05:27<00:27,  1.08it/s]Loading train:  90%|████████▉ | 256/285 [05:28<00:26,  1.08it/s]Loading train:  90%|█████████ | 257/285 [05:28<00:25,  1.11it/s]Loading train:  91%|█████████ | 258/285 [05:29<00:24,  1.10it/s]Loading train:  91%|█████████ | 259/285 [05:30<00:24,  1.07it/s]Loading train:  91%|█████████ | 260/285 [05:31<00:23,  1.08it/s]Loading train:  92%|█████████▏| 261/285 [05:32<00:21,  1.10it/s]Loading train:  92%|█████████▏| 262/285 [05:33<00:20,  1.14it/s]Loading train:  92%|█████████▏| 263/285 [05:34<00:19,  1.13it/s]Loading train:  93%|█████████▎| 264/285 [05:35<00:19,  1.10it/s]Loading train:  93%|█████████▎| 265/285 [05:36<00:18,  1.10it/s]Loading train:  93%|█████████▎| 266/285 [05:37<00:17,  1.07it/s]Loading train:  94%|█████████▎| 267/285 [05:38<00:16,  1.07it/s]Loading train:  94%|█████████▍| 268/285 [05:39<00:16,  1.01it/s]Loading train:  94%|█████████▍| 269/285 [05:40<00:16,  1.04s/it]Loading train:  95%|█████████▍| 270/285 [05:41<00:16,  1.08s/it]Loading train:  95%|█████████▌| 271/285 [05:42<00:15,  1.07s/it]Loading train:  95%|█████████▌| 272/285 [05:43<00:14,  1.10s/it]Loading train:  96%|█████████▌| 273/285 [05:44<00:13,  1.11s/it]Loading train:  96%|█████████▌| 274/285 [05:46<00:12,  1.11s/it]Loading train:  96%|█████████▋| 275/285 [05:46<00:10,  1.07s/it]Loading train:  97%|█████████▋| 276/285 [05:48<00:09,  1.05s/it]Loading train:  97%|█████████▋| 277/285 [05:49<00:08,  1.05s/it]Loading train:  98%|█████████▊| 278/285 [05:50<00:07,  1.04s/it]Loading train:  98%|█████████▊| 279/285 [05:51<00:06,  1.02s/it]Loading train:  98%|█████████▊| 280/285 [05:52<00:05,  1.02s/it]Loading train:  99%|█████████▊| 281/285 [05:53<00:04,  1.01s/it]Loading train:  99%|█████████▉| 282/285 [05:54<00:03,  1.07s/it]Loading train:  99%|█████████▉| 283/285 [05:55<00:02,  1.08s/it]Loading train: 100%|█████████▉| 284/285 [05:56<00:01,  1.11s/it]Loading train: 100%|██████████| 285/285 [05:57<00:00,  1.12s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:01, 248.39it/s]concatenating: train:  20%|██        | 57/285 [00:00<00:00, 265.27it/s]concatenating: train:  31%|███       | 88/285 [00:00<00:00, 276.83it/s]concatenating: train:  41%|████▏     | 118/285 [00:00<00:00, 283.31it/s]concatenating: train:  53%|█████▎    | 150/285 [00:00<00:00, 291.91it/s]concatenating: train:  64%|██████▍   | 183/285 [00:00<00:00, 301.29it/s]concatenating: train:  75%|███████▌  | 215/285 [00:00<00:00, 304.31it/s]concatenating: train:  87%|████████▋ | 248/285 [00:00<00:00, 311.11it/s]concatenating: train:  98%|█████████▊| 279/285 [00:00<00:00, 309.75it/s]concatenating: train: 100%|██████████| 285/285 [00:00<00:00, 306.98it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.45s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.41s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.34s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 144.54it/s]

---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 6  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
