2020-01-20 22:18:19.959694: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:23.240504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:23.240566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:23.653890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:23.653962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:23.653973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:23.654425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['a']
TypeExperiment 9
CrossVal ['a']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:26.910031: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:29.410564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:29.410636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:29.832980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:29.833054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:29.833068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:29.833544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 9
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:34.570343: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:39.131720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:39.131759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:39.557867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:39.557927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:39.557940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:39.558423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 9
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:43.053303: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:45.594057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:45.594119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:46.013993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:46.014055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:46.014067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:46.014521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 9
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:21:38.885964: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:21:42.138959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:21:42.139021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:21:42.544223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:21:42.544295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:21:42.544308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:21:42.544753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:44,  2.53it/s]Loading train:   1%|          | 2/266 [00:00<01:34,  2.79it/s]Loading train:   1%|          | 3/266 [00:00<01:23,  3.16it/s]Loading train:   2%|▏         | 4/266 [00:01<01:15,  3.46it/s]Loading train:   2%|▏         | 5/266 [00:01<01:12,  3.58it/s]Loading train:   2%|▏         | 6/266 [00:01<01:10,  3.71it/s]Loading train:   3%|▎         | 7/266 [00:01<01:08,  3.77it/s]Loading train:   3%|▎         | 8/266 [00:02<01:07,  3.84it/s]Loading train:   3%|▎         | 9/266 [00:02<01:06,  3.87it/s]Loading train:   4%|▍         | 10/266 [00:02<01:05,  3.90it/s]Loading train:   4%|▍         | 11/266 [00:02<01:04,  3.93it/s]Loading train:   5%|▍         | 12/266 [00:03<01:04,  3.95it/s]Loading train:   5%|▍         | 13/266 [00:03<01:03,  3.96it/s]Loading train:   5%|▌         | 14/266 [00:03<01:03,  3.95it/s]Loading train:   6%|▌         | 15/266 [00:03<01:03,  3.96it/s]Loading train:   6%|▌         | 16/266 [00:04<01:03,  3.95it/s]Loading train:   6%|▋         | 17/266 [00:04<01:02,  3.97it/s]Loading train:   7%|▋         | 18/266 [00:04<01:02,  3.98it/s]Loading train:   7%|▋         | 19/266 [00:04<01:01,  3.99it/s]Loading train:   8%|▊         | 20/266 [00:05<01:01,  3.98it/s]Loading train:   8%|▊         | 21/266 [00:05<01:01,  3.97it/s]Loading train:   8%|▊         | 22/266 [00:05<01:01,  3.98it/s]Loading train:   9%|▊         | 23/266 [00:05<01:00,  3.99it/s]Loading train:   9%|▉         | 24/266 [00:06<00:59,  4.06it/s]Loading train:   9%|▉         | 25/266 [00:06<00:58,  4.09it/s]Loading train:  10%|▉         | 26/266 [00:06<00:58,  4.12it/s]Loading train:  10%|█         | 27/266 [00:06<00:57,  4.12it/s]Loading train:  11%|█         | 28/266 [00:07<00:58,  4.10it/s]Loading train:  11%|█         | 29/266 [00:07<00:57,  4.11it/s]Loading train:  11%|█▏        | 30/266 [00:07<00:57,  4.13it/s]Loading train:  12%|█▏        | 31/266 [00:07<00:56,  4.15it/s]Loading train:  12%|█▏        | 32/266 [00:08<00:56,  4.15it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:56,  4.14it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:55,  4.15it/s]Loading train:  13%|█▎        | 35/266 [00:08<00:55,  4.16it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:55,  4.12it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:55,  4.12it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:55,  4.13it/s]Loading train:  15%|█▍        | 39/266 [00:09<00:54,  4.13it/s]Loading train:  15%|█▌        | 40/266 [00:09<00:54,  4.14it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:54,  4.10it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:53,  4.18it/s]Loading train:  16%|█▌        | 43/266 [00:10<00:51,  4.35it/s]Loading train:  17%|█▋        | 44/266 [00:10<00:52,  4.26it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:49,  4.44it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:47,  4.59it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.71it/s]Loading train:  18%|█▊        | 48/266 [00:11<00:45,  4.78it/s]Loading train:  18%|█▊        | 49/266 [00:11<00:44,  4.83it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:44,  4.85it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:44,  4.87it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:43,  4.88it/s]Loading train:  20%|█▉        | 53/266 [00:12<00:43,  4.91it/s]Loading train:  20%|██        | 54/266 [00:12<00:43,  4.92it/s]Loading train:  21%|██        | 55/266 [00:13<00:42,  4.94it/s]Loading train:  21%|██        | 56/266 [00:13<00:42,  4.92it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.94it/s]Loading train:  22%|██▏       | 58/266 [00:13<00:42,  4.95it/s]Loading train:  22%|██▏       | 59/266 [00:13<00:41,  4.95it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:42,  4.89it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:42,  4.84it/s]Loading train:  23%|██▎       | 62/266 [00:14<00:42,  4.80it/s]Loading train:  24%|██▎       | 63/266 [00:14<00:42,  4.76it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:42,  4.73it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:42,  4.72it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:42,  4.71it/s]Loading train:  25%|██▌       | 67/266 [00:15<00:42,  4.71it/s]Loading train:  26%|██▌       | 68/266 [00:15<00:41,  4.72it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:41,  4.73it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:41,  4.73it/s]Loading train:  27%|██▋       | 71/266 [00:16<00:41,  4.75it/s]Loading train:  27%|██▋       | 72/266 [00:16<00:40,  4.74it/s]Loading train:  27%|██▋       | 73/266 [00:16<00:40,  4.75it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:40,  4.74it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.73it/s]Loading train:  29%|██▊       | 76/266 [00:17<00:40,  4.71it/s]Loading train:  29%|██▉       | 77/266 [00:17<00:40,  4.70it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:41,  4.53it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:42,  4.43it/s]Loading train:  30%|███       | 80/266 [00:18<00:42,  4.37it/s]Loading train:  30%|███       | 81/266 [00:18<00:42,  4.32it/s]Loading train:  31%|███       | 82/266 [00:18<00:42,  4.30it/s]Loading train:  31%|███       | 83/266 [00:19<00:42,  4.27it/s]Loading train:  32%|███▏      | 84/266 [00:19<00:42,  4.25it/s]Loading train:  32%|███▏      | 85/266 [00:19<00:42,  4.23it/s]Loading train:  32%|███▏      | 86/266 [00:19<00:42,  4.21it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:42,  4.21it/s]Loading train:  33%|███▎      | 88/266 [00:20<00:42,  4.22it/s]Loading train:  33%|███▎      | 89/266 [00:20<00:41,  4.22it/s]Loading train:  34%|███▍      | 90/266 [00:20<00:41,  4.22it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:41,  4.22it/s]Loading train:  35%|███▍      | 92/266 [00:21<00:41,  4.22it/s]Loading train:  35%|███▍      | 93/266 [00:21<00:41,  4.20it/s]Loading train:  35%|███▌      | 94/266 [00:21<00:40,  4.21it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:40,  4.22it/s]Loading train:  36%|███▌      | 96/266 [00:22<00:39,  4.27it/s]Loading train:  36%|███▋      | 97/266 [00:22<00:41,  4.07it/s]Loading train:  37%|███▋      | 98/266 [00:22<00:40,  4.12it/s]Loading train:  37%|███▋      | 99/266 [00:22<00:38,  4.35it/s]Loading train:  38%|███▊      | 100/266 [00:23<00:38,  4.35it/s]Loading train:  38%|███▊      | 101/266 [00:23<00:37,  4.45it/s]Loading train:  38%|███▊      | 102/266 [00:23<00:36,  4.52it/s]Loading train:  39%|███▊      | 103/266 [00:23<00:35,  4.58it/s]Loading train:  39%|███▉      | 104/266 [00:24<00:35,  4.62it/s]Loading train:  39%|███▉      | 105/266 [00:24<00:34,  4.65it/s]Loading train:  40%|███▉      | 106/266 [00:24<00:34,  4.67it/s]Loading train:  40%|████      | 107/266 [00:24<00:33,  4.69it/s]Loading train:  41%|████      | 108/266 [00:24<00:33,  4.70it/s]Loading train:  41%|████      | 109/266 [00:25<00:33,  4.68it/s]Loading train:  41%|████▏     | 110/266 [00:25<00:33,  4.65it/s]Loading train:  42%|████▏     | 111/266 [00:25<00:33,  4.66it/s]Loading train:  42%|████▏     | 112/266 [00:25<00:32,  4.69it/s]Loading train:  42%|████▏     | 113/266 [00:25<00:32,  4.71it/s]Loading train:  43%|████▎     | 114/266 [00:26<00:32,  4.71it/s]Loading train:  43%|████▎     | 115/266 [00:26<00:32,  4.71it/s]Loading train:  44%|████▎     | 116/266 [00:26<00:31,  4.71it/s]Loading train:  44%|████▍     | 117/266 [00:26<00:31,  4.72it/s]Loading train:  44%|████▍     | 118/266 [00:27<00:31,  4.67it/s]Loading train:  45%|████▍     | 119/266 [00:27<00:33,  4.45it/s]Loading train:  45%|████▌     | 120/266 [00:27<00:33,  4.31it/s]Loading train:  45%|████▌     | 121/266 [00:27<00:34,  4.21it/s]Loading train:  46%|████▌     | 122/266 [00:28<00:34,  4.15it/s]Loading train:  46%|████▌     | 123/266 [00:28<00:34,  4.10it/s]Loading train:  47%|████▋     | 124/266 [00:28<00:34,  4.07it/s]Loading train:  47%|████▋     | 125/266 [00:28<00:34,  4.03it/s]Loading train:  47%|████▋     | 126/266 [00:29<00:34,  4.03it/s]Loading train:  48%|████▊     | 127/266 [00:29<00:34,  4.04it/s]Loading train:  48%|████▊     | 128/266 [00:29<00:34,  4.01it/s]Loading train:  48%|████▊     | 129/266 [00:29<00:34,  4.02it/s]Loading train:  49%|████▉     | 130/266 [00:30<00:33,  4.01it/s]Loading train:  49%|████▉     | 131/266 [00:30<00:33,  4.00it/s]Loading train:  50%|████▉     | 132/266 [00:30<00:33,  4.00it/s]Loading train:  50%|█████     | 133/266 [00:30<00:33,  4.01it/s]Loading train:  50%|█████     | 134/266 [00:31<00:33,  4.00it/s]Loading train:  51%|█████     | 135/266 [00:31<00:32,  3.99it/s]Loading train:  51%|█████     | 136/266 [00:31<00:32,  3.99it/s]Loading train:  52%|█████▏    | 137/266 [00:31<00:35,  3.60it/s]Loading train:  52%|█████▏    | 138/266 [00:32<00:48,  2.62it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:55,  2.31it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:58,  2.14it/s]Loading train:  53%|█████▎    | 141/266 [00:34<01:01,  2.02it/s]Loading train:  53%|█████▎    | 142/266 [00:34<01:10,  1.77it/s]Loading train:  54%|█████▍    | 143/266 [00:35<01:09,  1.78it/s]Loading train:  54%|█████▍    | 144/266 [00:35<01:04,  1.90it/s]Loading train:  55%|█████▍    | 145/266 [00:36<01:05,  1.84it/s]Loading train:  55%|█████▍    | 146/266 [00:36<01:03,  1.90it/s]Loading train:  55%|█████▌    | 147/266 [00:37<00:58,  2.02it/s]Loading train:  56%|█████▌    | 148/266 [00:37<00:52,  2.24it/s]Loading train:  56%|█████▌    | 149/266 [00:38<00:53,  2.20it/s]Loading train:  56%|█████▋    | 150/266 [00:38<00:51,  2.24it/s]Loading train:  57%|█████▋    | 151/266 [00:39<00:50,  2.28it/s]Loading train:  57%|█████▋    | 152/266 [00:39<00:48,  2.34it/s]Loading train:  58%|█████▊    | 153/266 [00:39<00:45,  2.50it/s]Loading train:  58%|█████▊    | 154/266 [00:39<00:38,  2.89it/s]Loading train:  58%|█████▊    | 155/266 [00:40<00:35,  3.15it/s]Loading train:  59%|█████▊    | 156/266 [00:40<00:32,  3.37it/s]Loading train:  59%|█████▉    | 157/266 [00:40<00:32,  3.31it/s]Loading train:  59%|█████▉    | 158/266 [00:41<00:32,  3.29it/s]Loading train:  60%|█████▉    | 159/266 [00:41<00:33,  3.23it/s]Loading train:  60%|██████    | 160/266 [00:41<00:34,  3.07it/s]Loading train:  61%|██████    | 161/266 [00:42<00:37,  2.77it/s]Loading train:  61%|██████    | 162/266 [00:42<00:36,  2.81it/s]Loading train:  61%|██████▏   | 163/266 [00:42<00:36,  2.80it/s]Loading train:  62%|██████▏   | 164/266 [00:43<00:35,  2.88it/s]Loading train:  62%|██████▏   | 165/266 [00:43<00:32,  3.10it/s]Loading train:  62%|██████▏   | 166/266 [00:43<00:30,  3.24it/s]Loading train:  63%|██████▎   | 167/266 [00:44<00:27,  3.58it/s]Loading train:  63%|██████▎   | 168/266 [00:44<00:27,  3.54it/s]Loading train:  64%|██████▎   | 169/266 [00:44<00:27,  3.56it/s]Loading train:  64%|██████▍   | 170/266 [00:44<00:27,  3.52it/s]Loading train:  64%|██████▍   | 171/266 [00:45<00:26,  3.61it/s]Loading train:  65%|██████▍   | 172/266 [00:45<00:26,  3.55it/s]Loading train:  65%|██████▌   | 173/266 [00:46<00:35,  2.64it/s]Loading train:  65%|██████▌   | 174/266 [00:46<00:39,  2.33it/s]Loading train:  66%|██████▌   | 175/266 [00:47<00:46,  1.95it/s]Loading train:  66%|██████▌   | 176/266 [00:47<00:49,  1.80it/s]Loading train:  67%|██████▋   | 177/266 [00:48<00:55,  1.60it/s]Loading train:  67%|██████▋   | 178/266 [00:49<00:58,  1.50it/s]Loading train:  67%|██████▋   | 179/266 [00:50<00:55,  1.57it/s]Loading train:  68%|██████▊   | 180/266 [00:50<00:54,  1.57it/s]Loading train:  68%|██████▊   | 181/266 [00:51<00:52,  1.61it/s]Loading train:  68%|██████▊   | 182/266 [00:51<00:51,  1.62it/s]Loading train:  69%|██████▉   | 183/266 [00:52<00:51,  1.61it/s]Loading train:  69%|██████▉   | 184/266 [00:53<00:53,  1.54it/s]Loading train:  70%|██████▉   | 185/266 [00:53<00:53,  1.50it/s]Loading train:  70%|██████▉   | 186/266 [00:54<00:51,  1.56it/s]Loading train:  70%|███████   | 187/266 [00:55<00:53,  1.48it/s]Loading train:  71%|███████   | 188/266 [00:55<00:52,  1.49it/s]Loading train:  71%|███████   | 189/266 [00:56<00:54,  1.41it/s]Loading train:  71%|███████▏  | 190/266 [00:57<00:49,  1.52it/s]Loading train:  72%|███████▏  | 191/266 [00:57<00:50,  1.49it/s]Loading train:  72%|███████▏  | 192/266 [00:58<00:48,  1.52it/s]Loading train:  73%|███████▎  | 193/266 [00:59<00:46,  1.58it/s]Loading train:  73%|███████▎  | 194/266 [00:59<00:46,  1.53it/s]Loading train:  73%|███████▎  | 195/266 [01:00<00:43,  1.62it/s]Loading train:  74%|███████▎  | 196/266 [01:01<00:42,  1.64it/s]Loading train:  74%|███████▍  | 197/266 [01:01<00:41,  1.67it/s]Loading train:  74%|███████▍  | 198/266 [01:02<00:39,  1.71it/s]Loading train:  75%|███████▍  | 199/266 [01:02<00:36,  1.82it/s]Loading train:  75%|███████▌  | 200/266 [01:03<00:34,  1.92it/s]Loading train:  76%|███████▌  | 201/266 [01:03<00:38,  1.70it/s]Loading train:  76%|███████▌  | 202/266 [01:04<00:38,  1.68it/s]Loading train:  76%|███████▋  | 203/266 [01:04<00:35,  1.77it/s]Loading train:  77%|███████▋  | 204/266 [01:05<00:36,  1.68it/s]Loading train:  77%|███████▋  | 205/266 [01:06<00:35,  1.70it/s]Loading train:  77%|███████▋  | 206/266 [01:06<00:34,  1.73it/s]Loading train:  78%|███████▊  | 207/266 [01:07<00:33,  1.75it/s]Loading train:  78%|███████▊  | 208/266 [01:07<00:30,  1.87it/s]Loading train:  79%|███████▊  | 209/266 [01:08<00:31,  1.80it/s]Loading train:  79%|███████▉  | 210/266 [01:08<00:32,  1.71it/s]Loading train:  79%|███████▉  | 211/266 [01:09<00:31,  1.74it/s]Loading train:  80%|███████▉  | 212/266 [01:10<00:34,  1.57it/s]Loading train:  80%|████████  | 213/266 [01:10<00:32,  1.62it/s]Loading train:  80%|████████  | 214/266 [01:11<00:32,  1.62it/s]Loading train:  81%|████████  | 215/266 [01:12<00:30,  1.67it/s]Loading train:  81%|████████  | 216/266 [01:12<00:30,  1.64it/s]Loading train:  82%|████████▏ | 217/266 [01:13<00:28,  1.72it/s]Loading train:  82%|████████▏ | 218/266 [01:13<00:28,  1.69it/s]Loading train:  82%|████████▏ | 219/266 [01:14<00:27,  1.72it/s]Loading train:  83%|████████▎ | 220/266 [01:14<00:26,  1.77it/s]Loading train:  83%|████████▎ | 221/266 [01:15<00:24,  1.80it/s]Loading train:  83%|████████▎ | 222/266 [01:16<00:24,  1.77it/s]Loading train:  84%|████████▍ | 223/266 [01:16<00:25,  1.69it/s]Loading train:  84%|████████▍ | 224/266 [01:17<00:26,  1.59it/s]Loading train:  85%|████████▍ | 225/266 [01:17<00:24,  1.65it/s]Loading train:  85%|████████▍ | 226/266 [01:18<00:24,  1.65it/s]Loading train:  85%|████████▌ | 227/266 [01:19<00:24,  1.60it/s]Loading train:  86%|████████▌ | 228/266 [01:19<00:23,  1.59it/s]Loading train:  86%|████████▌ | 229/266 [01:20<00:21,  1.69it/s]Loading train:  86%|████████▋ | 230/266 [01:20<00:20,  1.75it/s]Loading train:  87%|████████▋ | 231/266 [01:21<00:19,  1.77it/s]Loading train:  87%|████████▋ | 232/266 [01:21<00:19,  1.79it/s]Loading train:  88%|████████▊ | 233/266 [01:22<00:18,  1.74it/s]Loading train:  88%|████████▊ | 234/266 [01:23<00:19,  1.68it/s]Loading train:  88%|████████▊ | 235/266 [01:23<00:19,  1.61it/s]Loading train:  89%|████████▊ | 236/266 [01:24<00:18,  1.64it/s]Loading train:  89%|████████▉ | 237/266 [01:25<00:17,  1.67it/s]Loading train:  89%|████████▉ | 238/266 [01:25<00:16,  1.68it/s]Loading train:  90%|████████▉ | 239/266 [01:26<00:15,  1.72it/s]Loading train:  90%|█████████ | 240/266 [01:26<00:15,  1.68it/s]Loading train:  91%|█████████ | 241/266 [01:27<00:14,  1.69it/s]Loading train:  91%|█████████ | 242/266 [01:28<00:14,  1.66it/s]Loading train:  91%|█████████▏| 243/266 [01:28<00:13,  1.65it/s]Loading train:  92%|█████████▏| 244/266 [01:29<00:13,  1.66it/s]Loading train:  92%|█████████▏| 245/266 [01:29<00:12,  1.63it/s]Loading train:  92%|█████████▏| 246/266 [01:30<00:12,  1.59it/s]Loading train:  93%|█████████▎| 247/266 [01:31<00:12,  1.51it/s]Loading train:  93%|█████████▎| 248/266 [01:31<00:11,  1.53it/s]Loading train:  94%|█████████▎| 249/266 [01:32<00:11,  1.53it/s]Loading train:  94%|█████████▍| 250/266 [01:33<00:11,  1.42it/s]Loading train:  94%|█████████▍| 251/266 [01:34<00:10,  1.43it/s]Loading train:  95%|█████████▍| 252/266 [01:34<00:09,  1.45it/s]Loading train:  95%|█████████▌| 253/266 [01:35<00:08,  1.45it/s]Loading train:  95%|█████████▌| 254/266 [01:36<00:08,  1.43it/s]Loading train:  96%|█████████▌| 255/266 [01:36<00:07,  1.44it/s]Loading train:  96%|█████████▌| 256/266 [01:37<00:06,  1.47it/s]Loading train:  97%|█████████▋| 257/266 [01:38<00:06,  1.45it/s]Loading train:  97%|█████████▋| 258/266 [01:38<00:05,  1.47it/s]Loading train:  97%|█████████▋| 259/266 [01:39<00:04,  1.48it/s]Loading train:  98%|█████████▊| 260/266 [01:40<00:03,  1.53it/s]Loading train:  98%|█████████▊| 261/266 [01:40<00:03,  1.54it/s]Loading train:  98%|█████████▊| 262/266 [01:41<00:02,  1.53it/s]Loading train:  99%|█████████▉| 263/266 [01:42<00:01,  1.51it/s]Loading train:  99%|█████████▉| 264/266 [01:42<00:01,  1.53it/s]Loading train: 100%|█████████▉| 265/266 [01:43<00:00,  1.50it/s]Loading train: 100%|██████████| 266/266 [01:44<00:00,  1.50it/s]Loading train: 100%|██████████| 266/266 [01:44<00:00,  2.55it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/266 [00:00<00:11, 23.47it/s]concatenating: train:   3%|▎         | 8/266 [00:00<00:09, 27.58it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:07, 31.70it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:07, 34.60it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:06, 37.98it/s]concatenating: train:  11%|█         | 28/266 [00:00<00:05, 39.91it/s]concatenating: train:  12%|█▏        | 33/266 [00:00<00:05, 41.84it/s]concatenating: train:  15%|█▍        | 39/266 [00:00<00:05, 44.82it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:04, 49.15it/s]concatenating: train:  20%|█▉        | 53/266 [00:01<00:03, 53.30it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:03, 56.60it/s]concatenating: train:  25%|██▌       | 67/266 [00:01<00:03, 58.18it/s]concatenating: train:  28%|██▊       | 74/266 [00:01<00:03, 59.21it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 59.11it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 58.63it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:02, 58.26it/s]concatenating: train:  37%|███▋      | 99/266 [00:01<00:02, 58.75it/s]concatenating: train:  40%|███▉      | 106/266 [00:01<00:02, 60.00it/s]concatenating: train:  42%|████▏     | 113/266 [00:02<00:02, 60.82it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 60.59it/s]concatenating: train:  48%|████▊     | 127/266 [00:02<00:02, 58.37it/s]concatenating: train:  50%|█████     | 133/266 [00:02<00:02, 57.27it/s]concatenating: train:  52%|█████▏    | 139/266 [00:02<00:02, 56.09it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 54.55it/s]concatenating: train:  57%|█████▋    | 151/266 [00:02<00:02, 54.99it/s]concatenating: train:  59%|█████▉    | 158/266 [00:02<00:01, 57.62it/s]concatenating: train:  62%|██████▏   | 165/266 [00:02<00:01, 59.08it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 61.85it/s]concatenating: train:  67%|██████▋   | 179/266 [00:03<00:01, 57.96it/s]concatenating: train:  70%|██████▉   | 185/266 [00:03<00:01, 55.38it/s]concatenating: train:  72%|███████▏  | 191/266 [00:03<00:01, 53.78it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 52.30it/s]concatenating: train:  76%|███████▋  | 203/266 [00:03<00:01, 51.55it/s]concatenating: train:  79%|███████▊  | 209/266 [00:03<00:01, 50.08it/s]concatenating: train:  81%|████████  | 215/266 [00:03<00:01, 49.55it/s]concatenating: train:  83%|████████▎ | 220/266 [00:04<00:00, 49.11it/s]concatenating: train:  85%|████████▍ | 226/266 [00:04<00:00, 49.37it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 48.54it/s]concatenating: train:  89%|████████▊ | 236/266 [00:04<00:00, 46.59it/s]concatenating: train:  91%|█████████ | 241/266 [00:04<00:00, 46.75it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 47.18it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 45.13it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 42.73it/s]concatenating: train:  98%|█████████▊| 261/266 [00:04<00:00, 43.51it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 43.80it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 52.18it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:01,  1.62it/s]Loading test:  50%|█████     | 2/4 [00:01<00:01,  1.64it/s]Loading test:  75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 66.46it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<02:44,  1.61it/s]Loading trainS:   1%|          | 2/266 [00:01<02:50,  1.55it/s]Loading trainS:   1%|          | 3/266 [00:01<02:43,  1.61it/s]Loading trainS:   2%|▏         | 4/266 [00:02<02:33,  1.71it/s]Loading trainS:   2%|▏         | 5/266 [00:02<02:31,  1.72it/s]Loading trainS:   2%|▏         | 6/266 [00:03<02:36,  1.66it/s]Loading trainS:   3%|▎         | 7/266 [00:04<02:30,  1.72it/s]Loading trainS:   3%|▎         | 8/266 [00:04<02:32,  1.69it/s]Loading trainS:   3%|▎         | 9/266 [00:05<02:41,  1.59it/s]Loading trainS:   4%|▍         | 10/266 [00:06<02:44,  1.55it/s]Loading trainS:   4%|▍         | 11/266 [00:06<02:39,  1.60it/s]Loading trainS:   5%|▍         | 12/266 [00:07<02:34,  1.64it/s]Loading trainS:   5%|▍         | 13/266 [00:07<02:34,  1.64it/s]Loading trainS:   5%|▌         | 14/266 [00:08<02:31,  1.66it/s]Loading trainS:   6%|▌         | 15/266 [00:09<02:37,  1.59it/s]Loading trainS:   6%|▌         | 16/266 [00:09<02:37,  1.59it/s]Loading trainS:   6%|▋         | 17/266 [00:10<02:37,  1.58it/s]Loading trainS:   7%|▋         | 18/266 [00:11<02:39,  1.55it/s]Loading trainS:   7%|▋         | 19/266 [00:11<02:32,  1.62it/s]Loading trainS:   8%|▊         | 20/266 [00:12<02:34,  1.59it/s]Loading trainS:   8%|▊         | 21/266 [00:13<02:40,  1.53it/s]Loading trainS:   8%|▊         | 22/266 [00:13<02:37,  1.55it/s]Loading trainS:   9%|▊         | 23/266 [00:14<02:37,  1.54it/s]Loading trainS:   9%|▉         | 24/266 [00:14<02:29,  1.62it/s]Loading trainS:   9%|▉         | 25/266 [00:15<02:28,  1.62it/s]Loading trainS:  10%|▉         | 26/266 [00:16<02:32,  1.57it/s]Loading trainS:  10%|█         | 27/266 [00:16<02:29,  1.60it/s]Loading trainS:  11%|█         | 28/266 [00:17<02:27,  1.61it/s]Loading trainS:  11%|█         | 29/266 [00:17<02:23,  1.65it/s]Loading trainS:  11%|█▏        | 30/266 [00:18<02:23,  1.64it/s]Loading trainS:  12%|█▏        | 31/266 [00:19<02:28,  1.58it/s]Loading trainS:  12%|█▏        | 32/266 [00:19<02:24,  1.62it/s]Loading trainS:  12%|█▏        | 33/266 [00:20<02:27,  1.58it/s]Loading trainS:  13%|█▎        | 34/266 [00:21<02:23,  1.62it/s]Loading trainS:  13%|█▎        | 35/266 [00:21<02:23,  1.61it/s]Loading trainS:  14%|█▎        | 36/266 [00:22<02:20,  1.63it/s]Loading trainS:  14%|█▍        | 37/266 [00:22<02:16,  1.68it/s]Loading trainS:  14%|█▍        | 38/266 [00:23<02:17,  1.65it/s]Loading trainS:  15%|█▍        | 39/266 [00:24<02:17,  1.65it/s]Loading trainS:  15%|█▌        | 40/266 [00:24<02:22,  1.59it/s]Loading trainS:  15%|█▌        | 41/266 [00:25<02:15,  1.66it/s]Loading trainS:  16%|█▌        | 42/266 [00:25<02:12,  1.69it/s]Loading trainS:  16%|█▌        | 43/266 [00:26<02:02,  1.83it/s]Loading trainS:  17%|█▋        | 44/266 [00:26<01:58,  1.87it/s]Loading trainS:  17%|█▋        | 45/266 [00:27<02:00,  1.83it/s]Loading trainS:  17%|█▋        | 46/266 [00:27<01:55,  1.91it/s]Loading trainS:  18%|█▊        | 47/266 [00:28<01:49,  2.00it/s]Loading trainS:  18%|█▊        | 48/266 [00:28<01:44,  2.09it/s]Loading trainS:  18%|█▊        | 49/266 [00:29<01:46,  2.05it/s]Loading trainS:  19%|█▉        | 50/266 [00:29<01:46,  2.03it/s]Loading trainS:  19%|█▉        | 51/266 [00:30<01:47,  2.01it/s]Loading trainS:  20%|█▉        | 52/266 [00:30<01:42,  2.08it/s]Loading trainS:  20%|█▉        | 53/266 [00:31<01:40,  2.11it/s]Loading trainS:  20%|██        | 54/266 [00:31<01:46,  1.99it/s]Loading trainS:  21%|██        | 55/266 [00:32<01:44,  2.02it/s]Loading trainS:  21%|██        | 56/266 [00:32<01:42,  2.05it/s]Loading trainS:  21%|██▏       | 57/266 [00:33<01:47,  1.94it/s]Loading trainS:  22%|██▏       | 58/266 [00:33<01:46,  1.95it/s]Loading trainS:  22%|██▏       | 59/266 [00:34<01:49,  1.90it/s]Loading trainS:  23%|██▎       | 60/266 [00:34<01:48,  1.89it/s]Loading trainS:  23%|██▎       | 61/266 [00:35<01:48,  1.89it/s]Loading trainS:  23%|██▎       | 62/266 [00:36<01:50,  1.84it/s]Loading trainS:  24%|██▎       | 63/266 [00:36<01:51,  1.83it/s]Loading trainS:  24%|██▍       | 64/266 [00:37<01:47,  1.88it/s]Loading trainS:  24%|██▍       | 65/266 [00:37<01:42,  1.97it/s]Loading trainS:  25%|██▍       | 66/266 [00:38<01:53,  1.76it/s]Loading trainS:  25%|██▌       | 67/266 [00:38<01:48,  1.83it/s]Loading trainS:  26%|██▌       | 68/266 [00:39<01:48,  1.83it/s]Loading trainS:  26%|██▌       | 69/266 [00:39<01:52,  1.76it/s]Loading trainS:  26%|██▋       | 70/266 [00:40<01:49,  1.78it/s]Loading trainS:  27%|██▋       | 71/266 [00:40<01:46,  1.83it/s]Loading trainS:  27%|██▋       | 72/266 [00:41<01:40,  1.93it/s]Loading trainS:  27%|██▋       | 73/266 [00:41<01:41,  1.90it/s]Loading trainS:  28%|██▊       | 74/266 [00:42<01:38,  1.94it/s]Loading trainS:  28%|██▊       | 75/266 [00:42<01:37,  1.95it/s]Loading trainS:  29%|██▊       | 76/266 [00:43<01:39,  1.90it/s]Loading trainS:  29%|██▉       | 77/266 [00:44<01:42,  1.84it/s]Loading trainS:  29%|██▉       | 78/266 [00:44<01:48,  1.73it/s]Loading trainS:  30%|██▉       | 79/266 [00:45<01:51,  1.67it/s]Loading trainS:  30%|███       | 80/266 [00:45<01:46,  1.74it/s]Loading trainS:  30%|███       | 81/266 [00:46<01:50,  1.67it/s]Loading trainS:  31%|███       | 82/266 [00:47<01:49,  1.68it/s]Loading trainS:  31%|███       | 83/266 [00:47<01:48,  1.68it/s]Loading trainS:  32%|███▏      | 84/266 [00:48<01:49,  1.66it/s]Loading trainS:  32%|███▏      | 85/266 [00:48<01:50,  1.63it/s]Loading trainS:  32%|███▏      | 86/266 [00:49<01:49,  1.64it/s]Loading trainS:  33%|███▎      | 87/266 [00:50<01:49,  1.63it/s]Loading trainS:  33%|███▎      | 88/266 [00:50<01:50,  1.61it/s]Loading trainS:  33%|███▎      | 89/266 [00:51<01:45,  1.68it/s]Loading trainS:  34%|███▍      | 90/266 [00:52<01:46,  1.65it/s]Loading trainS:  34%|███▍      | 91/266 [00:52<01:48,  1.61it/s]Loading trainS:  35%|███▍      | 92/266 [00:53<01:47,  1.61it/s]Loading trainS:  35%|███▍      | 93/266 [00:53<01:50,  1.57it/s]Loading trainS:  35%|███▌      | 94/266 [00:54<01:48,  1.58it/s]Loading trainS:  36%|███▌      | 95/266 [00:55<01:52,  1.53it/s]Loading trainS:  36%|███▌      | 96/266 [00:55<01:47,  1.58it/s]Loading trainS:  36%|███▋      | 97/266 [00:56<01:52,  1.51it/s]Loading trainS:  37%|███▋      | 98/266 [00:57<01:52,  1.49it/s]Loading trainS:  37%|███▋      | 99/266 [00:57<01:40,  1.67it/s]Loading trainS:  38%|███▊      | 100/266 [00:58<01:36,  1.72it/s]Loading trainS:  38%|███▊      | 101/266 [00:58<01:31,  1.80it/s]Loading trainS:  38%|███▊      | 102/266 [00:59<01:27,  1.88it/s]Loading trainS:  39%|███▊      | 103/266 [00:59<01:26,  1.88it/s]Loading trainS:  39%|███▉      | 104/266 [01:00<01:26,  1.88it/s]Loading trainS:  39%|███▉      | 105/266 [01:00<01:26,  1.85it/s]Loading trainS:  40%|███▉      | 106/266 [01:01<01:26,  1.86it/s]Loading trainS:  40%|████      | 107/266 [01:01<01:23,  1.89it/s]Loading trainS:  41%|████      | 108/266 [01:02<01:24,  1.87it/s]Loading trainS:  41%|████      | 109/266 [01:02<01:20,  1.94it/s]Loading trainS:  41%|████▏     | 110/266 [01:03<01:20,  1.95it/s]Loading trainS:  42%|████▏     | 111/266 [01:03<01:18,  1.96it/s]Loading trainS:  42%|████▏     | 112/266 [01:04<01:19,  1.93it/s]Loading trainS:  42%|████▏     | 113/266 [01:04<01:19,  1.93it/s]Loading trainS:  43%|████▎     | 114/266 [01:05<01:23,  1.82it/s]Loading trainS:  43%|████▎     | 115/266 [01:06<01:22,  1.84it/s]Loading trainS:  44%|████▎     | 116/266 [01:06<01:17,  1.93it/s]Loading trainS:  44%|████▍     | 117/266 [01:07<01:21,  1.82it/s]Loading trainS:  44%|████▍     | 118/266 [01:07<01:20,  1.84it/s]Loading trainS:  45%|████▍     | 119/266 [01:08<01:24,  1.75it/s]Loading trainS:  45%|████▌     | 120/266 [01:09<01:26,  1.68it/s]Loading trainS:  45%|████▌     | 121/266 [01:09<01:26,  1.67it/s]Loading trainS:  46%|████▌     | 122/266 [01:10<01:26,  1.66it/s]Loading trainS:  46%|████▌     | 123/266 [01:10<01:28,  1.62it/s]Loading trainS:  47%|████▋     | 124/266 [01:11<01:32,  1.53it/s]Loading trainS:  47%|████▋     | 125/266 [01:12<01:32,  1.53it/s]Loading trainS:  47%|████▋     | 126/266 [01:12<01:29,  1.57it/s]Loading trainS:  48%|████▊     | 127/266 [01:13<01:28,  1.58it/s]Loading trainS:  48%|████▊     | 128/266 [01:14<01:27,  1.58it/s]Loading trainS:  48%|████▊     | 129/266 [01:14<01:26,  1.58it/s]Loading trainS:  49%|████▉     | 130/266 [01:15<01:27,  1.55it/s]Loading trainS:  49%|████▉     | 131/266 [01:16<01:26,  1.55it/s]Loading trainS:  50%|████▉     | 132/266 [01:16<01:24,  1.58it/s]Loading trainS:  50%|█████     | 133/266 [01:17<01:25,  1.56it/s]Loading trainS:  50%|█████     | 134/266 [01:17<01:24,  1.57it/s]Loading trainS:  51%|█████     | 135/266 [01:18<01:22,  1.59it/s]Loading trainS:  51%|█████     | 136/266 [01:19<01:18,  1.66it/s]Loading trainS:  52%|█████▏    | 137/266 [01:19<01:16,  1.70it/s]Loading trainS:  52%|█████▏    | 138/266 [01:20<01:13,  1.75it/s]Loading trainS:  52%|█████▏    | 139/266 [01:20<01:11,  1.78it/s]Loading trainS:  53%|█████▎    | 140/266 [01:21<01:11,  1.77it/s]Loading trainS:  53%|█████▎    | 141/266 [01:21<01:11,  1.76it/s]Loading trainS:  53%|█████▎    | 142/266 [01:22<01:09,  1.78it/s]Loading trainS:  54%|█████▍    | 143/266 [01:23<01:08,  1.79it/s]Loading trainS:  54%|█████▍    | 144/266 [01:23<01:08,  1.78it/s]Loading trainS:  55%|█████▍    | 145/266 [01:24<01:05,  1.86it/s]Loading trainS:  55%|█████▍    | 146/266 [01:24<01:04,  1.86it/s]Loading trainS:  55%|█████▌    | 147/266 [01:25<01:05,  1.80it/s]Loading trainS:  56%|█████▌    | 148/266 [01:25<01:07,  1.76it/s]Loading trainS:  56%|█████▌    | 149/266 [01:26<01:10,  1.66it/s]Loading trainS:  56%|█████▋    | 150/266 [01:27<01:09,  1.67it/s]Loading trainS:  57%|█████▋    | 151/266 [01:27<01:05,  1.74it/s]Loading trainS:  57%|█████▋    | 152/266 [01:28<01:04,  1.77it/s]Loading trainS:  58%|█████▊    | 153/266 [01:28<01:05,  1.73it/s]Loading trainS:  58%|█████▊    | 154/266 [01:29<01:03,  1.76it/s]Loading trainS:  58%|█████▊    | 155/266 [01:29<00:58,  1.89it/s]Loading trainS:  59%|█████▊    | 156/266 [01:30<00:56,  1.95it/s]Loading trainS:  59%|█████▉    | 157/266 [01:30<00:51,  2.10it/s]Loading trainS:  59%|█████▉    | 158/266 [01:31<00:51,  2.10it/s]Loading trainS:  60%|█████▉    | 159/266 [01:31<00:51,  2.09it/s]Loading trainS:  60%|██████    | 160/266 [01:32<00:53,  1.99it/s]Loading trainS:  61%|██████    | 161/266 [01:32<00:53,  1.95it/s]Loading trainS:  61%|██████    | 162/266 [01:33<00:52,  1.97it/s]Loading trainS:  61%|██████▏   | 163/266 [01:33<00:49,  2.09it/s]Loading trainS:  62%|██████▏   | 164/266 [01:34<00:49,  2.08it/s]Loading trainS:  62%|██████▏   | 165/266 [01:34<00:48,  2.07it/s]Loading trainS:  62%|██████▏   | 166/266 [01:34<00:47,  2.11it/s]Loading trainS:  63%|██████▎   | 167/266 [01:35<00:46,  2.12it/s]Loading trainS:  63%|██████▎   | 168/266 [01:35<00:45,  2.17it/s]Loading trainS:  64%|██████▎   | 169/266 [01:36<00:42,  2.30it/s]Loading trainS:  64%|██████▍   | 170/266 [01:36<00:44,  2.13it/s]Loading trainS:  64%|██████▍   | 171/266 [01:37<00:46,  2.04it/s]Loading trainS:  65%|██████▍   | 172/266 [01:37<00:47,  1.98it/s]Loading trainS:  65%|██████▌   | 173/266 [01:38<00:49,  1.89it/s]Loading trainS:  65%|██████▌   | 174/266 [01:38<00:47,  1.95it/s]Loading trainS:  66%|██████▌   | 175/266 [01:39<00:44,  2.03it/s]Loading trainS:  66%|██████▌   | 176/266 [01:39<00:44,  2.01it/s]Loading trainS:  67%|██████▋   | 177/266 [01:40<00:45,  1.94it/s]Loading trainS:  67%|██████▋   | 178/266 [01:41<00:47,  1.86it/s]Loading trainS:  67%|██████▋   | 179/266 [01:41<00:46,  1.89it/s]Loading trainS:  68%|██████▊   | 180/266 [01:42<00:47,  1.81it/s]Loading trainS:  68%|██████▊   | 181/266 [01:42<00:48,  1.76it/s]Loading trainS:  68%|██████▊   | 182/266 [01:43<00:49,  1.70it/s]Loading trainS:  69%|██████▉   | 183/266 [01:44<00:49,  1.68it/s]Loading trainS:  69%|██████▉   | 184/266 [01:44<00:49,  1.67it/s]Loading trainS:  70%|██████▉   | 185/266 [01:45<00:50,  1.60it/s]Loading trainS:  70%|██████▉   | 186/266 [01:45<00:50,  1.59it/s]Loading trainS:  70%|███████   | 187/266 [01:46<00:48,  1.63it/s]Loading trainS:  71%|███████   | 188/266 [01:47<00:45,  1.73it/s]Loading trainS:  71%|███████   | 189/266 [01:47<00:43,  1.77it/s]Loading trainS:  71%|███████▏  | 190/266 [01:48<00:40,  1.87it/s]Loading trainS:  72%|███████▏  | 191/266 [01:48<00:34,  2.16it/s]Loading trainS:  72%|███████▏  | 192/266 [01:48<00:29,  2.47it/s]Loading trainS:  73%|███████▎  | 193/266 [01:48<00:26,  2.75it/s]Loading trainS:  73%|███████▎  | 194/266 [01:49<00:25,  2.78it/s]Loading trainS:  73%|███████▎  | 195/266 [01:49<00:26,  2.66it/s]Loading trainS:  74%|███████▎  | 196/266 [01:49<00:25,  2.69it/s]Loading trainS:  74%|███████▍  | 197/266 [01:50<00:28,  2.43it/s]Loading trainS:  74%|███████▍  | 198/266 [01:50<00:29,  2.32it/s]Loading trainS:  75%|███████▍  | 199/266 [01:51<00:28,  2.33it/s]Loading trainS:  75%|███████▌  | 200/266 [01:51<00:28,  2.31it/s]Loading trainS:  76%|███████▌  | 201/266 [01:52<00:29,  2.24it/s]Loading trainS:  76%|███████▌  | 202/266 [01:52<00:28,  2.22it/s]Loading trainS:  76%|███████▋  | 203/266 [01:53<00:28,  2.23it/s]Loading trainS:  77%|███████▋  | 204/266 [01:53<00:27,  2.25it/s]Loading trainS:  77%|███████▋  | 205/266 [01:54<00:26,  2.31it/s]Loading trainS:  77%|███████▋  | 206/266 [01:54<00:27,  2.16it/s]Loading trainS:  78%|███████▊  | 207/266 [01:55<00:27,  2.17it/s]Loading trainS:  78%|███████▊  | 208/266 [01:55<00:25,  2.24it/s]Loading trainS:  79%|███████▊  | 209/266 [01:55<00:23,  2.46it/s]Loading trainS:  79%|███████▉  | 210/266 [01:56<00:21,  2.57it/s]Loading trainS:  79%|███████▉  | 211/266 [01:56<00:24,  2.28it/s]Loading trainS:  80%|███████▉  | 212/266 [01:57<00:23,  2.32it/s]Loading trainS:  80%|████████  | 213/266 [01:57<00:22,  2.33it/s]Loading trainS:  80%|████████  | 214/266 [01:57<00:21,  2.42it/s]Loading trainS:  81%|████████  | 215/266 [01:58<00:21,  2.38it/s]Loading trainS:  81%|████████  | 216/266 [01:58<00:21,  2.31it/s]Loading trainS:  82%|████████▏ | 217/266 [01:59<00:20,  2.36it/s]Loading trainS:  82%|████████▏ | 218/266 [01:59<00:20,  2.33it/s]Loading trainS:  82%|████████▏ | 219/266 [02:00<00:23,  2.01it/s]Loading trainS:  83%|████████▎ | 220/266 [02:00<00:21,  2.16it/s]Loading trainS:  83%|████████▎ | 221/266 [02:01<00:19,  2.33it/s]Loading trainS:  83%|████████▎ | 222/266 [02:01<00:18,  2.33it/s]Loading trainS:  84%|████████▍ | 223/266 [02:01<00:17,  2.48it/s]Loading trainS:  84%|████████▍ | 224/266 [02:02<00:19,  2.18it/s]Loading trainS:  85%|████████▍ | 225/266 [02:02<00:18,  2.26it/s]Loading trainS:  85%|████████▍ | 226/266 [02:03<00:16,  2.43it/s]Loading trainS:  85%|████████▌ | 227/266 [02:03<00:15,  2.50it/s]Loading trainS:  86%|████████▌ | 228/266 [02:03<00:14,  2.60it/s]Loading trainS:  86%|████████▌ | 229/266 [02:04<00:14,  2.47it/s]Loading trainS:  86%|████████▋ | 230/266 [02:04<00:14,  2.51it/s]Loading trainS:  87%|████████▋ | 231/266 [02:05<00:15,  2.32it/s]Loading trainS:  87%|████████▋ | 232/266 [02:05<00:13,  2.45it/s]Loading trainS:  88%|████████▊ | 233/266 [02:06<00:14,  2.21it/s]Loading trainS:  88%|████████▊ | 234/266 [02:06<00:14,  2.19it/s]Loading trainS:  88%|████████▊ | 235/266 [02:06<00:13,  2.31it/s]Loading trainS:  89%|████████▊ | 236/266 [02:07<00:12,  2.32it/s]Loading trainS:  89%|████████▉ | 237/266 [02:07<00:12,  2.23it/s]Loading trainS:  89%|████████▉ | 238/266 [02:08<00:12,  2.26it/s]Loading trainS:  90%|████████▉ | 239/266 [02:08<00:13,  2.05it/s]Loading trainS:  90%|█████████ | 240/266 [02:09<00:13,  1.99it/s]Loading trainS:  91%|█████████ | 241/266 [02:09<00:11,  2.14it/s]Loading trainS:  91%|█████████ | 242/266 [02:10<00:11,  2.01it/s]Loading trainS:  91%|█████████▏| 243/266 [02:10<00:10,  2.11it/s]Loading trainS:  92%|█████████▏| 244/266 [02:11<00:09,  2.23it/s]Loading trainS:  92%|█████████▏| 245/266 [02:11<00:09,  2.28it/s]Loading trainS:  92%|█████████▏| 246/266 [02:12<00:08,  2.31it/s]Loading trainS:  93%|█████████▎| 247/266 [02:12<00:07,  2.46it/s]Loading trainS:  93%|█████████▎| 248/266 [02:12<00:07,  2.49it/s]Loading trainS:  94%|█████████▎| 249/266 [02:13<00:06,  2.44it/s]Loading trainS:  94%|█████████▍| 250/266 [02:13<00:07,  2.28it/s]Loading trainS:  94%|█████████▍| 251/266 [02:14<00:06,  2.23it/s]Loading trainS:  95%|█████████▍| 252/266 [02:14<00:06,  2.12it/s]Loading trainS:  95%|█████████▌| 253/266 [02:15<00:06,  2.02it/s]Loading trainS:  95%|█████████▌| 254/266 [02:15<00:05,  2.09it/s]Loading trainS:  96%|█████████▌| 255/266 [02:16<00:05,  2.08it/s]Loading trainS:  96%|█████████▌| 256/266 [02:16<00:04,  2.06it/s]Loading trainS:  97%|█████████▋| 257/266 [02:17<00:04,  2.06it/s]Loading trainS:  97%|█████████▋| 258/266 [02:17<00:04,  1.92it/s]Loading trainS:  97%|█████████▋| 259/266 [02:18<00:03,  1.92it/s]Loading trainS:  98%|█████████▊| 260/266 [02:18<00:03,  1.83it/s]Loading trainS:  98%|█████████▊| 261/266 [02:19<00:02,  1.82it/s]Loading trainS:  98%|█████████▊| 262/266 [02:19<00:02,  1.92it/s]Loading trainS:  99%|█████████▉| 263/266 [02:20<00:01,  2.18it/s]Loading trainS:  99%|█████████▉| 264/266 [02:20<00:00,  2.39it/s]Loading trainS: 100%|█████████▉| 265/266 [02:20<00:00,  2.63it/s]Loading trainS: 100%|██████████| 266/266 [02:21<00:00,  2.60it/s]Loading trainS: 100%|██████████| 266/266 [02:21<00:00,  1.88it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  2.20it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:00,  2.24it/s]Loading testS:  75%|███████▌  | 3/4 [00:01<00:00,  2.07it/s]Loading testS: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]Loading testS: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]----------+++ 
CrossVal ['a']
TypeExperiment 9
CrossVal ['a']
(0/4) test vimp2_A_CSFn2
(1/4) test vimp2_ANON967_CSFn2
(2/4) test vimp2_B_CSFn2
(3/4) test vimp2_E_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from WMn   /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 82s - loss: 0.0957 - acc: 0.9896 - mDice: 0.8140 - val_loss: 0.0999 - val_acc: 0.9919 - val_mDice: 0.4486

Epoch 00001: val_mDice improved from -inf to 0.44859, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 77s - loss: 0.0640 - acc: 0.9931 - mDice: 0.8755 - val_loss: 0.0716 - val_acc: 0.9908 - val_mDice: 0.4716

Epoch 00002: val_mDice improved from 0.44859 to 0.47159, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 77s - loss: 0.0555 - acc: 0.9939 - mDice: 0.8920 - val_loss: 0.3076 - val_acc: 0.9800 - val_mDice: 0.3962

Epoch 00003: val_mDice did not improve from 0.47159
Epoch 4/300
 - 77s - loss: 0.0520 - acc: 0.9943 - mDice: 0.8989 - val_loss: 0.1942 - val_acc: 0.9893 - val_mDice: 0.4613

Epoch 00004: val_mDice did not improve from 0.47159
Epoch 5/300
 - 79s - loss: 0.0482 - acc: 0.9946 - mDice: 0.9064 - val_loss: 0.1078 - val_acc: 0.9934 - val_mDice: 0.5066

Epoch 00005: val_mDice improved from 0.47159 to 0.50661, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 79s - loss: 0.0458 - acc: 0.9948 - mDice: 0.9109 - val_loss: 0.0353 - val_acc: 0.9936 - val_mDice: 0.4954

Epoch 00006: val_mDice did not improve from 0.50661
Epoch 7/300
 - 80s - loss: 0.0459 - acc: 0.9949 - mDice: 0.9108 - val_loss: 0.0431 - val_acc: 0.9940 - val_mDice: 0.4795

Epoch 00007: val_mDice did not improve from 0.50661
Epoch 8/300
 - 79s - loss: 0.0427 - acc: 0.9952 - mDice: 0.9170 - val_loss: 0.0366 - val_acc: 0.9942 - val_mDice: 0.4925

Epoch 00008: val_mDice did not improve from 0.50661
Epoch 9/300
 - 80s - loss: 0.0419 - acc: 0.9952 - mDice: 0.9186 - val_loss: 0.0717 - val_acc: 0.9936 - val_mDice: 0.5009

Epoch 00009: val_mDice did not improve from 0.50661
Epoch 10/300
 - 80s - loss: 0.0398 - acc: 0.9954 - mDice: 0.9226 - val_loss: -2.1922e-02 - val_acc: 0.9933 - val_mDice: 0.4701

Epoch 00010: val_mDice did not improve from 0.50661
Epoch 11/300
 - 79s - loss: 0.0401 - acc: 0.9954 - mDice: 0.9220 - val_loss: 0.1019 - val_acc: 0.9915 - val_mDice: 0.4887

Epoch 00011: val_mDice did not improve from 0.50661
Epoch 12/300
 - 79s - loss: 0.0383 - acc: 0.9956 - mDice: 0.9256 - val_loss: 0.0764 - val_acc: 0.9940 - val_mDice: 0.5037

Epoch 00012: val_mDice did not improve from 0.50661
Epoch 13/300
 - 79s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9274 - val_loss: 0.0270 - val_acc: 0.9939 - val_mDice: 0.5121

Epoch 00013: val_mDice improved from 0.50661 to 0.51206, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 79s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9296 - val_loss: 0.0711 - val_acc: 0.9940 - val_mDice: 0.5017

Epoch 00014: val_mDice did not improve from 0.51206
Epoch 15/300
 - 80s - loss: 0.0368 - acc: 0.9956 - mDice: 0.9286 - val_loss: 0.0389 - val_acc: 0.9938 - val_mDice: 0.4940

Epoch 00015: val_mDice did not improve from 0.51206
Epoch 16/300
 - 79s - loss: 0.0359 - acc: 0.9957 - mDice: 0.9304 - val_loss: 0.1132 - val_acc: 0.9939 - val_mDice: 0.4960

Epoch 00016: val_mDice did not improve from 0.51206
Epoch 17/300
 - 79s - loss: 0.0346 - acc: 0.9958 - mDice: 0.9329 - val_loss: -9.0228e-04 - val_acc: 0.9942 - val_mDice: 0.4899

Epoch 00017: val_mDice did not improve from 0.51206
Epoch 18/300
 - 79s - loss: 0.0346 - acc: 0.9959 - mDice: 0.9327 - val_loss: 0.1643 - val_acc: 0.9933 - val_mDice: 0.5059

Epoch 00018: val_mDice did not improve from 0.51206
Epoch 19/300
 - 80s - loss: 0.0343 - acc: 0.9959 - mDice: 0.9334 - val_loss: 0.0037 - val_acc: 0.9937 - val_mDice: 0.4803

Epoch 00019: val_mDice did not improve from 0.51206
Epoch 20/300
 - 79s - loss: 0.0333 - acc: 0.9959 - mDice: 0.9355 - val_loss: 0.2233 - val_acc: 0.9919 - val_mDice: 0.4802

Epoch 00020: val_mDice did not improve from 0.51206
Epoch 21/300
 - 79s - loss: 0.0335 - acc: 0.9960 - mDice: 0.9349 - val_loss: 0.0491 - val_acc: 0.9925 - val_mDice: 0.4704

Epoch 00021: val_mDice did not improve from 0.51206
Epoch 22/300
 - 79s - loss: 0.0322 - acc: 0.9960 - mDice: 0.9374 - val_loss: -4.4497e-02 - val_acc: 0.9941 - val_mDice: 0.4986

Epoch 00022: val_mDice did not improve from 0.51206
Epoch 23/300
 - 80s - loss: 0.0328 - acc: 0.9960 - mDice: 0.9363 - val_loss: -6.6090e-04 - val_acc: 0.9942 - val_mDice: 0.4889

Epoch 00023: val_mDice did not improve from 0.51206
Epoch 24/300
 - 80s - loss: 0.0333 - acc: 0.9960 - mDice: 0.9354 - val_loss: 0.0363 - val_acc: 0.9940 - val_mDice: 0.4932

Epoch 00024: val_mDice did not improve from 0.51206
Epoch 25/300
 - 79s - loss: 0.0327 - acc: 0.9961 - mDice: 0.9365 - val_loss: -1.0214e-02 - val_acc: 0.9939 - val_mDice: 0.5080

Epoch 00025: val_mDice did not improve from 0.51206
Epoch 26/300
 - 80s - loss: 0.0322 - acc: 0.9960 - mDice: 0.9375 - val_loss: 0.0043 - val_acc: 0.9939 - val_mDice: 0.4791

Epoch 00026: val_mDice did not improve from 0.51206
Epoch 27/300
 - 80s - loss: 0.0324 - acc: 0.9961 - mDice: 0.9371 - val_loss: -5.2688e-03 - val_acc: 0.9943 - val_mDice: 0.4985

Epoch 00027: val_mDice did not improve from 0.51206
Epoch 28/300
 - 80s - loss: 0.0311 - acc: 0.9961 - mDice: 0.9396 - val_loss: -7.6872e-03 - val_acc: 0.9941 - val_mDice: 0.5032

Epoch 00028: val_mDice did not improve from 0.51206

Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 29/300
 - 81s - loss: 0.0297 - acc: 0.9963 - mDice: 0.9423 - val_loss: -2.5447e-03 - val_acc: 0.9941 - val_mDice: 0.4947

Epoch 00029: val_mDice did not improve from 0.51206
Epoch 30/300
 - 80s - loss: 0.0288 - acc: 0.9963 - mDice: 0.9441 - val_loss: 0.0723 - val_acc: 0.9936 - val_mDice: 0.4997

Epoch 00030: val_mDice did not improve from 0.51206
Epoch 31/300
 - 81s - loss: 0.0293 - acc: 0.9963 - mDice: 0.9432 - val_loss: 0.0335 - val_acc: 0.9937 - val_mDice: 0.4991

Epoch 00031: val_mDice did not improve from 0.51206
Epoch 32/300
 - 80s - loss: 0.0289 - acc: 0.9963 - mDice: 0.9440 - val_loss: -4.7593e-03 - val_acc: 0.9943 - val_mDice: 0.4972

Epoch 00032: val_mDice did not improve from 0.51206
Epoch 33/300
 - 80s - loss: 0.0285 - acc: 0.9964 - mDice: 0.9448 - val_loss: 0.0324 - val_acc: 0.9935 - val_mDice: 0.5021

Epoch 00033: val_mDice did not improve from 0.51206
Epoch 34/300
 - 79s - loss: 0.0293 - acc: 0.9964 - mDice: 0.9431 - val_loss: -3.2453e-03 - val_acc: 0.9942 - val_mDice: 0.4942

Epoch 00034: val_mDice did not improve from 0.51206
Epoch 35/300
 - 80s - loss: 0.0282 - acc: 0.9964 - mDice: 0.9454 - val_loss: -4.0119e-02 - val_acc: 0.9942 - val_mDice: 0.4897

Epoch 00035: val_mDice did not improve from 0.51206
Epoch 36/300
 - 80s - loss: 0.0282 - acc: 0.9964 - mDice: 0.9453 - val_loss: 0.0384 - val_acc: 0.9926 - val_mDice: 0.4901

Epoch 00036: val_mDice did not improve from 0.51206
Epoch 37/300
 - 79s - loss: 0.0278 - acc: 0.9964 - mDice: 0.9461 - val_loss: -4.7261e-03 - val_acc: 0.9939 - val_mDice: 0.4985

Epoch 00037: val_mDice did not improve from 0.51206
Epoch 38/300
 - 79s - loss: 0.0277 - acc: 0.9964 - mDice: 0.9464 - val_loss: -3.5563e-02 - val_acc: 0.9941 - val_mDice: 0.4806

Epoch 00038: val_mDice did not improve from 0.51206
Epoch 39/300
 - 79s - loss: 0.0277 - acc: 0.9965 - mDice: 0.9463 - val_loss: -2.1227e-03 - val_acc: 0.9938 - val_mDice: 0.4922

Epoch 00039: val_mDice did not improve from 0.51206
Epoch 40/300
 - 80s - loss: 0.0270 - acc: 0.9964 - mDice: 0.9477 - val_loss: 0.0291 - val_acc: 0.9938 - val_mDice: 0.5005

Epoch 00040: val_mDice did not improve from 0.51206
Epoch 41/300
 - 80s - loss: 0.0271 - acc: 0.9964 - mDice: 0.9475 - val_loss: -4.5342e-03 - val_acc: 0.9941 - val_mDice: 0.4969

Epoch 00041: val_mDice did not improve from 0.51206
Epoch 42/300
 - 80s - loss: 0.0279 - acc: 0.9965 - mDice: 0.9460 - val_loss: 0.0015 - val_acc: 0.9938 - val_mDice: 0.4847

Epoch 00042: val_mDice did not improve from 0.51206
Epoch 43/300
 - 80s - loss: 0.0268 - acc: 0.9965 - mDice: 0.9481 - val_loss: 0.0723 - val_acc: 0.9937 - val_mDice: 0.4997

Epoch 00043: val_mDice did not improve from 0.51206

Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 44/300
 - 80s - loss: 0.0259 - acc: 0.9965 - mDice: 0.9498 - val_loss: 0.0328 - val_acc: 0.9937 - val_mDice: 0.5007

Epoch 00044: val_mDice did not improve from 0.51206
Epoch 45/300
 - 79s - loss: 0.0259 - acc: 0.9966 - mDice: 0.9499 - val_loss: 0.0361 - val_acc: 0.9943 - val_mDice: 0.4933

Epoch 00045: val_mDice did not improve from 0.51206
Epoch 46/300
 - 80s - loss: 0.0260 - acc: 0.9966 - mDice: 0.9496 - val_loss: 0.0699 - val_acc: 0.9937 - val_mDice: 0.5045

Epoch 00046: val_mDice did not improve from 0.51206
Epoch 47/300
 - 79s - loss: 0.0256 - acc: 0.9966 - mDice: 0.9504 - val_loss: -8.7727e-03 - val_acc: 0.9940 - val_mDice: 0.5054

Epoch 00047: val_mDice did not improve from 0.51206
Epoch 48/300
 - 80s - loss: 0.0253 - acc: 0.9966 - mDice: 0.9511 - val_loss: 0.0361 - val_acc: 0.9941 - val_mDice: 0.4938

Epoch 00048: val_mDice did not improve from 0.51206
Epoch 49/300
 - 80s - loss: 0.0255 - acc: 0.9966 - mDice: 0.9505 - val_loss: 0.0316 - val_acc: 0.9935 - val_mDice: 0.5031

Epoch 00049: val_mDice did not improve from 0.51206
Epoch 50/300
 - 80s - loss: 0.0255 - acc: 0.9966 - mDice: 0.9506 - val_loss: 0.0750 - val_acc: 0.9941 - val_mDice: 0.4924

Epoch 00050: val_mDice did not improve from 0.51206
Epoch 51/300
 - 80s - loss: 0.0253 - acc: 0.9966 - mDice: 0.9510 - val_loss: 0.0359 - val_acc: 0.9942 - val_mDice: 0.4942

Epoch 00051: val_mDice did not improve from 0.51206
Epoch 52/300
 - 80s - loss: 0.0258 - acc: 0.9966 - mDice: 0.9501 - val_loss: 0.0313 - val_acc: 0.9938 - val_mDice: 0.5036

Epoch 00052: val_mDice did not improve from 0.51206
Epoch 53/300
 - 80s - loss: 0.0253 - acc: 0.9966 - mDice: 0.9510 - val_loss: -6.2407e-03 - val_acc: 0.9942 - val_mDice: 0.5002

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.34it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.77it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.28it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.80it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.33it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:04,  4.14it/s]predicting train subjects:   1%|          | 2/266 [00:00<00:58,  4.53it/s]predicting train subjects:   1%|          | 3/266 [00:00<01:04,  4.07it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:10,  3.71it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:07,  3.84it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<01:02,  4.19it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:58,  4.44it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:54,  4.70it/s]predicting train subjects:   3%|▎         | 9/266 [00:02<00:52,  4.89it/s]predicting train subjects:   4%|▍         | 10/266 [00:02<00:50,  5.04it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:49,  5.17it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:48,  5.26it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:48,  5.26it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:47,  5.29it/s]predicting train subjects:   6%|▌         | 15/266 [00:03<00:47,  5.31it/s]predicting train subjects:   6%|▌         | 16/266 [00:03<00:47,  5.30it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:46,  5.33it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:46,  5.35it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:45,  5.39it/s]predicting train subjects:   8%|▊         | 20/266 [00:04<00:45,  5.42it/s]predicting train subjects:   8%|▊         | 21/266 [00:04<00:45,  5.42it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:44,  5.48it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:43,  5.54it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:42,  5.65it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:43,  5.60it/s]predicting train subjects:  10%|▉         | 26/266 [00:05<00:42,  5.66it/s]predicting train subjects:  10%|█         | 27/266 [00:05<00:41,  5.71it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:41,  5.75it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:41,  5.77it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:40,  5.81it/s]predicting train subjects:  12%|█▏        | 31/266 [00:05<00:40,  5.80it/s]predicting train subjects:  12%|█▏        | 32/266 [00:06<00:40,  5.81it/s]predicting train subjects:  12%|█▏        | 33/266 [00:06<00:40,  5.82it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:39,  5.82it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:39,  5.86it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:39,  5.89it/s]predicting train subjects:  14%|█▍        | 37/266 [00:07<00:38,  5.90it/s]predicting train subjects:  14%|█▍        | 38/266 [00:07<00:38,  5.89it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:38,  5.89it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:38,  5.86it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:38,  5.82it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:36,  6.12it/s]predicting train subjects:  16%|█▌        | 43/266 [00:07<00:35,  6.36it/s]predicting train subjects:  17%|█▋        | 44/266 [00:08<00:34,  6.52it/s]predicting train subjects:  17%|█▋        | 45/266 [00:08<00:33,  6.66it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:32,  6.76it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:31,  6.85it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:31,  6.89it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:31,  6.92it/s]predicting train subjects:  19%|█▉        | 50/266 [00:08<00:31,  6.94it/s]predicting train subjects:  19%|█▉        | 51/266 [00:09<00:30,  6.97it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:31,  6.90it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:30,  6.90it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:30,  6.95it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:30,  7.00it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:29,  7.05it/s]predicting train subjects:  21%|██▏       | 57/266 [00:09<00:29,  7.09it/s]predicting train subjects:  22%|██▏       | 58/266 [00:10<00:29,  7.11it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:29,  7.13it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:29,  7.07it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:29,  7.04it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:28,  7.04it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:28,  7.02it/s]predicting train subjects:  24%|██▍       | 64/266 [00:10<00:28,  7.02it/s]predicting train subjects:  24%|██▍       | 65/266 [00:11<00:28,  7.00it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:28,  6.97it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:28,  6.96it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:28,  6.95it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:28,  6.98it/s]predicting train subjects:  26%|██▋       | 70/266 [00:11<00:28,  6.97it/s]predicting train subjects:  27%|██▋       | 71/266 [00:11<00:28,  6.96it/s]predicting train subjects:  27%|██▋       | 72/266 [00:12<00:27,  6.95it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:27,  6.92it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:27,  6.93it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:27,  6.92it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:27,  6.90it/s]predicting train subjects:  29%|██▉       | 77/266 [00:12<00:27,  6.90it/s]predicting train subjects:  29%|██▉       | 78/266 [00:13<00:28,  6.56it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:29,  6.34it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:29,  6.21it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:32,  5.78it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:31,  5.81it/s]predicting train subjects:  31%|███       | 83/266 [00:13<00:31,  5.84it/s]predicting train subjects:  32%|███▏      | 84/266 [00:14<00:30,  5.88it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:31,  5.83it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:30,  5.83it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:30,  5.87it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:30,  5.91it/s]predicting train subjects:  33%|███▎      | 89/266 [00:14<00:30,  5.79it/s]predicting train subjects:  34%|███▍      | 90/266 [00:15<00:30,  5.85it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:29,  5.88it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:29,  5.90it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:29,  5.89it/s]predicting train subjects:  35%|███▌      | 94/266 [00:15<00:29,  5.89it/s]predicting train subjects:  36%|███▌      | 95/266 [00:15<00:28,  5.90it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:36,  4.68it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:38,  4.43it/s]predicting train subjects:  37%|███▋      | 98/266 [00:16<00:37,  4.53it/s]predicting train subjects:  37%|███▋      | 99/266 [00:16<00:39,  4.25it/s]predicting train subjects:  38%|███▊      | 100/266 [00:17<00:34,  4.79it/s]predicting train subjects:  38%|███▊      | 101/266 [00:17<00:31,  5.25it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:29,  5.60it/s]predicting train subjects:  39%|███▊      | 103/266 [00:17<00:27,  5.92it/s]predicting train subjects:  39%|███▉      | 104/266 [00:17<00:26,  6.18it/s]predicting train subjects:  39%|███▉      | 105/266 [00:17<00:25,  6.35it/s]predicting train subjects:  40%|███▉      | 106/266 [00:18<00:24,  6.46it/s]predicting train subjects:  40%|████      | 107/266 [00:18<00:24,  6.57it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:23,  6.61it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:23,  6.62it/s]predicting train subjects:  41%|████▏     | 110/266 [00:18<00:23,  6.63it/s]predicting train subjects:  42%|████▏     | 111/266 [00:18<00:23,  6.68it/s]predicting train subjects:  42%|████▏     | 112/266 [00:18<00:23,  6.68it/s]predicting train subjects:  42%|████▏     | 113/266 [00:19<00:22,  6.69it/s]predicting train subjects:  43%|████▎     | 114/266 [00:19<00:22,  6.70it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:22,  6.69it/s]predicting train subjects:  44%|████▎     | 116/266 [00:19<00:22,  6.65it/s]predicting train subjects:  44%|████▍     | 117/266 [00:19<00:22,  6.55it/s]predicting train subjects:  44%|████▍     | 118/266 [00:19<00:22,  6.52it/s]predicting train subjects:  45%|████▍     | 119/266 [00:20<00:24,  5.97it/s]predicting train subjects:  45%|████▌     | 120/266 [00:20<00:24,  5.89it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:24,  5.85it/s]predicting train subjects:  46%|████▌     | 122/266 [00:20<00:24,  5.81it/s]predicting train subjects:  46%|████▌     | 123/266 [00:20<00:24,  5.79it/s]predicting train subjects:  47%|████▋     | 124/266 [00:20<00:24,  5.75it/s]predicting train subjects:  47%|████▋     | 125/266 [00:21<00:24,  5.71it/s]predicting train subjects:  47%|████▋     | 126/266 [00:21<00:25,  5.60it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:25,  5.54it/s]predicting train subjects:  48%|████▊     | 128/266 [00:21<00:24,  5.60it/s]predicting train subjects:  48%|████▊     | 129/266 [00:21<00:24,  5.63it/s]predicting train subjects:  49%|████▉     | 130/266 [00:21<00:24,  5.66it/s]predicting train subjects:  49%|████▉     | 131/266 [00:22<00:23,  5.67it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:23,  5.70it/s]predicting train subjects:  50%|█████     | 133/266 [00:22<00:23,  5.66it/s]predicting train subjects:  50%|█████     | 134/266 [00:22<00:23,  5.67it/s]predicting train subjects:  51%|█████     | 135/266 [00:22<00:22,  5.71it/s]predicting train subjects:  51%|█████     | 136/266 [00:23<00:22,  5.66it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:23<00:22,  5.85it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:21,  5.93it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:23<00:21,  5.96it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:23<00:21,  5.94it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:23<00:20,  5.95it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:24<00:20,  5.96it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:24<00:20,  6.02it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:20,  6.08it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:24<00:19,  6.13it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:24<00:19,  6.12it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:24<00:19,  6.15it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:25<00:19,  6.04it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:25<00:19,  5.95it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:19,  6.02it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:25<00:19,  5.91it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:25<00:18,  6.01it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:25<00:18,  6.08it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:25<00:18,  6.12it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:26<00:17,  6.51it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:26<00:16,  6.69it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:15,  6.88it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:26<00:15,  7.07it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:26<00:14,  7.26it/s]predicting train subjects:  60%|██████    | 160/266 [00:26<00:14,  7.38it/s]predicting train subjects:  61%|██████    | 161/266 [00:26<00:14,  7.41it/s]predicting train subjects:  61%|██████    | 162/266 [00:27<00:13,  7.45it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:27<00:14,  7.35it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:13,  7.44it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:27<00:13,  7.45it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:27<00:13,  7.50it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:27<00:13,  7.56it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:27<00:12,  7.60it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:27<00:12,  7.58it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:28<00:12,  7.49it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:28<00:12,  7.42it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:12,  7.38it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:28<00:12,  7.21it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:28<00:13,  7.04it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:28<00:13,  6.96it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:28<00:13,  6.89it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:29<00:12,  6.87it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:12,  6.85it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:29<00:12,  6.85it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:29<00:12,  6.85it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:29<00:12,  6.68it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:29<00:12,  6.60it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:30<00:12,  6.64it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:30<00:12,  6.68it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:12,  6.71it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:30<00:11,  6.71it/s]predicting train subjects:  70%|███████   | 187/266 [00:30<00:11,  6.73it/s]predicting train subjects:  71%|███████   | 188/266 [00:30<00:11,  6.72it/s]predicting train subjects:  71%|███████   | 189/266 [00:30<00:11,  6.71it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:31<00:11,  6.62it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:31<00:11,  6.62it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:31<00:14,  5.02it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:31<00:13,  5.44it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:31<00:12,  5.56it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:32<00:12,  5.86it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:32<00:11,  5.97it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:32<00:11,  6.17it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:32<00:10,  6.32it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:32<00:10,  6.42it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:32<00:10,  6.39it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:32<00:10,  6.32it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:33<00:10,  6.30it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:33<00:10,  6.27it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:33<00:09,  6.28it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:33<00:09,  6.37it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:33<00:09,  6.47it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:33<00:09,  6.52it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:34<00:08,  6.59it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:34<00:08,  6.58it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:34<00:08,  6.36it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:34<00:08,  6.24it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:34<00:08,  6.19it/s]predicting train subjects:  80%|████████  | 213/266 [00:34<00:08,  6.26it/s]predicting train subjects:  80%|████████  | 214/266 [00:34<00:08,  6.39it/s]predicting train subjects:  81%|████████  | 215/266 [00:35<00:07,  6.43it/s]predicting train subjects:  81%|████████  | 216/266 [00:35<00:07,  6.48it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:35<00:07,  6.66it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:35<00:07,  6.75it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:35<00:06,  6.85it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:35<00:06,  6.91it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:35<00:06,  6.88it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:36<00:06,  6.87it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:36<00:06,  6.80it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:36<00:06,  6.73it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:36<00:06,  6.60it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:36<00:06,  6.50it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:36<00:05,  6.58it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:37<00:05,  6.72it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:37<00:05,  6.71it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:37<00:05,  6.66it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:37<00:05,  6.56it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:37<00:05,  6.54it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:37<00:05,  6.59it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:37<00:04,  6.51it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:38<00:04,  6.56it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:38<00:04,  6.24it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:38<00:04,  6.32it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:38<00:04,  6.40it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:38<00:04,  6.40it/s]predicting train subjects:  90%|█████████ | 240/266 [00:38<00:04,  6.38it/s]predicting train subjects:  91%|█████████ | 241/266 [00:39<00:03,  6.33it/s]predicting train subjects:  91%|█████████ | 242/266 [00:39<00:03,  6.30it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:39<00:03,  6.44it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:39<00:03,  6.29it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:39<00:03,  6.29it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:39<00:03,  6.42it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:40<00:02,  6.38it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:40<00:02,  6.46it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:40<00:02,  6.26it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:40<00:02,  6.11it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:40<00:02,  5.99it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:40<00:02,  5.96it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:41<00:02,  5.82it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:41<00:02,  5.74it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:41<00:01,  5.68it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:41<00:01,  5.68it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:41<00:01,  5.75it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:41<00:01,  5.81it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:42<00:01,  5.73it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:42<00:01,  5.67it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:42<00:00,  5.59it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:42<00:00,  5.68it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:42<00:00,  5.70it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:42<00:00,  5.75it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:43<00:00,  5.79it/s]predicting train subjects: 100%|██████████| 266/266 [00:43<00:00,  5.76it/s]predicting train subjects: 100%|██████████| 266/266 [00:43<00:00,  6.14it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  6.37it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  6.49it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  6.56it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.42it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.49it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:49,  5.37it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:49,  5.38it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:45,  5.72it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:43,  6.01it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:44,  5.80it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<00:45,  5.65it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<00:46,  5.54it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:46,  5.58it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:46,  5.57it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:45,  5.57it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:46,  5.54it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:02<00:46,  5.52it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:02<00:45,  5.54it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:02<00:45,  5.56it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:45,  5.50it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:45,  5.53it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:03<00:44,  5.56it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:03<00:44,  5.59it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:03<00:44,  5.60it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:03<00:43,  5.61it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:03<00:44,  5.56it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:44,  5.52it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:04<00:44,  5.51it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:04<00:44,  5.49it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:04<00:43,  5.49it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:04<00:43,  5.53it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:04<00:43,  5.52it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:05<00:42,  5.54it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:05<00:43,  5.43it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:05<00:42,  5.50it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:05<00:42,  5.47it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:05<00:42,  5.56it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:05<00:41,  5.63it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:06<00:41,  5.59it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:06<00:42,  5.46it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:06<00:42,  5.46it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:06<00:41,  5.54it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:06<00:40,  5.61it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:07<00:40,  5.59it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:07<00:40,  5.64it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:07<00:39,  5.66it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:07<00:37,  5.94it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:07<00:36,  6.16it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:07<00:34,  6.37it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:07<00:33,  6.53it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:08<00:33,  6.60it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:08<00:33,  6.56it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:08<00:32,  6.69it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:08<00:32,  6.78it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:08<00:31,  6.86it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:08<00:31,  6.92it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:08<00:30,  6.91it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:09<00:31,  6.87it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:09<00:30,  6.91it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:09<00:30,  6.90it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:09<00:30,  6.92it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:09<00:30,  6.95it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:09<00:30,  6.93it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:09<00:29,  6.91it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:10<00:30,  6.83it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:10<00:30,  6.78it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:10<00:30,  6.78it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:10<00:29,  6.77it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:10<00:29,  6.78it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:10<00:29,  6.82it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:11<00:29,  6.82it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:11<00:29,  6.81it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:11<00:28,  6.85it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:11<00:28,  6.80it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:11<00:29,  6.75it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:11<00:28,  6.73it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:11<00:29,  6.69it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:12<00:28,  6.71it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:12<00:28,  6.66it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:12<00:28,  6.59it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:12<00:29,  6.53it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:12<00:28,  6.61it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:12<00:29,  6.31it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:13<00:30,  6.16it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:13<00:31,  5.98it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:13<00:32,  5.63it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:13<00:32,  5.68it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:13<00:32,  5.65it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:13<00:32,  5.65it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:14<00:32,  5.56it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:14<00:31,  5.63it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:14<00:31,  5.60it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:14<00:31,  5.64it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:14<00:31,  5.56it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:14<00:31,  5.52it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:15<00:31,  5.53it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:15<00:31,  5.60it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:15<00:30,  5.61it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:15<00:30,  5.55it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:15<00:30,  5.64it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:16<00:28,  5.94it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:16<00:28,  5.83it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:16<00:28,  5.92it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:16<00:26,  6.31it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:16<00:25,  6.46it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:16<00:25,  6.40it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:16<00:25,  6.38it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:17<00:25,  6.37it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:17<00:25,  6.45it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:17<00:25,  6.32it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:17<00:25,  6.35it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:17<00:25,  6.34it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:17<00:24,  6.41it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:18<00:24,  6.48it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:18<00:24,  6.48it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:18<00:24,  6.43it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:18<00:24,  6.28it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:18<00:24,  6.21it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:18<00:24,  6.33it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:19<00:23,  6.32it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:19<00:23,  6.38it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:19<00:23,  6.44it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:19<00:22,  6.50it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:19<00:23,  6.18it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:19<00:24,  6.03it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:19<00:24,  5.93it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:20<00:24,  5.84it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:20<00:24,  5.79it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:20<00:24,  5.75it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:20<00:25,  5.61it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:20<00:25,  5.50it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:21<00:25,  5.51it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:21<00:24,  5.53it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:21<00:24,  5.57it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:21<00:24,  5.59it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:21<00:24,  5.59it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:21<00:23,  5.59it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:22<00:23,  5.59it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:22<00:23,  5.57it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:22<00:23,  5.58it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:22<00:23,  5.58it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:22<00:22,  5.72it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:23<00:22,  5.80it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:23<00:21,  5.90it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:23<00:21,  5.98it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:23<00:20,  6.04it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:23<00:20,  5.97it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:23<00:20,  5.93it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:24<00:20,  5.85it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:24<00:20,  5.86it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:24<00:20,  5.98it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:24<00:20,  5.86it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:24<00:20,  5.81it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:24<00:19,  5.90it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:25<00:19,  5.91it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:25<00:19,  5.93it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:25<00:19,  5.98it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:25<00:18,  6.04it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:25<00:18,  5.98it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:25<00:17,  6.36it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:25<00:16,  6.70it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:26<00:15,  6.95it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:26<00:15,  6.89it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:26<00:15,  6.87it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:26<00:15,  6.78it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:26<00:15,  6.94it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:26<00:14,  7.10it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:26<00:14,  7.23it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:27<00:14,  7.28it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:27<00:13,  7.35it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:27<00:13,  7.37it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:27<00:13,  7.38it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:27<00:13,  7.42it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:27<00:13,  7.41it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:27<00:13,  7.17it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:28<00:13,  7.23it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:28<00:12,  7.28it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:28<00:13,  7.14it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:28<00:13,  7.01it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:28<00:13,  6.96it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:28<00:12,  6.94it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:28<00:12,  6.87it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:29<00:12,  6.80it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:29<00:12,  6.76it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:29<00:13,  6.31it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:29<00:13,  6.40it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:29<00:13,  6.45it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:29<00:12,  6.53it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:30<00:12,  6.62it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:30<00:12,  6.60it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:30<00:12,  6.57it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:30<00:11,  6.60it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:30<00:11,  6.63it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:30<00:11,  6.50it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:30<00:11,  6.53it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:31<00:11,  6.60it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:31<00:11,  6.71it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:31<00:11,  6.61it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:31<00:11,  6.19it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:31<00:11,  6.37it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:31<00:11,  6.20it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:32<00:11,  6.12it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:32<00:10,  6.27it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:32<00:10,  6.22it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:32<00:10,  6.28it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:32<00:10,  6.36it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:32<00:10,  6.26it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:32<00:09,  6.37it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:33<00:09,  6.45it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:33<00:09,  6.31it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:33<00:09,  6.45it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:33<00:09,  6.43it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:33<00:08,  6.47it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:33<00:08,  6.39it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:34<00:08,  6.33it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:34<00:08,  6.36it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:34<00:08,  6.30it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:34<00:08,  6.34it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:34<00:08,  6.46it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:34<00:07,  6.61it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:34<00:07,  6.59it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:35<00:07,  6.63it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:35<00:07,  6.77it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:35<00:06,  6.86it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:35<00:06,  6.93it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:35<00:06,  6.98it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:35<00:06,  6.99it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:35<00:06,  7.01it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:36<00:06,  6.85it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:36<00:05,  6.90it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:36<00:05,  6.93it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:36<00:05,  6.96it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:36<00:05,  6.98it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:36<00:05,  7.02it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:37<00:05,  6.90it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:37<00:05,  6.69it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:37<00:05,  6.59it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:37<00:05,  6.50it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:37<00:04,  6.53it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:37<00:04,  6.61it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:37<00:04,  6.62it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:38<00:04,  6.65it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:38<00:04,  6.70it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:38<00:04,  6.69it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:38<00:03,  6.73it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:38<00:03,  6.74it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:38<00:03,  6.76it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:38<00:03,  6.62it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:39<00:03,  6.67it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:39<00:03,  6.69it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:39<00:02,  6.71it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:39<00:02,  6.74it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:39<00:02,  6.74it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:39<00:02,  6.44it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:40<00:02,  6.23it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:40<00:02,  6.08it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:40<00:02,  6.03it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:40<00:02,  5.93it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:40<00:02,  5.82it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:40<00:01,  5.87it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:41<00:01,  5.89it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:41<00:01,  5.92it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:41<00:01,  5.87it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:41<00:01,  5.82it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:41<00:01,  5.73it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:41<00:00,  5.70it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:42<00:00,  5.73it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:42<00:00,  5.78it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:42<00:00,  5.84it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:42<00:00,  5.82it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:42<00:00,  5.83it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:42<00:00,  6.21it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 73.35it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:03, 66.58it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:03, 65.23it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 63.94it/s]saving BB  train1-THALAMUS:  11%|█         | 28/266 [00:00<00:03, 65.36it/s]saving BB  train1-THALAMUS:  14%|█▎        | 36/266 [00:00<00:03, 66.80it/s]saving BB  train1-THALAMUS:  17%|█▋        | 44/266 [00:00<00:03, 68.27it/s]saving BB  train1-THALAMUS:  20%|█▉        | 52/266 [00:00<00:03, 69.34it/s]saving BB  train1-THALAMUS:  23%|██▎       | 60/266 [00:00<00:02, 72.21it/s]saving BB  train1-THALAMUS:  26%|██▌       | 68/266 [00:00<00:02, 72.53it/s]saving BB  train1-THALAMUS:  29%|██▉       | 77/266 [00:01<00:02, 74.80it/s]saving BB  train1-THALAMUS:  32%|███▏      | 85/266 [00:01<00:02, 72.86it/s]saving BB  train1-THALAMUS:  35%|███▍      | 93/266 [00:01<00:02, 71.63it/s]saving BB  train1-THALAMUS:  38%|███▊      | 101/266 [00:01<00:02, 72.19it/s]saving BB  train1-THALAMUS:  41%|████      | 109/266 [00:01<00:02, 72.68it/s]saving BB  train1-THALAMUS:  44%|████▍     | 117/266 [00:01<00:02, 72.52it/s]saving BB  train1-THALAMUS:  47%|████▋     | 125/266 [00:01<00:01, 72.71it/s]saving BB  train1-THALAMUS:  50%|█████     | 133/266 [00:01<00:01, 71.42it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 141/266 [00:01<00:01, 71.37it/s]saving BB  train1-THALAMUS:  56%|█████▌    | 149/266 [00:02<00:01, 71.84it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 157/266 [00:02<00:01, 72.52it/s]saving BB  train1-THALAMUS:  62%|██████▏   | 166/266 [00:02<00:01, 75.78it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 175/266 [00:02<00:01, 79.24it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 184/266 [00:02<00:01, 81.65it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 193/266 [00:02<00:00, 82.33it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 202/266 [00:02<00:00, 80.52it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 211/266 [00:02<00:00, 80.43it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 220/266 [00:02<00:00, 81.14it/s]saving BB  train1-THALAMUS:  86%|████████▌ | 229/266 [00:03<00:00, 80.60it/s]saving BB  train1-THALAMUS:  89%|████████▉ | 238/266 [00:03<00:00, 81.46it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 247/266 [00:03<00:00, 81.90it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 256/266 [00:03<00:00, 78.57it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 264/266 [00:03<00:00, 75.05it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 74.63it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 81.76it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 8/266 [00:00<00:03, 74.10it/s]saving BB  train1-THALAMUS Sagittal:   6%|▌         | 15/266 [00:00<00:03, 72.69it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 22/266 [00:00<00:03, 71.20it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 29/266 [00:00<00:03, 70.39it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 37/266 [00:00<00:03, 72.03it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 45/266 [00:00<00:02, 74.07it/s]saving BB  train1-THALAMUS Sagittal:  20%|██        | 54/266 [00:00<00:02, 76.69it/s]saving BB  train1-THALAMUS Sagittal:  24%|██▎       | 63/266 [00:00<00:02, 79.70it/s]saving BB  train1-THALAMUS Sagittal:  27%|██▋       | 72/266 [00:00<00:02, 82.19it/s]saving BB  train1-THALAMUS Sagittal:  30%|███       | 80/266 [00:01<00:02, 80.87it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 88/266 [00:01<00:02, 78.17it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 96/266 [00:01<00:02, 78.22it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▉      | 105/266 [00:01<00:02, 79.54it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 114/266 [00:01<00:01, 79.93it/s]saving BB  train1-THALAMUS Sagittal:  46%|████▌     | 122/266 [00:01<00:01, 78.65it/s]saving BB  train1-THALAMUS Sagittal:  49%|████▉     | 130/266 [00:01<00:01, 76.43it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 138/266 [00:01<00:01, 76.77it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▍    | 146/266 [00:01<00:01, 75.55it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 154/266 [00:02<00:01, 75.84it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████▏   | 163/266 [00:02<00:01, 79.46it/s]saving BB  train1-THALAMUS Sagittal:  65%|██████▍   | 172/266 [00:02<00:01, 81.73it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 182/266 [00:02<00:00, 84.44it/s]saving BB  train1-THALAMUS Sagittal:  72%|███████▏  | 192/266 [00:02<00:00, 86.74it/s]saving BB  train1-THALAMUS Sagittal:  76%|███████▌  | 201/266 [00:02<00:00, 84.75it/s]saving BB  train1-THALAMUS Sagittal:  79%|███████▉  | 210/266 [00:02<00:00, 83.60it/s]saving BB  train1-THALAMUS Sagittal:  82%|████████▏ | 219/266 [00:02<00:00, 83.95it/s]saving BB  train1-THALAMUS Sagittal:  86%|████████▌ | 228/266 [00:02<00:00, 83.78it/s]saving BB  train1-THALAMUS Sagittal:  89%|████████▉ | 237/266 [00:02<00:00, 84.86it/s]saving BB  train1-THALAMUS Sagittal:  92%|█████████▏| 246/266 [00:03<00:00, 85.22it/s]saving BB  train1-THALAMUS Sagittal:  96%|█████████▌| 255/266 [00:03<00:00, 78.91it/s]saving BB  train1-THALAMUS Sagittal:  99%|█████████▉| 263/266 [00:03<00:00, 74.10it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 78.93it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:51,  1.10s/it]Loading train:   1%|          | 2/266 [00:01<04:30,  1.03s/it]Loading train:   1%|          | 3/266 [00:02<04:12,  1.04it/s]Loading train:   2%|▏         | 4/266 [00:03<03:53,  1.12it/s]Loading train:   2%|▏         | 5/266 [00:04<03:58,  1.10it/s]Loading train:   2%|▏         | 6/266 [00:05<03:38,  1.19it/s]Loading train:   3%|▎         | 7/266 [00:05<03:18,  1.31it/s]Loading train:   3%|▎         | 8/266 [00:06<03:08,  1.37it/s]Loading train:   3%|▎         | 9/266 [00:06<02:56,  1.46it/s]Loading train:   4%|▍         | 10/266 [00:07<02:48,  1.52it/s]Loading train:   4%|▍         | 11/266 [00:08<02:47,  1.52it/s]Loading train:   5%|▍         | 12/266 [00:08<02:47,  1.51it/s]Loading train:   5%|▍         | 13/266 [00:09<02:48,  1.50it/s]Loading train:   5%|▌         | 14/266 [00:10<02:42,  1.55it/s]Loading train:   6%|▌         | 15/266 [00:10<02:38,  1.59it/s]Loading train:   6%|▌         | 16/266 [00:11<02:36,  1.59it/s]Loading train:   6%|▋         | 17/266 [00:11<02:34,  1.61it/s]Loading train:   7%|▋         | 18/266 [00:12<02:33,  1.61it/s]Loading train:   7%|▋         | 19/266 [00:13<02:32,  1.62it/s]Loading train:   8%|▊         | 20/266 [00:13<02:30,  1.64it/s]Loading train:   8%|▊         | 21/266 [00:14<02:29,  1.64it/s]Loading train:   8%|▊         | 22/266 [00:15<02:30,  1.63it/s]Loading train:   9%|▊         | 23/266 [00:15<02:33,  1.58it/s]Loading train:   9%|▉         | 24/266 [00:16<02:31,  1.60it/s]Loading train:   9%|▉         | 25/266 [00:16<02:28,  1.62it/s]Loading train:  10%|▉         | 26/266 [00:17<02:24,  1.66it/s]Loading train:  10%|█         | 27/266 [00:18<02:23,  1.67it/s]Loading train:  11%|█         | 28/266 [00:18<02:21,  1.68it/s]Loading train:  11%|█         | 29/266 [00:19<02:26,  1.62it/s]Loading train:  11%|█▏        | 30/266 [00:19<02:28,  1.59it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:25,  1.61it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:21,  1.66it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:18,  1.69it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:17,  1.69it/s]Loading train:  13%|█▎        | 35/266 [00:22<02:19,  1.66it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:21,  1.63it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:21,  1.62it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:18,  1.65it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:15,  1.67it/s]Loading train:  15%|█▌        | 40/266 [00:25<02:12,  1.71it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:14,  1.67it/s]Loading train:  16%|█▌        | 42/266 [00:27<02:13,  1.68it/s]Loading train:  16%|█▌        | 43/266 [00:27<02:11,  1.69it/s]Loading train:  17%|█▋        | 44/266 [00:28<02:09,  1.72it/s]Loading train:  17%|█▋        | 45/266 [00:28<02:04,  1.78it/s]Loading train:  17%|█▋        | 46/266 [00:29<02:00,  1.83it/s]Loading train:  18%|█▊        | 47/266 [00:29<01:57,  1.86it/s]Loading train:  18%|█▊        | 48/266 [00:30<01:54,  1.90it/s]Loading train:  18%|█▊        | 49/266 [00:30<01:57,  1.84it/s]Loading train:  19%|█▉        | 50/266 [00:31<01:59,  1.80it/s]Loading train:  19%|█▉        | 51/266 [00:32<02:01,  1.77it/s]Loading train:  20%|█▉        | 52/266 [00:32<02:04,  1.72it/s]Loading train:  20%|█▉        | 53/266 [00:33<01:59,  1.78it/s]Loading train:  20%|██        | 54/266 [00:33<01:56,  1.82it/s]Loading train:  21%|██        | 55/266 [00:34<01:53,  1.86it/s]Loading train:  21%|██        | 56/266 [00:34<01:52,  1.87it/s]Loading train:  21%|██▏       | 57/266 [00:35<01:53,  1.84it/s]Loading train:  22%|██▏       | 58/266 [00:35<01:56,  1.79it/s]Loading train:  22%|██▏       | 59/266 [00:36<01:57,  1.76it/s]Loading train:  23%|██▎       | 60/266 [00:37<01:54,  1.81it/s]Loading train:  23%|██▎       | 61/266 [00:37<01:49,  1.87it/s]Loading train:  23%|██▎       | 62/266 [00:38<01:47,  1.90it/s]Loading train:  24%|██▎       | 63/266 [00:38<01:46,  1.91it/s]Loading train:  24%|██▍       | 64/266 [00:39<01:47,  1.89it/s]Loading train:  24%|██▍       | 65/266 [00:39<01:45,  1.90it/s]Loading train:  25%|██▍       | 66/266 [00:40<01:44,  1.91it/s]Loading train:  25%|██▌       | 67/266 [00:40<01:44,  1.90it/s]Loading train:  26%|██▌       | 68/266 [00:41<01:44,  1.90it/s]Loading train:  26%|██▌       | 69/266 [00:41<01:46,  1.84it/s]Loading train:  26%|██▋       | 70/266 [00:42<01:46,  1.84it/s]Loading train:  27%|██▋       | 71/266 [00:42<01:44,  1.86it/s]Loading train:  27%|██▋       | 72/266 [00:43<01:42,  1.89it/s]Loading train:  27%|██▋       | 73/266 [00:43<01:41,  1.91it/s]Loading train:  28%|██▊       | 74/266 [00:44<01:41,  1.90it/s]Loading train:  28%|██▊       | 75/266 [00:44<01:40,  1.89it/s]Loading train:  29%|██▊       | 76/266 [00:45<01:39,  1.90it/s]Loading train:  29%|██▉       | 77/266 [00:45<01:40,  1.88it/s]Loading train:  29%|██▉       | 78/266 [00:46<01:44,  1.79it/s]Loading train:  30%|██▉       | 79/266 [00:47<01:44,  1.80it/s]Loading train:  30%|███       | 80/266 [00:47<01:44,  1.78it/s]Loading train:  30%|███       | 81/266 [00:48<01:45,  1.76it/s]Loading train:  31%|███       | 82/266 [00:48<01:43,  1.78it/s]Loading train:  31%|███       | 83/266 [00:49<01:42,  1.79it/s]Loading train:  32%|███▏      | 84/266 [00:49<01:43,  1.76it/s]Loading train:  32%|███▏      | 85/266 [00:50<01:48,  1.67it/s]Loading train:  32%|███▏      | 86/266 [00:51<01:45,  1.70it/s]Loading train:  33%|███▎      | 87/266 [00:51<01:43,  1.73it/s]Loading train:  33%|███▎      | 88/266 [00:52<01:42,  1.74it/s]Loading train:  33%|███▎      | 89/266 [00:52<01:42,  1.72it/s]Loading train:  34%|███▍      | 90/266 [00:53<01:44,  1.69it/s]Loading train:  34%|███▍      | 91/266 [00:54<01:44,  1.68it/s]Loading train:  35%|███▍      | 92/266 [00:54<01:45,  1.65it/s]Loading train:  35%|███▍      | 93/266 [00:55<01:42,  1.69it/s]Loading train:  35%|███▌      | 94/266 [00:55<01:38,  1.74it/s]Loading train:  36%|███▌      | 95/266 [00:56<01:36,  1.77it/s]Loading train:  36%|███▌      | 96/266 [00:57<01:49,  1.55it/s]Loading train:  36%|███▋      | 97/266 [00:58<02:05,  1.35it/s]
Epoch 00053: val_mDice did not improve from 0.51206
Restoring model weights from the end of the best epoch
Epoch 00053: early stopping
{'val_loss': [0.09986785938963294, 0.07156979385763407, 0.30758321535540745, 0.19419414771255106, 0.10784523462643847, 0.03532366204308346, 0.0431231155525893, 0.036597750906366855, 0.07169246260309592, -0.021922422281932086, 0.10187858174322173, 0.0763804167509079, 0.026974771346431226, 0.07112657523248345, 0.03894763172138482, 0.1131659725215286, -0.000902276486158371, 0.16429159278050065, 0.003731583943590522, 0.22327902168035507, 0.04912947118282318, -0.04449744720477611, -0.0006609021220356226, 0.036275913764256984, -0.010213958797976375, 0.0042569502256810665, -0.00526880967663601, -0.007687181408982724, -0.002544670831412077, 0.07234926801174879, 0.033518117445055395, -0.0047593124327249825, 0.0323747638030909, -0.0032453107996843755, -0.04011858388548717, 0.038424963131546974, -0.004726120503619313, -0.03556344611570239, -0.0021227342658676207, 0.029103946406394243, -0.004534180508926511, 0.0014919955865480006, 0.07230428466573358, 0.03278892068192363, 0.03606267849681899, 0.06992719793925062, -0.00877266877796501, 0.036060256126802415, 0.03163311694515869, 0.07502212759573013, 0.03585594811011106, 0.03128623869270086, -0.006240694900043309], 'val_acc': [0.9919467908330262, 0.9908383362926543, 0.9800316081382334, 0.9892799509689212, 0.993422229308635, 0.9936220981180668, 0.9940077983774245, 0.994152789004147, 0.9935694015584886, 0.9933031317777932, 0.9915065243840218, 0.9939579050987959, 0.9938955483958125, 0.9939837958663702, 0.9937642808072269, 0.9938821396790445, 0.9942366639152169, 0.9933227710425854, 0.9937190650962293, 0.991948033683002, 0.9925117730163038, 0.9941088235937059, 0.9941686843521893, 0.9939990672282875, 0.9939348362386227, 0.9939083331264555, 0.994321777485311, 0.9941063253208995, 0.9941125605255365, 0.9935840535908937, 0.9936751089990139, 0.9942650366574526, 0.9935347959399223, 0.9941752338781953, 0.99419768108055, 0.9925688304938376, 0.993884636554867, 0.9940869980491698, 0.9938194663263857, 0.9937689630314708, 0.9940904253162444, 0.9938163510523736, 0.9937281082384288, 0.9936919449828565, 0.9942859238944948, 0.9937387160025537, 0.9940343014895916, 0.9940661061555147, 0.99354601604864, 0.9941365723498166, 0.9941518530249596, 0.9938116711564362, 0.9942163936793804], 'val_mDice': [0.44858676516014384, 0.4715941607719287, 0.3961816076480318, 0.4613316376344301, 0.5066093994537368, 0.495389923453331, 0.47951861354704306, 0.49253767356305755, 0.5008680262835696, 0.4700940061593428, 0.48873866500798613, 0.5037092190468873, 0.5120563082164153, 0.5017330462578684, 0.49401015625335276, 0.4959794017486274, 0.4898964674794115, 0.5058882522916974, 0.4802713019307703, 0.48020172107499093, 0.4704015757306479, 0.4985843750182539, 0.48893185070483014, 0.4932179124443792, 0.5080014292616397, 0.4791341128293425, 0.49852353520691395, 0.5031691762269475, 0.4947227530647069, 0.49967615865170956, 0.49913742346689105, 0.4972313990646171, 0.5021098209545016, 0.49415262474212795, 0.48973853646771204, 0.49007223336957395, 0.49854621523991227, 0.4805747283618278, 0.49221169618249405, 0.5005212698597461, 0.49688493832945824, 0.48474573995918036, 0.4996815510094166, 0.5006514559499919, 0.49332647293340415, 0.504459809162654, 0.5054354615276679, 0.49379319789068177, 0.5030839168466628, 0.4924098948686151, 0.49415766145102685, 0.5035609193146229, 0.5001680861460045], 'loss': [0.09572891879403052, 0.06403578045085788, 0.055548460230679685, 0.052013026032109035, 0.04818313134624468, 0.045836995033225046, 0.045894356617822846, 0.042720283149973476, 0.04191888563323688, 0.03983911877536768, 0.040137541790803276, 0.0382934647098407, 0.03739268741302822, 0.03625935746396448, 0.036780949078212014, 0.03585018751145351, 0.0345850746680029, 0.0346453700829663, 0.03429202715514276, 0.03325865884559612, 0.03352362180477764, 0.032247154606306914, 0.03280956836369058, 0.03329007212079969, 0.03269976604458512, 0.032222508476962466, 0.0324199657298151, 0.03112509945630196, 0.029729019278735943, 0.02881863589564225, 0.029282535516670028, 0.028895624522978912, 0.02848029539787189, 0.02931670581486301, 0.028188652853988198, 0.028207458060551176, 0.027794070054502233, 0.02768434139027667, 0.027715477248863904, 0.027014139359720607, 0.027120610253901185, 0.02785044902983451, 0.026802828040073955, 0.02593068199054115, 0.025866130999056488, 0.025988443866173012, 0.025592487601019495, 0.0252786400165585, 0.025537261703232655, 0.025513537098261155, 0.025317544663140786, 0.025750225323950413, 0.025291185533043343], 'acc': [0.9895930308296773, 0.9931354175755769, 0.9939396441419102, 0.9943153518922406, 0.9946100796239916, 0.9948386739836873, 0.9948501038235544, 0.9951511897095628, 0.9951929177530997, 0.995385297661641, 0.9954171159880438, 0.9955562301472641, 0.9956114243599573, 0.9956684540522227, 0.9956431554088724, 0.9957260607814379, 0.9958434181026677, 0.9958542547215425, 0.9958783627401263, 0.9959399505637727, 0.9959537330454369, 0.9960306043601056, 0.9960077667438542, 0.9959813083895438, 0.9960520778907859, 0.9960092086541025, 0.9960677181024841, 0.9961060288546045, 0.9962608353094702, 0.9963243945877325, 0.9963300429640352, 0.9963431883678212, 0.996368673622809, 0.996355991649118, 0.9963955720141054, 0.996421438296996, 0.9964230150534908, 0.9964081154435274, 0.9964504791383643, 0.9964276419306959, 0.9964208809066569, 0.9964744104898058, 0.9964841715865979, 0.9965410771250268, 0.9965548862476887, 0.9965686349069891, 0.9965824429701052, 0.9965752515350192, 0.9965794528072216, 0.9966063276738313, 0.9965917402282193, 0.9965923432898297, 0.9966237687532725], 'mDice': [0.8139751716856544, 0.8754571143157315, 0.8919911899821288, 0.898856159541559, 0.906358457706151, 0.9109246252000547, 0.9108010365722818, 0.9169827773690957, 0.9185651437762141, 0.9226149679878468, 0.9220043726038633, 0.9256181625022392, 0.9273916433410694, 0.9296168776509708, 0.9285894869802388, 0.930400262376746, 0.932871005753129, 0.9327429929122535, 0.933443036574, 0.9354733079239039, 0.9349281791922842, 0.9374419398059604, 0.936330330133632, 0.935386238249066, 0.9365251926341455, 0.9375070896546323, 0.9370811867758249, 0.939642297448743, 0.9423419376158945, 0.9441294829244604, 0.9431973278861823, 0.9439551286807082, 0.9447830018853703, 0.9431181069698513, 0.9453535277530182, 0.9452974535702302, 0.9461176219171551, 0.9463539722143904, 0.9462612898310704, 0.9476646204242394, 0.9474601709110002, 0.9459843478358497, 0.9480694546106179, 0.9497771220186716, 0.9499004190497466, 0.9496431023495336, 0.9504301239350843, 0.9510620239028341, 0.9505382887616561, 0.9505777611949379, 0.9509718294507662, 0.9501040623386632, 0.9510135688376756], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label valuesLoading train:  37%|███▋      | 98/266 [00:59<02:09,  1.30it/s]Loading train:  37%|███▋      | 99/266 [00:59<02:04,  1.35it/s]Loading train:  38%|███▊      | 100/266 [01:00<02:03,  1.34it/s]Loading train:  38%|███▊      | 101/266 [01:01<01:51,  1.48it/s]Loading train:  38%|███▊      | 102/266 [01:01<01:43,  1.59it/s]Loading train:  39%|███▊      | 103/266 [01:02<01:37,  1.68it/s]Loading train:  39%|███▉      | 104/266 [01:02<01:34,  1.72it/s]Loading train:  39%|███▉      | 105/266 [01:03<01:31,  1.77it/s]Loading train:  40%|███▉      | 106/266 [01:03<01:29,  1.79it/s]Loading train:  40%|████      | 107/266 [01:04<01:26,  1.84it/s]Loading train:  41%|████      | 108/266 [01:04<01:25,  1.85it/s]Loading train:  41%|████      | 109/266 [01:05<01:22,  1.90it/s]Loading train:  41%|████▏     | 110/266 [01:05<01:21,  1.90it/s]Loading train:  42%|████▏     | 111/266 [01:06<01:20,  1.93it/s]Loading train:  42%|████▏     | 112/266 [01:06<01:19,  1.93it/s]Loading train:  42%|████▏     | 113/266 [01:07<01:20,  1.89it/s]Loading train:  43%|████▎     | 114/266 [01:07<01:20,  1.89it/s]Loading train:  43%|████▎     | 115/266 [01:08<01:19,  1.91it/s]Loading train:  44%|████▎     | 116/266 [01:08<01:17,  1.92it/s]Loading train:  44%|████▍     | 117/266 [01:09<01:17,  1.92it/s]Loading train:  44%|████▍     | 118/266 [01:09<01:16,  1.94it/s]Loading train:  45%|████▍     | 119/266 [01:10<01:16,  1.91it/s]Loading train:  45%|████▌     | 120/266 [01:10<01:18,  1.87it/s]Loading train:  45%|████▌     | 121/266 [01:11<01:18,  1.85it/s]Loading train:  46%|████▌     | 122/266 [01:12<01:19,  1.82it/s]Loading train:  46%|████▌     | 123/266 [01:12<01:19,  1.81it/s]Loading train:  47%|████▋     | 124/266 [01:13<01:18,  1.81it/s]Loading train:  47%|████▋     | 125/266 [01:13<01:17,  1.81it/s]Loading train:  47%|████▋     | 126/266 [01:14<01:17,  1.82it/s]Loading train:  48%|████▊     | 127/266 [01:14<01:16,  1.82it/s]Loading train:  48%|████▊     | 128/266 [01:15<01:17,  1.78it/s]Loading train:  48%|████▊     | 129/266 [01:15<01:16,  1.79it/s]Loading train:  49%|████▉     | 130/266 [01:16<01:16,  1.78it/s]Loading train:  49%|████▉     | 131/266 [01:17<01:15,  1.78it/s]Loading train:  50%|████▉     | 132/266 [01:17<01:15,  1.78it/s]Loading train:  50%|█████     | 133/266 [01:18<01:15,  1.77it/s]Loading train:  50%|█████     | 134/266 [01:18<01:16,  1.73it/s]Loading train:  51%|█████     | 135/266 [01:19<01:15,  1.74it/s]Loading train:  51%|█████     | 136/266 [01:19<01:13,  1.77it/s]Loading train:  52%|█████▏    | 137/266 [01:20<01:13,  1.76it/s]Loading train:  52%|█████▏    | 138/266 [01:21<01:12,  1.77it/s]Loading train:  52%|█████▏    | 139/266 [01:21<01:11,  1.77it/s]Loading train:  53%|█████▎    | 140/266 [01:22<01:12,  1.74it/s]Loading train:  53%|█████▎    | 141/266 [01:22<01:12,  1.73it/s]Loading train:  53%|█████▎    | 142/266 [01:23<01:11,  1.73it/s]Loading train:  54%|█████▍    | 143/266 [01:24<01:11,  1.72it/s]Loading train:  54%|█████▍    | 144/266 [01:24<01:10,  1.72it/s]Loading train:  55%|█████▍    | 145/266 [01:25<01:10,  1.71it/s]Loading train:  55%|█████▍    | 146/266 [01:25<01:10,  1.71it/s]Loading train:  55%|█████▌    | 147/266 [01:26<01:09,  1.71it/s]Loading train:  56%|█████▌    | 148/266 [01:26<01:08,  1.73it/s]Loading train:  56%|█████▌    | 149/266 [01:27<01:09,  1.69it/s]Loading train:  56%|█████▋    | 150/266 [01:28<01:08,  1.70it/s]Loading train:  57%|█████▋    | 151/266 [01:28<01:08,  1.68it/s]Loading train:  57%|█████▋    | 152/266 [01:29<01:06,  1.70it/s]Loading train:  58%|█████▊    | 153/266 [01:29<01:06,  1.71it/s]Loading train:  58%|█████▊    | 154/266 [01:30<01:05,  1.72it/s]Loading train:  58%|█████▊    | 155/266 [01:31<01:03,  1.76it/s]Loading train:  59%|█████▊    | 156/266 [01:31<01:00,  1.82it/s]Loading train:  59%|█████▉    | 157/266 [01:32<00:58,  1.87it/s]Loading train:  59%|█████▉    | 158/266 [01:32<00:56,  1.90it/s]Loading train:  60%|█████▉    | 159/266 [01:33<00:55,  1.94it/s]Loading train:  60%|██████    | 160/266 [01:33<00:53,  1.98it/s]Loading train:  61%|██████    | 161/266 [01:33<00:52,  2.01it/s]Loading train:  61%|██████    | 162/266 [01:34<00:51,  2.03it/s]Loading train:  61%|██████▏   | 163/266 [01:34<00:50,  2.03it/s]Loading train:  62%|██████▏   | 164/266 [01:35<00:49,  2.05it/s]Loading train:  62%|██████▏   | 165/266 [01:35<00:49,  2.04it/s]Loading train:  62%|██████▏   | 166/266 [01:36<00:49,  2.02it/s]Loading train:  63%|██████▎   | 167/266 [01:36<00:49,  2.02it/s]Loading train:  63%|██████▎   | 168/266 [01:37<00:48,  2.02it/s]Loading train:  64%|██████▎   | 169/266 [01:37<00:48,  2.02it/s]Loading train:  64%|██████▍   | 170/266 [01:38<00:47,  2.04it/s]Loading train:  64%|██████▍   | 171/266 [01:38<00:46,  2.03it/s]Loading train:  65%|██████▍   | 172/266 [01:39<00:45,  2.05it/s]Loading train:  65%|██████▌   | 173/266 [01:39<00:45,  2.06it/s]Loading train:  65%|██████▌   | 174/266 [01:40<00:44,  2.08it/s]Loading train:  66%|██████▌   | 175/266 [01:40<00:44,  2.07it/s]Loading train:  66%|██████▌   | 176/266 [01:41<00:44,  2.02it/s]Loading train:  67%|██████▋   | 177/266 [01:41<00:44,  2.01it/s]Loading train:  67%|██████▋   | 178/266 [01:42<00:43,  2.02it/s]Loading train:  67%|██████▋   | 179/266 [01:42<00:43,  1.99it/s]Loading train:  68%|██████▊   | 180/266 [01:43<00:44,  1.95it/s]Loading train:  68%|██████▊   | 181/266 [01:43<00:42,  1.98it/s]Loading train:  68%|██████▊   | 182/266 [01:44<00:42,  1.99it/s]Loading train:  69%|██████▉   | 183/266 [01:44<00:41,  1.99it/s]Loading train:  69%|██████▉   | 184/266 [01:45<00:40,  2.02it/s]Loading train:  70%|██████▉   | 185/266 [01:45<00:39,  2.04it/s]Loading train:  70%|██████▉   | 186/266 [01:46<00:39,  2.02it/s]Loading train:  70%|███████   | 187/266 [01:46<00:39,  2.02it/s]Loading train:  71%|███████   | 188/266 [01:47<00:38,  2.01it/s]Loading train:  71%|███████   | 189/266 [01:47<00:39,  1.94it/s]Loading train:  71%|███████▏  | 190/266 [01:48<00:41,  1.85it/s]Loading train:  72%|███████▏  | 191/266 [01:49<00:46,  1.60it/s]Loading train:  72%|███████▏  | 192/266 [01:50<00:48,  1.53it/s]Loading train:  73%|███████▎  | 193/266 [01:50<00:48,  1.49it/s]Loading train:  73%|███████▎  | 194/266 [01:51<00:53,  1.36it/s]Loading train:  73%|███████▎  | 195/266 [01:52<00:48,  1.47it/s]Loading train:  74%|███████▎  | 196/266 [01:52<00:45,  1.55it/s]Loading train:  74%|███████▍  | 197/266 [01:53<00:42,  1.61it/s]Loading train:  74%|███████▍  | 198/266 [01:53<00:41,  1.63it/s]Loading train:  75%|███████▍  | 199/266 [01:54<00:39,  1.70it/s]Loading train:  75%|███████▌  | 200/266 [01:54<00:37,  1.74it/s]Loading train:  76%|███████▌  | 201/266 [01:55<00:37,  1.75it/s]Loading train:  76%|███████▌  | 202/266 [01:56<00:36,  1.75it/s]Loading train:  76%|███████▋  | 203/266 [01:56<00:36,  1.74it/s]Loading train:  77%|███████▋  | 204/266 [01:57<00:35,  1.73it/s]Loading train:  77%|███████▋  | 205/266 [01:57<00:34,  1.76it/s]Loading train:  77%|███████▋  | 206/266 [01:58<00:34,  1.73it/s]Loading train:  78%|███████▊  | 207/266 [01:59<00:34,  1.72it/s]Loading train:  78%|███████▊  | 208/266 [01:59<00:33,  1.73it/s]Loading train:  79%|███████▊  | 209/266 [02:00<00:32,  1.74it/s]Loading train:  79%|███████▉  | 210/266 [02:00<00:31,  1.75it/s]Loading train:  79%|███████▉  | 211/266 [02:01<00:30,  1.80it/s]Loading train:  80%|███████▉  | 212/266 [02:01<00:29,  1.83it/s]Loading train:  80%|████████  | 213/266 [02:02<00:28,  1.83it/s]Loading train:  80%|████████  | 214/266 [02:02<00:28,  1.82it/s]Loading train:  81%|████████  | 215/266 [02:03<00:27,  1.84it/s]Loading train:  81%|████████  | 216/266 [02:03<00:27,  1.83it/s]Loading train:  82%|████████▏ | 217/266 [02:04<00:26,  1.84it/s]Loading train:  82%|████████▏ | 218/266 [02:05<00:25,  1.86it/s]Loading train:  82%|████████▏ | 219/266 [02:05<00:24,  1.88it/s]Loading train:  83%|████████▎ | 220/266 [02:06<00:24,  1.89it/s]Loading train:  83%|████████▎ | 221/266 [02:06<00:23,  1.90it/s]Loading train:  83%|████████▎ | 222/266 [02:07<00:23,  1.89it/s]Loading train:  84%|████████▍ | 223/266 [02:07<00:23,  1.85it/s]Loading train:  84%|████████▍ | 224/266 [02:08<00:23,  1.82it/s]Loading train:  85%|████████▍ | 225/266 [02:08<00:22,  1.81it/s]Loading train:  85%|████████▍ | 226/266 [02:10<00:37,  1.05it/s]Loading train:  85%|████████▌ | 227/266 [02:13<00:55,  1.42s/it]Loading train:  86%|████████▌ | 228/266 [02:16<01:13,  1.94s/it]Loading train:  86%|████████▌ | 229/266 [02:19<01:24,  2.27s/it]Loading train:  86%|████████▋ | 230/266 [02:22<01:30,  2.52s/it]Loading train:  87%|████████▋ | 231/266 [02:25<01:34,  2.69s/it]Loading train:  87%|████████▋ | 232/266 [02:30<01:49,  3.23s/it]Loading train:  88%|████████▊ | 233/266 [02:35<02:05,  3.80s/it]Loading train:  88%|████████▊ | 234/266 [02:40<02:11,  4.12s/it]Loading train:  88%|████████▊ | 235/266 [02:45<02:20,  4.54s/it]Loading train:  89%|████████▊ | 236/266 [02:49<02:13,  4.46s/it]Loading train:  89%|████████▉ | 237/266 [02:54<02:09,  4.47s/it]Loading train:  89%|████████▉ | 238/266 [02:59<02:06,  4.53s/it]Loading train:  90%|████████▉ | 239/266 [03:03<01:58,  4.38s/it]Loading train:  90%|█████████ | 240/266 [03:06<01:49,  4.20s/it]Loading train:  91%|█████████ | 241/266 [03:10<01:39,  4.00s/it]Loading train:  91%|█████████ | 242/266 [03:14<01:35,  3.96s/it]Loading train:  91%|█████████▏| 243/266 [03:17<01:29,  3.87s/it]Loading train:  92%|█████████▏| 244/266 [03:21<01:24,  3.83s/it]Loading train:  92%|█████████▏| 245/266 [03:25<01:18,  3.75s/it]Loading train:  92%|█████████▏| 246/266 [03:28<01:11,  3.58s/it]Loading train:  93%|█████████▎| 247/266 [03:31<01:04,  3.38s/it]Loading train:  93%|█████████▎| 248/266 [03:34<00:59,  3.32s/it]Loading train:  94%|█████████▎| 249/266 [03:41<01:16,  4.51s/it]Loading train:  94%|█████████▍| 250/266 [03:46<01:12,  4.55s/it]Loading train:  94%|█████████▍| 251/266 [03:51<01:09,  4.67s/it]Loading train:  95%|█████████▍| 252/266 [03:56<01:06,  4.72s/it]Loading train:  95%|█████████▌| 253/266 [04:00<01:00,  4.64s/it]Loading train:  95%|█████████▌| 254/266 [04:05<00:56,  4.72s/it]Loading train:  96%|█████████▌| 255/266 [04:10<00:51,  4.66s/it]Loading train:  96%|█████████▌| 256/266 [04:14<00:45,  4.57s/it]Loading train:  97%|█████████▋| 257/266 [04:19<00:41,  4.59s/it]Loading train:  97%|█████████▋| 258/266 [04:23<00:37,  4.67s/it]Loading train:  97%|█████████▋| 259/266 [04:28<00:33,  4.73s/it]Loading train:  98%|█████████▊| 260/266 [04:33<00:27,  4.65s/it]Loading train:  98%|█████████▊| 261/266 [04:37<00:23,  4.61s/it]Loading train:  98%|█████████▊| 262/266 [04:42<00:18,  4.58s/it]Loading train:  99%|█████████▉| 263/266 [04:46<00:13,  4.62s/it]Loading train:  99%|█████████▉| 264/266 [04:51<00:09,  4.70s/it]Loading train: 100%|█████████▉| 265/266 [04:56<00:04,  4.65s/it]Loading train: 100%|██████████| 266/266 [05:01<00:00,  4.75s/it]Loading train: 100%|██████████| 266/266 [05:01<00:00,  1.13s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 59.45it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 57.91it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 56.70it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:04, 56.57it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:04, 55.55it/s]concatenating: train:  14%|█▎        | 36/266 [00:00<00:04, 54.02it/s]concatenating: train:  15%|█▌        | 41/266 [00:00<00:04, 52.49it/s]concatenating: train:  18%|█▊        | 47/266 [00:00<00:04, 53.17it/s]concatenating: train:  20%|█▉        | 53/266 [00:00<00:04, 52.08it/s]concatenating: train:  22%|██▏       | 59/266 [00:01<00:04, 51.25it/s]concatenating: train:  25%|██▍       | 66/266 [00:01<00:03, 53.82it/s]concatenating: train:  27%|██▋       | 72/266 [00:01<00:03, 54.29it/s]concatenating: train:  29%|██▉       | 78/266 [00:01<00:03, 55.27it/s]concatenating: train:  32%|███▏      | 84/266 [00:01<00:03, 55.51it/s]concatenating: train:  34%|███▍      | 91/266 [00:01<00:03, 56.86it/s]concatenating: train:  36%|███▋      | 97/266 [00:01<00:02, 57.47it/s]concatenating: train:  39%|███▊      | 103/266 [00:01<00:02, 57.72it/s]concatenating: train:  41%|████▏     | 110/266 [00:01<00:02, 58.69it/s]concatenating: train:  44%|████▍     | 117/266 [00:02<00:02, 59.10it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:02, 60.13it/s]concatenating: train:  49%|████▉     | 131/266 [00:02<00:02, 60.03it/s]concatenating: train:  52%|█████▏    | 138/266 [00:02<00:02, 57.76it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 60.41it/s]concatenating: train:  57%|█████▋    | 152/266 [00:02<00:02, 56.59it/s]concatenating: train:  59%|█████▉    | 158/266 [00:02<00:01, 56.50it/s]concatenating: train:  62%|██████▏   | 165/266 [00:02<00:01, 58.54it/s]concatenating: train:  64%|██████▍   | 171/266 [00:03<00:01, 56.83it/s]concatenating: train:  67%|██████▋   | 177/266 [00:03<00:01, 57.15it/s]concatenating: train:  69%|██████▉   | 183/266 [00:03<00:01, 57.24it/s]concatenating: train:  71%|███████   | 189/266 [00:03<00:01, 57.70it/s]concatenating: train:  73%|███████▎  | 195/266 [00:03<00:01, 56.36it/s]concatenating: train:  76%|███████▌  | 201/266 [00:03<00:01, 54.02it/s]concatenating: train:  78%|███████▊  | 207/266 [00:03<00:01, 52.95it/s]concatenating: train:  80%|████████  | 213/266 [00:03<00:01, 52.15it/s]concatenating: train:  82%|████████▏ | 219/266 [00:03<00:00, 51.08it/s]concatenating: train:  85%|████████▍ | 225/266 [00:04<00:00, 50.63it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 50.46it/s]concatenating: train:  89%|████████▉ | 237/266 [00:04<00:00, 50.83it/s]concatenating: train:  91%|█████████▏| 243/266 [00:04<00:00, 49.71it/s]concatenating: train:  93%|█████████▎| 248/266 [00:04<00:00, 49.75it/s]concatenating: train:  95%|█████████▌| 253/266 [00:04<00:00, 48.08it/s]concatenating: train:  97%|█████████▋| 258/266 [00:04<00:00, 46.41it/s]concatenating: train:  99%|█████████▉| 263/266 [00:04<00:00, 42.73it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 53.69it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:22<01:06, 22.08s/it]Loading test:  50%|█████     | 2/4 [00:33<00:37, 18.91s/it]Loading test:  75%|███████▌  | 3/4 [00:46<00:17, 17.08s/it]Loading test: 100%|██████████| 4/4 [01:02<00:00, 16.83s/it]Loading test: 100%|██████████| 4/4 [01:02<00:00, 15.66s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 56.68it/s] min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 48, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 48, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 48, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 48, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 48, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 48, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 24, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 24, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 24, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 24, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 24, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 24, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 24, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 24, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 24, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 12, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 12, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 12, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 12, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 12, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 12, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 12, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 12, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 12, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 12, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 24, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 24, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 24, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 24, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 24, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 24, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 24, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 24, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 24, 280)  0           concatenate_3[0][0]              2020-01-20 23:48:51.716723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 23:48:51.716820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 23:48:51.716834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 23:48:51.716841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 23:48:51.718492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 24, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 48, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 48, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 48, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 48, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 48, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 48, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 48, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 48, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 48, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 48, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 48, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34333306e-02 3.28766145e-02 7.68774413e-02 9.55240866e-03
 2.76468458e-02 7.23301791e-03 8.42576865e-02 1.14265660e-01
 8.97210969e-02 1.36317568e-02 2.90893443e-01 1.89347409e-01
 2.63288998e-04]
Train on 10397 samples, validate on 154 samples
Epoch 1/300
 - 30s - loss: 0.5447 - acc: 0.9105 - mDice: 0.4134 - val_loss: 0.6967 - val_acc: 0.9371 - val_mDice: 0.2473

Epoch 00001: val_mDice improved from -inf to 0.24730, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 25s - loss: 0.3830 - acc: 0.9371 - mDice: 0.5873 - val_loss: 0.6746 - val_acc: 0.9411 - val_mDice: 0.2706

Epoch 00002: val_mDice improved from 0.24730 to 0.27057, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 25s - loss: 0.3538 - acc: 0.9416 - mDice: 0.6189 - val_loss: 0.6714 - val_acc: 0.9365 - val_mDice: 0.2749

Epoch 00003: val_mDice improved from 0.27057 to 0.27493, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 25s - loss: 0.3366 - acc: 0.9436 - mDice: 0.6374 - val_loss: 0.6464 - val_acc: 0.9419 - val_mDice: 0.2993

Epoch 00004: val_mDice improved from 0.27493 to 0.29930, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 25s - loss: 0.3223 - acc: 0.9459 - mDice: 0.6528 - val_loss: 0.6535 - val_acc: 0.9471 - val_mDice: 0.2893

Epoch 00005: val_mDice did not improve from 0.29930
Epoch 6/300
 - 25s - loss: 0.3091 - acc: 0.9469 - mDice: 0.6671 - val_loss: 0.6363 - val_acc: 0.9507 - val_mDice: 0.3048

Epoch 00006: val_mDice improved from 0.29930 to 0.30479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 25s - loss: 0.3008 - acc: 0.9481 - mDice: 0.6760 - val_loss: 0.6407 - val_acc: 0.9485 - val_mDice: 0.2990

Epoch 00007: val_mDice did not improve from 0.30479
Epoch 8/300
 - 24s - loss: 0.2926 - acc: 0.9489 - mDice: 0.6849 - val_loss: 0.6139 - val_acc: 0.9494 - val_mDice: 0.3053

Epoch 00008: val_mDice improved from 0.30479 to 0.30527, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300
 - 24s - loss: 0.2892 - acc: 0.9495 - mDice: 0.6885 - val_loss: 0.5855 - val_acc: 0.9473 - val_mDice: 0.2823

Epoch 00009: val_mDice did not improve from 0.30527
Epoch 10/300
 - 25s - loss: 0.2850 - acc: 0.9499 - mDice: 0.6930 - val_loss: 0.5016 - val_acc: 0.9474 - val_mDice: 0.3027

Epoch 00010: val_mDice did not improve from 0.30527
Epoch 11/300
 - 25s - loss: 0.2792 - acc: 0.9509 - mDice: 0.6994 - val_loss: 0.3849 - val_acc: 0.9471 - val_mDice: 0.3037

Epoch 00011: val_mDice did not improve from 0.30527
Epoch 12/300
 - 24s - loss: 0.2712 - acc: 0.9514 - mDice: 0.7079 - val_loss: 0.3772 - val_acc: 0.9484 - val_mDice: 0.3013

Epoch 00012: val_mDice did not improve from 0.30527
Epoch 13/300
 - 24s - loss: 0.2718 - acc: 0.9515 - mDice: 0.7074 - val_loss: 0.5724 - val_acc: 0.9455 - val_mDice: 0.2923

Epoch 00013: val_mDice did not improve from 0.30527
Epoch 14/300
 - 24s - loss: 0.2660 - acc: 0.9519 - mDice: 0.7136 - val_loss: 0.3619 - val_acc: 0.9462 - val_mDice: 0.2988

Epoch 00014: val_mDice did not improve from 0.30527
Epoch 15/300
 - 24s - loss: 0.2649 - acc: 0.9523 - mDice: 0.7147 - val_loss: 0.4383 - val_acc: 0.9459 - val_mDice: 0.2901

Epoch 00015: val_mDice did not improve from 0.30527
Epoch 16/300
 - 24s - loss: 0.2585 - acc: 0.9529 - mDice: 0.7217 - val_loss: 0.3527 - val_acc: 0.9475 - val_mDice: 0.2777

Epoch 00016: val_mDice did not improve from 0.30527
Epoch 17/300
 - 24s - loss: 0.2574 - acc: 0.9531 - mDice: 0.7228 - val_loss: 0.3896 - val_acc: 0.9478 - val_mDice: 0.2983

Epoch 00017: val_mDice did not improve from 0.30527
Epoch 18/300
 - 24s - loss: 0.2593 - acc: 0.9532 - mDice: 0.7207 - val_loss: 0.1571 - val_acc: 0.9490 - val_mDice: 0.3093

Epoch 00018: val_mDice improved from 0.30527 to 0.30927, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300
 - 24s - loss: 0.2544 - acc: 0.9532 - mDice: 0.7261 - val_loss: 0.0751 - val_acc: 0.9522 - val_mDice: 0.3011

Epoch 00019: val_mDice did not improve from 0.30927
Epoch 20/300
 - 24s - loss: 0.2543 - acc: 0.9531 - mDice: 0.7262 - val_loss: 0.0252 - val_acc: 0.9474 - val_mDice: 0.2740

Epoch 00020: val_mDice did not improve from 0.30927
Epoch 21/300
 - 24s - loss: 0.2598 - acc: 0.9531 - mDice: 0.7203 - val_loss: 0.0691 - val_acc: 0.9482 - val_mDice: 0.3020

Epoch 00021: val_mDice did not improve from 0.30927
Epoch 22/300
 - 24s - loss: 0.2530 - acc: 0.9539 - mDice: 0.7276 - val_loss: 0.1233 - val_acc: 0.9489 - val_mDice: 0.3036

Epoch 00022: val_mDice did not improve from 0.30927
Epoch 23/300
 - 25s - loss: 0.2487 - acc: 0.9544 - mDice: 0.7322 - val_loss: 0.0865 - val_acc: 0.9485 - val_mDice: 0.3062

Epoch 00023: val_mDice did not improve from 0.30927
Epoch 24/300
 - 24s - loss: 0.2496 - acc: 0.9543 - mDice: 0.7312 - val_loss: 0.0871 - val_acc: 0.9488 - val_mDice: 0.3040

Epoch 00024: val_mDice did not improve from 0.30927
Epoch 25/300
 - 25s - loss: 0.2461 - acc: 0.9545 - mDice: 0.7350 - val_loss: 0.1876 - val_acc: 0.9460 - val_mDice: 0.2964

Epoch 00025: val_mDice did not improve from 0.30927
Epoch 26/300
 - 25s - loss: 0.2468 - acc: 0.9546 - mDice: 0.7343 - val_loss: 0.0356 - val_acc: 0.9509 - val_mDice: 0.2983

Epoch 00026: val_mDice did not improve from 0.30927
Epoch 27/300
 - 25s - loss: 0.2465 - acc: 0.9549 - mDice: 0.7346 - val_loss: 0.0457 - val_acc: 0.9467 - val_mDice: 0.2907

Epoch 00027: val_mDice did not improve from 0.30927
Epoch 28/300
 - 25s - loss: 0.2431 - acc: 0.9550 - mDice: 0.7383 - val_loss: 0.0981 - val_acc: 0.9468 - val_mDice: 0.2956

Epoch 00028: val_mDice did not improve from 0.30927
Epoch 29/300
 - 25s - loss: 0.2448 - acc: 0.9548 - mDice: 0.7364 - val_loss: 0.0842 - val_acc: 0.9483 - val_mDice: 0.2789

Epoch 00029: val_mDice did not improve from 0.30927
Epoch 30/300
 - 25s - loss: 0.2395 - acc: 0.9553 - mDice: 0.7421 - val_loss: -1.0107e-02 - val_acc: 0.9515 - val_mDice: 0.3100

Epoch 00030: val_mDice improved from 0.30927 to 0.31000, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 31/300
 - 24s - loss: 0.2380 - acc: 0.9553 - mDice: 0.7437 - val_loss: 0.0305 - val_acc: 0.9490 - val_mDice: 0.3032

Epoch 00031: val_mDice did not improve from 0.31000
Epoch 32/300
 - 24s - loss: 0.2395 - acc: 0.9555 - mDice: 0.7421 - val_loss: -2.8664e-02 - val_acc: 0.9499 - val_mDice: 0.2989

Epoch 00032: val_mDice did not improve from 0.31000
Epoch 33/300
 - 25s - loss: 0.2412 - acc: 0.9552 - mDice: 0.7394 - val_loss: 0.2442 - val_acc: 0.9414 - val_mDice: 0.2701

Epoch 00033: val_mDice did not improve from 0.31000

Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 34/300
 - 25s - loss: 0.2268 - acc: 0.9543 - mDice: 0.7349 - val_loss: -3.7564e-02 - val_acc: 0.9495 - val_mDice: 0.2898

Epoch 00034: val_mDice did not improve from 0.31000
Epoch 35/300
 - 25s - loss: 0.2175 - acc: 0.9550 - mDice: 0.7382 - val_loss: -7.2026e-02 - val_acc: 0.9505 - val_mDice: 0.3000

Epoch 00035: val_mDice did not improve from 0.31000
Epoch 36/300
 - 26s - loss: 0.2095 - acc: 0.9557 - mDice: 0.7459 - val_loss: -7.8160e-02 - val_acc: 0.9507 - val_mDice: 0.3032

Epoch 00036: val_mDice did not improve from 0.31000
Epoch 37/300
 - 26s - loss: 0.2154 - acc: 0.9544 - mDice: 0.7385 - val_loss: -2.4217e-02 - val_acc: 0.9474 - val_mDice: 0.2940

Epoch 00037: val_mDice did not improve from 0.31000
Epoch 38/300
 - 26s - loss: 0.2155 - acc: 0.9546 - mDice: 0.7344 - val_loss: -8.5954e-02 - val_acc: 0.9512 - val_mDice: 0.3063

Epoch 00038: val_mDice did not improve from 0.31000
Epoch 39/300
 - 26s - loss: 0.2081 - acc: 0.9544 - mDice: 0.7372 - val_loss: -1.9751e-02 - val_acc: 0.9477 - val_mDice: 0.2807

Epoch 00039: val_mDice did not improve from 0.31000
Epoch 40/300
 - 26s - loss: 0.2055 - acc: 0.9546 - mDice: 0.7381 - val_loss: -9.6195e-02 - val_acc: 0.9504 - val_mDice: 0.3012

Epoch 00040: val_mDice did not improve from 0.31000
Epoch 41/300
 - 26s - loss: 0.2024 - acc: 0.9548 - mDice: 0.7422 - val_loss: -8.4997e-02 - val_acc: 0.9502 - val_mDice: 0.2951

Epoch 00041: val_mDice did not improve from 0.31000
Epoch 42/300
 - 26s - loss: 0.2019 - acc: 0.9538 - mDice: 0.7357 - val_loss: -7.0298e-02 - val_acc: 0.9494 - val_mDice: 0.3000

Epoch 00042: val_mDice did not improve from 0.31000
Epoch 43/300
 - 26s - loss: 0.2046 - acc: 0.9538 - mDice: 0.7322 - val_loss: -5.9407e-02 - val_acc: 0.9499 - val_mDice: 0.3001

Epoch 00043: val_mDice did not improve from 0.31000
Epoch 44/300
 - 26s - loss: 0.2090 - acc: 0.9531 - mDice: 0.7250 - val_loss: -9.9689e-02 - val_acc: 0.9524 - val_mDice: 0.2895

Epoch 00044: val_mDice did not improve from 0.31000
Epoch 45/300
 - 26s - loss: 0.2074 - acc: 0.9532 - mDice: 0.7221 - val_loss: -9.5436e-02 - val_acc: 0.9524 - val_mDice: 0.2850

Epoch 00045: val_mDice did not improve from 0.31000
Epoch 46/300
 - 26s - loss: 0.2055 - acc: 0.9536 - mDice: 0.7284 - val_loss: -7.5726e-02 - val_acc: 0.9504 - val_mDice: 0.2968

Epoch 00046: val_mDice did not improve from 0.31000
Epoch 47/300
 - 26s - loss: 0.2067 - acc: 0.9536 - mDice: 0.7262 - val_loss: -2.3972e-02 - val_acc: 0.9485 - val_mDice: 0.2931

Epoch 00047: val_mDice did not improve from 0.31000
Epoch 48/300
 - 26s - loss: 0.2084 - acc: 0.9532 - mDice: 0.7241 - val_loss: -4.3587e-02 - val_acc: 0.9495 - val_mDice: 0.2716

Epoch 00048: val_mDice did not improve from 0.31000

Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 49/300
 - 26s - loss: 0.2017 - acc: 0.9536 - mDice: 0.7259 - val_loss: -8.5514e-02 - val_acc: 0.9501 - val_mDice: 0.2929

Epoch 00049: val_mDice did not improve from 0.31000
Epoch 50/300
 - 26s - loss: 0.1943 - acc: 0.9543 - mDice: 0.7335 - val_loss: -1.0580e-01 - val_acc: 0.9510 - val_mDice: 0.2962

Epoch 00050: val_mDice did not improve from 0.31000
Epoch 51/300
 - 26s - loss: 0.1940 - acc: 0.9547 - mDice: 0.7388 - val_loss: -1.0323e-01 - val_acc: 0.9510 - val_mDice: 0.2935

Epoch 00051: val_mDice did not improve from 0.31000
Epoch 52/300
 - 26s - loss: 0.1884 - acc: 0.9551 - mDice: 0.7408 - val_loss: -8.9393e-02 - val_acc: 0.9510 - val_mDice: 0.3011

Epoch 00052: val_mDice did not improve from 0.31000
Epoch 53/300
 - 25s - loss: 0.1929 - acc: 0.9546 - mDice: 0.7352 - val_loss: -1.0703e-01 - val_acc: 0.9517 - val_mDice: 0.2887

Epoch 00053: val_mDice did not improve from 0.31000
Epoch 54/300
 - 25s - loss: 0.1935 - acc: 0.9548 - mDice: 0.7329 - val_loss: -1.1633e-01 - val_acc: 0.9517 - val_mDice: 0.2972

Epoch 00054: val_mDice did not improve from 0.31000
Epoch 55/300
 - 25s - loss: 0.1885 - acc: 0.9550 - mDice: 0.7377 - val_loss: -9.5266e-02 - val_acc: 0.9503 - val_mDice: 0.2917

Epoch 00055: val_mDice did not improve from 0.31000
Epoch 56/300
 - 24s - loss: 0.1852 - acc: 0.9553 - mDice: 0.7412 - val_loss: -1.2512e-01 - val_acc: 0.9532 - val_mDice: 0.2956

Epoch 00056: val_mDice did not improve from 0.31000
Epoch 57/300
 - 25s - loss: 0.1815 - acc: 0.9553 - mDice: 0.7412 - val_loss: -1.2903e-01 - val_acc: 0.9516 - val_mDice: 0.2999

Epoch 00057: val_mDice did not improve from 0.31000
Epoch 58/300
 - 24s - loss: 0.1809 - acc: 0.9554 - mDice: 0.7418 - val_loss: -9.9778e-02 - val_acc: 0.9512 - val_mDice: 0.2904

Epoch 00058: val_mDice did not improve from 0.31000
Epoch 59/300
 - 24s - loss: 0.1845 - acc: 0.9553 - mDice: 0.7382 - val_loss: -1.0507e-01 - val_acc: 0.9509 - val_mDice: 0.2846

Epoch 00059: val_mDice did not improve from 0.31000
Epoch 60/300
 - 24s - loss: 0.1868 - acc: 0.9553 - mDice: 0.7410 - val_loss: -9.7474e-02 - val_acc: 0.9475 - val_mDice: 0.2765

Epoch 00060: val_mDice did not improve from 0.31000
Epoch 61/300
 - 24s - loss: 0.1838 - acc: 0.9552 - mDice: 0.7439 - val_loss: -1.0339e-01 - val_acc: 0.9513 - val_mDice: 0.2934

Epoch 00061: val_mDice did not improve from 0.31000
Epoch 62/300
 - 24s - loss: 0.1808 - acc: 0.9557 - mDice: 0.7485 - val_loss: -1.1050e-01 - val_acc: 0.9510 - val_mDice: 0.2966

Epoch 00062: val_mDice did not improve from 0.31000
Epoch 63/300
 - 24s - loss: 0.1779 - acc: 0.9559 - mDice: 0.7508 - val_loss: -1.2203e-01 - val_acc: 0.9515 - val_mDice: 0.3038

Epoch 00063: val_mDice did not improve from 0.31000

Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 64/300
 - 24s - loss: 0.1770 - acc: 0.9562 - mDice: 0.7517 - val_loss: -8.8664e-02 - val_acc: 0.9496 - val_mDice: 0.2994

Epoch 00064: val_mDice did not improve from 0.31000
Epoch 65/300
 - 25s - loss: 0.1785 - acc: 0.9557 - mDice: 0.7464 - val_loss: -9.8891e-02 - val_acc: 0.9503 - val_mDice: 0.2908

Epoch 00065: val_mDice did not improve from 0.31000
Epoch 66/300
 - 24s - loss: 0.1741 - acc: 0.9561 - mDice: 0.7499 - val_loss: -8.3649e-02 - val_acc: 0.9512 - val_mDice: 0.2926

Epoch 00066: val_mDice did not improve from 0.31000
Epoch 67/300
 - 24s - loss: 0.1756 - acc: 0.9563 - mDice: 0.7517 - val_loss: -9.1377e-02 - val_acc: 0.9503 - val_mDice: 0.2921

Epoch 00067: val_mDice did not improve from 0.31000
Epoch 68/300
 - 25s - loss: 0.1751 - acc: 0.9561 - mDice: 0.7475 - val_loss: -1.0693e-01 - val_acc: 0.9509 - val_mDice: 0.2955

Epoch 00068: val_mDice did not improve from 0.31000
Epoch 69/300
 - 24s - loss: 0.1738 - acc: 0.9564 - mDice: 0.7543 - val_loss: -8.1578e-02 - val_acc: 0.9508 - val_mDice: 0.2947

Epoch 00069: val_mDice did not improve from 0.31000
Epoch 70/300
 - 24s - loss: 0.1728 - acc: 0.9565 - mDice: 0.7522 - val_loss: -9.4970e-02 - val_acc: 0.9508 - val_mDice: 0.2985

Epoch 00070: val_mDice did not improve from 0.31000
Restoring model weights from the end of the best epoch
Epoch 00070: early stopping
{'val_loss': [0.6967442786538756, 0.674641935082225, 0.671386478009162, 0.6464237183719487, 0.6534538915405026, 0.6362587205775372, 0.6406529565136154, 0.6139487333886036, 0.5854510320471479, 0.5016396192761211, 0.3849101422669052, 0.3772001936064138, 0.5724390714199512, 0.3619210830369553, 0.43830148217739995, 0.3526903555377737, 0.3895802517215927, 0.1571336929843604, 0.07513057300216192, 0.025190591908894576, 0.06905036845377513, 0.12332199671148479, 0.08653197725236948, 0.08707903562621637, 0.18756242420572738, 0.03561544331250253, 0.045698560858992016, 0.09805800956855347, 0.08420954973666699, -0.010107285669678218, 0.03049476442302202, -0.028663628553221753, 0.24418278154614684, -0.037564418587059556, -0.07202592280439356, -0.07815975270101003, -0.024217440837437845, -0.085953796451742, -0.01975131905698157, -0.09619515430932114, -0.08499661392786285, -0.07029772272072513, -0.05940730945364415, -0.09968916514840026, -0.09543625831108105, -0.07572638329312012, -0.0239718370956018, -0.04358659140378624, -0.08551397312783285, -0.10579678544157801, -0.10323241856493443, -0.0893934353393829, -0.10702628879951519, -0.11632780346100206, -0.09526570584600816, -0.12512244310762202, -0.12903123757765664, -0.09977802337153344, -0.10506564560164879, -0.09747446473542747, -0.10339105658326903, -0.11049504738071232, -0.1220314237954361, -0.08866435900369664, -0.0988907995754732, -0.0836492231744644, -0.09137668218705561, -0.10693302676807363, -0.08157761150354205, -0.09497001583010635], 'val_acc': [0.9371440805398025, 0.9411429321611082, 0.9364612280548393, 0.9418708782691461, 0.9471436289997844, 0.9507140865573636, 0.948472290070026, 0.9494095978798804, 0.9472901844359064, 0.9473787670011644, 0.9471065866482722, 0.948423973925702, 0.9455315308137373, 0.9461934458125721, 0.9459132095435997, 0.9474770047447898, 0.9477668899994391, 0.9490424046268711, 0.9521828618916598, 0.947446398920827, 0.9482468180842214, 0.9488652534299082, 0.948535091691203, 0.9487573473484485, 0.9460388298158522, 0.950934716633388, 0.9466733576415421, 0.9468279720900895, 0.9483192880432327, 0.9515338323333047, 0.9489666989871434, 0.949937832819951, 0.9413796756174657, 0.9495029952618983, 0.9504660693081942, 0.9507269789646198, 0.9474222396875357, 0.9511650320771453, 0.947689584323338, 0.950429034697545, 0.9502341685357032, 0.9493950953731289, 0.9499072347368512, 0.9524260457459982, 0.9523519734283546, 0.9504048793346851, 0.9484545689124566, 0.949501392903266, 0.9501133816582816, 0.9510458421397519, 0.9509685534935493, 0.9509717736925397, 0.9517142114701209, 0.9517480274299522, 0.9503307915353155, 0.9532441736815812, 0.9515918129450315, 0.9512230111406995, 0.9508558078245684, 0.9475398195254339, 0.9512520006724766, 0.9509637154542007, 0.9514774726582812, 0.9495690364342231, 0.9502985934158424, 0.9512358841957984, 0.9502905313070719, 0.9509041262911512, 0.9507978403723085, 0.9508268337745172], 'val_mDice': [0.2472950750357145, 0.2705665868888428, 0.2749319727738182, 0.2993021992506919, 0.2893145251777265, 0.3047883118708412, 0.2989829070382304, 0.305265478499524, 0.2822940607930159, 0.3026782406808494, 0.30368598831164373, 0.3012877010873386, 0.29227776477089173, 0.2987560513731721, 0.2900570901957425, 0.2776970191822424, 0.29827210423234224, 0.3092741569528332, 0.30112927223180797, 0.27398880958170085, 0.30203471032830026, 0.3036029777163035, 0.30617981616939816, 0.3040104533557768, 0.29636018703897277, 0.29829801338446604, 0.29074887466895116, 0.2956030203343986, 0.2789490839297121, 0.3099950896455096, 0.30321774689795133, 0.2988565480941302, 0.27005131416893624, 0.289817883983835, 0.3000421057660858, 0.30318293139919056, 0.29396837649794366, 0.30632135252673903, 0.28072956078625344, 0.3011859445409341, 0.2951382574709979, 0.2999731404827787, 0.30009871324548476, 0.28951552152246623, 0.2849673383034669, 0.29678458178585226, 0.293118886456087, 0.27159635364622264, 0.2928737028465643, 0.2962344183356731, 0.2934705548278697, 0.30107455303916686, 0.28867945607219425, 0.2972100626725655, 0.2916807652293862, 0.2956386800710257, 0.2998757410746116, 0.29035531952009574, 0.2846132189809502, 0.2764825039095693, 0.2933785807970282, 0.2966276720746771, 0.30383797144735014, 0.29937821210591825, 0.29078300510134014, 0.2925891521108615, 0.29213403145988265, 0.29546350627750545, 0.2946757128486386, 0.2984943448916658], 'loss': [0.5446607354995069, 0.38304800185559174, 0.35376220224074956, 0.33659548840064596, 0.3222972773939208, 0.3090979533893988, 0.30081524747294214, 0.29260797358492513, 0.2892065717329644, 0.2850486536667853, 0.27916190670272, 0.27124676273466936, 0.2717539051814046, 0.265987954561534, 0.26494606320670194, 0.2584759134925007, 0.2574143601350169, 0.25934635909506937, 0.25435399793312735, 0.2543365840279759, 0.25976910135190373, 0.25295179828355047, 0.24867069718928775, 0.2496355331906037, 0.24609302614209191, 0.2467584683194096, 0.2465287765035311, 0.24308451153090727, 0.24479024870826543, 0.23951902317980156, 0.23803590647564538, 0.23951133110207457, 0.24119719209677443, 0.22683705941145038, 0.2175373012156822, 0.2094873095505786, 0.21538487048677235, 0.21546295520339748, 0.208113624102779, 0.20548773080844088, 0.20243255558928996, 0.2019327410762902, 0.20456175612226793, 0.2090129300769581, 0.2073693275976128, 0.20546132495724984, 0.20673855248268241, 0.20844322246245592, 0.2017164980755455, 0.19432259006554728, 0.19404007336486664, 0.18844821819017724, 0.19293902159634108, 0.19350439559411128, 0.18854595968694987, 0.18521793899085595, 0.18148013122777698, 0.180922714173311, 0.1845320901228433, 0.1867504794761679, 0.1838444888489896, 0.18083485877389258, 0.17785866073985468, 0.17704229848324998, 0.1785269681289457, 0.17408471916376025, 0.17563758415146255, 0.1750733528803834, 0.17378259383325312, 0.1727651978620855], 'acc': [0.9105434865841524, 0.9371110758116878, 0.9415677272380287, 0.9435750171979299, 0.9458686569636632, 0.9468579770819985, 0.948103900596455, 0.9489465392850116, 0.9495471503466978, 0.9498703311621781, 0.9508817883206065, 0.9513584265310374, 0.9514666313882905, 0.9518644292790053, 0.952257170887696, 0.9529293930804579, 0.9531266452934325, 0.9532255701483517, 0.953248470875464, 0.9531334679585598, 0.9530588985234714, 0.9539100773015526, 0.9544162239273477, 0.9542650813390008, 0.9545247142929704, 0.9545804620809575, 0.954881673949813, 0.9550094385897872, 0.9548421462499214, 0.9552783038084858, 0.95532291154168, 0.9554875556512488, 0.9552125371487379, 0.9542939926989321, 0.9550219387449507, 0.9556694228087089, 0.9544410559951519, 0.9546200364693387, 0.9544428927778934, 0.9546344204511713, 0.9547682690874961, 0.9537988925273686, 0.953798080248547, 0.9531115692109228, 0.953238190475927, 0.9536390666825457, 0.9535631848388284, 0.9532224463478607, 0.9536422385015522, 0.9543295353894694, 0.9546937958212945, 0.955121412252685, 0.9545692034360249, 0.954814618707242, 0.9550211516678235, 0.9553036137568305, 0.9553242713726059, 0.955351990674039, 0.9552733903723726, 0.9552786851589686, 0.9551562156667615, 0.9556935877768801, 0.9559318234717099, 0.9561902875623715, 0.9557429189932877, 0.9561363752099016, 0.9563267347766101, 0.9560918151799149, 0.9564088909766119, 0.9565303816730683], 'mDice': [0.4134042645196631, 0.5873221791976115, 0.6189042094683227, 0.6374259544981664, 0.6528230036573134, 0.6670947429525448, 0.6760056572567216, 0.6848653324455025, 0.6885394371449058, 0.6930348913784473, 0.699360911704215, 0.7079237905605328, 0.7073854280799997, 0.7136037985820134, 0.7147153856158772, 0.7217050406397839, 0.7228344417746576, 0.7207399913470562, 0.7261449483520562, 0.7261713406346424, 0.7202797396097939, 0.7276459650499403, 0.7322481779211976, 0.7312042377849742, 0.7350477237729851, 0.7343072613707137, 0.7345550553528314, 0.7382797265472442, 0.7364390039628568, 0.7421208040134327, 0.7437200261111717, 0.7420970273841123, 0.7394364141977623, 0.7349056363839581, 0.7382182213071563, 0.7458657848384388, 0.738461473119526, 0.7344144854314444, 0.7371985456846512, 0.7381057173079615, 0.7422069186280069, 0.7357040575986827, 0.732224700679524, 0.725017181150934, 0.7220915379610912, 0.7284153432723156, 0.7261620402519573, 0.724061881594673, 0.7258858137765022, 0.7334733255737113, 0.7387691200943283, 0.7408420597749042, 0.7352060588137443, 0.7329180708318602, 0.7377095800862354, 0.7411697685598146, 0.7412143753012224, 0.741755348805729, 0.7381753233621771, 0.7409519407340308, 0.7439353857244954, 0.7485253309485796, 0.7508139634712552, 0.7517182603528594, 0.7463597864890037, 0.7499082066112268, 0.7516527479344106, 0.7474510034506305, 0.7542709257266287, 0.7522069993151648], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.26s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:36,  2.76it/s]Loading train:   1%|          | 2/266 [00:00<01:33,  2.84it/s]Loading train:   1%|          | 3/266 [00:00<01:28,  2.98it/s]Loading train:   2%|▏         | 4/266 [00:01<01:23,  3.15it/s]Loading train:   2%|▏         | 5/266 [00:01<01:24,  3.09it/s]Loading train:   2%|▏         | 6/266 [00:01<01:24,  3.08it/s]Loading train:   3%|▎         | 7/266 [00:02<01:24,  3.06it/s]Loading train:   3%|▎         | 8/266 [00:02<01:24,  3.06it/s]Loading train:   3%|▎         | 9/266 [00:02<01:24,  3.05it/s]Loading train:   4%|▍         | 10/266 [00:03<01:23,  3.08it/s]Loading train:   4%|▍         | 11/266 [00:03<01:22,  3.07it/s]Loading train:   5%|▍         | 12/266 [00:03<01:23,  3.05it/s]Loading train:   5%|▍         | 13/266 [00:04<01:22,  3.06it/s]Loading train:   5%|▌         | 14/266 [00:04<01:22,  3.06it/s]Loading train:   6%|▌         | 15/266 [00:04<01:21,  3.07it/s]Loading train:   6%|▌         | 16/266 [00:05<01:20,  3.09it/s]Loading train:   6%|▋         | 17/266 [00:05<01:20,  3.09it/s]Loading train:   7%|▋         | 18/266 [00:05<01:20,  3.07it/s]Loading train:   7%|▋         | 19/266 [00:06<01:23,  2.96it/s]Loading train:   8%|▊         | 20/266 [00:06<01:22,  2.99it/s]Loading train:   8%|▊         | 21/266 [00:06<01:20,  3.03it/s]Loading train:   8%|▊         | 22/266 [00:07<01:19,  3.05it/s]Loading train:   9%|▊         | 23/266 [00:07<01:19,  3.06it/s]Loading train:   9%|▉         | 24/266 [00:07<01:18,  3.10it/s]Loading train:   9%|▉         | 25/266 [00:08<01:17,  3.13it/s]Loading train:  10%|▉         | 26/266 [00:08<01:16,  3.14it/s]Loading train:  10%|█         | 27/266 [00:08<01:16,  3.13it/s]Loading train:  11%|█         | 28/266 [00:09<01:14,  3.18it/s]Loading train:  11%|█         | 29/266 [00:09<01:13,  3.21it/s]Loading train:  11%|█▏        | 30/266 [00:09<01:12,  3.25it/s]Loading train:  12%|█▏        | 31/266 [00:09<01:12,  3.26it/s]Loading train:  12%|█▏        | 32/266 [00:10<01:11,  3.28it/s]Loading train:  12%|█▏        | 33/266 [00:10<01:10,  3.30it/s]Loading train:  13%|█▎        | 34/266 [00:10<01:10,  3.30it/s]Loading train:  13%|█▎        | 35/266 [00:11<01:09,  3.33it/s]Loading train:  14%|█▎        | 36/266 [00:11<01:09,  3.33it/s]Loading train:  14%|█▍        | 37/266 [00:11<01:09,  3.28it/s]Loading train:  14%|█▍        | 38/266 [00:12<01:09,  3.26it/s]Loading train:  15%|█▍        | 39/266 [00:12<01:09,  3.26it/s]Loading train:  15%|█▌        | 40/266 [00:12<01:09,  3.26it/s]Loading train:  15%|█▌        | 41/266 [00:13<01:09,  3.24it/s]Loading train:  16%|█▌        | 42/266 [00:13<01:07,  3.31it/s]Loading train:  16%|█▌        | 43/266 [00:13<01:07,  3.32it/s]Loading train:  17%|█▋        | 44/266 [00:13<01:06,  3.32it/s]Loading train:  17%|█▋        | 45/266 [00:14<01:05,  3.40it/s]Loading train:  17%|█▋        | 46/266 [00:14<01:03,  3.45it/s]Loading train:  18%|█▊        | 47/266 [00:14<01:02,  3.48it/s]Loading train:  18%|█▊        | 48/266 [00:15<01:01,  3.53it/s]Loading train:  18%|█▊        | 49/266 [00:15<01:01,  3.55it/s]Loading train:  19%|█▉        | 50/266 [00:15<01:00,  3.58it/s]Loading train:  19%|█▉        | 51/266 [00:15<01:01,  3.50it/s]Loading train:  20%|█▉        | 52/266 [00:16<01:00,  3.53it/s]Loading train:  20%|█▉        | 53/266 [00:16<00:59,  3.57it/s]Loading train:  20%|██        | 54/266 [00:16<00:59,  3.56it/s]Loading train:  21%|██        | 55/266 [00:16<00:59,  3.57it/s]Loading train:  21%|██        | 56/266 [00:17<00:58,  3.57it/s]Loading train:  21%|██▏       | 57/266 [00:17<00:58,  3.58it/s]Loading train:  22%|██▏       | 58/266 [00:17<00:58,  3.53it/s]Loading train:  22%|██▏       | 59/266 [00:18<01:00,  3.44it/s]Loading train:  23%|██▎       | 60/266 [00:18<00:59,  3.49it/s]Loading train:  23%|██▎       | 61/266 [00:18<00:57,  3.56it/s]Loading train:  23%|██▎       | 62/266 [00:18<00:56,  3.60it/s]Loading train:  24%|██▎       | 63/266 [00:19<00:56,  3.60it/s]Loading train:  24%|██▍       | 64/266 [00:19<00:56,  3.60it/s]Loading train:  24%|██▍       | 65/266 [00:19<00:55,  3.64it/s]Loading train:  25%|██▍       | 66/266 [00:20<00:54,  3.69it/s]Loading train:  25%|██▌       | 67/266 [00:20<00:53,  3.71it/s]Loading train:  26%|██▌       | 68/266 [00:20<00:53,  3.73it/s]Loading train:  26%|██▌       | 69/266 [00:20<00:53,  3.70it/s]Loading train:  26%|██▋       | 70/266 [00:21<00:52,  3.73it/s]Loading train:  27%|██▋       | 71/266 [00:21<00:52,  3.72it/s]Loading train:  27%|██▋       | 72/266 [00:21<00:52,  3.72it/s]Loading train:  27%|██▋       | 73/266 [00:21<00:51,  3.74it/s]Loading train:  28%|██▊       | 74/266 [00:22<00:51,  3.75it/s]Loading train:  28%|██▊       | 75/266 [00:22<00:51,  3.72it/s]Loading train:  29%|██▊       | 76/266 [00:22<00:51,  3.67it/s]Loading train:  29%|██▉       | 77/266 [00:22<00:50,  3.73it/s]Loading train:  29%|██▉       | 78/266 [00:23<00:52,  3.56it/s]Loading train:  30%|██▉       | 79/266 [00:23<00:54,  3.44it/s]Loading train:  30%|███       | 80/266 [00:23<00:55,  3.38it/s]Loading train:  30%|███       | 81/266 [00:24<00:55,  3.31it/s]Loading train:  31%|███       | 82/266 [00:24<00:56,  3.28it/s]Loading train:  31%|███       | 83/266 [00:24<00:56,  3.25it/s]Loading train:  32%|███▏      | 84/266 [00:25<00:56,  3.24it/s]Loading train:  32%|███▏      | 85/266 [00:25<00:56,  3.21it/s]Loading train:  32%|███▏      | 86/266 [00:25<00:56,  3.20it/s]Loading train:  33%|███▎      | 87/266 [00:26<00:56,  3.16it/s]Loading train:  33%|███▎      | 88/266 [00:26<00:56,  3.17it/s]Loading train:  33%|███▎      | 89/266 [00:26<00:55,  3.19it/s]Loading train:  34%|███▍      | 90/266 [00:27<00:55,  3.19it/s]Loading train:  34%|███▍      | 91/266 [00:27<00:54,  3.19it/s]Loading train:  35%|███▍      | 92/266 [00:27<00:54,  3.19it/s]Loading train:  35%|███▍      | 93/266 [00:28<00:54,  3.18it/s]Loading train:  35%|███▌      | 94/266 [00:28<00:54,  3.16it/s]Loading train:  36%|███▌      | 95/266 [00:28<00:55,  3.11it/s]Loading train:  36%|███▌      | 96/266 [00:28<00:53,  3.17it/s]Loading train:  36%|███▋      | 97/266 [00:29<00:53,  3.14it/s]Loading train:  37%|███▋      | 98/266 [00:29<00:52,  3.20it/s]Loading train:  37%|███▋      | 99/266 [00:29<00:50,  3.34it/s]Loading train:  38%|███▊      | 100/266 [00:30<00:49,  3.39it/s]Loading train:  38%|███▊      | 101/266 [00:30<00:48,  3.38it/s]Loading train:  38%|███▊      | 102/266 [00:30<00:48,  3.39it/s]Loading train:  39%|███▊      | 103/266 [00:31<00:48,  3.38it/s]Loading train:  39%|███▉      | 104/266 [00:31<00:48,  3.36it/s]Loading train:  39%|███▉      | 105/266 [00:31<00:48,  3.33it/s]Loading train:  40%|███▉      | 106/266 [00:31<00:48,  3.33it/s]Loading train:  40%|████      | 107/266 [00:32<00:47,  3.33it/s]Loading train:  41%|████      | 108/266 [00:32<00:48,  3.27it/s]Loading train:  41%|████      | 109/266 [00:32<00:47,  3.32it/s]Loading train:  41%|████▏     | 110/266 [00:33<00:47,  3.28it/s]Loading train:  42%|████▏     | 111/266 [00:33<00:46,  3.32it/s]Loading train:  42%|████▏     | 112/266 [00:33<00:45,  3.35it/s]Loading train:  42%|████▏     | 113/266 [00:34<00:45,  3.37it/s]Loading train:  43%|████▎     | 114/266 [00:34<00:45,  3.36it/s]Loading train:  43%|████▎     | 115/266 [00:34<00:45,  3.34it/s]Loading train:  44%|████▎     | 116/266 [00:34<00:44,  3.35it/s]Loading train:  44%|████▍     | 117/266 [00:35<00:44,  3.34it/s]Loading train:  44%|████▍     | 118/266 [00:35<00:44,  3.35it/s]Loading train:  45%|████▍     | 119/266 [00:35<00:45,  3.25it/s]Loading train:  45%|████▌     | 120/266 [00:36<00:45,  3.19it/s]Loading train:  45%|████▌     | 121/266 [00:36<00:45,  3.17it/s]Loading train:  46%|████▌     | 122/266 [00:36<00:45,  3.15it/s]Loading train:  46%|████▌     | 123/266 [00:37<00:45,  3.14it/s]Loading train:  47%|████▋     | 124/266 [00:37<00:45,  3.09it/s]Loading train:  47%|████▋     | 125/266 [00:37<00:47,  2.98it/s]Loading train:  47%|████▋     | 126/266 [00:38<00:46,  3.00it/s]Loading train:  48%|████▊     | 127/266 [00:38<00:45,  3.02it/s]Loading train:  48%|████▊     | 128/266 [00:38<00:45,  3.05it/s]Loading train:  48%|████▊     | 129/266 [00:39<00:44,  3.09it/s]Loading train:  49%|████▉     | 130/266 [00:39<00:43,  3.13it/s]Loading train:  49%|████▉     | 131/266 [00:39<00:42,  3.15it/s]Loading train:  50%|████▉     | 132/266 [00:40<00:42,  3.17it/s]Loading train:  50%|█████     | 133/266 [00:40<00:41,  3.18it/s]Loading train:  50%|█████     | 134/266 [00:40<00:41,  3.16it/s]Loading train:  51%|█████     | 135/266 [00:41<00:41,  3.17it/s]Loading train:  51%|█████     | 136/266 [00:41<00:40,  3.20it/s]Loading train:  52%|█████▏    | 137/266 [00:41<00:39,  3.24it/s]Loading train:  52%|█████▏    | 138/266 [00:41<00:38,  3.30it/s]Loading train:  52%|█████▏    | 139/266 [00:42<00:38,  3.34it/s]Loading train:  53%|█████▎    | 140/266 [00:42<00:37,  3.33it/s]Loading train:  53%|█████▎    | 141/266 [00:42<00:37,  3.32it/s]Loading train:  53%|█████▎    | 142/266 [00:43<00:36,  3.37it/s]Loading train:  54%|█████▍    | 143/266 [00:43<00:36,  3.38it/s]Loading train:  54%|█████▍    | 144/266 [00:43<00:35,  3.41it/s]Loading train:  55%|█████▍    | 145/266 [00:43<00:35,  3.42it/s]Loading train:  55%|█████▍    | 146/266 [00:44<00:34,  3.44it/s]Loading train:  55%|█████▌    | 147/266 [00:44<00:34,  3.45it/s]Loading train:  56%|█████▌    | 148/266 [00:44<00:34,  3.43it/s]Loading train:  56%|█████▌    | 149/266 [00:45<00:34,  3.43it/s]Loading train:  56%|█████▋    | 150/266 [00:45<00:33,  3.42it/s]Loading train:  57%|█████▋    | 151/266 [00:45<00:33,  3.42it/s]Loading train:  57%|█████▋    | 152/266 [00:46<00:33,  3.42it/s]Loading train:  58%|█████▊    | 153/266 [00:46<00:33,  3.36it/s]Loading train:  58%|█████▊    | 154/266 [00:46<00:33,  3.38it/s]Loading train:  58%|█████▊    | 155/266 [00:46<00:31,  3.51it/s]Loading train:  59%|█████▊    | 156/266 [00:47<00:30,  3.59it/s]Loading train:  59%|█████▉    | 157/266 [00:47<00:29,  3.71it/s]Loading train:  59%|█████▉    | 158/266 [00:47<00:28,  3.77it/s]Loading train:  60%|█████▉    | 159/266 [00:47<00:28,  3.80it/s]Loading train:  60%|██████    | 160/266 [00:48<00:28,  3.77it/s]Loading train:  61%|██████    | 161/266 [00:48<00:27,  3.76it/s]Loading train:  61%|██████    | 162/266 [00:48<00:27,  3.78it/s]Loading train:  61%|██████▏   | 163/266 [00:48<00:27,  3.80it/s]Loading train:  62%|██████▏   | 164/266 [00:49<00:26,  3.78it/s]Loading train:  62%|██████▏   | 165/266 [00:49<00:26,  3.81it/s]Loading train:  62%|██████▏   | 166/266 [00:49<00:26,  3.78it/s]Loading train:  63%|██████▎   | 167/266 [00:50<00:26,  3.81it/s]Loading train:  63%|██████▎   | 168/266 [00:50<00:25,  3.85it/s]Loading train:  64%|██████▎   | 169/266 [00:50<00:25,  3.86it/s]Loading train:  64%|██████▍   | 170/266 [00:50<00:24,  3.85it/s]Loading train:  64%|██████▍   | 171/266 [00:51<00:24,  3.88it/s]Loading train:  65%|██████▍   | 172/266 [00:51<00:24,  3.87it/s]Loading train:  65%|██████▌   | 173/266 [00:51<00:24,  3.82it/s]Loading train:  65%|██████▌   | 174/266 [00:51<00:24,  3.77it/s]Loading train:  66%|██████▌   | 175/266 [00:52<00:24,  3.74it/s]Loading train:  66%|██████▌   | 176/266 [00:52<00:24,  3.73it/s]Loading train:  67%|██████▋   | 177/266 [00:52<00:24,  3.70it/s]Loading train:  67%|██████▋   | 178/266 [00:52<00:24,  3.65it/s]Loading train:  67%|██████▋   | 179/266 [00:53<00:23,  3.65it/s]Loading train:  68%|██████▊   | 180/266 [00:53<00:23,  3.65it/s]Loading train:  68%|██████▊   | 181/266 [00:53<00:23,  3.65it/s]Loading train:  68%|██████▊   | 182/266 [00:54<00:22,  3.69it/s]Loading train:  69%|██████▉   | 183/266 [00:54<00:22,  3.70it/s]Loading train:  69%|██████▉   | 184/266 [00:54<00:22,  3.71it/s]Loading train:  70%|██████▉   | 185/266 [00:54<00:22,  3.67it/s]Loading train:  70%|██████▉   | 186/266 [00:55<00:21,  3.70it/s]Loading train:  70%|███████   | 187/266 [00:55<00:21,  3.69it/s]Loading train:  71%|███████   | 188/266 [00:55<00:21,  3.67it/s]Loading train:  71%|███████   | 189/266 [00:55<00:21,  3.65it/s]Loading train:  71%|███████▏  | 190/266 [00:56<00:20,  3.66it/s]Loading train:  72%|███████▏  | 191/266 [00:56<00:20,  3.57it/s]Loading train:  72%|███████▏  | 192/266 [00:56<00:20,  3.57it/s]Loading train:  73%|███████▎  | 193/266 [00:57<00:20,  3.58it/s]Loading train:  73%|███████▎  | 194/266 [00:57<00:20,  3.45it/s]Loading train:  73%|███████▎  | 195/266 [00:57<00:20,  3.55it/s]Loading train:  74%|███████▎  | 196/266 [00:57<00:19,  3.61it/s]Loading train:  74%|███████▍  | 197/266 [00:58<00:18,  3.66it/s]Loading train:  74%|███████▍  | 198/266 [00:58<00:18,  3.67it/s]Loading train:  75%|███████▍  | 199/266 [00:58<00:18,  3.68it/s]Loading train:  75%|███████▌  | 200/266 [00:58<00:17,  3.68it/s]Loading train:  76%|███████▌  | 201/266 [00:59<00:17,  3.63it/s]Loading train:  76%|███████▌  | 202/266 [00:59<00:17,  3.64it/s]Loading train:  76%|███████▋  | 203/266 [00:59<00:17,  3.64it/s]Loading train:  77%|███████▋  | 204/266 [01:00<00:16,  3.65it/s]Loading train:  77%|███████▋  | 205/266 [01:00<00:16,  3.68it/s]Loading train:  77%|███████▋  | 206/266 [01:00<00:16,  3.69it/s]Loading train:  78%|███████▊  | 207/266 [01:00<00:15,  3.70it/s]Loading train:  78%|███████▊  | 208/266 [01:01<00:15,  3.69it/s]Loading train:  79%|███████▊  | 209/266 [01:01<00:15,  3.68it/s]Loading train:  79%|███████▉  | 210/266 [01:01<00:15,  3.70it/s]Loading train:  79%|███████▉  | 211/266 [01:02<00:15,  3.61it/s]Loading train:  80%|███████▉  | 212/266 [01:02<00:14,  3.66it/s]Loading train:  80%|████████  | 213/266 [01:02<00:14,  3.75it/s]Loading train:  80%|████████  | 214/266 [01:02<00:13,  3.82it/s]Loading train:  81%|████████  | 215/266 [01:03<00:13,  3.86it/s]Loading train:  81%|████████  | 216/266 [01:03<00:12,  3.90it/s]Loading train:  82%|████████▏ | 217/266 [01:03<00:12,  3.93it/s]Loading train:  82%|████████▏ | 218/266 [01:03<00:12,  3.95it/s]Loading train:  82%|████████▏ | 219/266 [01:04<00:11,  3.97it/s]Loading train:  83%|████████▎ | 220/266 [01:04<00:11,  3.95it/s]Loading train:  83%|████████▎ | 221/266 [01:04<00:11,  3.96it/s]Loading train:  83%|████████▎ | 222/266 [01:04<00:11,  3.96it/s]Loading train:  84%|████████▍ | 223/266 [01:05<00:10,  3.96it/s]Loading train:  84%|████████▍ | 224/266 [01:05<00:10,  3.95it/s]Loading train:  85%|████████▍ | 225/266 [01:05<00:10,  3.97it/s]Loading train:  85%|████████▍ | 226/266 [01:05<00:10,  3.98it/s]Loading train:  85%|████████▌ | 227/266 [01:06<00:09,  3.92it/s]Loading train:  86%|████████▌ | 228/266 [01:06<00:09,  3.94it/s]Loading train:  86%|████████▌ | 229/266 [01:06<00:09,  3.96it/s]Loading train:  86%|████████▋ | 230/266 [01:06<00:09,  3.94it/s]Loading train:  87%|████████▋ | 231/266 [01:07<00:09,  3.84it/s]Loading train:  87%|████████▋ | 232/266 [01:07<00:09,  3.72it/s]Loading train:  88%|████████▊ | 233/266 [01:07<00:08,  3.70it/s]Loading train:  88%|████████▊ | 234/266 [01:07<00:08,  3.60it/s]Loading train:  88%|████████▊ | 235/266 [01:08<00:08,  3.58it/s]Loading train:  89%|████████▊ | 236/266 [01:08<00:08,  3.60it/s]Loading train:  89%|████████▉ | 237/266 [01:08<00:08,  3.61it/s]Loading train:  89%|████████▉ | 238/266 [01:09<00:07,  3.65it/s]Loading train:  90%|████████▉ | 239/266 [01:09<00:07,  3.67it/s]Loading train:  90%|█████████ | 240/266 [01:09<00:07,  3.69it/s]Loading train:  91%|█████████ | 241/266 [01:09<00:06,  3.70it/s]Loading train:  91%|█████████ | 242/266 [01:10<00:06,  3.71it/s]Loading train:  91%|█████████▏| 243/266 [01:10<00:06,  3.72it/s]Loading train:  92%|█████████▏| 244/266 [01:10<00:05,  3.72it/s]Loading train:  92%|█████████▏| 245/266 [01:10<00:05,  3.73it/s]Loading train:  92%|█████████▏| 246/266 [01:11<00:05,  3.69it/s]Loading train:  93%|█████████▎| 247/266 [01:11<00:05,  3.70it/s]Loading train:  93%|█████████▎| 248/266 [01:11<00:04,  3.71it/s]Loading train:  94%|█████████▎| 249/266 [01:12<00:04,  3.54it/s]Loading train:  94%|█████████▍| 250/266 [01:12<00:04,  3.44it/s]Loading train:  94%|█████████▍| 251/266 [01:12<00:04,  3.38it/s]Loading train:  95%|█████████▍| 252/266 [01:12<00:04,  3.31it/s]Loading train:  95%|█████████▌| 253/266 [01:13<00:03,  3.25it/s]Loading train:  95%|█████████▌| 254/266 [01:13<00:03,  3.20it/s]Loading train:  96%|█████████▌| 255/266 [01:13<00:03,  3.18it/s]Loading train:  96%|█████████▌| 256/266 [01:14<00:03,  3.20it/s]Loading train:  97%|█████████▋| 257/266 [01:14<00:02,  3.21it/s]Loading train:  97%|█████████▋| 258/266 [01:14<00:02,  3.23it/s]Loading train:  97%|█████████▋| 259/266 [01:15<00:02,  3.23it/s]Loading train:  98%|█████████▊| 260/266 [01:15<00:01,  3.24it/s]Loading train:  98%|█████████▊| 261/266 [01:15<00:01,  3.24it/s]Loading train:  98%|█████████▊| 262/266 [01:16<00:01,  3.23it/s]Loading train:  99%|█████████▉| 263/266 [01:16<00:00,  3.24it/s]Loading train:  99%|█████████▉| 264/266 [01:16<00:00,  3.24it/s]Loading train: 100%|█████████▉| 265/266 [01:17<00:00,  3.24it/s]Loading train: 100%|██████████| 266/266 [01:17<00:00,  3.21it/s]Loading train: 100%|██████████| 266/266 [01:17<00:00,  3.44it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 48.32it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 47.03it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 45.72it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 44.57it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 44.23it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 44.52it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:05, 44.21it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:05, 43.66it/s]concatenating: train:  17%|█▋        | 45/266 [00:01<00:05, 44.03it/s]concatenating: train:  19%|█▉        | 50/266 [00:01<00:04, 45.49it/s]concatenating: train:  21%|██        | 55/266 [00:01<00:04, 45.93it/s]concatenating: train:  23%|██▎       | 61/266 [00:01<00:04, 47.82it/s]concatenating: train:  25%|██▌       | 67/266 [00:01<00:04, 48.53it/s]concatenating: train:  27%|██▋       | 73/266 [00:01<00:03, 50.15it/s]concatenating: train:  30%|██▉       | 79/266 [00:01<00:03, 49.57it/s]concatenating: train:  32%|███▏      | 84/266 [00:01<00:03, 46.89it/s]concatenating: train:  33%|███▎      | 89/266 [00:01<00:03, 46.30it/s]concatenating: train:  35%|███▌      | 94/266 [00:02<00:03, 45.97it/s]concatenating: train:  37%|███▋      | 99/266 [00:02<00:03, 46.88it/s]concatenating: train:  39%|███▉      | 104/266 [00:02<00:03, 46.02it/s]concatenating: train:  41%|████      | 109/266 [00:02<00:03, 44.68it/s]concatenating: train:  43%|████▎     | 114/266 [00:02<00:03, 44.91it/s]concatenating: train:  45%|████▍     | 119/266 [00:02<00:03, 43.93it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:03, 42.74it/s]concatenating: train:  48%|████▊     | 129/266 [00:02<00:03, 41.36it/s]concatenating: train:  50%|█████     | 134/266 [00:02<00:03, 42.22it/s]concatenating: train:  52%|█████▏    | 139/266 [00:03<00:02, 43.44it/s]concatenating: train:  55%|█████▍    | 145/266 [00:03<00:02, 46.74it/s]concatenating: train:  56%|█████▋    | 150/266 [00:03<00:02, 47.42it/s]concatenating: train:  58%|█████▊    | 155/266 [00:03<00:02, 47.60it/s]concatenating: train:  61%|██████    | 161/266 [00:03<00:02, 48.96it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:02, 48.54it/s]concatenating: train:  64%|██████▍   | 171/266 [00:03<00:01, 48.45it/s]concatenating: train:  66%|██████▌   | 176/266 [00:03<00:01, 47.62it/s]concatenating: train:  68%|██████▊   | 181/266 [00:03<00:01, 47.10it/s]concatenating: train:  70%|███████   | 187/266 [00:04<00:01, 48.64it/s]concatenating: train:  73%|███████▎  | 193/266 [00:04<00:01, 50.08it/s]concatenating: train:  75%|███████▍  | 199/266 [00:04<00:01, 51.93it/s]concatenating: train:  77%|███████▋  | 205/266 [00:04<00:01, 50.12it/s]concatenating: train:  79%|███████▉  | 211/266 [00:04<00:01, 51.25it/s]concatenating: train:  82%|████████▏ | 217/266 [00:04<00:00, 51.60it/s]concatenating: train:  84%|████████▍ | 223/266 [00:04<00:00, 51.06it/s]concatenating: train:  86%|████████▌ | 229/266 [00:04<00:00, 50.88it/s]concatenating: train:  88%|████████▊ | 235/266 [00:04<00:00, 50.07it/s]concatenating: train:  91%|█████████ | 241/266 [00:05<00:00, 48.53it/s]concatenating: train:  92%|█████████▏| 246/266 [00:05<00:00, 47.87it/s]concatenating: train:  94%|█████████▍| 251/266 [00:05<00:00, 46.61it/s]concatenating: train:  96%|█████████▌| 256/266 [00:05<00:00, 45.06it/s]concatenating: train:  98%|█████████▊| 261/266 [00:05<00:00, 43.89it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 43.54it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 46.74it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  3.22it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  3.29it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  3.42it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.40it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.44it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 373.41it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2020-01-21 00:21:06.533653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 00:21:06.533750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 00:21:06.533764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 00:21:06.533772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 00:21:06.534050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from WMn   /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 396 samples
Epoch 1/300
 - 74s - loss: 0.0773 - acc: 0.9919 - mDice: 0.8496 - val_loss: -4.9015e-02 - val_acc: 0.9946 - val_mDice: 0.5665

Epoch 00001: val_mDice improved from -inf to 0.56649, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300
 - 71s - loss: 0.0505 - acc: 0.9944 - mDice: 0.9019 - val_loss: -1.6532e-02 - val_acc: 0.9947 - val_mDice: 0.5499

Epoch 00002: val_mDice did not improve from 0.56649
Epoch 3/300
 - 72s - loss: 0.0457 - acc: 0.9949 - mDice: 0.9112 - val_loss: -9.9586e-02 - val_acc: 0.9947 - val_mDice: 0.5657

Epoch 00003: val_mDice did not improve from 0.56649
Epoch 4/300
 - 72s - loss: 0.0419 - acc: 0.9952 - mDice: 0.9187 - val_loss: -9.5538e-02 - val_acc: 0.9950 - val_mDice: 0.5624

Epoch 00004: val_mDice did not improve from 0.56649
Epoch 5/300
 - 72s - loss: 0.0400 - acc: 0.9954 - mDice: 0.9223 - val_loss: -9.5177e-02 - val_acc: 0.9948 - val_mDice: 0.5568

Epoch 00005: val_mDice did not improve from 0.56649
Epoch 6/300
 - 72s - loss: 0.0382 - acc: 0.9955 - mDice: 0.9258 - val_loss: -9.2349e-02 - val_acc: 0.9950 - val_mDice: 0.5516

Epoch 00006: val_mDice did not improve from 0.56649
Epoch 7/300
 - 72s - loss: 0.0370 - acc: 0.9957 - mDice: 0.9283 - val_loss: -6.9527e-02 - val_acc: 0.9945 - val_mDice: 0.5563

Epoch 00007: val_mDice did not improve from 0.56649
Epoch 8/300
 - 73s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9297 - val_loss: -9.3580e-02 - val_acc: 0.9951 - val_mDice: 0.5534

Epoch 00008: val_mDice did not improve from 0.56649
Epoch 9/300
 - 73s - loss: 0.0356 - acc: 0.9958 - mDice: 0.9310 - val_loss: -9.3031e-02 - val_acc: 0.9946 - val_mDice: 0.5527

Epoch 00009: val_mDice did not improve from 0.56649
Epoch 10/300
 - 70s - loss: 0.0345 - acc: 0.9959 - mDice: 0.9331 - val_loss: -9.8951e-02 - val_acc: 0.9948 - val_mDice: 0.5643

Epoch 00010: val_mDice did not improve from 0.56649
Epoch 11/300
 - 70s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9338 - val_loss: -7.2968e-02 - val_acc: 0.9949 - val_mDice: 0.5624

Epoch 00011: val_mDice did not improve from 0.56649
Epoch 12/300
 - 70s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9352 - val_loss: -6.8246e-02 - val_acc: 0.9951 - val_mDice: 0.5523

Epoch 00012: val_mDice did not improve from 0.56649
Epoch 13/300
 - 70s - loss: 0.0330 - acc: 0.9960 - mDice: 0.9360 - val_loss: -9.1383e-02 - val_acc: 0.9948 - val_mDice: 0.5498

Epoch 00013: val_mDice did not improve from 0.56649
Epoch 14/300
 - 70s - loss: 0.0325 - acc: 0.9961 - mDice: 0.9369 - val_loss: -9.0374e-02 - val_acc: 0.9948 - val_mDice: 0.5474

Epoch 00014: val_mDice did not improve from 0.56649
Epoch 15/300
 - 70s - loss: 0.0323 - acc: 0.9961 - mDice: 0.9373 - val_loss: -9.5307e-02 - val_acc: 0.9947 - val_mDice: 0.5576

Epoch 00015: val_mDice did not improve from 0.56649
Epoch 16/300
 - 69s - loss: 0.0317 - acc: 0.9961 - mDice: 0.9385 - val_loss: -9.7730e-02 - val_acc: 0.9948 - val_mDice: 0.5620

Epoch 00016: val_mDice did not improve from 0.56649

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 17/300
 - 70s - loss: 0.0303 - acc: 0.9963 - mDice: 0.9413 - val_loss: -9.8029e-02 - val_acc: 0.9948 - val_mDice: 0.5626

Epoch 00017: val_mDice did not improve from 0.56649
Epoch 18/300
 - 69s - loss: 0.0296 - acc: 0.9963 - mDice: 0.9425 - val_loss: -9.6553e-02 - val_acc: 0.9947 - val_mDice: 0.5597

Epoch 00018: val_mDice did not improve from 0.56649
Epoch 19/300
 - 69s - loss: 0.0299 - acc: 0.9963 - mDice: 0.9420 - val_loss: -8.8245e-02 - val_acc: 0.9948 - val_mDice: 0.5565

Epoch 00019: val_mDice did not improve from 0.56649
Epoch 20/300
 - 70s - loss: 0.0295 - acc: 0.9964 - mDice: 0.9428 - val_loss: -9.0283e-02 - val_acc: 0.9946 - val_mDice: 0.5602

Epoch 00020: val_mDice did not improve from 0.56649
Epoch 21/300
 - 70s - loss: 0.0289 - acc: 0.9964 - mDice: 0.9439 - val_loss: -9.3093e-02 - val_acc: 0.9948 - val_mDice: 0.5527

Epoch 00021: val_mDice did not improve from 0.56649
Epoch 22/300
 - 70s - loss: 0.0294 - acc: 0.9964 - mDice: 0.9430 - val_loss: -9.8280e-02 - val_acc: 0.9946 - val_mDice: 0.5632

Epoch 00022: val_mDice did not improve from 0.56649
Epoch 23/300
 - 70s - loss: 0.0287 - acc: 0.9964 - mDice: 0.9444 - val_loss: -9.2538e-02 - val_acc: 0.9947 - val_mDice: 0.5517

Epoch 00023: val_mDice did not improve from 0.56649
Epoch 24/300
 - 70s - loss: 0.0287 - acc: 0.9964 - mDice: 0.9443 - val_loss: -9.4076e-02 - val_acc: 0.9949 - val_mDice: 0.5546

Epoch 00024: val_mDice did not improve from 0.56649
Epoch 25/300
 - 69s - loss: 0.0286 - acc: 0.9964 - mDice: 0.9446 - val_loss: -9.3072e-02 - val_acc: 0.9949 - val_mDice: 0.5526

Epoch 00025: val_mDice did not improve from 0.56649
Epoch 26/300
 - 69s - loss: 0.0286 - acc: 0.9965 - mDice: 0.9445 - val_loss: -6.7172e-02 - val_acc: 0.9948 - val_mDice: 0.5514

Epoch 00026: val_mDice did not improve from 0.56649
Epoch 27/300
 - 69s - loss: 0.0280 - acc: 0.9965 - mDice: 0.9456 - val_loss: -9.0301e-02 - val_acc: 0.9948 - val_mDice: 0.5471

Epoch 00027: val_mDice did not improve from 0.56649
Epoch 28/300
 - 70s - loss: 0.0282 - acc: 0.9965 - mDice: 0.9453 - val_loss: -9.5607e-02 - val_acc: 0.9948 - val_mDice: 0.5579

Epoch 00028: val_mDice did not improve from 0.56649
Epoch 29/300
 - 70s - loss: 0.0279 - acc: 0.9965 - mDice: 0.9458 - val_loss: -9.5466e-02 - val_acc: 0.9948 - val_mDice: 0.5575

Epoch 00029: val_mDice did not improve from 0.56649
Epoch 30/300
 - 71s - loss: 0.0284 - acc: 0.9965 - mDice: 0.9449 - val_loss: -4.2339e-02 - val_acc: 0.9947 - val_mDice: 0.5522

Epoch 00030: val_mDice did not improve from 0.56649
Epoch 31/300
 - 70s - loss: 0.0278 - acc: 0.9965 - mDice: 0.9461 - val_loss: -7.0604e-02 - val_acc: 0.9945 - val_mDice: 0.5585

Epoch 00031: val_mDice did not improve from 0.56649

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 32/300
 - 70s - loss: 0.0271 - acc: 0.9966 - mDice: 0.9474 - val_loss: -7.0063e-02 - val_acc: 0.9945 - val_mDice: 0.5574

Epoch 00032: val_mDice did not improve from 0.56649
Epoch 33/300
 - 70s - loss: 0.0269 - acc: 0.9966 - mDice: 0.9478 - val_loss: -7.1736e-02 - val_acc: 0.9948 - val_mDice: 0.5605

Epoch 00033: val_mDice did not improve from 0.56649
Epoch 34/300
 - 70s - loss: 0.0271 - acc: 0.9966 - mDice: 0.9474 - val_loss: -6.7968e-02 - val_acc: 0.9947 - val_mDice: 0.5610

Epoch 00034: val_mDice did not improve from 0.56649
Epoch 35/300
 - 69s - loss: 0.0266 - acc: 0.9966 - mDice: 0.9485 - val_loss: -9.1276e-02 - val_acc: 0.9947 - val_mDice: 0.5492

Epoch 00035: val_mDice did not improve from 0.56649
Epoch 36/300
 - 69s - loss: 0.0265 - acc: 0.9966 - mDice: 0.9487 - val_loss: -9.6062e-02 - val_acc: 0.9947 - val_mDice: 0.5587

Epoch 00036: val_mDice did not improve from 0.56649
Epoch 37/300
 - 69s - loss: 0.0264 - acc: 0.9966 - mDice: 0.9488 - val_loss: -9.2442e-02 - val_acc: 0.9948 - val_mDice: 0.5514

Epoch 00037: val_mDice did not improve from 0.56649
Epoch 38/300
 - 69s - loss: 0.0266 - acc: 0.9966 - mDice: 0.9484 - val_loss: -9.3276e-02 - val_acc: 0.9948 - val_mDice: 0.5531

Epoch 00038: val_mDice did not improve from 0.56649
Epoch 39/300
 - 69s - loss: 0.0264 - acc: 0.9966 - mDice: 0.9487 - val_loss: -9.5181e-02 - val_acc: 0.9948 - val_mDice: 0.5569

Epoch 00039: val_mDice did not improve from 0.56649
Epoch 40/300
 - 70s - loss: 0.0262 - acc: 0.9966 - mDice: 0.9492 - val_loss: -9.3569e-02 - val_acc: 0.9949 - val_mDice: 0.5537

Epoch 00040: val_mDice did not improve from 0.56649
Epoch 41/300
 - 70s - loss: 0.0266 - acc: 0.9966 - mDice: 0.9485 - val_loss: -9.3636e-02 - val_acc: 0.9949 - val_mDice: 0.5538

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.46it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.89it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.30it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.28it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:04,  4.10it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:03,  4.19it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:55,  4.72it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:52,  4.98it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<00:55,  4.69it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:52,  4.96it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:49,  5.20it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:47,  5.38it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:49,  5.17it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:47,  5.36it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:46,  5.43it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:46,  5.48it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:45,  5.55it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:45,  5.58it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:44,  5.60it/s]predicting train subjects:   6%|▌         | 16/266 [00:03<00:47,  5.27it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:45,  5.43it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:44,  5.53it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:44,  5.56it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:44,  5.58it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:43,  5.60it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:43,  5.64it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:42,  5.65it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:42,  5.74it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:41,  5.84it/s]predicting train subjects:  10%|▉         | 26/266 [00:04<00:40,  5.89it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:40,  5.88it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:40,  5.90it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:39,  5.94it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:39,  5.94it/s]predicting train subjects:  12%|█▏        | 31/266 [00:05<00:39,  5.92it/s]predicting train subjects:  12%|█▏        | 32/266 [00:05<00:40,  5.75it/s]predicting train subjects:  12%|█▏        | 33/266 [00:05<00:39,  5.85it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:39,  5.90it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:39,  5.91it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:38,  5.92it/s]predicting train subjects:  14%|█▍        | 37/266 [00:06<00:38,  5.89it/s]predicting train subjects:  14%|█▍        | 38/266 [00:06<00:42,  5.36it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:41,  5.52it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:40,  5.61it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:40,  5.59it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:38,  5.82it/s]predicting train subjects:  16%|█▌        | 43/266 [00:07<00:36,  6.08it/s]predicting train subjects:  17%|█▋        | 44/266 [00:07<00:35,  6.28it/s]predicting train subjects:  17%|█▋        | 45/266 [00:07<00:34,  6.40it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:33,  6.52it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:33,  6.49it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:33,  6.53it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:32,  6.61it/s]predicting train subjects:  19%|█▉        | 50/266 [00:08<00:32,  6.59it/s]predicting train subjects:  19%|█▉        | 51/266 [00:08<00:32,  6.61it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:32,  6.68it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:31,  6.68it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:31,  6.69it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:31,  6.68it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:31,  6.72it/s]predicting train subjects:  21%|██▏       | 57/266 [00:09<00:30,  6.75it/s]predicting train subjects:  22%|██▏       | 58/266 [00:09<00:30,  6.80it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:30,  6.83it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:29,  6.91it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:29,  7.00it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:29,  7.01it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:28,  7.04it/s]predicting train subjects:  24%|██▍       | 64/266 [00:10<00:28,  7.08it/s]predicting train subjects:  24%|██▍       | 65/266 [00:10<00:28,  7.08it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:28,  7.00it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:28,  7.00it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:28,  6.88it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:28,  6.94it/s]predicting train subjects:  26%|██▋       | 70/266 [00:11<00:27,  7.01it/s]predicting train subjects:  27%|██▋       | 71/266 [00:11<00:27,  7.03it/s]predicting train subjects:  27%|██▋       | 72/266 [00:11<00:27,  7.01it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:27,  7.00it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:27,  7.03it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:27,  7.01it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:27,  6.90it/s]predicting train subjects:  29%|██▉       | 77/266 [00:12<00:27,  6.89it/s]predicting train subjects:  29%|██▉       | 78/266 [00:12<00:29,  6.46it/s]predicting train subjects:  30%|██▉       | 79/266 [00:12<00:30,  6.13it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:30,  6.01it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:31,  5.94it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:31,  5.91it/s]predicting train subjects:  31%|███       | 83/266 [00:13<00:31,  5.83it/s]predicting train subjects:  32%|███▏      | 84/266 [00:13<00:31,  5.79it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:31,  5.74it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:31,  5.73it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:31,  5.76it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:30,  5.79it/s]predicting train subjects:  33%|███▎      | 89/266 [00:14<00:30,  5.80it/s]predicting train subjects:  34%|███▍      | 90/266 [00:14<00:30,  5.82it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:30,  5.79it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:30,  5.74it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:30,  5.75it/s]predicting train subjects:  35%|███▌      | 94/266 [00:15<00:30,  5.73it/s]predicting train subjects:  36%|███▌      | 95/266 [00:15<00:29,  5.76it/s]predicting train subjects:  36%|███▌      | 96/266 [00:15<00:31,  5.42it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:33,  5.05it/s]predicting train subjects:  37%|███▋      | 98/266 [00:16<00:33,  4.98it/s]predicting train subjects:  37%|███▋      | 99/266 [00:16<00:30,  5.51it/s]predicting train subjects:  38%|███▊      | 100/266 [00:16<00:30,  5.43it/s]predicting train subjects:  38%|███▊      | 101/266 [00:16<00:29,  5.68it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:28,  5.81it/s]predicting train subjects:  39%|███▊      | 103/266 [00:17<00:27,  5.95it/s]predicting train subjects:  39%|███▉      | 104/266 [00:17<00:26,  6.02it/s]predicting train subjects:  39%|███▉      | 105/266 [00:17<00:26,  6.05it/s]predicting train subjects:  40%|███▉      | 106/266 [00:17<00:26,  6.15it/s]predicting train subjects:  40%|████      | 107/266 [00:17<00:25,  6.25it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:25,  6.31it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:24,  6.32it/s]predicting train subjects:  41%|████▏     | 110/266 [00:18<00:24,  6.30it/s]predicting train subjects:  42%|████▏     | 111/266 [00:18<00:24,  6.35it/s]predicting train subjects:  42%|████▏     | 112/266 [00:18<00:24,  6.29it/s]predicting train subjects:  42%|████▏     | 113/266 [00:18<00:24,  6.37it/s]predicting train subjects:  43%|████▎     | 114/266 [00:18<00:23,  6.43it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:23,  6.46it/s]predicting train subjects:  44%|████▎     | 116/266 [00:19<00:23,  6.45it/s]predicting train subjects:  44%|████▍     | 117/266 [00:19<00:23,  6.37it/s]predicting train subjects:  44%|████▍     | 118/266 [00:19<00:23,  6.41it/s]predicting train subjects:  45%|████▍     | 119/266 [00:19<00:23,  6.21it/s]predicting train subjects:  45%|████▌     | 120/266 [00:19<00:23,  6.13it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:23,  6.05it/s]predicting train subjects:  46%|████▌     | 122/266 [00:20<00:24,  5.94it/s]predicting train subjects:  46%|████▌     | 123/266 [00:20<00:23,  5.96it/s]predicting train subjects:  47%|████▋     | 124/266 [00:20<00:23,  5.92it/s]predicting train subjects:  47%|████▋     | 125/266 [00:20<00:24,  5.83it/s]predicting train subjects:  47%|████▋     | 126/266 [00:20<00:24,  5.63it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:24,  5.61it/s]predicting train subjects:  48%|████▊     | 128/266 [00:21<00:24,  5.65it/s]predicting train subjects:  48%|████▊     | 129/266 [00:21<00:24,  5.70it/s]predicting train subjects:  49%|████▉     | 130/266 [00:21<00:23,  5.73it/s]predicting train subjects:  49%|████▉     | 131/266 [00:21<00:23,  5.73it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:23,  5.70it/s]predicting train subjects:  50%|█████     | 133/266 [00:22<00:23,  5.73it/s]predicting train subjects:  50%|█████     | 134/266 [00:22<00:23,  5.51it/s]predicting train subjects:  51%|█████     | 135/266 [00:22<00:23,  5.57it/s]predicting train subjects:  51%|█████     | 136/266 [00:22<00:23,  5.64it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:22<00:22,  5.74it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:21,  5.86it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:23<00:21,  6.01it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:23<00:20,  6.15it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:23<00:20,  6.22it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:23<00:19,  6.30it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:23<00:19,  6.36it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:19,  6.32it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:24<00:19,  6.06it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:24<00:20,  5.78it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:24<00:20,  5.90it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:24<00:19,  6.00it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:24<00:19,  6.03it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:19,  6.01it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:25<00:18,  6.08it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:25<00:18,  6.16it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:25<00:18,  6.20it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:25<00:17,  6.24it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:25<00:16,  6.57it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:25<00:16,  6.80it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:15,  7.05it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:26<00:15,  7.10it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:26<00:15,  7.05it/s]predicting train subjects:  60%|██████    | 160/266 [00:26<00:14,  7.10it/s]predicting train subjects:  61%|██████    | 161/266 [00:26<00:14,  7.15it/s]predicting train subjects:  61%|██████    | 162/266 [00:26<00:14,  7.23it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:26<00:14,  7.20it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:14,  7.21it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:27<00:13,  7.24it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:27<00:13,  7.32it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:27<00:13,  7.37it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:27<00:13,  7.33it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:27<00:13,  7.24it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:27<00:13,  7.28it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:27<00:12,  7.33it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:12,  7.29it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:28<00:13,  7.10it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:28<00:13,  7.00it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:28<00:13,  6.89it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:28<00:13,  6.82it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:28<00:12,  6.86it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:12,  6.83it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:29<00:12,  6.88it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:29<00:12,  6.82it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:29<00:12,  6.87it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:29<00:12,  6.92it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:29<00:11,  6.97it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:29<00:11,  7.03it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:11,  7.06it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:30<00:11,  6.96it/s]predicting train subjects:  70%|███████   | 187/266 [00:30<00:11,  6.93it/s]predicting train subjects:  71%|███████   | 188/266 [00:30<00:11,  6.95it/s]predicting train subjects:  71%|███████   | 189/266 [00:30<00:11,  6.96it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:30<00:10,  6.97it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:30<00:11,  6.44it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:31<00:12,  5.81it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:31<00:12,  6.08it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:31<00:13,  5.42it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:31<00:12,  5.68it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:31<00:11,  5.88it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:31<00:11,  6.11it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:32<00:10,  6.30it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:32<00:10,  6.47it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:32<00:10,  6.60it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:32<00:09,  6.72it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:32<00:09,  6.81it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:32<00:09,  6.88it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:32<00:09,  6.89it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:33<00:09,  6.65it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:33<00:09,  6.52it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:33<00:08,  6.59it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:33<00:08,  6.57it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:33<00:08,  6.71it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:33<00:08,  6.76it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:34<00:08,  6.84it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:34<00:07,  6.88it/s]predicting train subjects:  80%|████████  | 213/266 [00:34<00:07,  7.02it/s]predicting train subjects:  80%|████████  | 214/266 [00:34<00:07,  7.04it/s]predicting train subjects:  81%|████████  | 215/266 [00:34<00:07,  7.18it/s]predicting train subjects:  81%|████████  | 216/266 [00:34<00:06,  7.26it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:34<00:06,  7.34it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:34<00:06,  7.39it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:35<00:06,  7.42it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:35<00:06,  7.45it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:35<00:06,  7.45it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:35<00:05,  7.51it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:35<00:05,  7.57it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:35<00:05,  7.59it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:35<00:05,  7.59it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:36<00:05,  7.62it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:36<00:05,  7.64it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:36<00:04,  7.61it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:36<00:04,  7.55it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:36<00:04,  7.57it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:36<00:04,  7.39it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:36<00:04,  7.23it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:36<00:04,  7.18it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:37<00:04,  7.17it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:37<00:04,  7.21it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:37<00:04,  7.11it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:37<00:04,  7.06it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:37<00:04,  6.99it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:37<00:03,  7.04it/s]predicting train subjects:  90%|█████████ | 240/266 [00:37<00:03,  7.05it/s]predicting train subjects:  91%|█████████ | 241/266 [00:38<00:03,  7.09it/s]predicting train subjects:  91%|█████████ | 242/266 [00:38<00:03,  7.11it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:38<00:03,  7.12it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:38<00:03,  7.10it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:38<00:02,  7.10it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:38<00:02,  7.13it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:38<00:02,  7.13it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:39<00:02,  7.13it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:39<00:02,  6.72it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:39<00:02,  6.45it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:39<00:02,  6.24it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:39<00:02,  6.14it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:39<00:02,  6.11it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:40<00:01,  6.06it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:40<00:01,  6.01it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:40<00:01,  5.89it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:40<00:01,  5.93it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:40<00:01,  5.84it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:41<00:01,  5.65it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:41<00:01,  5.69it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:41<00:00,  5.75it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:41<00:00,  5.83it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:41<00:00,  5.88it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:41<00:00,  5.96it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:42<00:00,  6.03it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.04it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.31it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 81.49it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 72.24it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 71.46it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 69.51it/s]saving BB  train1-THALAMUS:  11%|█         | 29/266 [00:00<00:03, 69.51it/s]saving BB  train1-THALAMUS:  14%|█▍        | 37/266 [00:00<00:03, 70.49it/s]saving BB  train1-THALAMUS:  17%|█▋        | 45/266 [00:00<00:03, 71.94it/s]saving BB  train1-THALAMUS:  20%|█▉        | 53/266 [00:00<00:02, 73.89it/s]saving BB  train1-THALAMUS:  23%|██▎       | 62/266 [00:00<00:02, 76.02it/s]saving BB  train1-THALAMUS:  27%|██▋       | 71/266 [00:00<00:02, 78.23it/s]saving BB  train1-THALAMUS:  30%|██▉       | 79/266 [00:01<00:02, 78.49it/s]saving BB  train1-THALAMUS:  33%|███▎      | 87/266 [00:01<00:02, 77.22it/s]saving BB  train1-THALAMUS:  36%|███▌      | 95/266 [00:01<00:02, 76.44it/s]saving BB  train1-THALAMUS:  39%|███▊      | 103/266 [00:01<00:02, 77.13it/s]saving BB  train1-THALAMUS:  42%|████▏     | 111/266 [00:01<00:01, 77.93it/s]saving BB  train1-THALAMUS:  45%|████▍     | 119/266 [00:01<00:01, 78.23it/s]saving BB  train1-THALAMUS:  48%|████▊     | 127/266 [00:01<00:01, 77.52it/s]saving BB  train1-THALAMUS:  51%|█████     | 135/266 [00:01<00:01, 76.61it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 143/266 [00:01<00:01, 76.40it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 151/266 [00:01<00:01, 76.30it/s]saving BB  train1-THALAMUS:  60%|██████    | 160/266 [00:02<00:01, 78.83it/s]saving BB  train1-THALAMUS:  64%|██████▎   | 169/266 [00:02<00:01, 79.93it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 179/266 [00:02<00:01, 82.96it/s]saving BB  train1-THALAMUS:  71%|███████   | 189/266 [00:02<00:00, 86.15it/s]saving BB  train1-THALAMUS:  74%|███████▍  | 198/266 [00:02<00:00, 82.21it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 207/266 [00:02<00:00, 80.68it/s]saving BB  train1-THALAMUS:  81%|████████  | 216/266 [00:02<00:00, 81.30it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 225/266 [00:02<00:00, 81.85it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 234/266 [00:02<00:00, 83.08it/s]saving BB  train1-THALAMUS:  91%|█████████▏| 243/266 [00:03<00:00, 84.48it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 252/266 [00:03<00:00, 83.46it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 261/266 [00:03<00:00, 80.67it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 78.61it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<05:30,  1.25s/it]Loading train:   1%|          | 2/266 [00:02<05:08,  1.17s/it]Loading train:   1%|          | 3/266 [00:03<04:48,  1.10s/it]Loading train:   2%|▏         | 4/266 [00:04<04:28,  1.03s/it]Loading train:   2%|▏         | 5/266 [00:05<04:29,  1.03s/it]Loading train:   2%|▏         | 6/266 [00:05<04:15,  1.02it/s]Loading train:   3%|▎         | 7/266 [00:06<03:56,  1.10it/s]Loading train:   3%|▎         | 8/266 [00:07<03:42,  1.16it/s]Loading train:   3%|▎         | 9/266 [00:08<03:34,  1.20it/s]Loading train:   4%|▍         | 10/266 [00:08<03:26,  1.24it/s]Loading train:   4%|▍         | 11/266 [00:09<03:25,  1.24it/s]Loading train:   5%|▍         | 12/266 [00:10<03:22,  1.25it/s]Loading train:   5%|▍         | 13/266 [00:11<03:18,  1.28it/s]Loading train:   5%|▌         | 14/266 [00:12<03:15,  1.29it/s]Loading train:   6%|▌         | 15/266 [00:12<03:15,  1.28it/s]Loading train:   6%|▌         | 16/266 [00:13<03:13,  1.29it/s]Loading train:   6%|▋         | 17/266 [00:14<03:18,  1.25it/s]Loading train:   7%|▋         | 18/266 [00:15<03:17,  1.26it/s]Loading train:   7%|▋         | 19/266 [00:15<03:12,  1.28it/s]Loading train:   8%|▊         | 20/266 [00:16<03:11,  1.29it/s]Loading train:   8%|▊         | 21/266 [00:17<03:08,  1.30it/s]Loading train:   8%|▊         | 22/266 [00:18<03:08,  1.29it/s]Loading train:   9%|▊         | 23/266 [00:19<03:08,  1.29it/s]Loading train:   9%|▉         | 24/266 [00:19<03:03,  1.32it/s]Loading train:   9%|▉         | 25/266 [00:20<02:51,  1.40it/s]Loading train:  10%|▉         | 26/266 [00:20<02:44,  1.46it/s]Loading train:  10%|█         | 27/266 [00:21<02:38,  1.50it/s]Loading train:  11%|█         | 28/266 [00:22<02:35,  1.53it/s]Loading train:  11%|█         | 29/266 [00:22<02:33,  1.55it/s]Loading train:  11%|█▏        | 30/266 [00:23<02:31,  1.55it/s]Loading train:  12%|█▏        | 31/266 [00:24<02:31,  1.55it/s]Loading train:  12%|█▏        | 32/266 [00:24<02:33,  1.52it/s]Loading train:  12%|█▏        | 33/266 [00:25<02:32,  1.53it/s]Loading train:  13%|█▎        | 34/266 [00:26<02:32,  1.53it/s]Loading train:  13%|█▎        | 35/266 [00:26<02:30,  1.53it/s]Loading train:  14%|█▎        | 36/266 [00:27<02:27,  1.56it/s]Loading train:  14%|█▍        | 37/266 [00:28<02:25,  1.58it/s]Loading train:  14%|█▍        | 38/266 [00:28<02:23,  1.58it/s]Loading train:  15%|█▍        | 39/266 [00:29<02:25,  1.56it/s]Loading train:  15%|█▌        | 40/266 [00:29<02:27,  1.53it/s]Loading train:  15%|█▌        | 41/266 [00:30<02:27,  1.53it/s]Loading train:  16%|█▌        | 42/266 [00:31<02:31,  1.48it/s]Loading train:  16%|█▌        | 43/266 [00:32<02:27,  1.51it/s]Loading train:  17%|█▋        | 44/266 [00:32<02:23,  1.54it/s]Loading train:  17%|█▋        | 45/266 [00:33<02:21,  1.56it/s]Loading train:  17%|█▋        | 46/266 [00:33<02:19,  1.58it/s]Loading train:  18%|█▊        | 47/266 [00:34<02:20,  1.56it/s]Loading train:  18%|█▊        | 48/266 [00:35<02:21,  1.54it/s]Loading train:  18%|█▊        | 49/266 [00:35<02:20,  1.55it/s]Loading train:  19%|█▉        | 50/266 [00:36<02:18,  1.56it/s]Loading train:  19%|█▉        | 51/266 [00:37<02:16,  1.57it/s]Loading train:  20%|█▉        | 52/266 [00:37<02:14,  1.59it/s]Loading train:  20%|█▉        | 53/266 [00:38<02:13,  1.59it/s]Loading train:  20%|██        | 54/266 [00:38<02:14,  1.57it/s]Loading train:  21%|██        | 55/266 [00:39<02:15,  1.56it/s]Loading train:  21%|██        | 56/266 [00:40<02:15,  1.55it/s]Loading train:  21%|██▏       | 57/266 [00:40<02:14,  1.55it/s]Loading train:  22%|██▏       | 58/266 [00:41<02:11,  1.58it/s]Loading train:  22%|██▏       | 59/266 [00:42<02:09,  1.60it/s]Loading train:  23%|██▎       | 60/266 [00:42<02:08,  1.61it/s]Loading train:  23%|██▎       | 61/266 [00:43<02:04,  1.65it/s]Loading train:  23%|██▎       | 62/266 [00:43<02:02,  1.67it/s]Loading train:  24%|██▎       | 63/266 [00:44<02:01,  1.67it/s]Loading train:  24%|██▍       | 64/266 [00:45<01:58,  1.70it/s]Loading train:  24%|██▍       | 65/266 [00:45<01:57,  1.71it/s]Loading train:  25%|██▍       | 66/266 [00:46<01:57,  1.70it/s]Loading train:  25%|██▌       | 67/266 [00:46<01:56,  1.71it/s]Loading train:  26%|██▌       | 68/266 [00:47<01:53,  1.74it/s]Loading train:  26%|██▌       | 69/266 [00:47<01:51,  1.76it/s]Loading train:  26%|██▋       | 70/266 [00:48<01:50,  1.78it/s]Loading train:  27%|██▋       | 71/266 [00:49<01:49,  1.78it/s]Loading train:  27%|██▋       | 72/266 [00:49<01:48,  1.79it/s]Loading train:  27%|██▋       | 73/266 [00:50<01:49,  1.76it/s]Loading train:  28%|██▊       | 74/266 [00:50<01:49,  1.75it/s]Loading train:  28%|██▊       | 75/266 [00:51<01:49,  1.75it/s]Loading train:  29%|██▊       | 76/266 [00:51<01:48,  1.75it/s]Loading train:  29%|██▉       | 77/266 [00:52<01:49,  1.73it/s]Loading train:  29%|██▉       | 78/266 [00:53<01:51,  1.69it/s]Loading train:  30%|██▉       | 79/266 [00:53<01:52,  1.67it/s]Loading train:  30%|███       | 80/266 [00:54<01:52,  1.65it/s]Loading train:  30%|███       | 81/266 [00:54<01:52,  1.64it/s]Loading train:  31%|███       | 82/266 [00:55<01:53,  1.62it/s]Loading train:  31%|███       | 83/266 [00:56<01:53,  1.62it/s]Loading train:  32%|███▏      | 84/266 [00:56<01:52,  1.62it/s]Loading train:  32%|███▏      | 85/266 [00:57<01:53,  1.59it/s]Loading train:  32%|███▏      | 86/266 [00:58<01:53,  1.59it/s]Loading train:  33%|███▎      | 87/266 [00:58<01:54,  1.57it/s]Loading train:  33%|███▎      | 88/266 [00:59<01:58,  1.51it/s]Loading train:  33%|███▎      | 89/266 [01:00<01:56,  1.52it/s]Loading train:  34%|███▍      | 90/266 [01:00<01:54,  1.54it/s]Loading train:  34%|███▍      | 91/266 [01:01<01:52,  1.55it/s]Loading train:  35%|███▍      | 92/266 [01:02<01:51,  1.55it/s]Loading train:  35%|███▍      | 93/266 [01:02<01:50,  1.56it/s]Loading train:  35%|███▌      | 94/266 [01:03<01:49,  1.57it/s]Loading train:  36%|███▌      | 95/266 [01:03<01:50,  1.55it/s]Loading train:  36%|███▌      | 96/266 [01:04<02:05,  1.35it/s]Loading train:  36%|███▋      | 97/266 [01:05<02:17,  1.23it/s]Loading train:  37%|███▋      | 98/266 [01:06<02:22,  1.18it/s]Loading train:  37%|███▋      | 99/266 [01:07<02:18,  1.21it/s]Loading train:  38%|███▊      | 100/266 [01:08<02:17,  1.21it/s]Loading train:  38%|███▊      | 101/266 [01:09<02:10,  1.27it/s]Loading train:  38%|███▊      | 102/266 [01:09<02:01,  1.35it/s]Loading train:  39%|███▊      | 103/266 [01:10<01:57,  1.39it/s]Loading train:  39%|███▉      | 104/266 [01:11<01:53,  1.43it/s]Loading train:  39%|███▉      | 105/266 [01:11<01:53,  1.42it/s]Loading train:  40%|███▉      | 106/266 [01:12<01:51,  1.43it/s]Loading train:  40%|████      | 107/266 [01:13<01:52,  1.42it/s]Loading train:  41%|████      | 108/266 [01:13<01:49,  1.44it/s]Loading train:  41%|████      | 109/266 [01:14<01:48,  1.45it/s]Loading train:  41%|████▏     | 110/266 [01:15<01:45,  1.48it/s]Loading train:  42%|████▏     | 111/266 [01:15<01:42,  1.51it/s]Loading train:  42%|████▏     | 112/266 [01:16<01:43,  1.49it/s]Loading train:  42%|████▏     | 113/266 [01:17<01:42,  1.49it/s]Loading train:  43%|████▎     | 114/266 [01:17<01:43,  1.46it/s]Loading train:  43%|████▎     | 115/266 [01:18<01:43,  1.46it/s]Loading train:  44%|████▎     | 116/266 [01:19<01:43,  1.45it/s]Loading train:  44%|████▍     | 117/266 [01:19<01:41,  1.47it/s]Loading train:  44%|████▍     | 118/266 [01:20<01:38,  1.50it/s]Loading train:  45%|████▍     | 119/266 [01:21<01:37,  1.50it/s]Loading train:  45%|████▌     | 120/266 [01:21<01:36,  1.51it/s]Loading train:  45%|████▌     | 121/266 [01:22<01:37,  1.49it/s]Loading train:  46%|████▌     | 122/266 [01:23<01:36,  1.49it/s]Loading train:  46%|████▌     | 123/266 [01:23<01:36,  1.48it/s]Loading train:  47%|████▋     | 124/266 [01:24<01:35,  1.49it/s]Loading train:  47%|████▋     | 125/266 [01:25<01:34,  1.50it/s]Loading train:  47%|████▋     | 126/266 [01:25<01:32,  1.51it/s]Loading train:  48%|████▊     | 127/266 [01:26<01:30,  1.54it/s]Loading train:  48%|████▊     | 128/266 [01:27<01:29,  1.55it/s]Loading train:  48%|████▊     | 129/266 [01:27<01:28,  1.55it/s]Loading train:  49%|████▉     | 130/266 [01:28<01:28,  1.54it/s]Loading train:  49%|████▉     | 131/266 [01:29<01:29,  1.51it/s]Loading train:  50%|████▉     | 132/266 [01:29<01:29,  1.50it/s]Loading train:  50%|█████     | 133/266 [01:30<01:29,  1.49it/s]Loading train:  50%|█████     | 134/266 [01:31<01:29,  1.48it/s]Loading train:  51%|█████     | 135/266 [01:31<01:29,  1.46it/s]Loading train:  51%|█████     | 136/266 [01:32<01:29,  1.45it/s]Loading train:  52%|█████▏    | 137/266 [01:33<01:29,  1.44it/s]Loading train:  52%|█████▏    | 138/266 [01:34<01:29,  1.44it/s]Loading train:  52%|█████▏    | 139/266 [01:34<01:27,  1.45it/s]Loading train:  53%|█████▎    | 140/266 [01:35<01:26,  1.45it/s]Loading train:  53%|█████▎    | 141/266 [01:36<01:25,  1.47it/s]Loading train:  53%|█████▎    | 142/266 [01:36<01:23,  1.48it/s]Loading train:  54%|█████▍    | 143/266 [01:37<01:22,  1.50it/s]Loading train:  54%|█████▍    | 144/266 [01:38<01:21,  1.49it/s]Loading train:  55%|█████▍    | 145/266 [01:38<01:22,  1.47it/s]Loading train:  55%|█████▍    | 146/266 [01:39<01:22,  1.46it/s]Loading train:  55%|█████▌    | 147/266 [01:40<01:22,  1.45it/s]Loading train:  56%|█████▌    | 148/266 [01:40<01:22,  1.43it/s]Loading train:  56%|█████▌    | 149/266 [01:41<01:20,  1.46it/s]Loading train:  56%|█████▋    | 150/266 [01:42<01:18,  1.47it/s]Loading train:  57%|█████▋    | 151/266 [01:42<01:17,  1.48it/s]Loading train:  57%|█████▋    | 152/266 [01:43<01:16,  1.49it/s]Loading train:  58%|█████▊    | 153/266 [01:44<01:16,  1.48it/s]Loading train:  58%|█████▊    | 154/266 [01:44<01:15,  1.49it/s]Loading train:  58%|█████▊    | 155/266 [01:45<01:12,  1.54it/s]Loading train:  59%|█████▊    | 156/266 [01:46<01:09,  1.58it/s]Loading train:  59%|█████▉    | 157/266 [01:46<01:07,  1.62it/s]Loading train:  59%|█████▉    | 158/266 [01:47<01:06,  1.63it/s]Loading train:  60%|█████▉    | 159/266 [01:47<01:04,  1.65it/s]Loading train:  60%|██████    | 160/266 [01:48<01:02,  1.68it/s]Loading train:  61%|██████    | 161/266 [01:48<01:01,  1.70it/s]Loading train:  61%|██████    | 162/266 [01:49<01:00,  1.72it/s]Loading train:  61%|██████▏   | 163/266 [01:50<00:59,  1.72it/s]Loading train:  62%|██████▏   | 164/266 [01:50<00:59,  1.72it/s]Loading train:  62%|██████▏   | 165/266 [01:51<00:58,  1.74it/s]Loading train:  62%|██████▏   | 166/266 [01:51<00:57,  1.73it/s]Loading train:  63%|██████▎   | 167/266 [01:52<00:57,  1.74it/s]Loading train:  63%|██████▎   | 168/266 [01:53<00:56,  1.74it/s]Loading train:  64%|██████▎   | 169/266 [01:53<00:56,  1.72it/s]Loading train:  64%|██████▍   | 170/266 [01:54<00:56,  1.69it/s]Loading train:  64%|██████▍   | 171/266 [01:54<00:56,  1.68it/s]Loading train:  65%|██████▍   | 172/266 [01:55<00:55,  1.70it/s]Loading train:  65%|██████▌   | 173/266 [01:55<00:55,  1.69it/s]Loading train:  65%|██████▌   | 174/266 [01:56<00:53,  1.73it/s]Loading train:  66%|██████▌   | 175/266 [01:57<00:53,  1.71it/s]Loading train:  66%|██████▌   | 176/266 [01:57<00:51,  1.73it/s]Loading train:  67%|██████▋   | 177/266 [01:58<00:50,  1.76it/s]Loading train:  67%|██████▋   | 178/266 [01:58<00:50,  1.75it/s]Loading train:  67%|██████▋   | 179/266 [01:59<00:49,  1.74it/s]Loading train:  68%|██████▊   | 180/266 [01:59<00:48,  1.77it/s]Loading train:  68%|██████▊   | 181/266 [02:00<00:47,  1.77it/s]Loading train:  68%|██████▊   | 182/266 [02:01<00:47,  1.77it/s]Loading train:  69%|██████▉   | 183/266 [02:01<00:46,  1.77it/s]Loading train:  69%|██████▉   | 184/266 [02:02<00:45,  1.79it/s]Loading train:  70%|██████▉   | 185/266 [02:02<00:45,  1.79it/s]Loading train:  70%|██████▉   | 186/266 [02:03<00:45,  1.75it/s]Loading train:  70%|███████   | 187/266 [02:03<00:45,  1.75it/s]Loading train:  71%|███████   | 188/266 [02:04<00:43,  1.77it/s]Loading train:  71%|███████   | 189/266 [02:05<00:42,  1.80it/s]Loading train:  71%|███████▏  | 190/266 [02:05<00:41,  1.83it/s]Loading train:  72%|███████▏  | 191/266 [02:06<00:48,  1.54it/s]Loading train:  72%|███████▏  | 192/266 [02:07<00:51,  1.43it/s]Loading train:  73%|███████▎  | 193/266 [02:08<00:53,  1.37it/s]Loading train:  73%|███████▎  | 194/266 [02:09<00:58,  1.22it/s]Loading train:  73%|███████▎  | 195/266 [02:09<00:53,  1.32it/s]Loading train:  74%|███████▎  | 196/266 [02:10<00:49,  1.40it/s]Loading train:  74%|███████▍  | 197/266 [02:10<00:46,  1.47it/s]Loading train:  74%|███████▍  | 198/266 [02:11<00:44,  1.53it/s]Loading train:  75%|███████▍  | 199/266 [02:12<00:43,  1.56it/s]Loading train:  75%|███████▌  | 200/266 [02:12<00:41,  1.58it/s]Loading train:  76%|███████▌  | 201/266 [02:13<00:40,  1.60it/s]Loading train:  76%|███████▌  | 202/266 [02:13<00:40,  1.60it/s]Loading train:  76%|███████▋  | 203/266 [02:14<00:40,  1.57it/s]Loading train:  77%|███████▋  | 204/266 [02:15<00:39,  1.59it/s]Loading train:  77%|███████▋  | 205/266 [02:15<00:38,  1.58it/s]Loading train:  77%|███████▋  | 206/266 [02:16<00:37,  1.61it/s]Loading train:  78%|███████▊  | 207/266 [02:17<00:36,  1.61it/s]Loading train:  78%|███████▊  | 208/266 [02:17<00:36,  1.61it/s]Loading train:  79%|███████▊  | 209/266 [02:18<00:35,  1.63it/s]Loading train:  79%|███████▉  | 210/266 [02:18<00:34,  1.64it/s]Loading train:  79%|███████▉  | 211/266 [02:19<00:34,  1.59it/s]Loading train:  80%|███████▉  | 212/266 [02:20<00:33,  1.61it/s]Loading train:  80%|████████  | 213/266 [02:21<00:52,  1.02it/s]Loading train:  80%|████████  | 214/266 [02:22<00:49,  1.04it/s]Loading train:  81%|████████  | 215/266 [02:25<01:15,  1.48s/it]Loading train:  81%|████████  | 216/266 [02:29<01:47,  2.14s/it]Loading train:  82%|████████▏ | 217/266 [02:32<02:08,  2.61s/it]Loading train:  82%|████████▏ | 218/266 [02:36<02:23,  2.99s/it]Loading train:  82%|████████▏ | 219/266 [02:40<02:31,  3.23s/it]Loading train:  83%|████████▎ | 220/266 [02:44<02:31,  3.30s/it]Loading train:  83%|████████▎ | 221/266 [02:48<02:36,  3.48s/it]Loading train:  83%|████████▎ | 222/266 [02:51<02:36,  3.55s/it]Loading train:  84%|████████▍ | 223/266 [02:54<02:27,  3.44s/it]Loading train:  84%|████████▍ | 224/266 [02:57<02:17,  3.27s/it]Loading train:  85%|████████▍ | 225/266 [03:00<02:09,  3.16s/it]Loading train:  85%|████████▍ | 226/266 [03:05<02:20,  3.51s/it]Loading train:  85%|████████▌ | 227/266 [03:11<02:47,  4.29s/it]Loading train:  86%|████████▌ | 228/266 [03:16<02:55,  4.63s/it]Loading train:  86%|████████▌ | 229/266 [03:22<03:05,  5.02s/it]Loading train:  86%|████████▋ | 230/266 [03:27<03:03,  5.09s/it]Loading train:  87%|████████▋ | 231/266 [03:33<03:01,  5.19s/it]Loading train:  87%|████████▋ | 232/266 [03:37<02:51,  5.05s/it]Loading train:  88%|████████▊ | 233/266 [03:42<02:44,  4.98s/it]Loading train:  88%|████████▊ | 234/266 [03:46<02:27,  4.62s/it]Loading train:  88%|████████▊ | 235/266 [03:50<02:16,  4.41s/it]Loading train:  89%|████████▊ | 236/266 [03:54<02:05,  4.18s/it]Loading train:  89%|████████▉ | 237/266 [03:57<01:58,  4.08s/it]Loading train:  89%|████████▉ | 238/266 [04:01<01:49,  3.93s/it]Loading train:  90%|████████▉ | 239/266 [04:04<01:42,  3.79s/it]Loading train:  90%|█████████ | 240/266 [04:08<01:36,  3.72s/it]Loading train:  91%|█████████ | 241/266 [04:11<01:30,  3.62s/it]Loading train:  91%|█████████ | 242/266 [04:15<01:26,  3.59s/it]Loading train:  91%|█████████▏| 243/266 [04:18<01:22,  3.58s/it]Loading train:  92%|█████████▏| 244/266 [04:22<01:18,  3.57s/it]Loading train:  92%|█████████▏| 245/266 [04:25<01:14,  3.54s/it]Loading train:  92%|█████████▏| 246/266 [04:29<01:11,  3.56s/it]Loading train:  93%|█████████▎| 247/266 [04:33<01:07,  3.55s/it]Loading train:  93%|█████████▎| 248/266 [04:36<01:03,  3.51s/it]Loading train:  94%|█████████▎| 249/266 [04:42<01:10,  4.14s/it]Loading train:  94%|█████████▍| 250/266 [04:47<01:12,  4.54s/it]Loading train:  94%|█████████▍| 251/266 [04:53<01:12,  4.81s/it]Loading train:  95%|█████████▍| 252/266 [04:58<01:10,  5.06s/it]Loading train:  95%|█████████▌| 253/266 [05:03<01:06,  5.13s/it]Loading train:  95%|█████████▌| 254/266 [05:09<01:03,  5.27s/it]Loading train:  96%|█████████▌| 255/266 [05:15<00:59,  5.38s/it]Loading train:  96%|█████████▌| 256/266 [05:20<00:53,  5.40s/it]Loading train:  97%|█████████▋| 257/266 [05:25<00:47,  5.33s/it]Loading train:  97%|█████████▋| 258/266 [05:31<00:43,  5.40s/it]Loading train:  97%|█████████▋| 259/266 [05:36<00:37,  5.30s/it]Loading train:  98%|█████████▊| 260/266 [05:41<00:32,  5.33s/it]Loading train:  98%|█████████▊| 261/266 [05:47<00:27,  5.44s/it]Loading train:  98%|█████████▊| 262/266 [05:53<00:21,  5.49s/it]Loading train:  99%|█████████▉| 263/266 [05:58<00:16,  5.57s/it]Loading train:  99%|█████████▉| 264/266 [06:04<00:11,  5.65s/it]Loading train: 100%|█████████▉| 265/266 [06:10<00:05,  5.55s/it]Loading train: 100%|██████████| 266/266 [06:15<00:00,  5.53s/it]Loading train: 100%|██████████| 266/266 [06:15<00:00,  1.41s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 55.74it/s]concatenating: train:   4%|▍         | 11/266 [00:00<00:04, 53.40it/s]concatenating: train:   6%|▌         | 16/266 [00:00<00:04, 50.54it/s]concatenating: train:   8%|▊         | 21/266 [00:00<00:05, 48.61it/s]concatenating: train:  10%|█         | 27/266 [00:00<00:04, 50.27it/s]concatenating: train:  12%|█▏        | 33/266 [00:00<00:04, 52.35it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 56.15it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:03, 55.90it/s]concatenating: train:  20%|█▉        | 52/266 [00:00<00:03, 54.52it/s]concatenating: train:  22%|██▏       | 58/266 [00:01<00:03, 53.47it/s]concatenating: train:  24%|██▍       | 64/266 [00:01<00:03, 54.06it/s]concatenating: train:  27%|██▋       | 71/266 [00:01<00:03, 56.07it/s]concatenating: train:  29%|██▉       | 77/266 [00:01<00:03, 55.77it/s]concatenating: train:  31%|███       | 83/266 [00:01<00:03, 55.70it/s]concatenating: train:  33%|███▎      | 89/266 [00:01<00:03, 55.91it/s]concatenating: train:  36%|███▌      | 96/266 [00:01<00:02, 57.31it/s]concatenating: train:  38%|███▊      | 102/266 [00:01<00:02, 55.75it/s]concatenating: train:  41%|████      | 108/266 [00:01<00:02, 55.05it/s]concatenating: train:  43%|████▎     | 114/266 [00:02<00:02, 54.91it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 53.74it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 54.24it/s]concatenating: train:  50%|█████     | 133/266 [00:02<00:02, 56.55it/s]concatenating: train:  52%|█████▏    | 139/266 [00:02<00:02, 56.42it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 56.83it/s]concatenating: train:  57%|█████▋    | 151/266 [00:02<00:02, 56.13it/s]concatenating: train:  59%|█████▉    | 157/266 [00:02<00:01, 54.80it/s]concatenating: train:  61%|██████▏   | 163/266 [00:02<00:01, 55.80it/s]concatenating: train:  64%|██████▎   | 169/266 [00:03<00:01, 55.70it/s]concatenating: train:  66%|██████▌   | 175/266 [00:03<00:01, 56.09it/s]concatenating: train:  68%|██████▊   | 181/266 [00:03<00:01, 55.17it/s]concatenating: train:  70%|███████   | 187/266 [00:03<00:01, 56.47it/s]concatenating: train:  73%|███████▎  | 193/266 [00:03<00:01, 56.63it/s]concatenating: train:  75%|███████▍  | 199/266 [00:03<00:01, 56.18it/s]concatenating: train:  77%|███████▋  | 205/266 [00:03<00:01, 55.19it/s]concatenating: train:  79%|███████▉  | 211/266 [00:03<00:01, 53.82it/s]concatenating: train:  82%|████████▏ | 217/266 [00:03<00:00, 53.67it/s]concatenating: train:  84%|████████▍ | 223/266 [00:04<00:00, 53.20it/s]concatenating: train:  86%|████████▌ | 229/266 [00:04<00:00, 52.99it/s]concatenating: train:  88%|████████▊ | 235/266 [00:04<00:00, 51.02it/s]concatenating: train:  91%|█████████ | 241/266 [00:04<00:00, 52.43it/s]concatenating: train:  93%|█████████▎| 247/266 [00:04<00:00, 52.56it/s]concatenating: train:  95%|█████████▌| 253/266 [00:04<00:00, 52.73it/s]concatenating: train:  97%|█████████▋| 259/266 [00:04<00:00, 49.39it/s]concatenating: train: 100%|█████████▉| 265/266 [00:04<00:00, 50.07it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 54.11it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:27<01:21, 27.24s/it]Loading test:  50%|█████     | 2/4 [00:40<00:46, 23.11s/it]Loading test:  75%|███████▌  | 3/4 [01:00<00:22, 22.03s/it]Loading test: 100%|██████████| 4/4 [01:26<00:00, 23.26s/it]Loading test: 100%|██████████| 4/4 [01:26<00:00, 21.59s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 59.31it/s]
Epoch 00041: val_mDice did not improve from 0.56649
Restoring model weights from the end of the best epoch
Epoch 00041: early stopping
{'val_loss': [-0.04901540546555712, -0.016532492419384946, -0.09958625487004867, -0.09553778284426892, -0.09517707954151462, -0.09234944629398259, -0.06952727484432134, -0.09358034937670737, -0.09303058834389002, -0.0989509519528259, -0.07296756331366722, -0.06824555635602787, -0.0913825227380401, -0.09037445899513033, -0.09530719922799052, -0.09773041759476517, -0.09802925059891711, -0.09655256941914558, -0.08824491730392581, -0.09028296304321048, -0.09309275026875313, -0.0982799543575807, -0.09253796686728795, -0.09407570574319724, -0.0930724935323903, -0.06717176551018099, -0.09030118422827335, -0.09560679742182145, -0.09546618088327273, -0.0423392037099058, -0.07060353209575017, -0.07006317346987098, -0.07173649466248473, -0.06796757801614627, -0.09127622214381141, -0.09606164746513271, -0.0924422728142353, -0.09327584527658694, -0.09518122289216879, -0.09356881997952557, -0.0936361974989525], 'val_acc': [0.9945646133085694, 0.9946882974619817, 0.9946612179881394, 0.9950079499471067, 0.9948098186892692, 0.9950051804383596, 0.9944550897737946, 0.9951125560986875, 0.9946073766308602, 0.994841506986907, 0.9948571968560267, 0.9950913272120736, 0.9947990477085114, 0.9948242788362984, 0.9946759897049027, 0.994782746741266, 0.9947578226677095, 0.9947119872979443, 0.9948282795723038, 0.9946163067914019, 0.9947842865279226, 0.99458738198184, 0.9946676826838291, 0.9948901118654193, 0.9948501225673791, 0.9947762880662475, 0.9948353538609515, 0.9947553632235286, 0.9947833608497273, 0.9946689139110874, 0.9944517046514184, 0.9945473896734642, 0.9948178231716156, 0.9947006052190607, 0.9947375194592909, 0.9947461320294274, 0.9948119710792195, 0.9947664397533493, 0.9947928960877236, 0.9948639655956114, 0.9948532006355247], 'val_mDice': [0.566492657167743, 0.5498532499327804, 0.56574717239298, 0.5623568331051354, 0.5567984141860948, 0.5515704956846406, 0.5562546424040891, 0.5534289688850292, 0.5526560944777847, 0.5643346574571397, 0.5624430626630783, 0.5523298464414447, 0.5497832732237499, 0.5473687068899978, 0.55759427833813, 0.5620318148849588, 0.5626034198535813, 0.5596648877241996, 0.556457084712747, 0.5602070602654207, 0.5527418720150234, 0.5632423918054561, 0.5516793368139652, 0.5545978357244019, 0.5526166032962124, 0.5513567989221727, 0.547061963665365, 0.5578984568516413, 0.557468097017269, 0.5522077818485823, 0.5585052468108408, 0.5573577366091989, 0.560482188758224, 0.5610463282827175, 0.549234573357748, 0.558725329812127, 0.5514106643623794, 0.5531150019831128, 0.5569160086187449, 0.5536542868599145, 0.5537577507772831], 'loss': [0.07733307954431215, 0.05049626924014745, 0.045704654447581816, 0.04187052208463977, 0.04002699225275242, 0.03823371254290765, 0.036950475792752444, 0.036207608146785385, 0.03555379541147627, 0.03446528711779672, 0.034096702052669715, 0.0334058655351033, 0.032998520871486914, 0.03251718783802843, 0.03231376947446213, 0.03170374554796277, 0.03026013368397326, 0.029641758744116148, 0.029902920224349948, 0.029491326975431432, 0.028949699408628, 0.029380297494283055, 0.028690179074041013, 0.02872916867283459, 0.02856977407106033, 0.028608879098879603, 0.028036077768297387, 0.028208937576210937, 0.027942013191283185, 0.028426608135621256, 0.027805084760418982, 0.027113184185364063, 0.026935958825460425, 0.027127662750861693, 0.026575353097616636, 0.026478678164819636, 0.026425788188690277, 0.02659045340577246, 0.026436731741607017, 0.026203585676464206, 0.026554721698000418], 'acc': [0.9919257020074574, 0.9943979531702268, 0.9948606266062006, 0.9951841680273585, 0.9953822196435071, 0.9955213519202792, 0.9956570707742615, 0.9957354712827875, 0.9958131104057086, 0.9959020417283602, 0.9959404406220591, 0.995992631174835, 0.9960270694211377, 0.9960926452088374, 0.9961159042288612, 0.9961496411009814, 0.996274399634713, 0.9963231762578242, 0.9963404887375198, 0.996366973977408, 0.9964070232640245, 0.9963734947430102, 0.9964220810750931, 0.9964372642503596, 0.996435140121165, 0.9964683117597148, 0.9964643456772575, 0.996472607630261, 0.9964814883581494, 0.9964796075246067, 0.9965155519728501, 0.9965550742331828, 0.9965791254346511, 0.9965872834059069, 0.9966117222004902, 0.9966115661895334, 0.9966072596788755, 0.996630088728494, 0.9966318251538278, 0.9966235930681032, 0.9966343721869965], 'mDice': [0.8496139181241499, 0.9018773287273822, 0.9112006050967183, 0.9186941853872802, 0.9222699279253704, 0.9257797327298727, 0.9282726393395283, 0.9297133009588876, 0.930979278078501, 0.9331042051196043, 0.9338193986816848, 0.9351687186438243, 0.9359664624815062, 0.9368923153273949, 0.9372856503778934, 0.938488969475049, 0.9413019052978565, 0.942510124204771, 0.9419802199176905, 0.9427886317984001, 0.9438504249770086, 0.9430077123451144, 0.9443618307825147, 0.9442775267157928, 0.9445907574726139, 0.944502352190183, 0.9456398892548322, 0.9452968404135205, 0.9458228584466676, 0.9448571489478894, 0.9460798752488067, 0.947434218823814, 0.9477764172974339, 0.9473903701590517, 0.9484824535440954, 0.9486770849627953, 0.9487826326790698, 0.948444082811765, 0.9487492269474146, 0.9492176997511232, 0.9485105720943682], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________2020-01-21 01:24:01.275634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 01:24:01.275752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 01:24:01.275766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 01:24:01.275773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 01:24:01.276067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights from thalamus:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights from thalamus:   2%|▏         | 1/44 [00:00<00:08,  5.35it/s]loading the weights from thalamus:   7%|▋         | 3/44 [00:00<00:06,  6.26it/s]loading the weights from thalamus:   9%|▉         | 4/44 [00:00<00:06,  5.81it/s]loading the weights from thalamus:  18%|█▊        | 8/44 [00:00<00:04,  7.38it/s]loading the weights from thalamus:  20%|██        | 9/44 [00:00<00:05,  6.41it/s]loading the weights from thalamus:  25%|██▌       | 11/44 [00:01<00:04,  7.19it/s]loading the weights from thalamus:  27%|██▋       | 12/44 [00:01<00:05,  6.15it/s]loading the weights from thalamus:  39%|███▊      | 17/44 [00:01<00:03,  7.94it/s]loading the weights from thalamus:  43%|████▎     | 19/44 [00:01<00:03,  8.15it/s]loading the weights from thalamus:  48%|████▊     | 21/44 [00:02<00:03,  6.61it/s]loading the weights from thalamus:  57%|█████▋    | 25/44 [00:02<00:02,  8.08it/s]loading the weights from thalamus:  61%|██████▏   | 27/44 [00:02<00:02,  7.99it/s]loading the weights from thalamus:  66%|██████▌   | 29/44 [00:03<00:01,  7.95it/s]loading the weights from thalamus:  68%|██████▊   | 30/44 [00:03<00:02,  6.26it/s]loading the weights from thalamus:  70%|███████   | 31/44 [00:03<00:02,  5.38it/s]loading the weights from thalamus:  80%|███████▉  | 35/44 [00:03<00:01,  6.75it/s]loading the weights from thalamus:  84%|████████▍ | 37/44 [00:03<00:00,  7.24it/s]loading the weights from thalamus:  86%|████████▋ | 38/44 [00:04<00:01,  5.88it/s]loading the weights from thalamus:  91%|█████████ | 40/44 [00:04<00:00,  6.47it/s]loading the weights from thalamus:  93%|█████████▎| 41/44 [00:04<00:00,  5.40it/s]loading the weights from thalamus: 100%|██████████| 44/44 [00:04<00:00,  9.34it/s]
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Thalamus /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [6.34593210e-02 3.28900850e-02 7.69089401e-02 9.55632253e-03
 2.76581735e-02 7.23603030e-03 8.42951544e-02 1.14312478e-01
 8.97578581e-02 1.36373421e-02 2.91012630e-01 1.89013139e-01
 2.62525705e-04]
Train on 16844 samples, validate on 245 samples
Epoch 1/300
 - 37s - loss: 0.5958 - acc: 0.8908 - mDice: 0.3583 - val_loss: 0.6847 - val_acc: 0.9404 - val_mDice: 0.2027

Epoch 00001: val_mDice improved from -inf to 0.20268, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 33s - loss: 0.4426 - acc: 0.9337 - mDice: 0.5228 - val_loss: 0.5280 - val_acc: 0.9423 - val_mDice: 0.2126

Epoch 00002: val_mDice improved from 0.20268 to 0.21261, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 33s - loss: 0.4070 - acc: 0.9383 - mDice: 0.5612 - val_loss: 0.3725 - val_acc: 0.9465 - val_mDice: 0.2186

Epoch 00003: val_mDice improved from 0.21261 to 0.21859, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 33s - loss: 0.3841 - acc: 0.9410 - mDice: 0.5860 - val_loss: 0.2253 - val_acc: 0.9472 - val_mDice: 0.2037

Epoch 00004: val_mDice did not improve from 0.21859
Epoch 5/300
 - 33s - loss: 0.3681 - acc: 0.9429 - mDice: 0.6033 - val_loss: 0.1275 - val_acc: 0.9475 - val_mDice: 0.2288

Epoch 00005: val_mDice improved from 0.21859 to 0.22876, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 33s - loss: 0.3587 - acc: 0.9440 - mDice: 0.6134 - val_loss: -9.3812e-02 - val_acc: 0.9508 - val_mDice: 0.2235

Epoch 00006: val_mDice did not improve from 0.22876
Epoch 7/300
 - 33s - loss: 0.3539 - acc: 0.9450 - mDice: 0.6186 - val_loss: -8.4525e-02 - val_acc: 0.9498 - val_mDice: 0.2167

Epoch 00007: val_mDice did not improve from 0.22876
Epoch 8/300
 - 33s - loss: 0.3448 - acc: 0.9462 - mDice: 0.6284 - val_loss: -1.0929e-01 - val_acc: 0.9489 - val_mDice: 0.2206

Epoch 00008: val_mDice did not improve from 0.22876
Epoch 9/300
 - 33s - loss: 0.3422 - acc: 0.9465 - mDice: 0.6313 - val_loss: -1.3378e-01 - val_acc: 0.9481 - val_mDice: 0.2277

Epoch 00009: val_mDice did not improve from 0.22876
Epoch 10/300
 - 33s - loss: 0.3314 - acc: 0.9476 - mDice: 0.6429 - val_loss: -1.4283e-01 - val_acc: 0.9494 - val_mDice: 0.2223

Epoch 00010: val_mDice did not improve from 0.22876
Epoch 11/300
 - 34s - loss: 0.3331 - acc: 0.9477 - mDice: 0.6410 - val_loss: -1.2794e-01 - val_acc: 0.9486 - val_mDice: 0.2305

Epoch 00011: val_mDice improved from 0.22876 to 0.23055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300
 - 34s - loss: 0.3279 - acc: 0.9484 - mDice: 0.6467 - val_loss: -1.3647e-01 - val_acc: 0.9487 - val_mDice: 0.2256

Epoch 00012: val_mDice did not improve from 0.23055
Epoch 13/300
 - 34s - loss: 0.3240 - acc: 0.9488 - mDice: 0.6509 - val_loss: -1.5434e-01 - val_acc: 0.9501 - val_mDice: 0.2250

Epoch 00013: val_mDice did not improve from 0.23055
Epoch 14/300
 - 34s - loss: 0.3219 - acc: 0.9491 - mDice: 0.6531 - val_loss: -1.6843e-01 - val_acc: 0.9497 - val_mDice: 0.2304

Epoch 00014: val_mDice did not improve from 0.23055
Epoch 15/300
 - 34s - loss: 0.3175 - acc: 0.9495 - mDice: 0.6580 - val_loss: -1.8294e-01 - val_acc: 0.9520 - val_mDice: 0.2223

Epoch 00015: val_mDice did not improve from 0.23055
Epoch 16/300
 - 34s - loss: 0.3149 - acc: 0.9498 - mDice: 0.6608 - val_loss: -1.6864e-01 - val_acc: 0.9499 - val_mDice: 0.2231

Epoch 00016: val_mDice did not improve from 0.23055
Epoch 17/300
 - 34s - loss: 0.3109 - acc: 0.9500 - mDice: 0.6650 - val_loss: -1.8027e-01 - val_acc: 0.9503 - val_mDice: 0.2155

Epoch 00017: val_mDice did not improve from 0.23055
Epoch 18/300
 - 33s - loss: 0.3103 - acc: 0.9502 - mDice: 0.6657 - val_loss: -1.3599e-01 - val_acc: 0.9456 - val_mDice: 0.2140

Epoch 00018: val_mDice did not improve from 0.23055
Epoch 19/300
 - 34s - loss: 0.3098 - acc: 0.9505 - mDice: 0.6662 - val_loss: -1.4649e-01 - val_acc: 0.9466 - val_mDice: 0.2168

Epoch 00019: val_mDice did not improve from 0.23055
Epoch 20/300
 - 34s - loss: 0.3065 - acc: 0.9508 - mDice: 0.6697 - val_loss: -1.4819e-01 - val_acc: 0.9502 - val_mDice: 0.2261

Epoch 00020: val_mDice did not improve from 0.23055
Epoch 21/300
 - 35s - loss: 0.3098 - acc: 0.9497 - mDice: 0.6592 - val_loss: -2.1267e-01 - val_acc: 0.9506 - val_mDice: 0.2209

Epoch 00021: val_mDice did not improve from 0.23055
Epoch 22/300
 - 36s - loss: 0.3039 - acc: 0.9488 - mDice: 0.6454 - val_loss: -2.3236e-01 - val_acc: 0.9521 - val_mDice: 0.2328

Epoch 00022: val_mDice improved from 0.23055 to 0.23283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 23/300
 - 36s - loss: 0.3064 - acc: 0.9483 - mDice: 0.6362 - val_loss: -2.2507e-01 - val_acc: 0.9504 - val_mDice: 0.2220

Epoch 00023: val_mDice did not improve from 0.23283
Epoch 24/300
 - 37s - loss: 0.2944 - acc: 0.9491 - mDice: 0.6475 - val_loss: -2.3626e-01 - val_acc: 0.9504 - val_mDice: 0.2290

Epoch 00024: val_mDice did not improve from 0.23283
Epoch 25/300
 - 36s - loss: 0.2931 - acc: 0.9494 - mDice: 0.6492 - val_loss: -2.2202e-01 - val_acc: 0.9472 - val_mDice: 0.2149

Epoch 00025: val_mDice did not improve from 0.23283
Epoch 26/300
 - 36s - loss: 0.2899 - acc: 0.9488 - mDice: 0.6490 - val_loss: -2.0457e-01 - val_acc: 0.9492 - val_mDice: 0.2039

Epoch 00026: val_mDice did not improve from 0.23283
Epoch 27/300
 - 37s - loss: 0.2946 - acc: 0.9486 - mDice: 0.6388 - val_loss: -2.2126e-01 - val_acc: 0.9511 - val_mDice: 0.2219

Epoch 00027: val_mDice did not improve from 0.23283
Epoch 28/300
 - 36s - loss: 0.2981 - acc: 0.9472 - mDice: 0.6257 - val_loss: -2.1584e-01 - val_acc: 0.9513 - val_mDice: 0.2180

Epoch 00028: val_mDice did not improve from 0.23283
Epoch 29/300
 - 37s - loss: 0.2919 - acc: 0.9467 - mDice: 0.6211 - val_loss: -1.6939e-01 - val_acc: 0.9438 - val_mDice: 0.1733

Epoch 00029: val_mDice did not improve from 0.23283
Epoch 30/300
 - 36s - loss: 0.3005 - acc: 0.9464 - mDice: 0.6015 - val_loss: -2.4203e-01 - val_acc: 0.9463 - val_mDice: 0.2075

Epoch 00030: val_mDice did not improve from 0.23283
Epoch 31/300
 - 36s - loss: 0.2933 - acc: 0.9465 - mDice: 0.6133 - val_loss: -2.5432e-01 - val_acc: 0.9516 - val_mDice: 0.2188

Epoch 00031: val_mDice did not improve from 0.23283
Epoch 32/300
 - 34s - loss: 0.2877 - acc: 0.9469 - mDice: 0.6186 - val_loss: -2.4886e-01 - val_acc: 0.9504 - val_mDice: 0.2172

Epoch 00032: val_mDice did not improve from 0.23283
Epoch 33/300
 - 34s - loss: 0.2880 - acc: 0.9469 - mDice: 0.6173 - val_loss: -2.4245e-01 - val_acc: 0.9507 - val_mDice: 0.2172

Epoch 00033: val_mDice did not improve from 0.23283
Epoch 34/300
 - 33s - loss: 0.2729 - acc: 0.9474 - mDice: 0.6263 - val_loss: -1.9211e-01 - val_acc: 0.9464 - val_mDice: 0.1964

Epoch 00034: val_mDice did not improve from 0.23283
Epoch 35/300
 - 33s - loss: 0.2702 - acc: 0.9478 - mDice: 0.6294 - val_loss: -1.9593e-01 - val_acc: 0.9428 - val_mDice: 0.2046

Epoch 00035: val_mDice did not improve from 0.23283
Epoch 36/300
 - 34s - loss: 0.2803 - acc: 0.9473 - mDice: 0.6170 - val_loss: -2.0244e-01 - val_acc: 0.9471 - val_mDice: 0.2010

Epoch 00036: val_mDice did not improve from 0.23283
Epoch 37/300
 - 34s - loss: 0.2854 - acc: 0.9468 - mDice: 0.6138 - val_loss: -2.3227e-01 - val_acc: 0.9493 - val_mDice: 0.2125

Epoch 00037: val_mDice did not improve from 0.23283

Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 38/300
 - 34s - loss: 0.2724 - acc: 0.9473 - mDice: 0.6226 - val_loss: -2.5880e-01 - val_acc: 0.9517 - val_mDice: 0.2208

Epoch 00038: val_mDice did not improve from 0.23283
Epoch 39/300
 - 34s - loss: 0.2637 - acc: 0.9485 - mDice: 0.6387 - val_loss: -2.3724e-01 - val_acc: 0.9499 - val_mDice: 0.2214

Epoch 00039: val_mDice did not improve from 0.23283
Epoch 40/300
 - 34s - loss: 0.2542 - acc: 0.9491 - mDice: 0.6461 - val_loss: -2.4201e-01 - val_acc: 0.9509 - val_mDice: 0.2154

Epoch 00040: val_mDice did not improve from 0.23283
Epoch 41/300
 - 33s - loss: 0.2533 - acc: 0.9489 - mDice: 0.6388 - val_loss: -2.4058e-01 - val_acc: 0.9500 - val_mDice: 0.2187

Epoch 00041: val_mDice did not improve from 0.23283
Epoch 42/300
 - 33s - loss: 0.2538 - acc: 0.9491 - mDice: 0.6386 - val_loss: -2.2007e-01 - val_acc: 0.9480 - val_mDice: 0.2064

Epoch 00042: val_mDice did not improve from 0.23283
Epoch 43/300
 - 33s - loss: 0.2594 - acc: 0.9485 - mDice: 0.6358 - val_loss: -2.6780e-01 - val_acc: 0.9516 - val_mDice: 0.2245

Epoch 00043: val_mDice did not improve from 0.23283
Epoch 44/300
 - 33s - loss: 0.2531 - acc: 0.9491 - mDice: 0.6424 - val_loss: -2.3506e-01 - val_acc: 0.9490 - val_mDice: 0.2145

Epoch 00044: val_mDice did not improve from 0.23283
Epoch 45/300
 - 33s - loss: 0.2494 - acc: 0.9493 - mDice: 0.6457 - val_loss: -2.7603e-01 - val_acc: 0.9525 - val_mDice: 0.2262

Epoch 00045: val_mDice did not improve from 0.23283
Epoch 46/300
 - 33s - loss: 0.2435 - acc: 0.9497 - mDice: 0.6512 - val_loss: -2.2237e-01 - val_acc: 0.9504 - val_mDice: 0.1982

Epoch 00046: val_mDice did not improve from 0.23283
Epoch 47/300
 - 33s - loss: 0.2466 - acc: 0.9499 - mDice: 0.6553 - val_loss: -2.4955e-01 - val_acc: 0.9513 - val_mDice: 0.2045

Epoch 00047: val_mDice did not improve from 0.23283
Epoch 48/300
 - 33s - loss: 0.2640 - acc: 0.9489 - mDice: 0.6336 - val_loss: -2.4992e-01 - val_acc: 0.9503 - val_mDice: 0.1978

Epoch 00048: val_mDice did not improve from 0.23283
Epoch 49/300
 - 33s - loss: 0.2545 - acc: 0.9493 - mDice: 0.6377 - val_loss: -2.6078e-01 - val_acc: 0.9520 - val_mDice: 0.2196

Epoch 00049: val_mDice did not improve from 0.23283
Epoch 50/300
 - 33s - loss: 0.2466 - acc: 0.9498 - mDice: 0.6470 - val_loss: -2.7519e-01 - val_acc: 0.9527 - val_mDice: 0.2249

Epoch 00050: val_mDice did not improve from 0.23283
Epoch 51/300
 - 33s - loss: 0.2484 - acc: 0.9494 - mDice: 0.6445 - val_loss: -2.5970e-01 - val_acc: 0.9507 - val_mDice: 0.2222

Epoch 00051: val_mDice did not improve from 0.23283
Epoch 52/300
 - 34s - loss: 0.2518 - acc: 0.9492 - mDice: 0.6406 - val_loss: -2.7710e-01 - val_acc: 0.9533 - val_mDice: 0.2136

Epoch 00052: val_mDice did not improve from 0.23283

Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 53/300
 - 35s - loss: 0.2427 - acc: 0.9495 - mDice: 0.6420 - val_loss: -2.6507e-01 - val_acc: 0.9521 - val_mDice: 0.2210

Epoch 00053: val_mDice did not improve from 0.23283
Epoch 54/300
 - 34s - loss: 0.2383 - acc: 0.9501 - mDice: 0.6534 - val_loss: -2.7146e-01 - val_acc: 0.9523 - val_mDice: 0.2212

Epoch 00054: val_mDice did not improve from 0.23283
Epoch 55/300
 - 35s - loss: 0.2349 - acc: 0.9503 - mDice: 0.6530 - val_loss: -2.6104e-01 - val_acc: 0.9517 - val_mDice: 0.2167

Epoch 00055: val_mDice did not improve from 0.23283
Epoch 56/300
 - 35s - loss: 0.2376 - acc: 0.9504 - mDice: 0.6533 - val_loss: -2.7007e-01 - val_acc: 0.9521 - val_mDice: 0.2148

Epoch 00056: val_mDice did not improve from 0.23283
Epoch 57/300
 - 35s - loss: 0.2332 - acc: 0.9506 - mDice: 0.6552 - val_loss: -2.6002e-01 - val_acc: 0.9521 - val_mDice: 0.2262

Epoch 00057: val_mDice did not improve from 0.23283
Epoch 58/300
 - 35s - loss: 0.2327 - acc: 0.9506 - mDice: 0.6562 - val_loss: -2.6784e-01 - val_acc: 0.9519 - val_mDice: 0.2173

Epoch 00058: val_mDice did not improve from 0.23283
Epoch 59/300
 - 36s - loss: 0.2282 - acc: 0.9508 - mDice: 0.6584 - val_loss: -2.5842e-01 - val_acc: 0.9531 - val_mDice: 0.2126

Epoch 00059: val_mDice did not improve from 0.23283
Epoch 60/300
 - 36s - loss: 0.2341 - acc: 0.9506 - mDice: 0.6538 - val_loss: -2.6938e-01 - val_acc: 0.9529 - val_mDice: 0.2189

Epoch 00060: val_mDice did not improve from 0.23283
Epoch 61/300
 - 36s - loss: 0.2339 - acc: 0.9505 - mDice: 0.6517 - val_loss: -2.8410e-01 - val_acc: 0.9525 - val_mDice: 0.2212

Epoch 00061: val_mDice did not improve from 0.23283
Epoch 62/300
 - 35s - loss: 0.2287 - acc: 0.9510 - mDice: 0.6607 - val_loss: -2.7715e-01 - val_acc: 0.9518 - val_mDice: 0.2210

Epoch 00062: val_mDice did not improve from 0.23283
Restoring model weights from the end of the best epoch
Epoch 00062: early stopping
{'val_loss': [0.684696550271949, 0.5280236297724198, 0.37253728828259874, 0.22527374138542433, 0.12745165297457453, -0.09381232712873999, -0.08452544882133299, -0.1092851068444398, -0.1337833963524626, -0.1428307781033978, -0.1279409936800295, -0.1364738488218234, -0.15434162584798677, -0.1684278710840308, -0.1829376360591577, -0.16864074363696333, -0.18026926991890888, -0.13599058575167947, -0.14649415706885427, -0.1481897738211009, -0.21267451334516613, -0.23235875683627566, -0.2250666992966922, -0.2362613872605927, -0.2220225843543909, -0.2045690270558912, -0.22125989557909115, -0.21583648801458125, -0.169388066863223, -0.2420269282420679, -0.2543215316397195, -0.24885501838004104, -0.24245127647811052, -0.19210600461430696, -0.19592938579770983, -0.2024416156702352, -0.23226862598438652, -0.2587999687815199, -0.23723771788027823, -0.24200941200311088, -0.2405778312941595, -0.2200695511942007, -0.26779704658808756, -0.235062143898436, -0.2760261191359284, -0.22236549987324647, -0.24955415021514102, -0.24991564440056302, -0.2607847723957835, -0.2751912567004257, -0.259704021394861, -0.2770954921119371, -0.26506757690590255, -0.27146318765851307, -0.2610438309837969, -0.2700679809311215, -0.26002068901244474, -0.26783717750590674, -0.2584217403616224, -0.26937798676746233, -0.28410305729021834, -0.2771546287005957], 'val_acc': [0.9404189586639404, 0.9422847993519842, 0.9465299552800704, 0.9471579035934137, 0.947468600711044, 0.9508323584284101, 0.9497988613284364, 0.9488716636385236, 0.9481210757275017, 0.9493736916658829, 0.9486149269707349, 0.9487490118766317, 0.9500506933854551, 0.9497187356559598, 0.9520391761040201, 0.9499427676200867, 0.9502976171824397, 0.9456436451600523, 0.9465626551180469, 0.9502158481247571, 0.95060668551192, 0.9521372987299549, 0.9503515751994386, 0.9504398898202546, 0.9472020596874003, 0.9492216158886345, 0.9510841819704795, 0.9513474642014017, 0.9438121403966632, 0.9463026535754301, 0.9516401716641018, 0.9504366164304772, 0.9507293311917052, 0.9463598922807344, 0.9428424251322843, 0.9470908593158333, 0.9492951959979777, 0.9516990501053479, 0.9499264152682557, 0.9508912222726005, 0.9500425226834356, 0.9480033310092225, 0.9516483472318066, 0.9490122989732392, 0.9525052321200468, 0.9503695648543689, 0.9513163919351539, 0.9503335964923002, 0.9519721342592823, 0.9526851189379789, 0.9507391416296667, 0.9533441273533568, 0.9521291219458288, 0.9523073568636057, 0.9516647117478507, 0.9520767939334013, 0.9520816985441714, 0.9519099897267868, 0.9531249987835787, 0.952936944912891, 0.95245126680452, 0.9518494861466544], 'val_mDice': [0.20268111432693442, 0.21261140673744436, 0.2185871129741474, 0.2037305244985892, 0.2287571372426286, 0.2234651499560901, 0.21665216511001392, 0.22063925314922722, 0.2277330905199051, 0.22234160954854926, 0.23054850299139412, 0.2255627257483346, 0.22499042414889045, 0.23037394181806214, 0.22232632050100637, 0.22309281813855075, 0.21545297743714587, 0.21399580884952935, 0.21684074249802804, 0.22612819714205606, 0.22089109782661712, 0.2328251293119119, 0.22196556977471527, 0.22897613443890397, 0.21487321917499816, 0.20393662230700862, 0.22194404945689805, 0.21796098974894504, 0.17328624016776376, 0.2075291780792937, 0.2188184547181032, 0.21718009971842475, 0.2171765882141736, 0.19643793954532973, 0.204649345911279, 0.20100821828355595, 0.21253197442512123, 0.22081116039534004, 0.22138700710267437, 0.2154430455091048, 0.2186599185272139, 0.2064478785103681, 0.22448478243788894, 0.21453163118994967, 0.22620114501641722, 0.19817619253786242, 0.20454010671498823, 0.1978285154517816, 0.2195585342694302, 0.2248642044712086, 0.2222027731489162, 0.21358354496104376, 0.2210341950460356, 0.22118683873998876, 0.2167356879431374, 0.2148063102242898, 0.22615264781883784, 0.2172675734880019, 0.21264939922459272, 0.21888601703911412, 0.22123700273888453, 0.22101806119388465], 'loss': [0.5958282780198579, 0.4426395439834103, 0.4070374350194211, 0.38407835579844163, 0.36810250817045015, 0.358744206558537, 0.3538700025081918, 0.34483234946686986, 0.34216989416516724, 0.33142977047068467, 0.33312846177570105, 0.32788925725684487, 0.32396147607504344, 0.3219426767938184, 0.31746214831336833, 0.3148526893293906, 0.31092111308262993, 0.31027453535173827, 0.30980201440280986, 0.30648268533309264, 0.3098463887037814, 0.30388748842441704, 0.30640651273705477, 0.294394474280182, 0.2931484673884384, 0.2899470971695985, 0.29461573931467955, 0.29806658256568086, 0.29193187688357464, 0.3004640627486008, 0.2933146365827236, 0.2877488955586102, 0.28795805072178127, 0.272943096754619, 0.2702438392012036, 0.2802708726932523, 0.28535517652899894, 0.27241570087344585, 0.26368562802645845, 0.25420217319519983, 0.2532509212829498, 0.2538061684747369, 0.2593962884997897, 0.2531373811432549, 0.2494090506866684, 0.24354915520030268, 0.24657852865127838, 0.26395371496964415, 0.25450945086180066, 0.24660315037712965, 0.24844412393125964, 0.2517617985040179, 0.24271293306535302, 0.2382564687760486, 0.2349230194372211, 0.23764298200736253, 0.23319587203534767, 0.2327212047701464, 0.2282185259201076, 0.23405094088467568, 0.2338802141263941, 0.22870106879246097], 'acc': [0.8907514662187475, 0.9336564955448657, 0.9382564943735856, 0.941040811261834, 0.9428570158930013, 0.9440413377497887, 0.9450462702984652, 0.9462311150936524, 0.9464728457645746, 0.9475516313414934, 0.9477381335177294, 0.9483912322547254, 0.9487769837986401, 0.9490865267772511, 0.9494554384449434, 0.9498430923834121, 0.9500137052366723, 0.9502039170200087, 0.9505466875144462, 0.9507845655681685, 0.9497022358786887, 0.9488439874166662, 0.9482665974208901, 0.9491362623804909, 0.9494225677501784, 0.9488359954968056, 0.9486334868619403, 0.9472105740594966, 0.9466572788698151, 0.9464330523463049, 0.9464992483221021, 0.9469361851671704, 0.9469328794236479, 0.9473542138364416, 0.947802163258892, 0.9473412746249058, 0.9467883593776791, 0.9473149210136765, 0.9485172954060861, 0.9491002746784497, 0.9489036647519316, 0.949116116767823, 0.9484837097174423, 0.9491044851995313, 0.9492692459057075, 0.9497276857597928, 0.9498727765864076, 0.9489039743323134, 0.949303949425606, 0.9498351956030955, 0.9494398351987845, 0.9492492427086496, 0.9495221806362332, 0.9500706952814534, 0.9503242470071027, 0.9504075909320489, 0.9506495362419721, 0.9506229435893652, 0.9507714595244916, 0.9506293897729564, 0.9505151246044704, 0.9509914750315124], 'mDice': [0.3582785820564805, 0.5227887705923882, 0.5611972196584528, 0.5860057849197257, 0.6032647651084329, 0.6133748980407561, 0.6186381592493786, 0.6284116126315606, 0.6312860928580246, 0.6428882540868229, 0.6410312681923114, 0.6467018567805929, 0.6509402729898188, 0.6531371831915153, 0.6579707501528241, 0.6607730862817468, 0.6650363023951739, 0.6657079854244519, 0.6662149394359648, 0.669655319620866, 0.6591766981310245, 0.645449443954494, 0.6362346002216777, 0.6475286013516415, 0.6491904229003554, 0.6489608941749676, 0.6388238245044969, 0.6257020051831503, 0.6210534790120762, 0.6014828330913934, 0.6133046912286378, 0.618572524117761, 0.6172611452294087, 0.6262663959982672, 0.629412673574761, 0.617005321464157, 0.6137720454591268, 0.6225951737298503, 0.6386962400404752, 0.6460795845067634, 0.6388109006187577, 0.6386114521926555, 0.6357677912830995, 0.642396784234262, 0.6457059924590053, 0.6512252431255824, 0.6552699369267142, 0.6335734638195937, 0.6377210885311102, 0.6469781346366117, 0.6444763721797789, 0.6405895390191516, 0.6419599812377861, 0.6534041973006496, 0.6530237006153129, 0.6533480361900202, 0.6552032951404465, 0.6561865019164157, 0.6584462841016837, 0.6537865830151436, 0.6517254072607512, 0.6607492448343174], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.39s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.25s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:24,  3.13it/s]Loading train:   1%|          | 2/266 [00:00<01:22,  3.21it/s]Loading train:   1%|          | 3/266 [00:00<01:17,  3.40it/s]Loading train:   2%|▏         | 4/266 [00:01<01:13,  3.56it/s]Loading train:   2%|▏         | 5/266 [00:01<01:13,  3.53it/s]Loading train:   2%|▏         | 6/266 [00:01<01:14,  3.50it/s]Loading train:   3%|▎         | 7/266 [00:01<01:13,  3.54it/s]Loading train:   3%|▎         | 8/266 [00:02<01:11,  3.59it/s]Loading train:   3%|▎         | 9/266 [00:02<01:10,  3.62it/s]Loading train:   4%|▍         | 10/266 [00:02<01:10,  3.65it/s]Loading train:   4%|▍         | 11/266 [00:03<01:09,  3.66it/s]Loading train:   5%|▍         | 12/266 [00:03<01:08,  3.68it/s]Loading train:   5%|▍         | 13/266 [00:03<01:08,  3.72it/s]Loading train:   5%|▌         | 14/266 [00:03<01:06,  3.77it/s]Loading train:   6%|▌         | 15/266 [00:04<01:07,  3.73it/s]Loading train:   6%|▌         | 16/266 [00:04<01:06,  3.75it/s]Loading train:   6%|▋         | 17/266 [00:04<01:05,  3.79it/s]Loading train:   7%|▋         | 18/266 [00:04<01:05,  3.81it/s]Loading train:   7%|▋         | 19/266 [00:05<01:04,  3.84it/s]Loading train:   8%|▊         | 20/266 [00:05<01:04,  3.81it/s]Loading train:   8%|▊         | 21/266 [00:05<01:04,  3.80it/s]Loading train:   8%|▊         | 22/266 [00:05<01:03,  3.82it/s]Loading train:   9%|▊         | 23/266 [00:06<01:03,  3.82it/s]Loading train:   9%|▉         | 24/266 [00:06<01:03,  3.81it/s]Loading train:   9%|▉         | 25/266 [00:06<01:02,  3.83it/s]Loading train:  10%|▉         | 26/266 [00:06<01:02,  3.84it/s]Loading train:  10%|█         | 27/266 [00:07<01:01,  3.87it/s]Loading train:  11%|█         | 28/266 [00:07<01:00,  3.91it/s]Loading train:  11%|█         | 29/266 [00:07<01:00,  3.93it/s]Loading train:  11%|█▏        | 30/266 [00:07<01:00,  3.93it/s]Loading train:  12%|█▏        | 31/266 [00:08<01:00,  3.90it/s]Loading train:  12%|█▏        | 32/266 [00:08<00:59,  3.92it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:59,  3.93it/s]Loading train:  13%|█▎        | 34/266 [00:09<00:58,  3.97it/s]Loading train:  13%|█▎        | 35/266 [00:09<00:58,  3.96it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:58,  3.92it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:58,  3.92it/s]Loading train:  14%|█▍        | 38/266 [00:10<00:58,  3.92it/s]Loading train:  15%|█▍        | 39/266 [00:10<00:58,  3.91it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:57,  3.94it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:56,  3.96it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:53,  4.15it/s]Loading train:  16%|█▌        | 43/266 [00:11<00:52,  4.27it/s]Loading train:  17%|█▋        | 44/266 [00:11<00:50,  4.37it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:49,  4.47it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:48,  4.56it/s]Loading train:  18%|█▊        | 47/266 [00:12<00:47,  4.66it/s]Loading train:  18%|█▊        | 48/266 [00:12<00:46,  4.70it/s]Loading train:  18%|█▊        | 49/266 [00:12<00:45,  4.77it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:45,  4.77it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:44,  4.82it/s]Loading train:  20%|█▉        | 52/266 [00:13<00:44,  4.78it/s]Loading train:  20%|█▉        | 53/266 [00:13<00:44,  4.83it/s]Loading train:  20%|██        | 54/266 [00:13<00:44,  4.81it/s]Loading train:  21%|██        | 55/266 [00:13<00:43,  4.86it/s]Loading train:  21%|██        | 56/266 [00:13<00:43,  4.87it/s]Loading train:  21%|██▏       | 57/266 [00:14<00:43,  4.85it/s]Loading train:  22%|██▏       | 58/266 [00:14<00:43,  4.82it/s]Loading train:  22%|██▏       | 59/266 [00:14<00:42,  4.85it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:43,  4.79it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:43,  4.72it/s]Loading train:  23%|██▎       | 62/266 [00:15<00:43,  4.71it/s]Loading train:  24%|██▎       | 63/266 [00:15<00:43,  4.68it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:43,  4.65it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:43,  4.62it/s]Loading train:  25%|██▍       | 66/266 [00:16<00:43,  4.64it/s]Loading train:  25%|██▌       | 67/266 [00:16<00:43,  4.58it/s]Loading train:  26%|██▌       | 68/266 [00:16<00:43,  4.59it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:42,  4.62it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:42,  4.61it/s]Loading train:  27%|██▋       | 71/266 [00:17<00:42,  4.60it/s]Loading train:  27%|██▋       | 72/266 [00:17<00:42,  4.54it/s]Loading train:  27%|██▋       | 73/266 [00:17<00:42,  4.57it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:41,  4.59it/s]Loading train:  28%|██▊       | 75/266 [00:18<00:41,  4.59it/s]Loading train:  29%|██▊       | 76/266 [00:18<00:41,  4.54it/s]Loading train:  29%|██▉       | 77/266 [00:18<00:41,  4.52it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:43,  4.35it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:44,  4.25it/s]Loading train:  30%|███       | 80/266 [00:19<00:44,  4.20it/s]Loading train:  30%|███       | 81/266 [00:19<00:44,  4.13it/s]Loading train:  31%|███       | 82/266 [00:19<00:45,  4.01it/s]Loading train:  31%|███       | 83/266 [00:19<00:45,  3.99it/s]Loading train:  32%|███▏      | 84/266 [00:20<00:45,  3.99it/s]Loading train:  32%|███▏      | 85/266 [00:20<00:45,  3.98it/s]Loading train:  32%|███▏      | 86/266 [00:20<00:45,  3.99it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:44,  4.02it/s]Loading train:  33%|███▎      | 88/266 [00:21<00:44,  4.00it/s]Loading train:  33%|███▎      | 89/266 [00:21<00:44,  4.02it/s]Loading train:  34%|███▍      | 90/266 [00:21<00:43,  4.02it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:43,  4.04it/s]Loading train:  35%|███▍      | 92/266 [00:22<00:42,  4.08it/s]Loading train:  35%|███▍      | 93/266 [00:22<00:42,  4.07it/s]Loading train:  35%|███▌      | 94/266 [00:22<00:42,  4.07it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:41,  4.09it/s]Loading train:  36%|███▌      | 96/266 [00:23<00:41,  4.09it/s]Loading train:  36%|███▋      | 97/266 [00:23<00:43,  3.87it/s]Loading train:  37%|███▋      | 98/266 [00:23<00:42,  3.92it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:40,  4.13it/s]Loading train:  38%|███▊      | 100/266 [00:24<00:40,  4.14it/s]Loading train:  38%|███▊      | 101/266 [00:24<00:39,  4.20it/s]Loading train:  38%|███▊      | 102/266 [00:24<00:38,  4.29it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:37,  4.34it/s]Loading train:  39%|███▉      | 104/266 [00:25<00:36,  4.42it/s]Loading train:  39%|███▉      | 105/266 [00:25<00:36,  4.47it/s]Loading train:  40%|███▉      | 106/266 [00:25<00:35,  4.49it/s]Loading train:  40%|████      | 107/266 [00:25<00:35,  4.47it/s]Loading train:  41%|████      | 108/266 [00:25<00:34,  4.52it/s]Loading train:  41%|████      | 109/266 [00:26<00:37,  4.24it/s]Loading train:  41%|████▏     | 110/266 [00:26<00:35,  4.34it/s]Loading train:  42%|████▏     | 111/266 [00:26<00:35,  4.33it/s]Loading train:  42%|████▏     | 112/266 [00:26<00:35,  4.38it/s]Loading train:  42%|████▏     | 113/266 [00:27<00:34,  4.39it/s]Loading train:  43%|████▎     | 114/266 [00:27<00:34,  4.39it/s]Loading train:  43%|████▎     | 115/266 [00:27<00:34,  4.43it/s]Loading train:  44%|████▎     | 116/266 [00:27<00:33,  4.45it/s]Loading train:  44%|████▍     | 117/266 [00:28<00:33,  4.43it/s]Loading train:  44%|████▍     | 118/266 [00:28<00:34,  4.34it/s]Loading train:  45%|████▍     | 119/266 [00:28<00:35,  4.11it/s]Loading train:  45%|████▌     | 120/266 [00:28<00:36,  4.05it/s]Loading train:  45%|████▌     | 121/266 [00:29<00:36,  4.03it/s]Loading train:  46%|████▌     | 122/266 [00:29<00:36,  4.00it/s]Loading train:  46%|████▌     | 123/266 [00:29<00:36,  3.94it/s]Loading train:  47%|████▋     | 124/266 [00:29<00:35,  3.96it/s]Loading train:  47%|████▋     | 125/266 [00:30<00:35,  3.94it/s]Loading train:  47%|████▋     | 126/266 [00:30<00:35,  3.94it/s]Loading train:  48%|████▊     | 127/266 [00:30<00:35,  3.93it/s]Loading train:  48%|████▊     | 128/266 [00:30<00:35,  3.88it/s]Loading train:  48%|████▊     | 129/266 [00:31<00:35,  3.91it/s]Loading train:  49%|████▉     | 130/266 [00:31<00:35,  3.86it/s]Loading train:  49%|████▉     | 131/266 [00:31<00:34,  3.88it/s]Loading train:  50%|████▉     | 132/266 [00:31<00:34,  3.87it/s]Loading train:  50%|█████     | 133/266 [00:32<00:34,  3.87it/s]Loading train:  50%|█████     | 134/266 [00:32<00:34,  3.81it/s]Loading train:  51%|█████     | 135/266 [00:32<00:34,  3.80it/s]Loading train:  51%|█████     | 136/266 [00:32<00:34,  3.80it/s]Loading train:  52%|█████▏    | 137/266 [00:33<00:32,  3.92it/s]Loading train:  52%|█████▏    | 138/266 [00:33<00:31,  4.02it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:33,  3.83it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:31,  3.98it/s]Loading train:  53%|█████▎    | 141/266 [00:34<00:30,  4.08it/s]Loading train:  53%|█████▎    | 142/266 [00:34<00:29,  4.16it/s]Loading train:  54%|█████▍    | 143/266 [00:34<00:29,  4.19it/s]Loading train:  54%|█████▍    | 144/266 [00:34<00:29,  4.20it/s]Loading train:  55%|█████▍    | 145/266 [00:35<00:28,  4.25it/s]Loading train:  55%|█████▍    | 146/266 [00:35<00:28,  4.25it/s]Loading train:  55%|█████▌    | 147/266 [00:35<00:27,  4.28it/s]Loading train:  56%|█████▌    | 148/266 [00:35<00:27,  4.30it/s]Loading train:  56%|█████▌    | 149/266 [00:36<00:27,  4.29it/s]Loading train:  56%|█████▋    | 150/266 [00:36<00:27,  4.29it/s]Loading train:  57%|█████▋    | 151/266 [00:36<00:27,  4.24it/s]Loading train:  57%|█████▋    | 152/266 [00:36<00:26,  4.27it/s]Loading train:  58%|█████▊    | 153/266 [00:36<00:26,  4.26it/s]Loading train:  58%|█████▊    | 154/266 [00:37<00:26,  4.25it/s]Loading train:  58%|█████▊    | 155/266 [00:37<00:24,  4.45it/s]Loading train:  59%|█████▊    | 156/266 [00:37<00:24,  4.57it/s]Loading train:  59%|█████▉    | 157/266 [00:37<00:23,  4.69it/s]Loading train:  59%|█████▉    | 158/266 [00:37<00:22,  4.79it/s]Loading train:  60%|█████▉    | 159/266 [00:38<00:21,  4.89it/s]Loading train:  60%|██████    | 160/266 [00:38<00:21,  4.97it/s]Loading train:  61%|██████    | 161/266 [00:38<00:20,  5.04it/s]Loading train:  61%|██████    | 162/266 [00:38<00:20,  5.03it/s]Loading train:  61%|██████▏   | 163/266 [00:38<00:20,  5.06it/s]Loading train:  62%|██████▏   | 164/266 [00:39<00:20,  5.08it/s]Loading train:  62%|██████▏   | 165/266 [00:39<00:19,  5.09it/s]Loading train:  62%|██████▏   | 166/266 [00:39<00:19,  5.09it/s]Loading train:  63%|██████▎   | 167/266 [00:39<00:19,  5.11it/s]Loading train:  63%|██████▎   | 168/266 [00:39<00:19,  5.12it/s]Loading train:  64%|██████▎   | 169/266 [00:40<00:18,  5.16it/s]Loading train:  64%|██████▍   | 170/266 [00:40<00:18,  5.11it/s]Loading train:  64%|██████▍   | 171/266 [00:40<00:18,  5.12it/s]Loading train:  65%|██████▍   | 172/266 [00:40<00:18,  5.13it/s]Loading train:  65%|██████▌   | 173/266 [00:40<00:18,  4.92it/s]Loading train:  65%|██████▌   | 174/266 [00:41<00:19,  4.79it/s]Loading train:  66%|██████▌   | 175/266 [00:41<00:19,  4.71it/s]Loading train:  66%|██████▌   | 176/266 [00:41<00:19,  4.66it/s]Loading train:  67%|██████▋   | 177/266 [00:41<00:19,  4.61it/s]Loading train:  67%|██████▋   | 178/266 [00:42<00:19,  4.56it/s]Loading train:  67%|██████▋   | 179/266 [00:42<00:19,  4.56it/s]Loading train:  68%|██████▊   | 180/266 [00:42<00:18,  4.53it/s]Loading train:  68%|██████▊   | 181/266 [00:42<00:18,  4.55it/s]Loading train:  68%|██████▊   | 182/266 [00:42<00:18,  4.57it/s]Loading train:  69%|██████▉   | 183/266 [00:43<00:18,  4.58it/s]Loading train:  69%|██████▉   | 184/266 [00:43<00:17,  4.60it/s]Loading train:  70%|██████▉   | 185/266 [00:43<00:17,  4.58it/s]Loading train:  70%|██████▉   | 186/266 [00:43<00:17,  4.60it/s]Loading train:  70%|███████   | 187/266 [00:44<00:17,  4.60it/s]Loading train:  71%|███████   | 188/266 [00:44<00:17,  4.51it/s]Loading train:  71%|███████   | 189/266 [00:44<00:17,  4.49it/s]Loading train:  71%|███████▏  | 190/266 [00:44<00:16,  4.52it/s]Loading train:  72%|███████▏  | 191/266 [00:44<00:17,  4.40it/s]Loading train:  72%|███████▏  | 192/266 [00:45<00:16,  4.41it/s]Loading train:  73%|███████▎  | 193/266 [00:45<00:16,  4.34it/s]Loading train:  73%|███████▎  | 194/266 [00:45<00:17,  4.16it/s]Loading train:  73%|███████▎  | 195/266 [00:45<00:16,  4.29it/s]Loading train:  74%|███████▎  | 196/266 [00:46<00:16,  4.35it/s]Loading train:  74%|███████▍  | 197/266 [00:46<00:15,  4.42it/s]Loading train:  74%|███████▍  | 198/266 [00:46<00:15,  4.45it/s]Loading train:  75%|███████▍  | 199/266 [00:46<00:14,  4.51it/s]Loading train:  75%|███████▌  | 200/266 [00:46<00:14,  4.53it/s]Loading train:  76%|███████▌  | 201/266 [00:47<00:14,  4.53it/s]Loading train:  76%|███████▌  | 202/266 [00:47<00:13,  4.59it/s]Loading train:  76%|███████▋  | 203/266 [00:47<00:13,  4.59it/s]Loading train:  77%|███████▋  | 204/266 [00:47<00:13,  4.61it/s]Loading train:  77%|███████▋  | 205/266 [00:48<00:13,  4.61it/s]Loading train:  77%|███████▋  | 206/266 [00:48<00:12,  4.63it/s]Loading train:  78%|███████▊  | 207/266 [00:48<00:12,  4.65it/s]Loading train:  78%|███████▊  | 208/266 [00:48<00:12,  4.65it/s]Loading train:  79%|███████▊  | 209/266 [00:48<00:12,  4.63it/s]Loading train:  79%|███████▉  | 210/266 [00:49<00:12,  4.59it/s]Loading train:  79%|███████▉  | 211/266 [00:49<00:11,  4.60it/s]Loading train:  80%|███████▉  | 212/266 [00:49<00:11,  4.59it/s]Loading train:  80%|████████  | 213/266 [00:49<00:11,  4.61it/s]Loading train:  80%|████████  | 214/266 [00:49<00:11,  4.65it/s]Loading train:  81%|████████  | 215/266 [00:50<00:10,  4.65it/s]Loading train:  81%|████████  | 216/266 [00:50<00:10,  4.68it/s]Loading train:  82%|████████▏ | 217/266 [00:50<00:10,  4.68it/s]Loading train:  82%|████████▏ | 218/266 [00:50<00:10,  4.70it/s]Loading train:  82%|████████▏ | 219/266 [00:51<00:10,  4.70it/s]Loading train:  83%|████████▎ | 220/266 [00:51<00:09,  4.68it/s]Loading train:  83%|████████▎ | 221/266 [00:51<00:09,  4.71it/s]Loading train:  83%|████████▎ | 222/266 [00:51<00:09,  4.71it/s]Loading train:  84%|████████▍ | 223/266 [00:51<00:09,  4.68it/s]Loading train:  84%|████████▍ | 224/266 [00:52<00:08,  4.68it/s]Loading train:  85%|████████▍ | 225/266 [00:52<00:08,  4.68it/s]Loading train:  85%|████████▍ | 226/266 [00:52<00:08,  4.66it/s]Loading train:  85%|████████▌ | 227/266 [00:52<00:08,  4.67it/s]Loading train:  86%|████████▌ | 228/266 [00:52<00:08,  4.60it/s]Loading train:  86%|████████▌ | 229/266 [00:53<00:07,  4.63it/s]Loading train:  86%|████████▋ | 230/266 [00:53<00:07,  4.66it/s]Loading train:  87%|████████▋ | 231/266 [00:53<00:07,  4.56it/s]Loading train:  87%|████████▋ | 232/266 [00:53<00:07,  4.55it/s]Loading train:  88%|████████▊ | 233/266 [00:54<00:07,  4.53it/s]Loading train:  88%|████████▊ | 234/266 [00:54<00:07,  4.50it/s]Loading train:  88%|████████▊ | 235/266 [00:54<00:06,  4.49it/s]Loading train:  89%|████████▊ | 236/266 [00:54<00:06,  4.48it/s]Loading train:  89%|████████▉ | 237/266 [00:54<00:06,  4.45it/s]Loading train:  89%|████████▉ | 238/266 [00:55<00:06,  4.38it/s]Loading train:  90%|████████▉ | 239/266 [00:55<00:06,  4.33it/s]Loading train:  90%|█████████ | 240/266 [00:55<00:05,  4.36it/s]Loading train:  91%|█████████ | 241/266 [00:55<00:05,  4.40it/s]Loading train:  91%|█████████ | 242/266 [00:56<00:05,  4.38it/s]Loading train:  91%|█████████▏| 243/266 [00:56<00:05,  4.36it/s]Loading train:  92%|█████████▏| 244/266 [00:56<00:05,  4.25it/s]Loading train:  92%|█████████▏| 245/266 [00:56<00:04,  4.23it/s]Loading train:  92%|█████████▏| 246/266 [00:57<00:04,  4.13it/s]Loading train:  93%|█████████▎| 247/266 [00:57<00:04,  4.08it/s]Loading train:  93%|█████████▎| 248/266 [00:57<00:04,  4.07it/s]Loading train:  94%|█████████▎| 249/266 [00:57<00:04,  3.90it/s]Loading train:  94%|█████████▍| 250/266 [00:58<00:04,  3.89it/s]Loading train:  94%|█████████▍| 251/266 [00:58<00:03,  3.87it/s]Loading train:  95%|█████████▍| 252/266 [00:58<00:03,  3.85it/s]Loading train:  95%|█████████▌| 253/266 [00:58<00:03,  3.84it/s]Loading train:  95%|█████████▌| 254/266 [00:59<00:03,  3.87it/s]Loading train:  96%|█████████▌| 255/266 [00:59<00:02,  3.84it/s]Loading train:  96%|█████████▌| 256/266 [00:59<00:02,  3.84it/s]Loading train:  97%|█████████▋| 257/266 [00:59<00:02,  3.80it/s]Loading train:  97%|█████████▋| 258/266 [01:00<00:02,  3.76it/s]Loading train:  97%|█████████▋| 259/266 [01:00<00:01,  3.79it/s]Loading train:  98%|█████████▊| 260/266 [01:00<00:01,  3.83it/s]Loading train:  98%|█████████▊| 261/266 [01:01<00:01,  3.82it/s]Loading train:  98%|█████████▊| 262/266 [01:01<00:01,  3.81it/s]Loading train:  99%|█████████▉| 263/266 [01:01<00:00,  3.82it/s]Loading train:  99%|█████████▉| 264/266 [01:01<00:00,  3.81it/s]Loading train: 100%|█████████▉| 265/266 [01:02<00:00,  3.76it/s]Loading train: 100%|██████████| 266/266 [01:02<00:00,  3.77it/s]Loading train: 100%|██████████| 266/266 [01:02<00:00,  4.26it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 46.64it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 45.63it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 45.17it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 45.20it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 45.01it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 45.93it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 46.31it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 45.66it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:04, 47.74it/s]concatenating: train:  20%|█▉        | 52/266 [00:01<00:04, 50.49it/s]concatenating: train:  22%|██▏       | 58/266 [00:01<00:04, 51.75it/s]concatenating: train:  24%|██▍       | 64/266 [00:01<00:03, 53.36it/s]concatenating: train:  26%|██▋       | 70/266 [00:01<00:03, 53.88it/s]concatenating: train:  29%|██▊       | 76/266 [00:01<00:03, 53.87it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:03, 52.72it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:03, 50.23it/s]concatenating: train:  35%|███▌      | 94/266 [00:01<00:03, 49.87it/s]concatenating: train:  38%|███▊      | 100/266 [00:02<00:03, 51.01it/s]concatenating: train:  40%|███▉      | 106/266 [00:02<00:03, 52.06it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:02, 52.03it/s]concatenating: train:  44%|████▍     | 118/266 [00:02<00:02, 52.10it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:02, 51.87it/s]concatenating: train:  49%|████▉     | 130/266 [00:02<00:02, 49.61it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:02, 46.43it/s]concatenating: train:  53%|█████▎    | 140/266 [00:02<00:02, 46.08it/s]concatenating: train:  55%|█████▍    | 146/266 [00:02<00:02, 48.12it/s]concatenating: train:  57%|█████▋    | 152/266 [00:03<00:02, 49.03it/s]concatenating: train:  59%|█████▉    | 158/266 [00:03<00:02, 49.73it/s]concatenating: train:  62%|██████▏   | 165/266 [00:03<00:01, 52.88it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 55.87it/s]concatenating: train:  67%|██████▋   | 178/266 [00:03<00:01, 53.85it/s]concatenating: train:  69%|██████▉   | 184/266 [00:03<00:01, 54.43it/s]concatenating: train:  71%|███████▏  | 190/266 [00:03<00:01, 53.63it/s]concatenating: train:  74%|███████▎  | 196/266 [00:03<00:01, 51.71it/s]concatenating: train:  76%|███████▋  | 203/266 [00:03<00:01, 54.20it/s]concatenating: train:  79%|███████▉  | 210/266 [00:04<00:00, 56.62it/s]concatenating: train:  81%|████████  | 216/266 [00:04<00:00, 54.56it/s]concatenating: train:  83%|████████▎ | 222/266 [00:04<00:00, 53.42it/s]concatenating: train:  86%|████████▌ | 228/266 [00:04<00:00, 52.80it/s]concatenating: train:  88%|████████▊ | 234/266 [00:04<00:00, 51.42it/s]concatenating: train:  90%|█████████ | 240/266 [00:04<00:00, 50.42it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 49.54it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 48.68it/s]concatenating: train:  96%|█████████▌| 256/266 [00:05<00:00, 47.24it/s]concatenating: train:  98%|█████████▊| 261/266 [00:05<00:00, 46.73it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 44.66it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 50.34it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  3.96it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  4.04it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  4.12it/s]Loading test: 100%|██████████| 4/4 [00:00<00:00,  4.14it/s]Loading test: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 365.67it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<01:14,  3.58it/s]Loading trainS:   1%|          | 2/266 [00:00<01:13,  3.59it/s]Loading trainS:   1%|          | 3/266 [00:00<01:09,  3.78it/s]Loading trainS:   2%|▏         | 4/266 [00:01<01:06,  3.91it/s]Loading trainS:   2%|▏         | 5/266 [00:01<01:07,  3.86it/s]Loading trainS:   2%|▏         | 6/266 [00:01<01:08,  3.81it/s]Loading trainS:   3%|▎         | 7/266 [00:01<01:07,  3.82it/s]Loading trainS:   3%|▎         | 8/266 [00:02<01:07,  3.83it/s]Loading trainS:   3%|▎         | 9/266 [00:02<01:07,  3.82it/s]Loading trainS:   4%|▍         | 10/266 [00:02<01:07,  3.79it/s]Loading trainS:   4%|▍         | 11/266 [00:02<01:07,  3.80it/s]Loading trainS:   5%|▍         | 12/266 [00:03<01:06,  3.81it/s]Loading trainS:   5%|▍         | 13/266 [00:03<01:06,  3.82it/s]Loading trainS:   5%|▌         | 14/266 [00:03<01:05,  3.83it/s]Loading trainS:   6%|▌         | 15/266 [00:03<01:05,  3.84it/s]Loading trainS:   6%|▌         | 16/266 [00:04<01:05,  3.80it/s]Loading trainS:   6%|▋         | 17/266 [00:04<01:06,  3.73it/s]Loading trainS:   7%|▋         | 18/266 [00:04<01:05,  3.77it/s]Loading trainS:   7%|▋         | 19/266 [00:04<01:04,  3.81it/s]Loading trainS:   8%|▊         | 20/266 [00:05<01:03,  3.84it/s]Loading trainS:   8%|▊         | 21/266 [00:05<01:03,  3.85it/s]Loading trainS:   8%|▊         | 22/266 [00:05<01:03,  3.87it/s]Loading trainS:   9%|▊         | 23/266 [00:06<01:02,  3.89it/s]Loading trainS:   9%|▉         | 24/266 [00:06<01:01,  3.96it/s]Loading trainS:   9%|▉         | 25/266 [00:06<01:00,  4.01it/s]Loading trainS:  10%|▉         | 26/266 [00:06<00:59,  4.04it/s]Loading trainS:  10%|█         | 27/266 [00:06<00:58,  4.07it/s]Loading trainS:  11%|█         | 28/266 [00:07<00:58,  4.08it/s]Loading trainS:  11%|█         | 29/266 [00:07<00:57,  4.10it/s]Loading trainS:  11%|█▏        | 30/266 [00:07<00:57,  4.10it/s]Loading trainS:  12%|█▏        | 31/266 [00:07<00:57,  4.10it/s]Loading trainS:  12%|█▏        | 32/266 [00:08<00:56,  4.11it/s]Loading trainS:  12%|█▏        | 33/266 [00:08<00:56,  4.12it/s]Loading trainS:  13%|█▎        | 34/266 [00:08<00:56,  4.10it/s]Loading trainS:  13%|█▎        | 35/266 [00:08<00:56,  4.10it/s]Loading trainS:  14%|█▎        | 36/266 [00:09<00:55,  4.11it/s]Loading trainS:  14%|█▍        | 37/266 [00:09<00:55,  4.11it/s]Loading trainS:  14%|█▍        | 38/266 [00:09<00:55,  4.12it/s]Loading trainS:  15%|█▍        | 39/266 [00:09<00:54,  4.13it/s]Loading trainS:  15%|█▌        | 40/266 [00:10<00:54,  4.14it/s]Loading trainS:  15%|█▌        | 41/266 [00:10<00:54,  4.15it/s]Loading trainS:  16%|█▌        | 42/266 [00:10<00:51,  4.36it/s]Loading trainS:  16%|█▌        | 43/266 [00:10<00:49,  4.52it/s]Loading trainS:  17%|█▋        | 44/266 [00:10<00:47,  4.64it/s]Loading trainS:  17%|█▋        | 45/266 [00:11<00:46,  4.71it/s]Loading trainS:  17%|█▋        | 46/266 [00:11<00:46,  4.78it/s]Loading trainS:  18%|█▊        | 47/266 [00:11<00:46,  4.75it/s]Loading trainS:  18%|█▊        | 48/266 [00:11<00:45,  4.78it/s]Loading trainS:  18%|█▊        | 49/266 [00:11<00:45,  4.82it/s]Loading trainS:  19%|█▉        | 50/266 [00:12<00:44,  4.84it/s]Loading trainS:  19%|█▉        | 51/266 [00:12<00:44,  4.86it/s]Loading trainS:  20%|█▉        | 52/266 [00:12<00:44,  4.83it/s]Loading trainS:  20%|█▉        | 53/266 [00:12<00:44,  4.81it/s]Loading trainS:  20%|██        | 54/266 [00:13<00:44,  4.80it/s]Loading trainS:  21%|██        | 55/266 [00:13<00:44,  4.79it/s]Loading trainS:  21%|██        | 56/266 [00:13<00:43,  4.79it/s]Loading trainS:  21%|██▏       | 57/266 [00:13<00:43,  4.78it/s]Loading trainS:  22%|██▏       | 58/266 [00:13<00:43,  4.79it/s]Loading trainS:  22%|██▏       | 59/266 [00:14<00:43,  4.77it/s]Loading trainS:  23%|██▎       | 60/266 [00:14<00:43,  4.71it/s]Loading trainS:  23%|██▎       | 61/266 [00:14<00:44,  4.65it/s]Loading trainS:  23%|██▎       | 62/266 [00:14<00:44,  4.63it/s]Loading trainS:  24%|██▎       | 63/266 [00:14<00:44,  4.61it/s]Loading trainS:  24%|██▍       | 64/266 [00:15<00:44,  4.50it/s]Loading trainS:  24%|██▍       | 65/266 [00:15<00:44,  4.51it/s]Loading trainS:  25%|██▍       | 66/266 [00:15<00:44,  4.50it/s]Loading trainS:  25%|██▌       | 67/266 [00:15<00:44,  4.49it/s]Loading trainS:  26%|██▌       | 68/266 [00:16<00:43,  4.51it/s]Loading trainS:  26%|██▌       | 69/266 [00:16<00:43,  4.49it/s]Loading trainS:  26%|██▋       | 70/266 [00:16<00:43,  4.47it/s]Loading trainS:  27%|██▋       | 71/266 [00:16<00:43,  4.49it/s]Loading trainS:  27%|██▋       | 72/266 [00:16<00:43,  4.47it/s]Loading trainS:  27%|██▋       | 73/266 [00:17<00:43,  4.41it/s]Loading trainS:  28%|██▊       | 74/266 [00:17<00:42,  4.48it/s]Loading trainS:  28%|██▊       | 75/266 [00:17<00:42,  4.54it/s]Loading trainS:  29%|██▊       | 76/266 [00:17<00:41,  4.58it/s]Loading trainS:  29%|██▉       | 77/266 [00:18<00:40,  4.61it/s]Loading trainS:  29%|██▉       | 78/266 [00:18<00:42,  4.46it/s]Loading trainS:  30%|██▉       | 79/266 [00:18<00:42,  4.36it/s]Loading trainS:  30%|███       | 80/266 [00:18<00:43,  4.32it/s]Loading trainS:  30%|███       | 81/266 [00:19<00:43,  4.28it/s]Loading trainS:  31%|███       | 82/266 [00:19<00:43,  4.26it/s]Loading trainS:  31%|███       | 83/266 [00:19<00:43,  4.23it/s]Loading trainS:  32%|███▏      | 84/266 [00:19<00:43,  4.22it/s]Loading trainS:  32%|███▏      | 85/266 [00:19<00:42,  4.21it/s]Loading trainS:  32%|███▏      | 86/266 [00:20<00:42,  4.21it/s]Loading trainS:  33%|███▎      | 87/266 [00:20<00:42,  4.18it/s]Loading trainS:  33%|███▎      | 88/266 [00:20<00:42,  4.15it/s]Loading trainS:  33%|███▎      | 89/266 [00:20<00:42,  4.13it/s]Loading trainS:  34%|███▍      | 90/266 [00:21<00:43,  4.09it/s]Loading trainS:  34%|███▍      | 91/266 [00:21<00:43,  4.04it/s]Loading trainS:  35%|███▍      | 92/266 [00:21<00:43,  4.04it/s]Loading trainS:  35%|███▍      | 93/266 [00:21<00:42,  4.03it/s]Loading trainS:  35%|███▌      | 94/266 [00:22<00:42,  4.03it/s]Loading trainS:  36%|███▌      | 95/266 [00:22<00:42,  4.01it/s]Loading trainS:  36%|███▌      | 96/266 [00:22<00:42,  4.02it/s]Loading trainS:  36%|███▋      | 97/266 [00:22<00:43,  3.86it/s]Loading trainS:  37%|███▋      | 98/266 [00:23<00:43,  3.90it/s]Loading trainS:  37%|███▋      | 99/266 [00:23<00:40,  4.14it/s]Loading trainS:  38%|███▊      | 100/266 [00:23<00:39,  4.15it/s]Loading trainS:  38%|███▊      | 101/266 [00:23<00:39,  4.14it/s]Loading trainS:  38%|███▊      | 102/266 [00:24<00:38,  4.23it/s]Loading trainS:  39%|███▊      | 103/266 [00:24<00:39,  4.17it/s]Loading trainS:  39%|███▉      | 104/266 [00:24<00:38,  4.25it/s]Loading trainS:  39%|███▉      | 105/266 [00:24<00:37,  4.30it/s]Loading trainS:  40%|███▉      | 106/266 [00:25<00:36,  4.37it/s]Loading trainS:  40%|████      | 107/266 [00:25<00:35,  4.42it/s]Loading trainS:  41%|████      | 108/266 [00:25<00:35,  4.46it/s]Loading trainS:  41%|████      | 109/266 [00:25<00:34,  4.49it/s]Loading trainS:  41%|████▏     | 110/266 [00:25<00:34,  4.50it/s]Loading trainS:  42%|████▏     | 111/266 [00:26<00:34,  4.52it/s]Loading trainS:  42%|████▏     | 112/266 [00:26<00:34,  4.53it/s]Loading trainS:  42%|████▏     | 113/266 [00:26<00:33,  4.57it/s]Loading trainS:  43%|████▎     | 114/266 [00:26<00:33,  4.58it/s]Loading trainS:  43%|████▎     | 115/266 [00:27<00:32,  4.60it/s]Loading trainS:  44%|████▎     | 116/266 [00:27<00:32,  4.60it/s]Loading trainS:  44%|████▍     | 117/266 [00:27<00:32,  4.58it/s]Loading trainS:  44%|████▍     | 118/266 [00:27<00:32,  4.58it/s]Loading trainS:  45%|████▍     | 119/266 [00:27<00:33,  4.35it/s]Loading trainS:  45%|████▌     | 120/266 [00:28<00:35,  4.15it/s]Loading trainS:  45%|████▌     | 121/266 [00:28<00:35,  4.08it/s]Loading trainS:  46%|████▌     | 122/266 [00:28<00:35,  4.03it/s]Loading trainS:  46%|████▌     | 123/266 [00:28<00:35,  4.01it/s]Loading trainS:  47%|████▋     | 124/266 [00:29<00:35,  3.99it/s]Loading trainS:  47%|████▋     | 125/266 [00:29<00:35,  3.97it/s]Loading trainS:  47%|████▋     | 126/266 [00:29<00:35,  3.92it/s]Loading trainS:  48%|████▊     | 127/266 [00:30<00:35,  3.91it/s]Loading trainS:  48%|████▊     | 128/266 [00:30<00:35,  3.91it/s]Loading trainS:  48%|████▊     | 129/266 [00:30<00:35,  3.89it/s]Loading trainS:  49%|████▉     | 130/266 [00:30<00:35,  3.86it/s]Loading trainS:  49%|████▉     | 131/266 [00:31<00:34,  3.86it/s]Loading trainS:  50%|████▉     | 132/266 [00:31<00:35,  3.83it/s]Loading trainS:  50%|█████     | 133/266 [00:31<00:34,  3.85it/s]Loading trainS:  50%|█████     | 134/266 [00:31<00:34,  3.85it/s]Loading trainS:  51%|█████     | 135/266 [00:32<00:34,  3.76it/s]Loading trainS:  51%|█████     | 136/266 [00:32<00:34,  3.79it/s]Loading trainS:  52%|█████▏    | 137/266 [00:32<00:32,  3.93it/s]Loading trainS:  52%|█████▏    | 138/266 [00:32<00:31,  4.04it/s]Loading trainS:  52%|█████▏    | 139/266 [00:33<00:30,  4.10it/s]Loading trainS:  53%|█████▎    | 140/266 [00:33<00:30,  4.16it/s]Loading trainS:  53%|█████▎    | 141/266 [00:33<00:29,  4.21it/s]Loading trainS:  53%|█████▎    | 142/266 [00:33<00:29,  4.24it/s]Loading trainS:  54%|█████▍    | 143/266 [00:33<00:28,  4.28it/s]Loading trainS:  54%|█████▍    | 144/266 [00:34<00:28,  4.29it/s]Loading trainS:  55%|█████▍    | 145/266 [00:34<00:28,  4.30it/s]Loading trainS:  55%|█████▍    | 146/266 [00:34<00:27,  4.30it/s]Loading trainS:  55%|█████▌    | 147/266 [00:34<00:27,  4.31it/s]Loading trainS:  56%|█████▌    | 148/266 [00:35<00:27,  4.32it/s]Loading trainS:  56%|█████▌    | 149/266 [00:35<00:27,  4.32it/s]Loading trainS:  56%|█████▋    | 150/266 [00:35<00:26,  4.32it/s]Loading trainS:  57%|█████▋    | 151/266 [00:35<00:26,  4.30it/s]Loading trainS:  57%|█████▋    | 152/266 [00:36<00:26,  4.30it/s]Loading trainS:  58%|█████▊    | 153/266 [00:36<00:26,  4.30it/s]Loading trainS:  58%|█████▊    | 154/266 [00:36<00:26,  4.29it/s]Loading trainS:  58%|█████▊    | 155/266 [00:36<00:24,  4.49it/s]Loading trainS:  59%|█████▊    | 156/266 [00:36<00:23,  4.62it/s]Loading trainS:  59%|█████▉    | 157/266 [00:37<00:23,  4.70it/s]Loading trainS:  59%|█████▉    | 158/266 [00:37<00:22,  4.81it/s]Loading trainS:  60%|█████▉    | 159/266 [00:37<00:21,  4.87it/s]Loading trainS:  60%|██████    | 160/266 [00:37<00:21,  4.93it/s]Loading trainS:  61%|██████    | 161/266 [00:37<00:21,  4.98it/s]Loading trainS:  61%|██████    | 162/266 [00:38<00:20,  4.99it/s]Loading trainS:  61%|██████▏   | 163/266 [00:38<00:20,  5.01it/s]Loading trainS:  62%|██████▏   | 164/266 [00:38<00:20,  5.03it/s]Loading trainS:  62%|██████▏   | 165/266 [00:38<00:20,  5.05it/s]Loading trainS:  62%|██████▏   | 166/266 [00:38<00:19,  5.06it/s]Loading trainS:  63%|██████▎   | 167/266 [00:39<00:19,  5.07it/s]Loading trainS:  63%|██████▎   | 168/266 [00:39<00:19,  5.08it/s]Loading trainS:  64%|██████▎   | 169/266 [00:39<00:19,  5.07it/s]Loading trainS:  64%|██████▍   | 170/266 [00:39<00:18,  5.06it/s]Loading trainS:  64%|██████▍   | 171/266 [00:39<00:18,  5.06it/s]Loading trainS:  65%|██████▍   | 172/266 [00:40<00:18,  5.07it/s]Loading trainS:  65%|██████▌   | 173/266 [00:40<00:18,  4.91it/s]Loading trainS:  65%|██████▌   | 174/266 [00:40<00:19,  4.81it/s]Loading trainS:  66%|██████▌   | 175/266 [00:40<00:19,  4.74it/s]Loading trainS:  66%|██████▌   | 176/266 [00:40<00:19,  4.63it/s]Loading trainS:  67%|██████▋   | 177/266 [00:41<00:19,  4.63it/s]Loading trainS:  67%|██████▋   | 178/266 [00:41<00:19,  4.56it/s]Loading trainS:  67%|██████▋   | 179/266 [00:41<00:19,  4.48it/s]Loading trainS:  68%|██████▊   | 180/266 [00:41<00:19,  4.50it/s]Loading trainS:  68%|██████▊   | 181/266 [00:42<00:18,  4.53it/s]Loading trainS:  68%|██████▊   | 182/266 [00:42<00:18,  4.56it/s]Loading trainS:  69%|██████▉   | 183/266 [00:42<00:18,  4.59it/s]Loading trainS:  69%|██████▉   | 184/266 [00:42<00:17,  4.59it/s]Loading trainS:  70%|██████▉   | 185/266 [00:42<00:17,  4.61it/s]Loading trainS:  70%|██████▉   | 186/266 [00:43<00:17,  4.60it/s]Loading trainS:  70%|███████   | 187/266 [00:43<00:17,  4.62it/s]Loading trainS:  71%|███████   | 188/266 [00:43<00:16,  4.63it/s]Loading trainS:  71%|███████   | 189/266 [00:43<00:16,  4.64it/s]Loading trainS:  71%|███████▏  | 190/266 [00:44<00:16,  4.65it/s]Loading trainS:  72%|███████▏  | 191/266 [00:44<00:16,  4.53it/s]Loading trainS:  72%|███████▏  | 192/266 [00:44<00:16,  4.54it/s]Loading trainS:  73%|███████▎  | 193/266 [00:44<00:16,  4.48it/s]Loading trainS:  73%|███████▎  | 194/266 [00:44<00:16,  4.24it/s]Loading trainS:  73%|███████▎  | 195/266 [00:45<00:16,  4.36it/s]Loading trainS:  74%|███████▎  | 196/266 [00:45<00:15,  4.44it/s]Loading trainS:  74%|███████▍  | 197/266 [00:45<00:15,  4.48it/s]Loading trainS:  74%|███████▍  | 198/266 [00:45<00:15,  4.50it/s]Loading trainS:  75%|███████▍  | 199/266 [00:46<00:14,  4.50it/s]Loading trainS:  75%|███████▌  | 200/266 [00:46<00:14,  4.52it/s]Loading trainS:  76%|███████▌  | 201/266 [00:46<00:14,  4.52it/s]Loading trainS:  76%|███████▌  | 202/266 [00:46<00:14,  4.49it/s]Loading trainS:  76%|███████▋  | 203/266 [00:46<00:13,  4.54it/s]Loading trainS:  77%|███████▋  | 204/266 [00:47<00:13,  4.57it/s]Loading trainS:  77%|███████▋  | 205/266 [00:47<00:13,  4.58it/s]Loading trainS:  77%|███████▋  | 206/266 [00:47<00:13,  4.58it/s]Loading trainS:  78%|███████▊  | 207/266 [00:47<00:12,  4.57it/s]Loading trainS:  78%|███████▊  | 208/266 [00:48<00:12,  4.57it/s]Loading trainS:  79%|███████▊  | 209/266 [00:48<00:12,  4.55it/s]Loading trainS:  79%|███████▉  | 210/266 [00:48<00:12,  4.55it/s]Loading trainS:  79%|███████▉  | 211/266 [00:48<00:12,  4.55it/s]Loading trainS:  80%|███████▉  | 212/266 [00:48<00:11,  4.55it/s]Loading trainS:  80%|████████  | 213/266 [00:49<00:11,  4.62it/s]Loading trainS:  80%|████████  | 214/266 [00:49<00:11,  4.65it/s]Loading trainS:  81%|████████  | 215/266 [00:49<00:10,  4.68it/s]Loading trainS:  81%|████████  | 216/266 [00:49<00:10,  4.70it/s]Loading trainS:  82%|████████▏ | 217/266 [00:49<00:10,  4.70it/s]Loading trainS:  82%|████████▏ | 218/266 [00:50<00:10,  4.72it/s]Loading trainS:  82%|████████▏ | 219/266 [00:50<00:09,  4.72it/s]Loading trainS:  83%|████████▎ | 220/266 [00:50<00:09,  4.73it/s]Loading trainS:  83%|████████▎ | 221/266 [00:50<00:09,  4.75it/s]Loading trainS:  83%|████████▎ | 222/266 [00:51<00:09,  4.74it/s]Loading trainS:  84%|████████▍ | 223/266 [00:51<00:09,  4.70it/s]Loading trainS:  84%|████████▍ | 224/266 [00:51<00:08,  4.68it/s]Loading trainS:  85%|████████▍ | 225/266 [00:51<00:08,  4.63it/s]Loading trainS:  85%|████████▍ | 226/266 [00:51<00:08,  4.67it/s]Loading trainS:  85%|████████▌ | 227/266 [00:52<00:08,  4.64it/s]Loading trainS:  86%|████████▌ | 228/266 [00:52<00:08,  4.61it/s]Loading trainS:  86%|████████▌ | 229/266 [00:52<00:07,  4.65it/s]Loading trainS:  86%|████████▋ | 230/266 [00:52<00:07,  4.68it/s]Loading trainS:  87%|████████▋ | 231/266 [00:52<00:07,  4.65it/s]Loading trainS:  87%|████████▋ | 232/266 [00:53<00:07,  4.63it/s]Loading trainS:  88%|████████▊ | 233/266 [00:53<00:07,  4.61it/s]Loading trainS:  88%|████████▊ | 234/266 [00:53<00:06,  4.59it/s]Loading trainS:  88%|████████▊ | 235/266 [00:53<00:06,  4.58it/s]Loading trainS:  89%|████████▊ | 236/266 [00:54<00:06,  4.57it/s]Loading trainS:  89%|████████▉ | 237/266 [00:54<00:06,  4.57it/s]Loading trainS:  89%|████████▉ | 238/266 [00:54<00:06,  4.56it/s]Loading trainS:  90%|████████▉ | 239/266 [00:54<00:05,  4.56it/s]Loading trainS:  90%|█████████ | 240/266 [00:54<00:05,  4.55it/s]Loading trainS:  91%|█████████ | 241/266 [00:55<00:05,  4.54it/s]Loading trainS:  91%|█████████ | 242/266 [00:55<00:05,  4.55it/s]Loading trainS:  91%|█████████▏| 243/266 [00:55<00:05,  4.56it/s]Loading trainS:  92%|█████████▏| 244/266 [00:55<00:04,  4.56it/s]Loading trainS:  92%|█████████▏| 245/266 [00:56<00:04,  4.56it/s]Loading trainS:  92%|█████████▏| 246/266 [00:56<00:04,  4.55it/s]Loading trainS:  93%|█████████▎| 247/266 [00:56<00:04,  4.52it/s]Loading trainS:  93%|█████████▎| 248/266 [00:56<00:03,  4.54it/s]Loading trainS:  94%|█████████▎| 249/266 [00:56<00:03,  4.42it/s]Loading trainS:  94%|█████████▍| 250/266 [00:57<00:03,  4.34it/s]Loading trainS:  94%|█████████▍| 251/266 [00:57<00:03,  4.30it/s]Loading trainS:  95%|█████████▍| 252/266 [00:57<00:03,  4.26it/s]Loading trainS:  95%|█████████▌| 253/266 [00:57<00:03,  4.23it/s]Loading trainS:  95%|█████████▌| 254/266 [00:58<00:02,  4.21it/s]Loading trainS:  96%|█████████▌| 255/266 [00:58<00:02,  4.20it/s]Loading trainS:  96%|█████████▌| 256/266 [00:58<00:02,  4.19it/s]Loading trainS:  97%|█████████▋| 257/266 [00:58<00:02,  4.17it/s]Loading trainS:  97%|█████████▋| 258/266 [00:59<00:01,  4.15it/s]Loading trainS:  97%|█████████▋| 259/266 [00:59<00:01,  4.16it/s]Loading trainS:  98%|█████████▊| 260/266 [00:59<00:01,  4.16it/s]Loading trainS:  98%|█████████▊| 261/266 [01:00<00:01,  2.58it/s]Loading trainS:  98%|█████████▊| 262/266 [01:01<00:01,  2.10it/s]Loading trainS:  99%|█████████▉| 263/266 [01:01<00:01,  1.86it/s]Loading trainS:  99%|█████████▉| 264/266 [01:02<00:01,  1.65it/s]Loading trainS: 100%|█████████▉| 265/266 [01:03<00:00,  1.60it/s]Loading trainS: 100%|██████████| 266/266 [01:03<00:00,  1.54it/s]Loading trainS: 100%|██████████| 266/266 [01:03<00:00,  4.17it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  1.64it/s]Loading testS:  50%|█████     | 2/4 [00:01<00:01,  1.53it/s]Loading testS:  75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               2020-01-21 02:04:08.873338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 02:04:08.873442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 02:04:08.873457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 02:04:08.873465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 02:04:08.874222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from WMn   /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 48s - loss: 0.1325 - acc: 0.9866 - mDice: 0.7420 - val_loss: 0.2921 - val_acc: 0.9888 - val_mDice: 0.4197

Epoch 00001: val_mDice improved from -inf to 0.41970, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 44s - loss: 0.0856 - acc: 0.9911 - mDice: 0.8333 - val_loss: 0.2374 - val_acc: 0.9924 - val_mDice: 0.4421

Epoch 00002: val_mDice improved from 0.41970 to 0.44210, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 45s - loss: 0.0764 - acc: 0.9920 - mDice: 0.8513 - val_loss: 0.2025 - val_acc: 0.9927 - val_mDice: 0.4505

Epoch 00003: val_mDice improved from 0.44210 to 0.45048, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 46s - loss: 0.0703 - acc: 0.9925 - mDice: 0.8633 - val_loss: 0.2157 - val_acc: 0.9921 - val_mDice: 0.4503

Epoch 00004: val_mDice did not improve from 0.45048
Epoch 5/300
 - 45s - loss: 0.0665 - acc: 0.9930 - mDice: 0.8705 - val_loss: 0.1167 - val_acc: 0.9925 - val_mDice: 0.4628

Epoch 00005: val_mDice improved from 0.45048 to 0.46285, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 45s - loss: 0.0655 - acc: 0.9932 - mDice: 0.8726 - val_loss: 0.0156 - val_acc: 0.9932 - val_mDice: 0.4703

Epoch 00006: val_mDice improved from 0.46285 to 0.47028, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 46s - loss: 0.0613 - acc: 0.9934 - mDice: 0.8807 - val_loss: 0.1218 - val_acc: 0.9932 - val_mDice: 0.4717

Epoch 00007: val_mDice improved from 0.47028 to 0.47170, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 46s - loss: 0.0594 - acc: 0.9936 - mDice: 0.8845 - val_loss: 0.2354 - val_acc: 0.9910 - val_mDice: 0.4560

Epoch 00008: val_mDice did not improve from 0.47170
Epoch 9/300
 - 46s - loss: 0.0581 - acc: 0.9938 - mDice: 0.8870 - val_loss: 0.1047 - val_acc: 0.9935 - val_mDice: 0.4782

Epoch 00009: val_mDice improved from 0.47170 to 0.47818, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 46s - loss: 0.0578 - acc: 0.9938 - mDice: 0.8877 - val_loss: 0.1985 - val_acc: 0.9894 - val_mDice: 0.4527

Epoch 00010: val_mDice did not improve from 0.47818
Epoch 11/300
 - 46s - loss: 0.0559 - acc: 0.9940 - mDice: 0.8912 - val_loss: 0.0709 - val_acc: 0.9916 - val_mDice: 0.4784

Epoch 00011: val_mDice improved from 0.47818 to 0.47836, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 46s - loss: 0.0541 - acc: 0.9941 - mDice: 0.8949 - val_loss: -2.8688e-02 - val_acc: 0.9935 - val_mDice: 0.4711

Epoch 00012: val_mDice did not improve from 0.47836
Epoch 13/300
 - 46s - loss: 0.0541 - acc: 0.9942 - mDice: 0.8947 - val_loss: 0.0119 - val_acc: 0.9936 - val_mDice: 0.4731

Epoch 00013: val_mDice did not improve from 0.47836
Epoch 14/300
 - 45s - loss: 0.0517 - acc: 0.9943 - mDice: 0.8994 - val_loss: -3.9360e-02 - val_acc: 0.9939 - val_mDice: 0.4883

Epoch 00014: val_mDice improved from 0.47836 to 0.48834, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 45s - loss: 0.0517 - acc: 0.9943 - mDice: 0.8994 - val_loss: 0.0832 - val_acc: 0.9915 - val_mDice: 0.4054

Epoch 00015: val_mDice did not improve from 0.48834
Epoch 16/300
 - 45s - loss: 0.0509 - acc: 0.9945 - mDice: 0.9010 - val_loss: 0.1179 - val_acc: 0.9923 - val_mDice: 0.4853

Epoch 00016: val_mDice did not improve from 0.48834
Epoch 17/300
 - 45s - loss: 0.0501 - acc: 0.9945 - mDice: 0.9025 - val_loss: 0.1104 - val_acc: 0.9933 - val_mDice: 0.4910

Epoch 00017: val_mDice improved from 0.48834 to 0.49099, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300
 - 45s - loss: 0.0518 - acc: 0.9944 - mDice: 0.8993 - val_loss: 0.1055 - val_acc: 0.9931 - val_mDice: 0.4805

Epoch 00018: val_mDice did not improve from 0.49099
Epoch 19/300
 - 44s - loss: 0.0493 - acc: 0.9946 - mDice: 0.9042 - val_loss: -9.1105e-03 - val_acc: 0.9938 - val_mDice: 0.4868

Epoch 00019: val_mDice did not improve from 0.49099
Epoch 20/300
 - 43s - loss: 0.0485 - acc: 0.9946 - mDice: 0.9058 - val_loss: -2.9764e-02 - val_acc: 0.9928 - val_mDice: 0.4699

Epoch 00020: val_mDice did not improve from 0.49099
Epoch 21/300
 - 44s - loss: 0.0474 - acc: 0.9947 - mDice: 0.9080 - val_loss: 0.0133 - val_acc: 0.9936 - val_mDice: 0.4911

Epoch 00021: val_mDice improved from 0.49099 to 0.49115, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 22/300
 - 44s - loss: 0.0470 - acc: 0.9948 - mDice: 0.9086 - val_loss: 0.0341 - val_acc: 0.9938 - val_mDice: 0.4990

Epoch 00022: val_mDice improved from 0.49115 to 0.49900, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300
 - 43s - loss: 0.0486 - acc: 0.9948 - mDice: 0.9055 - val_loss: 0.0064 - val_acc: 0.9931 - val_mDice: 0.4927

Epoch 00023: val_mDice did not improve from 0.49900
Epoch 24/300
 - 44s - loss: 0.0461 - acc: 0.9949 - mDice: 0.9105 - val_loss: 0.0725 - val_acc: 0.9938 - val_mDice: 0.4982

Epoch 00024: val_mDice did not improve from 0.49900
Epoch 25/300
 - 43s - loss: 0.0468 - acc: 0.9948 - mDice: 0.9090 - val_loss: 0.0011 - val_acc: 0.9935 - val_mDice: 0.4929

Epoch 00025: val_mDice did not improve from 0.49900
Epoch 26/300
 - 44s - loss: 0.0461 - acc: 0.9948 - mDice: 0.9104 - val_loss: 0.0654 - val_acc: 0.9934 - val_mDice: 0.4916

Epoch 00026: val_mDice did not improve from 0.49900
Epoch 27/300
 - 43s - loss: 0.0451 - acc: 0.9949 - mDice: 0.9124 - val_loss: -4.5742e-02 - val_acc: 0.9940 - val_mDice: 0.5013

Epoch 00027: val_mDice improved from 0.49900 to 0.50134, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300
 - 44s - loss: 0.0452 - acc: 0.9949 - mDice: 0.9122 - val_loss: 0.0370 - val_acc: 0.9939 - val_mDice: 0.4945

Epoch 00028: val_mDice did not improve from 0.50134
Epoch 29/300
 - 43s - loss: 0.0447 - acc: 0.9950 - mDice: 0.9132 - val_loss: -3.2427e-02 - val_acc: 0.9940 - val_mDice: 0.5050

Epoch 00029: val_mDice improved from 0.50134 to 0.50504, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/300
 - 44s - loss: 0.0442 - acc: 0.9950 - mDice: 0.9141 - val_loss: -4.1977e-02 - val_acc: 0.9939 - val_mDice: 0.5071

Epoch 00030: val_mDice improved from 0.50504 to 0.50709, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 31/300
 - 44s - loss: 0.0443 - acc: 0.9950 - mDice: 0.9139 - val_loss: -6.3068e-03 - val_acc: 0.9941 - val_mDice: 0.5016

Epoch 00031: val_mDice did not improve from 0.50709
Epoch 32/300
 - 44s - loss: 0.0437 - acc: 0.9951 - mDice: 0.9150 - val_loss: 0.0312 - val_acc: 0.9932 - val_mDice: 0.4938

Epoch 00032: val_mDice did not improve from 0.50709
Epoch 33/300
 - 44s - loss: 0.0443 - acc: 0.9951 - mDice: 0.9139 - val_loss: 0.0289 - val_acc: 0.9939 - val_mDice: 0.4942

Epoch 00033: val_mDice did not improve from 0.50709
Epoch 34/300
 - 44s - loss: 0.0438 - acc: 0.9951 - mDice: 0.9149 - val_loss: -2.7251e-02 - val_acc: 0.9939 - val_mDice: 0.4939

Epoch 00034: val_mDice did not improve from 0.50709
Epoch 35/300
 - 44s - loss: 0.0434 - acc: 0.9951 - mDice: 0.9157 - val_loss: 0.0041 - val_acc: 0.9936 - val_mDice: 0.5029

Epoch 00035: val_mDice did not improve from 0.50709
Epoch 36/300
 - 44s - loss: 0.0431 - acc: 0.9951 - mDice: 0.9162 - val_loss: 0.0307 - val_acc: 0.9940 - val_mDice: 0.5065

Epoch 00036: val_mDice did not improve from 0.50709
Epoch 37/300
 - 43s - loss: 0.0429 - acc: 0.9951 - mDice: 0.9167 - val_loss: -4.6928e-03 - val_acc: 0.9939 - val_mDice: 0.4972

Epoch 00037: val_mDice did not improve from 0.50709
Epoch 38/300
 - 44s - loss: 0.0434 - acc: 0.9951 - mDice: 0.9156 - val_loss: -2.1589e-02 - val_acc: 0.9932 - val_mDice: 0.4821

Epoch 00038: val_mDice did not improve from 0.50709
Epoch 39/300
 - 43s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9169 - val_loss: 0.0129 - val_acc: 0.9934 - val_mDice: 0.4964

Epoch 00039: val_mDice did not improve from 0.50709
Epoch 40/300
 - 44s - loss: 0.0421 - acc: 0.9952 - mDice: 0.9183 - val_loss: -1.4361e-03 - val_acc: 0.9937 - val_mDice: 0.4909

Epoch 00040: val_mDice did not improve from 0.50709
Epoch 41/300
 - 43s - loss: 0.0424 - acc: 0.9952 - mDice: 0.9175 - val_loss: 0.0484 - val_acc: 0.9937 - val_mDice: 0.4933

Epoch 00041: val_mDice did not improve from 0.50709
Epoch 42/300
 - 43s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9168 - val_loss: 0.0766 - val_acc: 0.9939 - val_mDice: 0.5004

Epoch 00042: val_mDice did not improve from 0.50709
Epoch 43/300
 - 44s - loss: 0.0419 - acc: 0.9952 - mDice: 0.9186 - val_loss: 0.0205 - val_acc: 0.9938 - val_mDice: 0.4928

Epoch 00043: val_mDice did not improve from 0.50709
Epoch 44/300
 - 44s - loss: 0.0413 - acc: 0.9953 - mDice: 0.9199 - val_loss: -2.5649e-03 - val_acc: 0.9939 - val_mDice: 0.4933

Epoch 00044: val_mDice did not improve from 0.50709
Epoch 45/300
 - 44s - loss: 0.0409 - acc: 0.9953 - mDice: 0.9205 - val_loss: 0.0155 - val_acc: 0.9941 - val_mDice: 0.5027

Epoch 00045: val_mDice did not improve from 0.50709

Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 46/300
 - 44s - loss: 0.0403 - acc: 0.9954 - mDice: 0.9216 - val_loss: 0.0249 - val_acc: 0.9940 - val_mDice: 0.4979

Epoch 00046: val_mDice did not improve from 0.50709
Epoch 47/300
 - 45s - loss: 0.0402 - acc: 0.9954 - mDice: 0.9218 - val_loss: 0.0753 - val_acc: 0.9939 - val_mDice: 0.4968

Epoch 00047: val_mDice did not improve from 0.50709
Epoch 48/300
 - 46s - loss: 0.0396 - acc: 0.9955 - mDice: 0.9230 - val_loss: -3.3502e-02 - val_acc: 0.9939 - val_mDice: 0.4885

Epoch 00048: val_mDice did not improve from 0.50709
Epoch 49/300
 - 46s - loss: 0.0388 - acc: 0.9955 - mDice: 0.9246 - val_loss: -1.6215e-02 - val_acc: 0.9939 - val_mDice: 0.4968

Epoch 00049: val_mDice did not improve from 0.50709
Epoch 50/300
 - 45s - loss: 0.0384 - acc: 0.9955 - mDice: 0.9255 - val_loss: 0.0214 - val_acc: 0.9935 - val_mDice: 0.4940

Epoch 00050: val_mDice did not improve from 0.50709
Epoch 51/300
 - 46s - loss: 0.0384 - acc: 0.9955 - mDice: 0.9254 - val_loss: 0.0433 - val_acc: 0.9932 - val_mDice: 0.4915

Epoch 00051: val_mDice did not improve from 0.50709
Epoch 52/300
 - 46s - loss: 0.0377 - acc: 0.9956 - mDice: 0.9268 - val_loss: -3.2892e-02 - val_acc: 0.9940 - val_mDice: 0.4982

Epoch 00052: val_mDice did not improve from 0.50709
Epoch 53/300
 - 46s - loss: 0.0380 - acc: 0.9955 - mDice: 0.9262 - val_loss: -4.5163e-02 - val_acc: 0.9939 - val_mDice: 0.5011

Epoch 00053: val_mDice did not improve from 0.50709
Epoch 54/300
 - 46s - loss: 0.0375 - acc: 0.9956 - mDice: 0.9271 - val_loss: 7.7029e-04 - val_acc: 0.9939 - val_mDice: 0.4993

Epoch 00054: val_mDice did not improve from 0.50709
Epoch 55/300
 - 47s - loss: 0.0375 - acc: 0.9956 - mDice: 0.9271 - val_loss: -1.0874e-03 - val_acc: 0.9942 - val_mDice: 0.4965

Epoch 00055: val_mDice did not improve from 0.50709
Epoch 56/300
 - 46s - loss: 0.0376 - acc: 0.9956 - mDice: 0.9271 - val_loss: -2.7698e-03 - val_acc: 0.9941 - val_mDice: 0.4965

Epoch 00056: val_mDice did not improve from 0.50709
Epoch 57/300
 - 46s - loss: 0.0377 - acc: 0.9956 - mDice: 0.9268 - val_loss: -4.1966e-03 - val_acc: 0.9941 - val_mDice: 0.5001

Epoch 00057: val_mDice did not improve from 0.50709
Epoch 58/300
 - 45s - loss: 0.0378 - acc: 0.9956 - mDice: 0.9266 - val_loss: -2.6586e-03 - val_acc: 0.9941 - val_mDice: 0.4993

Epoch 00058: val_mDice did not improve from 0.50709
Epoch 59/300
 - 44s - loss: 0.0379 - acc: 0.9956 - mDice: 0.9264 - val_loss: 0.0314 - val_acc: 0.9940 - val_mDice: 0.5030

Epoch 00059: val_mDice did not improve from 0.50709
Epoch 60/300
 - 44s - loss: 0.0371 - acc: 0.9956 - mDice: 0.9279 - val_loss: 0.0406 - val_acc: 0.9936 - val_mDice: 0.4988

Epoch 00060: val_mDice did not improve from 0.50709

Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 61/300
 - 44s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9302 - val_loss: 0.0790 - val_acc: 0.9933 - val_mDice: 0.4926

Epoch 00061: val_mDice did not improve from 0.50709
Epoch 62/300
 - 44s - loss: 0.0374 - acc: 0.9957 - mDice: 0.9274 - val_loss: -2.9755e-03 - val_acc: 0.9940 - val_mDice: 0.4941

Epoch 00062: val_mDice did not improve from 0.50709
Epoch 63/300
 - 43s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9301 - val_loss: -5.7622e-03 - val_acc: 0.9940 - val_mDice: 0.5006

Epoch 00063: val_mDice did not improve from 0.50709
Epoch 64/300
 - 44s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9300 - val_loss: 0.0345 - val_acc: 0.9939 - val_mDice: 0.4970

Epoch 00064: val_mDice did not improve from 0.50709
Epoch 65/300
 - 44s - loss: 0.0358 - acc: 0.9957 - mDice: 0.9305 - val_loss: 0.0335 - val_acc: 0.9940 - val_mDice: 0.4990

Epoch 00065: val_mDice did not improve from 0.50709
Epoch 66/300
 - 44s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: -4.4895e-03 - val_acc: 0.9939 - val_mDice: 0.4971

Epoch 00066: val_mDice did not improve from 0.50709
Epoch 67/300
 - 44s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9304 - val_loss: 0.0347 - val_acc: 0.9940 - val_mDice: 0.4962

Epoch 00067: val_mDice did not improve from 0.50709
Epoch 68/300
 - 44s - loss: 0.0364 - acc: 0.9958 - mDice: 0.9293 - val_loss: 0.0345 - val_acc: 0.9941 - val_mDice: 0.4970

Epoch 00068: val_mDice did not improve from 0.50709
Epoch 69/300
 - 44s - loss: 0.0360 - acc: 0.9958 - mDice: 0.9301 - val_loss: -4.8564e-03 - val_acc: 0.9941 - val_mDice: 0.4975

Epoch 00069: val_mDice did not improve from 0.50709
Epoch 70/300
 - 44s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9290 - val_loss: -2.3829e-03 - val_acc: 0.9938 - val_mDice: 0.4947

Epoch 00070: val_mDice did not improve from 0.50709
Restoring model weights from the end of the best epoch
Epoch 00070: early stopping
{'val_loss': [0.2921042994130403, 0.23736988141899928, 0.20252397959120572, 0.21574869973119348, 0.11670799879357219, 0.015636734664440155, 0.12177395133767277, 0.23543095123022795, 0.10472930234391242, 0.19851101748645306, 0.07085648458451033, -0.02868778328411281, 0.011885931715369225, -0.03935991949401796, 0.08317219885066152, 0.11793367355130613, 0.11036623787367716, 0.10549648851156235, -0.009110455168411136, -0.029763857077341527, 0.01330040511675179, 0.03408159292303026, 0.006351398304104805, 0.07246849784860387, 0.0010901610367000103, 0.06544023100286722, -0.04574227653210983, 0.03703897673403844, -0.03242654836503789, -0.04197666922118515, -0.006306809140369296, 0.031175307114608586, 0.028901147132273763, -0.027250793238636106, 0.004097014316357672, 0.030687475053127855, -0.004692781309131533, -0.021588846866507083, 0.012945836700964719, -0.0014360853820107877, 0.04844933463027701, 0.07662558346055448, 0.020508599001914263, -0.002564921334851533, 0.015528488904237747, 0.024908518069423735, 0.07529796968447044, -0.03350170172052458, -0.016215424286201596, 0.02135492255911231, 0.04330730449873954, -0.032891779963392764, -0.04516334319487214, 0.0007702948059886694, -0.001087356940843165, -0.0027697644545696676, -0.004196610418148339, -0.0026586405001580715, 0.031379743420984596, 0.04063315415987745, 0.07897993549704552, -0.0029755207942798734, -0.005762221873737872, 0.03454255434917286, 0.03347401937935501, -0.00448951794533059, 0.03472520847572014, 0.0345085802837275, -0.004856375802773982, -0.002382865408435464], 'val_acc': [0.9888244061730802, 0.9923527496866882, 0.9927169410511851, 0.9921039352193475, 0.9924621968530118, 0.9932114593684673, 0.993228605017066, 0.9910076484084129, 0.9935491406358778, 0.989415897987783, 0.9915847857482731, 0.9935160810127854, 0.9935999582521617, 0.9938793387264013, 0.9915274162776768, 0.992338415235281, 0.9932666402310133, 0.9931213487870991, 0.9938197806477547, 0.9928098544478416, 0.9936283356510103, 0.9938403582200408, 0.9930988950654864, 0.9938359926454723, 0.993477426469326, 0.993430963717401, 0.9940115376375616, 0.9938503396697342, 0.9940361734479666, 0.9938918044790626, 0.9940739013254642, 0.9931540880352259, 0.9938709172420204, 0.9939201818779111, 0.9935522535815835, 0.9939706968143582, 0.9938715458847582, 0.9931955570355058, 0.9933573813177645, 0.9936959962360561, 0.9936769739724696, 0.9939145683310926, 0.9937823624350131, 0.9938515876419842, 0.9940782622434199, 0.9939859728328884, 0.993915194645524, 0.9939335859380662, 0.9939367058686912, 0.9934761733748019, 0.9931578268297017, 0.9940339871682227, 0.9938930571079254, 0.9938964890316129, 0.9941936298273504, 0.9940701574087143, 0.9941147421486676, 0.9940564390271902, 0.9940040544606745, 0.9935927893966436, 0.9933358584530652, 0.9939766223542392, 0.9940364854410291, 0.9939320283010602, 0.9939781753346324, 0.9939214298501611, 0.9940455262549222, 0.9940542574040592, 0.9941446771845222, 0.9937536800280213], 'val_mDice': [0.4196987202158198, 0.4421024885959923, 0.45048033469356596, 0.45025494939181954, 0.46284917247248814, 0.4702760433428921, 0.47169920668238774, 0.4560107388533652, 0.4781829909188673, 0.4527298759785481, 0.47835884615778923, 0.471137841741438, 0.4730581002695544, 0.4883415618678555, 0.4054019125760533, 0.4852897679666057, 0.4909885406959802, 0.4805042705265805, 0.48675992409698665, 0.4699326865375042, 0.49114645982626826, 0.4990016354713589, 0.4926908772904426, 0.4982181219384074, 0.4929341818206012, 0.4915991274174303, 0.5013402563054115, 0.4945430834777653, 0.5050421721534804, 0.5070919392164797, 0.5016378581058234, 0.4937856981996447, 0.4942207003477961, 0.493910675868392, 0.502894707606174, 0.5064881488215178, 0.49716581183020025, 0.4820746718905866, 0.4963602003408596, 0.49089666514191777, 0.4932922119041905, 0.5004101566737518, 0.4927908768877387, 0.49332169524859637, 0.5026597657706589, 0.4978832945926115, 0.49676189373712987, 0.4885474679758772, 0.49681323987897485, 0.49400667019654065, 0.49149024940561503, 0.49816215760074556, 0.501083986600861, 0.4993009159807116, 0.4965114110382274, 0.4965283634373918, 0.5001405370421708, 0.49925142200663686, 0.5029541673138738, 0.49882458813954145, 0.49264820758253336, 0.4941044287988916, 0.5005738249747083, 0.4969521320890635, 0.4990198148880154, 0.49707554455380887, 0.4962102114222944, 0.49701979267410934, 0.49752794904634356, 0.49474226078018546], 'loss': [0.13250001394573482, 0.08564945528669142, 0.0764040090710676, 0.07027874107911804, 0.06653056113326176, 0.06545025519000898, 0.061339890271636464, 0.05939119746942089, 0.058108091409771204, 0.057751715223077586, 0.0559333294735659, 0.05407814671228859, 0.054141970830254814, 0.051727759132779154, 0.051745963827913, 0.05088155247133424, 0.0501307368129613, 0.051772269030245836, 0.04926357334937398, 0.048470339278068444, 0.04735849559760695, 0.04701537131809587, 0.048565048445243376, 0.04606413818827431, 0.046794951800008096, 0.046122932882138326, 0.04510423832097867, 0.04517990640107383, 0.044667391369425075, 0.04420656641871447, 0.04429606385498744, 0.04372588425019456, 0.04431712055827484, 0.04377528782092113, 0.043382148508017554, 0.043107592731115224, 0.04288096458976715, 0.04343562681746009, 0.04276869620812317, 0.04206468144231963, 0.04244011663401655, 0.0427798793737836, 0.04189613802817082, 0.04126298627804779, 0.04091545011823541, 0.040348171233002064, 0.04023529564332253, 0.03964216456438754, 0.03882652364538787, 0.038377601496223455, 0.03840364035350099, 0.03770373611551579, 0.03798770027991938, 0.037547025115831786, 0.03754844061967123, 0.03757409623646495, 0.037718048067102314, 0.037798234021897734, 0.03791888532470823, 0.037124098817119616, 0.03595843930128519, 0.03737715803293646, 0.035990946760947135, 0.036040802731758574, 0.035785506140435075, 0.036314800537476016, 0.03585982993911669, 0.03638138354821835, 0.03600539833106437, 0.03655294991057551], 'acc': [0.9866048680014403, 0.991109539653084, 0.9919933692855232, 0.9925137879652637, 0.992992413408238, 0.9932499298847718, 0.993448327300135, 0.993617526985176, 0.9937770437997601, 0.9938247312413035, 0.993969196944726, 0.9940995470535967, 0.9942233782065543, 0.9943114058666976, 0.9943242232198706, 0.994483058746509, 0.9944977855466309, 0.9944455680066566, 0.9945662370248305, 0.994643304604776, 0.9947304839426956, 0.9947528290155805, 0.9947948915639572, 0.9948539022675261, 0.994824188025942, 0.9948136351160352, 0.9949127691013965, 0.9949446064303724, 0.994976721955908, 0.9950402717259588, 0.9950458873532937, 0.9950604799996827, 0.9950658075205884, 0.9950762269831264, 0.9951453694721624, 0.9951445574030251, 0.9951190307911256, 0.9951403611169205, 0.9951744345653566, 0.9952229925588101, 0.9952398580140293, 0.9952483759935246, 0.9952222834179678, 0.995259180341266, 0.9952919264212017, 0.9953601837047401, 0.9954433609996923, 0.9954848201817525, 0.9955310696603751, 0.9955162453404893, 0.9955173726695138, 0.9955789062208701, 0.9955456396130938, 0.995565908128575, 0.9955923437268889, 0.9955854298581752, 0.9956192612523358, 0.9955946151437668, 0.9956175032373338, 0.9956120781066374, 0.9957044105212891, 0.9957196842963457, 0.9957357980419123, 0.9957357383957166, 0.9957322264515427, 0.9957429478430811, 0.995765692440907, 0.9957545399804335, 0.9957548699562345, 0.9957365949865528], 'mDice': [0.7420013663425982, 0.8332954170926629, 0.8513243219360626, 0.8632973163098391, 0.870538045341997, 0.8725642014782017, 0.8806783661092219, 0.8844803111714068, 0.8869686185503785, 0.8876544243265586, 0.8912102882872642, 0.8948559710816414, 0.89466533166815, 0.8994387409004778, 0.8993989346202441, 0.9010412794684677, 0.9025375934079007, 0.8992792741171846, 0.9042347148649436, 0.9057765898123891, 0.9079509457949635, 0.9086268426365895, 0.90550945862763, 0.9104780994846497, 0.9090293288591003, 0.9103788882268906, 0.9123646941326409, 0.9121942724138793, 0.9132015642154829, 0.9140887914481473, 0.9139077238287537, 0.9150410306306008, 0.9138566164798239, 0.9149353222604882, 0.9156820795433757, 0.9162372348303577, 0.9167014818802602, 0.9155773161193614, 0.9168949850631588, 0.9182753403752997, 0.9175156853202329, 0.916829187584433, 0.9186116630598631, 0.9198590502016523, 0.9205336556958725, 0.9216275345618486, 0.9218071974935994, 0.9229766232800897, 0.924586032039961, 0.9254834883592817, 0.9254348476797055, 0.9268091339031496, 0.926245586751188, 0.9271239741928585, 0.9271019188468848, 0.9270521147583258, 0.9267546597418503, 0.9266106597815184, 0.9263525653237277, 0.9279387223778721, 0.9302227451671938, 0.9273770383090747, 0.930137816041386, 0.9300450783123798, 0.9305484846132176, 0.9294858647996683, 0.9303894841921679, 0.9293446746993815, 0.9300992672654191, 0.9290114179421513], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:01,  1.61it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:00,  2.12it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:00<00:00,  2.73it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  3.39it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  4.02it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<00:47,  5.57it/s]predicting train subjects:   1%|          | 2/266 [00:00<00:43,  6.02it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:45,  5.83it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:46,  5.61it/s]predicting train subjects:   2%|▏         | 5/266 [00:00<00:45,  5.71it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:42,  6.06it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:41,  6.32it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:39,  6.53it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:38,  6.65it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:38,  6.69it/s]predicting train subjects:   4%|▍         | 11/266 [00:01<00:37,  6.82it/s]predicting train subjects:   5%|▍         | 12/266 [00:01<00:36,  6.92it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:36,  6.87it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:36,  6.94it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:35,  7.02it/s]predicting train subjects:   6%|▌         | 16/266 [00:02<00:35,  7.01it/s]predicting train subjects:   6%|▋         | 17/266 [00:02<00:35,  7.03it/s]predicting train subjects:   7%|▋         | 18/266 [00:02<00:35,  7.01it/s]predicting train subjects:   7%|▋         | 19/266 [00:02<00:35,  6.94it/s]predicting train subjects:   8%|▊         | 20/266 [00:02<00:35,  7.00it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:35,  6.88it/s]predicting train subjects:   8%|▊         | 22/266 [00:03<00:35,  6.92it/s]predicting train subjects:   9%|▊         | 23/266 [00:03<00:35,  6.87it/s]predicting train subjects:   9%|▉         | 24/266 [00:03<00:36,  6.63it/s]predicting train subjects:   9%|▉         | 25/266 [00:03<00:35,  6.77it/s]predicting train subjects:  10%|▉         | 26/266 [00:03<00:35,  6.84it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:34,  6.93it/s]predicting train subjects:  11%|█         | 28/266 [00:04<00:33,  7.01it/s]predicting train subjects:  11%|█         | 29/266 [00:04<00:33,  7.05it/s]predicting train subjects:  11%|█▏        | 30/266 [00:04<00:33,  7.15it/s]predicting train subjects:  12%|█▏        | 31/266 [00:04<00:32,  7.19it/s]predicting train subjects:  12%|█▏        | 32/266 [00:04<00:32,  7.13it/s]predicting train subjects:  12%|█▏        | 33/266 [00:04<00:32,  7.12it/s]predicting train subjects:  13%|█▎        | 34/266 [00:05<00:32,  7.14it/s]predicting train subjects:  13%|█▎        | 35/266 [00:05<00:32,  7.11it/s]predicting train subjects:  14%|█▎        | 36/266 [00:05<00:32,  7.07it/s]predicting train subjects:  14%|█▍        | 37/266 [00:05<00:32,  7.05it/s]predicting train subjects:  14%|█▍        | 38/266 [00:05<00:32,  7.04it/s]predicting train subjects:  15%|█▍        | 39/266 [00:05<00:32,  7.08it/s]predicting train subjects:  15%|█▌        | 40/266 [00:05<00:31,  7.12it/s]predicting train subjects:  15%|█▌        | 41/266 [00:05<00:31,  7.14it/s]predicting train subjects:  16%|█▌        | 42/266 [00:06<00:29,  7.54it/s]predicting train subjects:  16%|█▌        | 43/266 [00:06<00:28,  7.83it/s]predicting train subjects:  17%|█▋        | 44/266 [00:06<00:27,  8.07it/s]predicting train subjects:  17%|█▋        | 45/266 [00:06<00:26,  8.21it/s]predicting train subjects:  17%|█▋        | 46/266 [00:06<00:26,  8.25it/s]predicting train subjects:  18%|█▊        | 47/266 [00:06<00:26,  8.30it/s]predicting train subjects:  18%|█▊        | 48/266 [00:06<00:25,  8.42it/s]predicting train subjects:  18%|█▊        | 49/266 [00:06<00:25,  8.56it/s]predicting train subjects:  19%|█▉        | 50/266 [00:07<00:24,  8.67it/s]predicting train subjects:  19%|█▉        | 51/266 [00:07<00:24,  8.68it/s]predicting train subjects:  20%|█▉        | 52/266 [00:07<00:24,  8.69it/s]predicting train subjects:  20%|█▉        | 53/266 [00:07<00:24,  8.65it/s]predicting train subjects:  20%|██        | 54/266 [00:07<00:24,  8.55it/s]predicting train subjects:  21%|██        | 55/266 [00:07<00:24,  8.59it/s]predicting train subjects:  21%|██        | 56/266 [00:07<00:24,  8.63it/s]predicting train subjects:  21%|██▏       | 57/266 [00:07<00:24,  8.66it/s]predicting train subjects:  22%|██▏       | 58/266 [00:07<00:24,  8.65it/s]predicting train subjects:  22%|██▏       | 59/266 [00:08<00:23,  8.64it/s]predicting train subjects:  23%|██▎       | 60/266 [00:08<00:23,  8.65it/s]predicting train subjects:  23%|██▎       | 61/266 [00:08<00:23,  8.67it/s]predicting train subjects:  23%|██▎       | 62/266 [00:08<00:23,  8.62it/s]predicting train subjects:  24%|██▎       | 63/266 [00:08<00:23,  8.62it/s]predicting train subjects:  24%|██▍       | 64/266 [00:08<00:23,  8.47it/s]predicting train subjects:  24%|██▍       | 65/266 [00:08<00:23,  8.55it/s]predicting train subjects:  25%|██▍       | 66/266 [00:08<00:23,  8.61it/s]predicting train subjects:  25%|██▌       | 67/266 [00:09<00:23,  8.61it/s]predicting train subjects:  26%|██▌       | 68/266 [00:09<00:23,  8.56it/s]predicting train subjects:  26%|██▌       | 69/266 [00:09<00:22,  8.57it/s]predicting train subjects:  26%|██▋       | 70/266 [00:09<00:22,  8.64it/s]predicting train subjects:  27%|██▋       | 71/266 [00:09<00:22,  8.66it/s]predicting train subjects:  27%|██▋       | 72/266 [00:09<00:22,  8.55it/s]predicting train subjects:  27%|██▋       | 73/266 [00:09<00:22,  8.57it/s]predicting train subjects:  28%|██▊       | 74/266 [00:09<00:22,  8.65it/s]predicting train subjects:  28%|██▊       | 75/266 [00:09<00:22,  8.62it/s]predicting train subjects:  29%|██▊       | 76/266 [00:10<00:22,  8.64it/s]predicting train subjects:  29%|██▉       | 77/266 [00:10<00:22,  8.53it/s]predicting train subjects:  29%|██▉       | 78/266 [00:10<00:23,  8.01it/s]predicting train subjects:  30%|██▉       | 79/266 [00:10<00:24,  7.70it/s]predicting train subjects:  30%|███       | 80/266 [00:10<00:24,  7.60it/s]predicting train subjects:  30%|███       | 81/266 [00:10<00:24,  7.40it/s]predicting train subjects:  31%|███       | 82/266 [00:10<00:25,  7.28it/s]predicting train subjects:  31%|███       | 83/266 [00:11<00:25,  7.21it/s]predicting train subjects:  32%|███▏      | 84/266 [00:11<00:25,  7.19it/s]predicting train subjects:  32%|███▏      | 85/266 [00:11<00:25,  7.16it/s]predicting train subjects:  32%|███▏      | 86/266 [00:11<00:25,  7.16it/s]predicting train subjects:  33%|███▎      | 87/266 [00:11<00:25,  7.09it/s]predicting train subjects:  33%|███▎      | 88/266 [00:11<00:25,  7.06it/s]predicting train subjects:  33%|███▎      | 89/266 [00:11<00:24,  7.11it/s]predicting train subjects:  34%|███▍      | 90/266 [00:11<00:24,  7.21it/s]predicting train subjects:  34%|███▍      | 91/266 [00:12<00:24,  7.14it/s]predicting train subjects:  35%|███▍      | 92/266 [00:12<00:24,  7.17it/s]predicting train subjects:  35%|███▍      | 93/266 [00:12<00:23,  7.23it/s]predicting train subjects:  35%|███▌      | 94/266 [00:12<00:24,  7.15it/s]predicting train subjects:  36%|███▌      | 95/266 [00:12<00:23,  7.15it/s]predicting train subjects:  36%|███▌      | 96/266 [00:12<00:27,  6.23it/s]predicting train subjects:  36%|███▋      | 97/266 [00:13<00:28,  5.96it/s]predicting train subjects:  37%|███▋      | 98/266 [00:13<00:27,  6.10it/s]predicting train subjects:  37%|███▋      | 99/266 [00:13<00:27,  6.11it/s]predicting train subjects:  38%|███▊      | 100/266 [00:13<00:24,  6.64it/s]predicting train subjects:  38%|███▊      | 101/266 [00:13<00:23,  7.09it/s]predicting train subjects:  38%|███▊      | 102/266 [00:13<00:22,  7.43it/s]predicting train subjects:  39%|███▊      | 103/266 [00:13<00:21,  7.70it/s]predicting train subjects:  39%|███▉      | 104/266 [00:14<00:20,  7.85it/s]predicting train subjects:  39%|███▉      | 105/266 [00:14<00:20,  7.95it/s]predicting train subjects:  40%|███▉      | 106/266 [00:14<00:19,  8.10it/s]predicting train subjects:  40%|████      | 107/266 [00:14<00:19,  8.24it/s]predicting train subjects:  41%|████      | 108/266 [00:14<00:19,  8.26it/s]predicting train subjects:  41%|████      | 109/266 [00:14<00:19,  8.25it/s]predicting train subjects:  41%|████▏     | 110/266 [00:14<00:18,  8.28it/s]predicting train subjects:  42%|████▏     | 111/266 [00:14<00:18,  8.31it/s]predicting train subjects:  42%|████▏     | 112/266 [00:14<00:18,  8.34it/s]predicting train subjects:  42%|████▏     | 113/266 [00:15<00:18,  8.11it/s]predicting train subjects:  43%|████▎     | 114/266 [00:15<00:18,  8.14it/s]predicting train subjects:  43%|████▎     | 115/266 [00:15<00:18,  8.16it/s]predicting train subjects:  44%|████▎     | 116/266 [00:15<00:18,  8.18it/s]predicting train subjects:  44%|████▍     | 117/266 [00:15<00:18,  8.23it/s]predicting train subjects:  44%|████▍     | 118/266 [00:15<00:18,  8.18it/s]predicting train subjects:  45%|████▍     | 119/266 [00:15<00:18,  7.79it/s]predicting train subjects:  45%|████▌     | 120/266 [00:15<00:19,  7.59it/s]predicting train subjects:  45%|████▌     | 121/266 [00:16<00:19,  7.49it/s]predicting train subjects:  46%|████▌     | 122/266 [00:16<00:19,  7.21it/s]predicting train subjects:  46%|████▌     | 123/266 [00:16<00:19,  7.23it/s]predicting train subjects:  47%|████▋     | 124/266 [00:16<00:19,  7.18it/s]predicting train subjects:  47%|████▋     | 125/266 [00:16<00:19,  7.12it/s]predicting train subjects:  47%|████▋     | 126/266 [00:16<00:19,  7.15it/s]predicting train subjects:  48%|████▊     | 127/266 [00:16<00:19,  7.17it/s]predicting train subjects:  48%|████▊     | 128/266 [00:17<00:19,  7.13it/s]predicting train subjects:  48%|████▊     | 129/266 [00:17<00:19,  7.16it/s]predicting train subjects:  49%|████▉     | 130/266 [00:17<00:18,  7.19it/s]predicting train subjects:  49%|████▉     | 131/266 [00:17<00:18,  7.17it/s]predicting train subjects:  50%|████▉     | 132/266 [00:17<00:18,  7.12it/s]predicting train subjects:  50%|█████     | 133/266 [00:17<00:18,  7.11it/s]predicting train subjects:  50%|█████     | 134/266 [00:17<00:18,  7.13it/s]predicting train subjects:  51%|█████     | 135/266 [00:18<00:18,  7.11it/s]predicting train subjects:  51%|█████     | 136/266 [00:18<00:18,  7.13it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:18<00:17,  7.30it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:18<00:17,  7.40it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:18<00:17,  7.46it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:18<00:16,  7.55it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:18<00:16,  7.62it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:19<00:16,  7.62it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:19<00:16,  7.47it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:19<00:16,  7.54it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:19<00:15,  7.58it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:19<00:15,  7.61it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:19<00:15,  7.61it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:19<00:15,  7.63it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:19<00:15,  7.68it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:20<00:15,  7.68it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:20<00:15,  7.64it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:20<00:14,  7.64it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:20<00:14,  7.64it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:20<00:14,  7.65it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:20<00:13,  8.10it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:20<00:13,  8.45it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:20<00:12,  8.73it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:21<00:12,  8.95it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:21<00:11,  9.18it/s]predicting train subjects:  60%|██████    | 160/266 [00:21<00:11,  9.31it/s]predicting train subjects:  61%|██████    | 161/266 [00:21<00:11,  9.41it/s]predicting train subjects:  61%|██████    | 162/266 [00:21<00:10,  9.47it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:21<00:10,  9.45it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:21<00:10,  9.45it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:21<00:10,  9.19it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:21<00:11,  9.07it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:21<00:10,  9.29it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:22<00:10,  9.45it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:22<00:10,  9.40it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:22<00:10,  9.50it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:22<00:09,  9.54it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:22<00:09,  9.55it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:22<00:10,  9.26it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:22<00:10,  9.06it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:22<00:10,  8.84it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:22<00:10,  8.79it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:23<00:10,  8.77it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:23<00:10,  8.74it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:23<00:10,  8.68it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:23<00:09,  8.67it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:23<00:09,  8.65it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:23<00:09,  8.71it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:23<00:09,  8.71it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:23<00:09,  8.77it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:23<00:09,  8.81it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:24<00:09,  8.77it/s]predicting train subjects:  70%|███████   | 187/266 [00:24<00:09,  8.70it/s]predicting train subjects:  71%|███████   | 188/266 [00:24<00:08,  8.68it/s]predicting train subjects:  71%|███████   | 189/266 [00:24<00:08,  8.67it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:24<00:08,  8.69it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:24<00:08,  8.57it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:24<00:10,  7.21it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:24<00:09,  7.59it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:25<00:09,  7.46it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:25<00:09,  7.71it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:25<00:08,  7.90it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:25<00:08,  7.98it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:25<00:08,  8.09it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:25<00:08,  8.25it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:25<00:07,  8.35it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:25<00:07,  8.37it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:26<00:07,  8.41it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:26<00:07,  8.47it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:26<00:07,  8.53it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:26<00:07,  8.54it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:26<00:07,  8.54it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:26<00:06,  8.60it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:26<00:06,  8.64it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:26<00:06,  8.62it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:27<00:06,  8.61it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:27<00:06,  8.57it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:27<00:06,  8.56it/s]predicting train subjects:  80%|████████  | 213/266 [00:27<00:06,  8.70it/s]predicting train subjects:  80%|████████  | 214/266 [00:27<00:06,  8.57it/s]predicting train subjects:  81%|████████  | 215/266 [00:27<00:05,  8.70it/s]predicting train subjects:  81%|████████  | 216/266 [00:27<00:05,  8.68it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:27<00:05,  8.59it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:27<00:05,  8.72it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:28<00:05,  8.59it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:28<00:05,  8.71it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:28<00:05,  8.78it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:28<00:04,  8.85it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:28<00:04,  8.80it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:28<00:04,  8.80it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:28<00:04,  8.84it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:28<00:04,  8.90it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:28<00:04,  8.96it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:29<00:04,  9.00it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:29<00:04,  8.93it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:29<00:04,  8.97it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:29<00:03,  8.90it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:29<00:03,  8.84it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:29<00:03,  8.61it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:29<00:03,  8.57it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:29<00:03,  8.54it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:29<00:03,  8.52it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:30<00:03,  8.54it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:30<00:03,  8.53it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:30<00:03,  8.53it/s]predicting train subjects:  90%|█████████ | 240/266 [00:30<00:03,  8.40it/s]predicting train subjects:  91%|█████████ | 241/266 [00:30<00:02,  8.47it/s]predicting train subjects:  91%|█████████ | 242/266 [00:30<00:02,  8.54it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:30<00:02,  8.58it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:30<00:02,  8.61it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:31<00:02,  8.57it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:31<00:02,  8.34it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:31<00:02,  8.35it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:31<00:02,  8.41it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:31<00:02,  8.06it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:31<00:02,  7.85it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:31<00:01,  7.66it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:31<00:01,  7.54it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:32<00:01,  7.46it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:32<00:01,  7.45it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:32<00:01,  7.27it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:32<00:01,  7.28it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:32<00:01,  7.31it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:32<00:01,  7.37it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:32<00:00,  7.41it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:33<00:00,  7.45it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:33<00:00,  7.44it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:33<00:00,  7.42it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:33<00:00,  7.42it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:33<00:00,  7.43it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:33<00:00,  7.45it/s]predicting train subjects: 100%|██████████| 266/266 [00:33<00:00,  7.46it/s]predicting train subjects: 100%|██████████| 266/266 [00:33<00:00,  7.86it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  8.32it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  8.46it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  8.56it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  8.43it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  8.50it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:38,  6.93it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:37,  7.06it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:35,  7.50it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:33,  7.84it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:34,  7.66it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:00<00:35,  7.42it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:00<00:35,  7.31it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:35,  7.25it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:35,  7.18it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:35,  7.12it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:35,  7.10it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:01<00:35,  7.10it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:01<00:35,  7.10it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:01<00:35,  7.08it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:36,  6.91it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:35,  6.98it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:02<00:35,  7.04it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:02<00:35,  6.90it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:02<00:35,  6.89it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:02<00:35,  6.93it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:02<00:35,  6.98it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:34,  7.02it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:03<00:34,  7.05it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:03<00:34,  7.09it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:03<00:33,  7.14it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:03<00:33,  7.21it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:03<00:32,  7.26it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:03<00:32,  7.32it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:04<00:32,  7.29it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:04<00:32,  7.25it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:04<00:32,  7.25it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:04<00:32,  7.26it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:04<00:31,  7.29it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:04<00:31,  7.29it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:04<00:31,  7.30it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:05<00:31,  7.27it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:05<00:32,  7.09it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:05<00:31,  7.16it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:05<00:32,  7.08it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:05<00:31,  7.22it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:05<00:31,  7.17it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:05<00:29,  7.62it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:05<00:28,  7.81it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:06<00:27,  8.11it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:06<00:26,  8.36it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:06<00:25,  8.53it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:06<00:25,  8.66it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:06<00:24,  8.75it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:06<00:24,  8.80it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:06<00:24,  8.85it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:06<00:24,  8.89it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:06<00:24,  8.82it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:07<00:24,  8.80it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:07<00:24,  8.80it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:07<00:23,  8.80it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:07<00:23,  8.77it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:07<00:24,  8.69it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:07<00:23,  8.71it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:07<00:23,  8.72it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:07<00:23,  8.69it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:07<00:23,  8.70it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:08<00:23,  8.69it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:08<00:23,  8.71it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:08<00:23,  8.70it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:08<00:23,  8.72it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:08<00:22,  8.74it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:08<00:22,  8.78it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:08<00:22,  8.80it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:08<00:22,  8.71it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:09<00:22,  8.71it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:09<00:23,  8.40it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:09<00:22,  8.60it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:09<00:22,  8.65it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:09<00:22,  8.71it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:09<00:21,  8.75it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:09<00:21,  8.77it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:09<00:21,  8.79it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:09<00:22,  8.21it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:10<00:23,  7.87it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:10<00:24,  7.67it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:10<00:24,  7.54it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:10<00:24,  7.46it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:10<00:24,  7.40it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:10<00:24,  7.34it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:10<00:24,  7.34it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:11<00:24,  7.38it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:11<00:24,  7.43it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:11<00:23,  7.44it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:11<00:23,  7.45it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:11<00:23,  7.44it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:11<00:23,  7.39it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:11<00:23,  7.38it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:11<00:23,  7.39it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:12<00:23,  7.38it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:12<00:23,  7.36it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:12<00:22,  7.60it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:12<00:22,  7.44it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:12<00:22,  7.52it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:12<00:20,  8.06it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:12<00:20,  8.21it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:13<00:20,  8.22it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:13<00:19,  8.28it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:13<00:19,  8.25it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:13<00:19,  8.16it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:13<00:19,  8.16it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:13<00:19,  8.20it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:13<00:19,  8.26it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:13<00:19,  8.29it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:13<00:18,  8.30it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:14<00:18,  8.37it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:14<00:18,  8.41it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:14<00:18,  8.41it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:14<00:18,  8.43it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:14<00:18,  8.31it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:14<00:18,  8.34it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:14<00:18,  8.31it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:14<00:18,  8.04it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:15<00:18,  8.05it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:15<00:19,  7.67it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:15<00:19,  7.38it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:15<00:20,  7.19it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:15<00:20,  6.95it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:15<00:20,  7.00it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:15<00:20,  6.99it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:16<00:20,  7.01it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:16<00:20,  6.94it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:16<00:20,  6.92it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:16<00:20,  6.81it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:16<00:19,  6.89it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:16<00:19,  6.93it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:16<00:19,  6.98it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:17<00:20,  6.66it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:17<00:20,  6.60it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:17<00:19,  6.65it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:17<00:19,  6.74it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:17<00:19,  6.58it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:17<00:18,  6.83it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:17<00:18,  7.02it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:18<00:17,  7.16it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:18<00:17,  7.08it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:18<00:17,  7.02it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:18<00:18,  6.85it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:18<00:17,  7.02it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:18<00:17,  7.16it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:18<00:16,  7.18it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:19<00:16,  7.31it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:19<00:16,  7.40it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:19<00:15,  7.42it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:19<00:15,  7.34it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:19<00:15,  7.40it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:19<00:15,  7.43it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:19<00:15,  7.38it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:20<00:15,  7.39it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:20<00:15,  7.41it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:20<00:14,  7.90it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:20<00:13,  8.26it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:20<00:12,  8.55it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:20<00:12,  8.82it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:20<00:11,  8.99it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:20<00:11,  9.15it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:20<00:11,  9.08it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:21<00:11,  9.08it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:21<00:11,  9.19it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:21<00:11,  9.26it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:21<00:10,  9.29it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:21<00:10,  9.29it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:21<00:10,  9.27it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:21<00:10,  9.28it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:21<00:10,  9.30it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:21<00:10,  9.26it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:22<00:10,  9.31it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:22<00:10,  9.32it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:22<00:10,  9.01it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:22<00:10,  8.82it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:22<00:10,  8.70it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:22<00:10,  8.58it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:22<00:10,  8.50it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:22<00:10,  8.42it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:22<00:10,  8.40it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:23<00:10,  8.43it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:23<00:10,  8.44it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:23<00:09,  8.43it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:23<00:09,  8.44it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:23<00:09,  8.42it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:23<00:09,  8.44it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:23<00:09,  8.45it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:23<00:09,  8.41it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:24<00:09,  8.41it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:24<00:09,  8.43it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:24<00:09,  8.42it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:24<00:08,  8.39it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:24<00:08,  8.51it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:24<00:08,  8.42it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:24<00:08,  8.06it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:24<00:08,  8.15it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:24<00:08,  8.16it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:25<00:08,  8.12it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:25<00:08,  8.19it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:25<00:08,  8.26it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:25<00:07,  8.27it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:25<00:07,  8.25it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:25<00:07,  8.31it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:25<00:07,  8.36it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:25<00:07,  8.34it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:26<00:07,  8.35it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:26<00:07,  8.38it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:26<00:07,  8.42it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:26<00:06,  8.29it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:26<00:06,  8.33it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:26<00:06,  8.35it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:26<00:06,  8.38it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:26<00:06,  8.31it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:27<00:06,  8.44it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:27<00:06,  8.55it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:27<00:05,  8.59it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:27<00:05,  8.67it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:27<00:05,  8.63it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:27<00:05,  8.66it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:27<00:05,  8.64it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:27<00:05,  8.59it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:27<00:05,  8.69it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:28<00:05,  8.73it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:28<00:04,  8.76it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:28<00:04,  8.79it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:28<00:04,  8.80it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:28<00:04,  8.72it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:28<00:04,  8.73it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:28<00:04,  8.71it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:28<00:04,  8.73it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:28<00:04,  8.72it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:29<00:04,  8.59it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:29<00:03,  8.58it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:29<00:03,  8.60it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:29<00:03,  8.59it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:29<00:03,  8.58it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:29<00:03,  8.54it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:29<00:03,  8.46it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:29<00:03,  8.47it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:30<00:03,  8.50it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:30<00:03,  8.45it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:30<00:02,  8.48it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:30<00:02,  8.44it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:30<00:02,  8.42it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:30<00:02,  8.49it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:30<00:02,  8.53it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:30<00:02,  8.56it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:30<00:02,  8.55it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:31<00:02,  8.56it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:31<00:02,  8.15it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:31<00:02,  7.91it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:31<00:01,  7.66it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:31<00:01,  7.45it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:31<00:01,  7.46it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:31<00:01,  7.46it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:32<00:01,  7.43it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:32<00:01,  7.10it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:32<00:01,  6.90it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:32<00:01,  7.01it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:32<00:00,  7.04it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:32<00:00,  7.08it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:32<00:00,  7.15it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:33<00:00,  7.21it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:33<00:00,  6.97it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:33<00:00,  7.05it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:33<00:00,  7.10it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:33<00:00,  7.12it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:33<00:00,  7.91it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 77.73it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 69.70it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 67.80it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 66.99it/s]saving BB  train1-THALAMUS:  11%|█▏        | 30/266 [00:00<00:03, 68.77it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 70.42it/s]saving BB  train1-THALAMUS:  17%|█▋        | 46/266 [00:00<00:03, 72.27it/s]saving BB  train1-THALAMUS:  20%|██        | 54/266 [00:00<00:02, 73.26it/s]saving BB  train1-THALAMUS:  24%|██▎       | 63/266 [00:00<00:02, 75.94it/s]saving BB  train1-THALAMUS:  27%|██▋       | 72/266 [00:00<00:02, 79.34it/s]saving BB  train1-THALAMUS:  30%|███       | 81/266 [00:01<00:02, 79.68it/s]saving BB  train1-THALAMUS:  33%|███▎      | 89/266 [00:01<00:02, 77.54it/s]saving BB  train1-THALAMUS:  36%|███▋      | 97/266 [00:01<00:02, 76.28it/s]saving BB  train1-THALAMUS:  39%|███▉      | 105/266 [00:01<00:02, 77.13it/s]saving BB  train1-THALAMUS:  42%|████▏     | 113/266 [00:01<00:01, 77.23it/s]saving BB  train1-THALAMUS:  45%|████▌     | 121/266 [00:01<00:01, 76.53it/s]saving BB  train1-THALAMUS:  48%|████▊     | 129/266 [00:01<00:01, 76.06it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 137/266 [00:01<00:01, 74.81it/s]saving BB  train1-THALAMUS:  55%|█████▍    | 145/266 [00:01<00:01, 74.47it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 153/266 [00:02<00:01, 73.10it/s]saving BB  train1-THALAMUS:  61%|██████    | 162/266 [00:02<00:01, 76.43it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 171/266 [00:02<00:01, 78.62it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 180/266 [00:02<00:01, 80.75it/s]saving BB  train1-THALAMUS:  71%|███████   | 189/266 [00:02<00:00, 82.47it/s]saving BB  train1-THALAMUS:  74%|███████▍  | 198/266 [00:02<00:00, 82.26it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 207/266 [00:02<00:00, 80.84it/s]saving BB  train1-THALAMUS:  81%|████████  | 216/266 [00:02<00:00, 80.76it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 225/266 [00:02<00:00, 81.34it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 234/266 [00:03<00:00, 81.24it/s]saving BB  train1-THALAMUS:  91%|█████████▏| 243/266 [00:03<00:00, 81.95it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 252/266 [00:03<00:00, 81.84it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 261/266 [00:03<00:00, 79.54it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 77.41it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 61.02it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 8/266 [00:00<00:03, 72.74it/s]saving BB  train1-THALAMUS Sagittal:   6%|▌         | 15/266 [00:00<00:03, 70.79it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 22/266 [00:00<00:03, 70.25it/s]saving BB  train1-THALAMUS Sagittal:  11%|█▏        | 30/266 [00:00<00:03, 70.94it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 38/266 [00:00<00:03, 72.01it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 46/266 [00:00<00:02, 73.50it/s]saving BB  train1-THALAMUS Sagittal:  21%|██        | 55/266 [00:00<00:02, 76.04it/s]saving BB  train1-THALAMUS Sagittal:  24%|██▍       | 64/266 [00:00<00:02, 79.32it/s]saving BB  train1-THALAMUS Sagittal:  27%|██▋       | 73/266 [00:00<00:02, 80.71it/s]saving BB  train1-THALAMUS Sagittal:  30%|███       | 81/266 [00:01<00:02, 79.16it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 89/266 [00:01<00:02, 77.42it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▋      | 97/266 [00:01<00:02, 75.51it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▉      | 105/266 [00:01<00:02, 75.95it/s]saving BB  train1-THALAMUS Sagittal:  42%|████▏     | 113/266 [00:01<00:02, 76.15it/s]saving BB  train1-THALAMUS Sagittal:  45%|████▌     | 121/266 [00:01<00:01, 76.09it/s]saving BB  train1-THALAMUS Sagittal:  48%|████▊     | 129/266 [00:01<00:01, 75.00it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 137/266 [00:01<00:01, 74.39it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▍    | 145/266 [00:01<00:01, 73.53it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 153/266 [00:02<00:01, 73.87it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████    | 162/266 [00:02<00:01, 77.59it/s]saving BB  train1-THALAMUS Sagittal:  64%|██████▍   | 171/266 [00:02<00:01, 80.72it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 180/266 [00:02<00:01, 82.78it/s]saving BB  train1-THALAMUS Sagittal:  71%|███████   | 189/266 [00:02<00:00, 84.26it/s]saving BB  train1-THALAMUS Sagittal:  74%|███████▍  | 198/266 [00:02<00:00, 83.35it/s]saving BB  train1-THALAMUS Sagittal:  78%|███████▊  | 207/266 [00:02<00:00, 82.96it/s]saving BB  train1-THALAMUS Sagittal:  81%|████████  | 216/266 [00:02<00:00, 78.77it/s]saving BB  train1-THALAMUS Sagittal:  84%|████████▍ | 224/266 [00:02<00:00, 75.04it/s]saving BB  train1-THALAMUS Sagittal:  88%|████████▊ | 233/266 [00:03<00:00, 77.06it/s]saving BB  train1-THALAMUS Sagittal:  91%|█████████ | 241/266 [00:03<00:00, 76.45it/s]saving BB  train1-THALAMUS Sagittal:  94%|█████████▍| 250/266 [00:03<00:00, 77.68it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 258/266 [00:03<00:00, 75.33it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 75.13it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 76.94it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:49,  1.09s/it]Loading train:   1%|          | 2/266 [00:02<04:37,  1.05s/it]Loading train:   1%|          | 3/266 [00:02<04:16,  1.02it/s]Loading train:   2%|▏         | 4/266 [00:03<04:01,  1.09it/s]Loading train:   2%|▏         | 5/266 [00:04<04:04,  1.07it/s]Loading train:   2%|▏         | 6/266 [00:05<03:42,  1.17it/s]Loading train:   3%|▎         | 7/266 [00:05<03:23,  1.27it/s]Loading train:   3%|▎         | 8/266 [00:06<03:07,  1.38it/s]Loading train:   3%|▎         | 9/266 [00:07<02:58,  1.44it/s]Loading train:   4%|▍         | 10/266 [00:07<02:50,  1.50it/s]Loading train:   4%|▍         | 11/266 [00:08<02:45,  1.54it/s]Loading train:   5%|▍         | 12/266 [00:08<02:44,  1.54it/s]Loading train:   5%|▍         | 13/266 [00:09<02:43,  1.55it/s]Loading train:   5%|▌         | 14/266 [00:10<02:39,  1.58it/s]Loading train:   6%|▌         | 15/266 [00:10<02:38,  1.58it/s]Loading train:   6%|▌         | 16/266 [00:11<02:31,  1.65it/s]Loading train:   6%|▋         | 17/266 [00:11<02:29,  1.66it/s]Loading train:   7%|▋         | 18/266 [00:12<02:29,  1.66it/s]Loading train:   7%|▋         | 19/266 [00:13<02:28,  1.67it/s]Loading train:   8%|▊         | 20/266 [00:13<02:29,  1.65it/s]Loading train:   8%|▊         | 21/266 [00:14<02:29,  1.64it/s]Loading train:   8%|▊         | 22/266 [00:15<02:31,  1.61it/s]Loading train:   9%|▊         | 23/266 [00:15<02:30,  1.62it/s]Loading train:   9%|▉         | 24/266 [00:16<02:28,  1.63it/s]Loading train:   9%|▉         | 25/266 [00:16<02:23,  1.68it/s]Loading train:  10%|▉         | 26/266 [00:17<02:19,  1.72it/s]Loading train:  10%|█         | 27/266 [00:17<02:20,  1.71it/s]Loading train:  11%|█         | 28/266 [00:18<02:17,  1.73it/s]Loading train:  11%|█         | 29/266 [00:19<02:15,  1.75it/s]Loading train:  11%|█▏        | 30/266 [00:19<02:13,  1.76it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:12,  1.78it/s]Loading train:  12%|█▏        | 32/266 [00:20<02:10,  1.79it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:11,  1.77it/s]Loading train:  13%|█▎        | 34/266 [00:21<02:13,  1.73it/s]Loading train:  13%|█▎        | 35/266 [00:22<02:13,  1.73it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:12,  1.74it/s]Loading train:  14%|█▍        | 37/266 [00:23<02:10,  1.75it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:10,  1.75it/s]Loading train:  15%|█▍        | 39/266 [00:24<02:08,  1.77it/s]Loading train:  15%|█▌        | 40/266 [00:25<02:07,  1.78it/s]Loading train:  15%|█▌        | 41/266 [00:25<02:04,  1.81it/s]Loading train:  16%|█▌        | 42/266 [00:26<02:01,  1.85it/s]Loading train:  16%|█▌        | 43/266 [00:26<01:57,  1.90it/s]Loading train:  17%|█▋        | 44/266 [00:27<01:54,  1.94it/s]Loading train:  17%|█▋        | 45/266 [00:27<01:52,  1.96it/s]Loading train:  17%|█▋        | 46/266 [00:28<01:51,  1.98it/s]Loading train:  18%|█▊        | 47/266 [00:28<01:51,  1.96it/s]Loading train:  18%|█▊        | 48/266 [00:29<01:50,  1.97it/s]Loading train:  18%|█▊        | 49/266 [00:29<01:50,  1.96it/s]Loading train:  19%|█▉        | 50/266 [00:30<01:49,  1.98it/s]Loading train:  19%|█▉        | 51/266 [00:30<01:47,  2.00it/s]Loading train:  20%|█▉        | 52/266 [00:31<01:46,  2.01it/s]Loading train:  20%|█▉        | 53/266 [00:31<01:46,  2.00it/s]Loading train:  20%|██        | 54/266 [00:32<01:46,  2.00it/s]Loading train:  21%|██        | 55/266 [00:32<01:45,  2.00it/s]Loading train:  21%|██        | 56/266 [00:33<01:45,  1.99it/s]Loading train:  21%|██▏       | 57/266 [00:33<01:45,  1.98it/s]Loading train:  22%|██▏       | 58/266 [00:34<01:46,  1.95it/s]Loading train:  22%|██▏       | 59/266 [00:34<01:45,  1.96it/s]Loading train:  23%|██▎       | 60/266 [00:35<01:47,  1.91it/s]Loading train:  23%|██▎       | 61/266 [00:35<01:43,  1.97it/s]Loading train:  23%|██▎       | 62/266 [00:36<01:44,  1.96it/s]Loading train:  24%|██▎       | 63/266 [00:36<01:41,  2.00it/s]Loading train:  24%|██▍       | 64/266 [00:37<01:39,  2.02it/s]Loading train:  24%|██▍       | 65/266 [00:37<01:38,  2.03it/s]Loading train:  25%|██▍       | 66/266 [00:38<01:36,  2.07it/s]Loading train:  25%|██▌       | 67/266 [00:38<01:35,  2.08it/s]Loading train:  26%|██▌       | 68/266 [00:39<01:35,  2.08it/s]Loading train:  26%|██▌       | 69/266 [00:39<01:34,  2.08it/s]Loading train:  26%|██▋       | 70/266 [00:40<01:35,  2.06it/s]Loading train:  27%|██▋       | 71/266 [00:40<01:34,  2.07it/s]Loading train:  27%|██▋       | 72/266 [00:41<01:34,  2.05it/s]Loading train:  27%|██▋       | 73/266 [00:41<01:33,  2.07it/s]Loading train:  28%|██▊       | 74/266 [00:42<01:33,  2.05it/s]Loading train:  28%|██▊       | 75/266 [00:42<01:32,  2.06it/s]Loading train:  29%|██▊       | 76/266 [00:43<01:32,  2.06it/s]Loading train:  29%|██▉       | 77/266 [00:43<01:29,  2.10it/s]Loading train:  29%|██▉       | 78/266 [00:44<01:36,  1.95it/s]Loading train:  30%|██▉       | 79/266 [00:44<01:38,  1.90it/s]Loading train:  30%|███       | 80/266 [00:45<01:40,  1.86it/s]Loading train:  30%|███       | 81/266 [00:45<01:41,  1.83it/s]Loading train:  31%|███       | 82/266 [00:46<01:45,  1.75it/s]Loading train:  31%|███       | 83/266 [00:47<01:46,  1.73it/s]Loading train:  32%|███▏      | 84/266 [00:47<01:46,  1.70it/s]Loading train:  32%|███▏      | 85/266 [00:48<01:46,  1.70it/s]Loading train:  32%|███▏      | 86/266 [00:48<01:45,  1.70it/s]Loading train:  33%|███▎      | 87/266 [00:49<01:44,  1.71it/s]Loading train:  33%|███▎      | 88/266 [00:50<01:43,  1.73it/s]Loading train:  33%|███▎      | 89/266 [00:50<01:41,  1.75it/s]Loading train:  34%|███▍      | 90/266 [00:51<01:40,  1.75it/s]Loading train:  34%|███▍      | 91/266 [00:51<01:41,  1.73it/s]Loading train:  35%|███▍      | 92/266 [00:52<01:41,  1.71it/s]Loading train:  35%|███▍      | 93/266 [00:52<01:41,  1.71it/s]Loading train:  35%|███▌      | 94/266 [00:53<01:42,  1.69it/s]Loading train:  36%|███▌      | 95/266 [00:54<01:41,  1.68it/s]Loading train:  36%|███▌      | 96/266 [00:55<01:51,  1.53it/s]Loading train:  36%|███▋      | 97/266 [00:55<02:04,  1.35it/s]Loading train:  37%|███▋      | 98/266 [00:56<02:11,  1.28it/s]Loading train:  37%|███▋      | 99/266 [00:57<02:05,  1.33it/s]Loading train:  38%|███▊      | 100/266 [00:58<02:05,  1.33it/s]Loading train:  38%|███▊      | 101/266 [00:58<01:55,  1.43it/s]Loading train:  38%|███▊      | 102/266 [00:59<01:46,  1.55it/s]Loading train:  39%|███▊      | 103/266 [00:59<01:41,  1.61it/s]Loading train:  39%|███▉      | 104/266 [01:00<01:34,  1.71it/s]Loading train:  39%|███▉      | 105/266 [01:00<01:30,  1.78it/s]Loading train:  40%|███▉      | 106/266 [01:01<01:27,  1.84it/s]Loading train:  40%|████      | 107/266 [01:01<01:24,  1.88it/s]Loading train:  41%|████      | 108/266 [01:02<01:23,  1.89it/s]Loading train:  41%|████      | 109/266 [01:02<01:23,  1.88it/s]Loading train:  41%|████▏     | 110/266 [01:03<01:22,  1.88it/s]Loading train:  42%|████▏     | 111/266 [01:04<01:23,  1.86it/s]Loading train:  42%|████▏     | 112/266 [01:04<01:22,  1.86it/s]Loading train:  42%|████▏     | 113/266 [01:05<01:21,  1.87it/s]Loading train:  43%|████▎     | 114/266 [01:05<01:20,  1.90it/s]Loading train:  43%|████▎     | 115/266 [01:06<01:21,  1.86it/s]Loading train:  44%|████▎     | 116/266 [01:06<01:20,  1.86it/s]Loading train:  44%|████▍     | 117/266 [01:07<01:19,  1.88it/s]Loading train:  44%|████▍     | 118/266 [01:07<01:18,  1.87it/s]Loading train:  45%|████▍     | 119/266 [01:08<01:21,  1.80it/s]Loading train:  45%|████▌     | 120/266 [01:08<01:21,  1.79it/s]Loading train:  45%|████▌     | 121/266 [01:09<01:22,  1.75it/s]Loading train:  46%|████▌     | 122/266 [01:10<01:23,  1.72it/s]Loading train:  46%|████▌     | 123/266 [01:10<01:24,  1.70it/s]Loading train:  47%|████▋     | 124/266 [01:11<01:23,  1.69it/s]Loading train:  47%|████▋     | 125/266 [01:11<01:22,  1.70it/s]Loading train:  47%|████▋     | 126/266 [01:12<01:22,  1.70it/s]Loading train:  48%|████▊     | 127/266 [01:13<01:23,  1.67it/s]Loading train:  48%|████▊     | 128/266 [01:13<01:22,  1.68it/s]Loading train:  48%|████▊     | 129/266 [01:14<01:20,  1.71it/s]Loading train:  49%|████▉     | 130/266 [01:14<01:19,  1.72it/s]Loading train:  49%|████▉     | 131/266 [01:15<01:18,  1.71it/s]Loading train:  50%|████▉     | 132/266 [01:16<01:17,  1.72it/s]Loading train:  50%|█████     | 133/266 [01:16<01:17,  1.72it/s]Loading train:  50%|█████     | 134/266 [01:17<01:17,  1.71it/s]Loading train:  51%|█████     | 135/266 [01:17<01:16,  1.71it/s]Loading train:  51%|█████     | 136/266 [01:18<01:16,  1.69it/s]Loading train:  52%|█████▏    | 137/266 [01:19<01:16,  1.69it/s]Loading train:  52%|█████▏    | 138/266 [01:19<01:14,  1.72it/s]Loading train:  52%|█████▏    | 139/266 [01:20<01:13,  1.72it/s]Loading train:  53%|█████▎    | 140/266 [01:20<01:11,  1.75it/s]Loading train:  53%|█████▎    | 141/266 [01:21<01:09,  1.79it/s]Loading train:  53%|█████▎    | 142/266 [01:21<01:08,  1.82it/s]Loading train:  54%|█████▍    | 143/266 [01:22<01:12,  1.69it/s]Loading train:  54%|█████▍    | 144/266 [01:24<02:01,  1.01it/s]Loading train:  55%|█████▍    | 145/266 [01:26<02:56,  1.46s/it]Loading train:  55%|█████▍    | 146/266 [01:31<05:00,  2.50s/it]Loading train:  55%|█████▌    | 147/266 [01:41<09:25,  4.75s/it]Loading train:  56%|█████▌    | 148/266 [01:53<13:36,  6.92s/it]Loading train:  56%|█████▌    | 149/266 [02:04<15:23,  7.90s/it]Loading train:  56%|█████▋    | 150/266 [02:14<17:00,  8.80s/it]Loading train:  57%|█████▋    | 151/266 [02:25<17:50,  9.30s/it]Loading train:  57%|█████▋    | 152/266 [02:34<17:25,  9.17s/it]Loading train:  58%|█████▊    | 153/266 [02:43<17:14,  9.16s/it]Loading train:  58%|█████▊    | 154/266 [02:52<16:55,  9.07s/it]Loading train:  58%|█████▊    | 155/266 [02:52<12:01,  6.50s/it]Loading train:  59%|█████▊    | 156/266 [02:53<08:35,  4.68s/it]Loading train:  59%|█████▉    | 157/266 [02:53<06:12,  3.42s/it]Loading train:  59%|█████▉    | 158/266 [02:54<04:33,  2.53s/it]Loading train:  60%|█████▉    | 159/266 [02:54<03:24,  1.91s/it]Loading train:  60%|██████    | 160/266 [02:58<04:21,  2.47s/it]Loading train:  61%|██████    | 161/266 [03:00<04:21,  2.49s/it]Loading train:  61%|██████    | 162/266 [03:03<04:09,  2.40s/it]Loading train:  61%|██████▏   | 163/266 [03:05<04:00,  2.34s/it]Loading train:  62%|██████▏   | 164/266 [03:07<03:44,  2.20s/it]Loading train:  62%|██████▏   | 165/266 [03:09<03:55,  2.33s/it]Loading train:  62%|██████▏   | 166/266 [03:11<03:48,  2.28s/it]Loading train:  63%|██████▎   | 167/266 [03:14<03:40,  2.23s/it]Loading train:  63%|██████▎   | 168/266 [03:16<03:39,  2.24s/it]Loading train:  64%|██████▎   | 169/266 [03:18<03:39,  2.26s/it]Loading train:  64%|██████▍   | 170/266 [03:20<03:34,  2.23s/it]Loading train:  64%|██████▍   | 171/266 [03:22<03:28,  2.19s/it]Loading train:  65%|██████▍   | 172/266 [03:24<03:21,  2.15s/it]Loading train:  65%|██████▌   | 173/266 [03:32<05:43,  3.69s/it]Loading train:  65%|██████▌   | 174/266 [03:42<08:44,  5.70s/it]Loading train:  66%|██████▌   | 175/266 [03:50<09:38,  6.36s/it]Loading train:  66%|██████▌   | 176/266 [03:55<09:05,  6.06s/it]Loading train:  67%|██████▋   | 177/266 [04:04<10:06,  6.81s/it]Loading train:  67%|██████▋   | 178/266 [04:13<10:48,  7.37s/it]Loading train:  67%|██████▋   | 179/266 [04:19<10:24,  7.18s/it]Loading train:  68%|██████▊   | 180/266 [04:28<10:55,  7.62s/it]Loading train:  68%|██████▊   | 181/266 [04:37<11:14,  7.94s/it]Loading train:  68%|██████▊   | 182/266 [04:44<10:54,  7.79s/it]Loading train:  69%|██████▉   | 183/266 [04:52<10:50,  7.84s/it]Loading train:  69%|██████▉   | 184/266 [04:58<09:53,  7.23s/it]Loading train:  70%|██████▉   | 185/266 [05:05<09:50,  7.29s/it]Loading train:  70%|██████▉   | 186/266 [05:12<09:22,  7.03s/it]Loading train:  70%|███████   | 187/266 [05:20<09:41,  7.35s/it]Loading train:  71%|███████   | 188/266 [05:28<09:54,  7.63s/it]Loading train:  71%|███████   | 189/266 [05:36<10:01,  7.81s/it]Loading train:  71%|███████▏  | 190/266 [05:46<10:27,  8.26s/it]Loading train:  72%|███████▏  | 191/266 [06:12<17:03, 13.65s/it]Loading train:  72%|███████▏  | 192/266 [06:25<16:46, 13.61s/it]Loading train:  73%|███████▎  | 193/266 [06:42<17:42, 14.56s/it]Loading train:  73%|███████▎  | 194/266 [07:16<24:24, 20.35s/it]Loading train:  73%|███████▎  | 195/266 [07:22<19:00, 16.06s/it]Loading train:  74%|███████▎  | 196/266 [07:29<15:31, 13.31s/it]Loading train:  74%|███████▍  | 197/266 [07:37<13:35, 11.82s/it]Loading train:  74%|███████▍  | 198/266 [07:46<12:10, 10.75s/it]Loading train:  75%|███████▍  | 199/266 [07:55<11:36, 10.40s/it]Loading train:  75%|███████▌  | 200/266 [08:05<11:07, 10.12s/it]Loading train:  76%|███████▌  | 201/266 [08:14<10:40,  9.85s/it]Loading train:  76%|███████▌  | 202/266 [08:25<10:54, 10.23s/it]Loading train:  76%|███████▋  | 203/266 [08:31<09:31,  9.07s/it]Loading train:  77%|███████▋  | 204/266 [08:40<09:07,  8.83s/it]Loading train:  77%|███████▋  | 205/266 [08:47<08:29,  8.36s/it]Loading train:  77%|███████▋  | 206/266 [08:55<08:10,  8.17s/it]Loading train:  78%|███████▊  | 207/266 [08:59<07:00,  7.13s/it]Loading train:  78%|███████▊  | 208/266 [09:04<06:07,  6.33s/it]Loading train:  79%|███████▊  | 209/266 [09:08<05:31,  5.81s/it]Loading train:  79%|███████▉  | 210/266 [09:14<05:19,  5.71s/it]Loading train:  79%|███████▉  | 211/266 [09:20<05:16,  5.76s/it]Loading train:  80%|███████▉  | 212/266 [09:26<05:20,  5.93s/it]Loading train:  80%|████████  | 213/266 [09:29<04:21,  4.93s/it]Loading train:  80%|████████  | 214/266 [09:30<03:22,  3.89s/it]Loading train:  81%|████████  | 215/266 [09:31<02:36,  3.07s/it]Loading train:  81%|████████  | 216/266 [09:33<02:08,  2.57s/it]Loading train:  82%|████████▏ | 217/266 [09:34<01:52,  2.30s/it]Loading train:  82%|████████▏ | 218/266 [09:36<01:36,  2.01s/it]Loading train:  82%|████████▏ | 219/266 [09:37<01:25,  1.82s/it]Loading train:  83%|████████▎ | 220/266 [09:39<01:19,  1.73s/it]Loading train:  83%|████████▎ | 221/266 [09:40<01:12,  1.61s/it]Loading train:  83%|████████▎ | 222/266 [09:41<01:03,  1.43s/it]Loading train:  84%|████████▍ | 223/266 [09:42<00:53,  1.25s/it]Loading train:  84%|████████▍ | 224/266 [09:43<00:52,  1.24s/it]Loading train:  85%|████████▍ | 225/266 [09:44<00:51,  1.26s/it]Loading train:  85%|████████▍ | 226/266 [09:46<00:55,  1.39s/it]Loading train:  85%|████████▌ | 227/266 [09:47<00:53,  1.37s/it]Loading train:  86%|████████▌ | 228/266 [09:49<00:50,  1.33s/it]Loading train:  86%|████████▌ | 229/266 [09:50<00:50,  1.37s/it]Loading train:  86%|████████▋ | 230/266 [09:51<00:49,  1.38s/it]Loading train:  87%|████████▋ | 231/266 [09:54<01:05,  1.87s/it]Loading train:  87%|████████▋ | 232/266 [09:59<01:33,  2.75s/it]Loading train:  88%|████████▊ | 233/266 [10:04<01:50,  3.36s/it]Loading train:  88%|████████▊ | 234/266 [10:09<02:04,  3.90s/it]Loading train:  88%|████████▊ | 235/266 [10:14<02:13,  4.30s/it]Loading train:  89%|████████▊ | 236/266 [10:20<02:22,  4.75s/it]Loading train:  89%|████████▉ | 237/266 [10:25<02:21,  4.89s/it]Loading train:  89%|████████▉ | 238/266 [10:31<02:25,  5.21s/it]Loading train:  90%|████████▉ | 239/266 [10:36<02:17,  5.08s/it]Loading train:  90%|█████████ | 240/266 [10:42<02:20,  5.40s/it]Loading train:  91%|█████████ | 241/266 [10:48<02:14,  5.37s/it]Loading train:  91%|█████████ | 242/266 [10:54<02:14,  5.61s/it]Loading train:  91%|█████████▏| 243/266 [10:58<02:00,  5.25s/it]Loading train:  92%|█████████▏| 244/266 [11:04<01:58,  5.40s/it]Loading train:  92%|█████████▏| 245/266 [11:09<01:49,  5.21s/it]Loading train:  92%|█████████▏| 246/266 [11:15<01:53,  5.69s/it]Loading train:  93%|█████████▎| 247/266 [11:21<01:45,  5.54s/it]Loading train:  93%|█████████▎| 248/266 [11:27<01:41,  5.65s/it]Loading train:  94%|█████████▎| 249/266 [11:35<01:52,  6.60s/it]Loading train:  94%|█████████▍| 250/266 [11:44<01:55,  7.22s/it]Loading train:  94%|█████████▍| 251/266 [11:52<01:52,  7.53s/it]Loading train:  95%|█████████▍| 252/266 [12:04<02:04,  8.86s/it]Loading train:  95%|█████████▌| 253/266 [12:15<02:01,  9.37s/it]Loading train:  95%|█████████▌| 254/266 [12:26<01:59,  9.96s/it]Loading train:  96%|█████████▌| 255/266 [12:39<01:59, 10.89s/it]Loading train:  96%|█████████▌| 256/266 [12:53<01:57, 11.74s/it]Loading train:  97%|█████████▋| 257/266 [13:07<01:51, 12.39s/it]Loading train:  97%|█████████▋| 258/266 [13:20<01:39, 12.50s/it]Loading train:  97%|█████████▋| 259/266 [13:31<01:25, 12.25s/it]Loading train:  98%|█████████▊| 260/266 [13:46<01:17, 12.89s/it]Loading train:  98%|█████████▊| 261/266 [13:58<01:03, 12.75s/it]Loading train:  98%|█████████▊| 262/266 [14:11<00:51, 12.90s/it]Loading train:  99%|█████████▉| 263/266 [14:26<00:39, 13.31s/it]Loading train:  99%|█████████▉| 264/266 [14:37<00:25, 12.84s/it]Loading train: 100%|█████████▉| 265/266 [14:49<00:12, 12.41s/it]Loading train: 100%|██████████| 266/266 [14:59<00:00, 11.86s/it]Loading train: 100%|██████████| 266/266 [14:59<00:00,  3.38s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 57.49it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 55.40it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 53.54it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:04, 51.20it/s]concatenating: train:  11%|█         | 28/266 [00:00<00:04, 50.81it/s]concatenating: train:  13%|█▎        | 34/266 [00:00<00:04, 51.03it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 52.27it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:04, 53.05it/s]concatenating: train:  20%|█▉        | 52/266 [00:00<00:03, 54.09it/s]concatenating: train:  22%|██▏       | 58/266 [00:01<00:03, 53.96it/s]concatenating: train:  24%|██▍       | 64/266 [00:01<00:03, 54.65it/s]concatenating: train:  26%|██▋       | 70/266 [00:01<00:03, 55.05it/s]concatenating: train:  29%|██▊       | 76/266 [00:01<00:03, 56.08it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:03, 54.56it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:03, 53.90it/s]concatenating: train:  35%|███▌      | 94/266 [00:01<00:03, 52.62it/s]concatenating: train:  38%|███▊      | 100/266 [00:01<00:03, 52.23it/s]concatenating: train:  40%|███▉      | 106/266 [00:01<00:02, 53.64it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:02, 53.07it/s]concatenating: train:  44%|████▍     | 118/266 [00:02<00:02, 52.62it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:02, 50.40it/s]concatenating: train:  49%|████▉     | 130/266 [00:02<00:02, 49.45it/s]concatenating: train:  51%|█████     | 136/266 [00:02<00:02, 49.54it/s]concatenating: train:  53%|█████▎    | 142/266 [00:02<00:02, 51.37it/s]concatenating: train:  56%|█████▌    | 149/266 [00:02<00:02, 54.77it/s]concatenating: train:  58%|█████▊    | 155/266 [00:02<00:02, 53.44it/s]concatenating: train:  61%|██████    | 161/266 [00:03<00:01, 54.00it/s]concatenating: train:  63%|██████▎   | 167/266 [00:03<00:01, 54.58it/s]concatenating: train:  65%|██████▌   | 173/266 [00:03<00:01, 55.41it/s]concatenating: train:  67%|██████▋   | 179/266 [00:03<00:01, 55.77it/s]concatenating: train:  70%|██████▉   | 185/266 [00:03<00:01, 55.54it/s]concatenating: train:  72%|███████▏  | 191/266 [00:03<00:01, 53.94it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 52.84it/s]concatenating: train:  76%|███████▋  | 203/266 [00:03<00:01, 51.04it/s]concatenating: train:  79%|███████▊  | 209/266 [00:03<00:01, 49.57it/s]concatenating: train:  80%|████████  | 214/266 [00:04<00:01, 49.61it/s]concatenating: train:  83%|████████▎ | 220/266 [00:04<00:00, 51.50it/s]concatenating: train:  85%|████████▍ | 226/266 [00:04<00:00, 53.42it/s]concatenating: train:  87%|████████▋ | 232/266 [00:04<00:00, 54.02it/s]concatenating: train:  90%|████████▉ | 239/266 [00:04<00:00, 56.14it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 57.72it/s]concatenating: train:  95%|█████████▍| 252/266 [00:04<00:00, 55.88it/s]concatenating: train:  97%|█████████▋| 258/266 [00:04<00:00, 55.05it/s]concatenating: train:  99%|█████████▉| 264/266 [00:04<00:00, 52.24it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 53.12it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:13<00:41, 13.68s/it]Loading test:  50%|█████     | 2/4 [00:19<00:22, 11.28s/it]Loading test:  75%|███████▌  | 3/4 [00:25<00:09,  9.72s/it]Loading test: 100%|██████████| 4/4 [00:35<00:00,  9.70s/it]Loading test: 100%|██████████| 4/4 [00:35<00:00,  8.77s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 62.78it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      2020-01-21 03:20:47.111952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 03:20:47.112051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 03:20:47.112065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 03:20:47.112072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 03:20:47.112377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)

loading the weights from thalamus:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights from thalamus:   2%|▏         | 1/44 [00:00<00:07,  5.40it/s]loading the weights from thalamus:   7%|▋         | 3/44 [00:00<00:06,  6.42it/s]loading the weights from thalamus:   9%|▉         | 4/44 [00:00<00:06,  6.09it/s]loading the weights from thalamus:  18%|█▊        | 8/44 [00:00<00:04,  7.83it/s]loading the weights from thalamus:  23%|██▎       | 10/44 [00:00<00:03,  8.64it/s]loading the weights from thalamus:  27%|██▋       | 12/44 [00:01<00:04,  7.22it/s]loading the weights from thalamus:  39%|███▊      | 17/44 [00:01<00:02,  9.22it/s]loading the weights from thalamus:  43%|████▎     | 19/44 [00:01<00:02,  9.29it/s]loading the weights from thalamus:  48%|████▊     | 21/44 [00:02<00:03,  6.99it/s]loading the weights from thalamus:  57%|█████▋    | 25/44 [00:02<00:02,  8.63it/s]loading the weights from thalamus:  61%|██████▏   | 27/44 [00:02<00:01,  8.90it/s]loading the weights from thalamus:  66%|██████▌   | 29/44 [00:02<00:01,  9.22it/s]loading the weights from thalamus:  70%|███████   | 31/44 [00:03<00:01,  7.12it/s]loading the weights from thalamus:  80%|███████▉  | 35/44 [00:03<00:01,  8.73it/s]loading the weights from thalamus:  84%|████████▍ | 37/44 [00:03<00:00,  9.10it/s]loading the weights from thalamus:  89%|████████▊ | 39/44 [00:03<00:00,  8.94it/s]loading the weights from thalamus:  93%|█████████▎| 41/44 [00:04<00:00,  7.01it/s]loading the weights from thalamus: 100%|██████████| 44/44 [00:04<00:00, 10.35it/s]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Thalamus /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [6.33657318e-02 3.28415790e-02 7.67955156e-02 9.54222897e-03
 2.76173835e-02 7.22530992e-03 8.43092963e-02 1.14143891e-01
 8.96254841e-02 1.36172299e-02 2.90583448e-01 1.90074696e-01
 2.58206748e-04]
Train on 9753 samples, validate on 142 samples
Epoch 1/300
 - 24s - loss: 0.7256 - acc: 0.8263 - mDice: 0.2194 - val_loss: 0.6708 - val_acc: 0.9291 - val_mDice: 0.2558

Epoch 00001: val_mDice improved from -inf to 0.25576, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 20s - loss: 0.5473 - acc: 0.9208 - mDice: 0.4099 - val_loss: 0.5440 - val_acc: 0.9373 - val_mDice: 0.2593

Epoch 00002: val_mDice improved from 0.25576 to 0.25934, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 19s - loss: 0.4782 - acc: 0.9263 - mDice: 0.4846 - val_loss: 0.4891 - val_acc: 0.9456 - val_mDice: 0.3011

Epoch 00003: val_mDice improved from 0.25934 to 0.30109, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 20s - loss: 0.4476 - acc: 0.9296 - mDice: 0.5177 - val_loss: 0.4608 - val_acc: 0.9477 - val_mDice: 0.3447

Epoch 00004: val_mDice improved from 0.30109 to 0.34471, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 20s - loss: 0.4299 - acc: 0.9318 - mDice: 0.5368 - val_loss: 0.3758 - val_acc: 0.9469 - val_mDice: 0.3489

Epoch 00005: val_mDice improved from 0.34471 to 0.34893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 20s - loss: 0.4151 - acc: 0.9333 - mDice: 0.5528 - val_loss: 0.3640 - val_acc: 0.9395 - val_mDice: 0.2674

Epoch 00006: val_mDice did not improve from 0.34893
Epoch 7/300
 - 20s - loss: 0.4084 - acc: 0.9343 - mDice: 0.5600 - val_loss: 0.3139 - val_acc: 0.9469 - val_mDice: 0.3460

Epoch 00007: val_mDice did not improve from 0.34893
Epoch 8/300
 - 20s - loss: 0.4000 - acc: 0.9358 - mDice: 0.5690 - val_loss: 0.3077 - val_acc: 0.9485 - val_mDice: 0.3180

Epoch 00008: val_mDice did not improve from 0.34893
Epoch 9/300
 - 20s - loss: 0.3952 - acc: 0.9368 - mDice: 0.5742 - val_loss: 0.3468 - val_acc: 0.9280 - val_mDice: 0.3189

Epoch 00009: val_mDice did not improve from 0.34893
Epoch 10/300
 - 20s - loss: 0.3893 - acc: 0.9376 - mDice: 0.5807 - val_loss: 0.2640 - val_acc: 0.9501 - val_mDice: 0.3522

Epoch 00010: val_mDice improved from 0.34893 to 0.35223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 20s - loss: 0.3835 - acc: 0.9382 - mDice: 0.5869 - val_loss: 0.2525 - val_acc: 0.9503 - val_mDice: 0.3552

Epoch 00011: val_mDice improved from 0.35223 to 0.35521, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 21s - loss: 0.3780 - acc: 0.9388 - mDice: 0.5928 - val_loss: 0.2611 - val_acc: 0.9515 - val_mDice: 0.3435

Epoch 00012: val_mDice did not improve from 0.35521
Epoch 13/300
 - 20s - loss: 0.3749 - acc: 0.9396 - mDice: 0.5962 - val_loss: 0.2425 - val_acc: 0.9507 - val_mDice: 0.3642

Epoch 00013: val_mDice improved from 0.35521 to 0.36420, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 20s - loss: 0.3698 - acc: 0.9403 - mDice: 0.6017 - val_loss: 0.2921 - val_acc: 0.9465 - val_mDice: 0.3514

Epoch 00014: val_mDice did not improve from 0.36420
Epoch 15/300
 - 20s - loss: 0.3677 - acc: 0.9406 - mDice: 0.6039 - val_loss: 0.2770 - val_acc: 0.9468 - val_mDice: 0.3591

Epoch 00015: val_mDice did not improve from 0.36420
Epoch 16/300
 - 20s - loss: 0.3635 - acc: 0.9414 - mDice: 0.6084 - val_loss: 0.2380 - val_acc: 0.9488 - val_mDice: 0.3604

Epoch 00016: val_mDice did not improve from 0.36420
Epoch 17/300
 - 21s - loss: 0.3612 - acc: 0.9416 - mDice: 0.6110 - val_loss: 0.2702 - val_acc: 0.9452 - val_mDice: 0.3504

Epoch 00017: val_mDice did not improve from 0.36420
Epoch 18/300
 - 21s - loss: 0.3603 - acc: 0.9419 - mDice: 0.6119 - val_loss: 0.2397 - val_acc: 0.9498 - val_mDice: 0.3619

Epoch 00018: val_mDice did not improve from 0.36420
Epoch 19/300
 - 22s - loss: 0.3567 - acc: 0.9424 - mDice: 0.6158 - val_loss: 0.2209 - val_acc: 0.9511 - val_mDice: 0.3286

Epoch 00019: val_mDice did not improve from 0.36420
Epoch 20/300
 - 21s - loss: 0.3534 - acc: 0.9425 - mDice: 0.6193 - val_loss: 0.2380 - val_acc: 0.9458 - val_mDice: 0.3475

Epoch 00020: val_mDice did not improve from 0.36420
Epoch 21/300
 - 22s - loss: 0.3534 - acc: 0.9428 - mDice: 0.6193 - val_loss: 0.2427 - val_acc: 0.9488 - val_mDice: 0.3469

Epoch 00021: val_mDice did not improve from 0.36420
Epoch 22/300
 - 22s - loss: 0.3494 - acc: 0.9431 - mDice: 0.6236 - val_loss: 0.1896 - val_acc: 0.9514 - val_mDice: 0.3223

Epoch 00022: val_mDice did not improve from 0.36420
Epoch 23/300
 - 21s - loss: 0.3504 - acc: 0.9432 - mDice: 0.6226 - val_loss: 0.1765 - val_acc: 0.9523 - val_mDice: 0.3597

Epoch 00023: val_mDice did not improve from 0.36420
Epoch 24/300
 - 20s - loss: 0.3454 - acc: 0.9436 - mDice: 0.6280 - val_loss: 0.2589 - val_acc: 0.9458 - val_mDice: 0.3522

Epoch 00024: val_mDice did not improve from 0.36420
Epoch 25/300
 - 20s - loss: 0.3475 - acc: 0.9436 - mDice: 0.6257 - val_loss: 0.2269 - val_acc: 0.9508 - val_mDice: 0.3540

Epoch 00025: val_mDice did not improve from 0.36420
Epoch 26/300
 - 21s - loss: 0.3440 - acc: 0.9439 - mDice: 0.6295 - val_loss: 0.2244 - val_acc: 0.9534 - val_mDice: 0.3536

Epoch 00026: val_mDice did not improve from 0.36420
Epoch 27/300
 - 20s - loss: 0.3402 - acc: 0.9441 - mDice: 0.6336 - val_loss: 0.1902 - val_acc: 0.9507 - val_mDice: 0.3238

Epoch 00027: val_mDice did not improve from 0.36420
Epoch 28/300
 - 20s - loss: 0.3374 - acc: 0.9442 - mDice: 0.6366 - val_loss: 0.2057 - val_acc: 0.9501 - val_mDice: 0.3589

Epoch 00028: val_mDice did not improve from 0.36420

Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 29/300
 - 20s - loss: 0.3293 - acc: 0.9448 - mDice: 0.6454 - val_loss: 0.1986 - val_acc: 0.9504 - val_mDice: 0.3650

Epoch 00029: val_mDice improved from 0.36420 to 0.36500, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 21s - loss: 0.3276 - acc: 0.9449 - mDice: 0.6473 - val_loss: 0.3694 - val_acc: 0.9227 - val_mDice: 0.3047

Epoch 00030: val_mDice did not improve from 0.36500
Epoch 31/300
 - 20s - loss: 0.3274 - acc: 0.9450 - mDice: 0.6475 - val_loss: 0.2743 - val_acc: 0.9489 - val_mDice: 0.3401

Epoch 00031: val_mDice did not improve from 0.36500
Epoch 32/300
 - 20s - loss: 0.3259 - acc: 0.9453 - mDice: 0.6491 - val_loss: 0.2032 - val_acc: 0.9499 - val_mDice: 0.3583

Epoch 00032: val_mDice did not improve from 0.36500
Epoch 33/300
 - 20s - loss: 0.3244 - acc: 0.9452 - mDice: 0.6507 - val_loss: 0.2138 - val_acc: 0.9529 - val_mDice: 0.3468

Epoch 00033: val_mDice did not improve from 0.36500
Epoch 34/300
 - 21s - loss: 0.3230 - acc: 0.9453 - mDice: 0.6523 - val_loss: 0.2023 - val_acc: 0.9516 - val_mDice: 0.3270

Epoch 00034: val_mDice did not improve from 0.36500
Epoch 35/300
 - 20s - loss: 0.3190 - acc: 0.9455 - mDice: 0.6566 - val_loss: 0.2150 - val_acc: 0.9524 - val_mDice: 0.3571

Epoch 00035: val_mDice did not improve from 0.36500
Epoch 36/300
 - 20s - loss: 0.3190 - acc: 0.9456 - mDice: 0.6566 - val_loss: 0.1978 - val_acc: 0.9528 - val_mDice: 0.3629

Epoch 00036: val_mDice did not improve from 0.36500
Epoch 37/300
 - 21s - loss: 0.3201 - acc: 0.9457 - mDice: 0.6554 - val_loss: 0.1912 - val_acc: 0.9534 - val_mDice: 0.3566

Epoch 00037: val_mDice did not improve from 0.36500
Epoch 38/300
 - 20s - loss: 0.3189 - acc: 0.9458 - mDice: 0.6567 - val_loss: 0.2076 - val_acc: 0.9490 - val_mDice: 0.3647

Epoch 00038: val_mDice did not improve from 0.36500
Epoch 39/300
 - 20s - loss: 0.3186 - acc: 0.9459 - mDice: 0.6570 - val_loss: 0.2187 - val_acc: 0.9489 - val_mDice: 0.3652

Epoch 00039: val_mDice improved from 0.36500 to 0.36517, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 40/300
 - 20s - loss: 0.3175 - acc: 0.9458 - mDice: 0.6582 - val_loss: 0.1999 - val_acc: 0.9537 - val_mDice: 0.3372

Epoch 00040: val_mDice did not improve from 0.36517
Epoch 41/300
 - 20s - loss: 0.3155 - acc: 0.9459 - mDice: 0.6604 - val_loss: 0.1973 - val_acc: 0.9524 - val_mDice: 0.3425

Epoch 00041: val_mDice did not improve from 0.36517
Epoch 42/300
 - 19s - loss: 0.3161 - acc: 0.9460 - mDice: 0.6597 - val_loss: 0.1993 - val_acc: 0.9520 - val_mDice: 0.3625

Epoch 00042: val_mDice did not improve from 0.36517
Epoch 43/300
 - 20s - loss: 0.3171 - acc: 0.9456 - mDice: 0.6587 - val_loss: 0.2009 - val_acc: 0.9520 - val_mDice: 0.3418

Epoch 00043: val_mDice did not improve from 0.36517

Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 44/300
 - 20s - loss: 0.3125 - acc: 0.9463 - mDice: 0.6636 - val_loss: 0.2242 - val_acc: 0.9529 - val_mDice: 0.3441

Epoch 00044: val_mDice did not improve from 0.36517
Epoch 45/300
 - 19s - loss: 0.3110 - acc: 0.9464 - mDice: 0.6652 - val_loss: 0.1791 - val_acc: 0.9540 - val_mDice: 0.3589

Epoch 00045: val_mDice did not improve from 0.36517
Epoch 46/300
 - 19s - loss: 0.3103 - acc: 0.9466 - mDice: 0.6660 - val_loss: 0.2047 - val_acc: 0.9523 - val_mDice: 0.3554

Epoch 00046: val_mDice did not improve from 0.36517
Epoch 47/300
 - 20s - loss: 0.3121 - acc: 0.9465 - mDice: 0.6641 - val_loss: 0.1983 - val_acc: 0.9521 - val_mDice: 0.3639

Epoch 00047: val_mDice did not improve from 0.36517
Epoch 48/300
 - 20s - loss: 0.3081 - acc: 0.9467 - mDice: 0.6683 - val_loss: 0.1805 - val_acc: 0.9519 - val_mDice: 0.3682

Epoch 00048: val_mDice improved from 0.36517 to 0.36824, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 49/300
 - 19s - loss: 0.3081 - acc: 0.9468 - mDice: 0.6683 - val_loss: 0.1660 - val_acc: 0.9539 - val_mDice: 0.3619

Epoch 00049: val_mDice did not improve from 0.36824
Epoch 50/300
 - 19s - loss: 0.3086 - acc: 0.9468 - mDice: 0.6679 - val_loss: 0.1497 - val_acc: 0.9540 - val_mDice: 0.3592

Epoch 00050: val_mDice did not improve from 0.36824
Epoch 51/300
 - 20s - loss: 0.3103 - acc: 0.9467 - mDice: 0.6660 - val_loss: 0.1774 - val_acc: 0.9536 - val_mDice: 0.3612

Epoch 00051: val_mDice did not improve from 0.36824
Epoch 52/300
 - 20s - loss: 0.3086 - acc: 0.9468 - mDice: 0.6678 - val_loss: 0.2024 - val_acc: 0.9523 - val_mDice: 0.3430

Epoch 00052: val_mDice did not improve from 0.36824
Epoch 53/300
 - 20s - loss: 0.3083 - acc: 0.9468 - mDice: 0.6680 - val_loss: 0.2243 - val_acc: 0.9512 - val_mDice: 0.3678

Epoch 00053: val_mDice did not improve from 0.36824
Epoch 54/300
 - 20s - loss: 0.3062 - acc: 0.9469 - mDice: 0.6704 - val_loss: 0.2095 - val_acc: 0.9493 - val_mDice: 0.3634

Epoch 00054: val_mDice did not improve from 0.36824
Epoch 55/300
 - 20s - loss: 0.3037 - acc: 0.9468 - mDice: 0.6732 - val_loss: 0.1895 - val_acc: 0.9532 - val_mDice: 0.3612

Epoch 00055: val_mDice did not improve from 0.36824
Epoch 56/300
 - 20s - loss: 0.3035 - acc: 0.9468 - mDice: 0.6733 - val_loss: 0.1846 - val_acc: 0.9534 - val_mDice: 0.3683

Epoch 00056: val_mDice improved from 0.36824 to 0.36829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 57/300
 - 19s - loss: 0.3015 - acc: 0.9469 - mDice: 0.6755 - val_loss: 0.2156 - val_acc: 0.9503 - val_mDice: 0.3673

Epoch 00057: val_mDice did not improve from 0.36829
Epoch 58/300
 - 20s - loss: 0.3022 - acc: 0.9468 - mDice: 0.6747 - val_loss: 0.1946 - val_acc: 0.9533 - val_mDice: 0.3592

Epoch 00058: val_mDice did not improve from 0.36829
Epoch 59/300
 - 19s - loss: 0.3006 - acc: 0.9470 - mDice: 0.6765 - val_loss: 0.2151 - val_acc: 0.9512 - val_mDice: 0.3628

Epoch 00059: val_mDice did not improve from 0.36829
Epoch 60/300
 - 20s - loss: 0.3008 - acc: 0.9470 - mDice: 0.6762 - val_loss: 0.1893 - val_acc: 0.9527 - val_mDice: 0.3507

Epoch 00060: val_mDice did not improve from 0.36829
Epoch 61/300
 - 19s - loss: 0.3029 - acc: 0.9470 - mDice: 0.6739 - val_loss: 0.1648 - val_acc: 0.9533 - val_mDice: 0.3650

Epoch 00061: val_mDice did not improve from 0.36829
Epoch 62/300
 - 20s - loss: 0.3020 - acc: 0.9469 - mDice: 0.6750 - val_loss: 0.1646 - val_acc: 0.9523 - val_mDice: 0.3648

Epoch 00062: val_mDice did not improve from 0.36829
Epoch 63/300
 - 19s - loss: 0.3011 - acc: 0.9472 - mDice: 0.6759 - val_loss: 0.1401 - val_acc: 0.9515 - val_mDice: 0.3630

Epoch 00063: val_mDice did not improve from 0.36829

Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 64/300
 - 20s - loss: 0.2949 - acc: 0.9473 - mDice: 0.6827 - val_loss: 0.1385 - val_acc: 0.9527 - val_mDice: 0.3614

Epoch 00064: val_mDice did not improve from 0.36829
Epoch 65/300
 - 20s - loss: 0.2967 - acc: 0.9475 - mDice: 0.6806 - val_loss: 0.1696 - val_acc: 0.9506 - val_mDice: 0.3646

Epoch 00065: val_mDice did not improve from 0.36829
Epoch 66/300
 - 20s - loss: 0.2982 - acc: 0.9475 - mDice: 0.6790 - val_loss: 0.1468 - val_acc: 0.9522 - val_mDice: 0.3652

Epoch 00066: val_mDice did not improve from 0.36829
Epoch 67/300
 - 20s - loss: 0.2997 - acc: 0.9474 - mDice: 0.6775 - val_loss: 0.1737 - val_acc: 0.9509 - val_mDice: 0.3618

Epoch 00067: val_mDice did not improve from 0.36829
Epoch 68/300
 - 21s - loss: 0.2940 - acc: 0.9476 - mDice: 0.6836 - val_loss: 0.1548 - val_acc: 0.9532 - val_mDice: 0.3598

Epoch 00068: val_mDice did not improve from 0.36829
Epoch 69/300
 - 21s - loss: 0.2965 - acc: 0.9476 - mDice: 0.6809 - val_loss: 0.1422 - val_acc: 0.9530 - val_mDice: 0.3561

Epoch 00069: val_mDice did not improve from 0.36829
Epoch 70/300
 - 22s - loss: 0.2958 - acc: 0.9474 - mDice: 0.6817 - val_loss: 0.1543 - val_acc: 0.9528 - val_mDice: 0.3638

Epoch 00070: val_mDice did not improve from 0.36829
Epoch 71/300
 - 22s - loss: 0.2957 - acc: 0.9475 - mDice: 0.6817 - val_loss: 0.1409 - val_acc: 0.9525 - val_mDice: 0.3644

Epoch 00071: val_mDice did not improve from 0.36829
Epoch 72/300
 - 21s - loss: 0.2965 - acc: 0.9475 - mDice: 0.6809 - val_loss: 0.1415 - val_acc: 0.9523 - val_mDice: 0.3629

Epoch 00072: val_mDice did not improve from 0.36829
Epoch 73/300
 - 21s - loss: 0.2961 - acc: 0.9476 - mDice: 0.6813 - val_loss: 0.1215 - val_acc: 0.9527 - val_mDice: 0.3636

Epoch 00073: val_mDice did not improve from 0.36829
Epoch 74/300
 - 22s - loss: 0.2970 - acc: 0.9475 - mDice: 0.6804 - val_loss: 0.1280 - val_acc: 0.9529 - val_mDice: 0.3603

Epoch 00074: val_mDice did not improve from 0.36829
Epoch 75/300
 - 21s - loss: 0.2934 - acc: 0.9476 - mDice: 0.6843 - val_loss: 0.1485 - val_acc: 0.9519 - val_mDice: 0.3550

Epoch 00075: val_mDice did not improve from 0.36829
Epoch 76/300
 - 22s - loss: 0.2958 - acc: 0.9477 - mDice: 0.6817 - val_loss: 0.1399 - val_acc: 0.9513 - val_mDice: 0.3656

Epoch 00076: val_mDice did not improve from 0.36829
Epoch 77/300
 - 22s - loss: 0.2951 - acc: 0.9476 - mDice: 0.6824 - val_loss: 0.1169 - val_acc: 0.9517 - val_mDice: 0.3633

Epoch 00077: val_mDice did not improve from 0.36829
Epoch 78/300
 - 22s - loss: 0.2940 - acc: 0.9477 - mDice: 0.6836 - val_loss: 0.1275 - val_acc: 0.9523 - val_mDice: 0.3626

Epoch 00078: val_mDice did not improve from 0.36829

Epoch 00078: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 79/300
 - 22s - loss: 0.2929 - acc: 0.9477 - mDice: 0.6847 - val_loss: 0.1283 - val_acc: 0.9533 - val_mDice: 0.3576

Epoch 00079: val_mDice did not improve from 0.36829
Epoch 80/300
 - 22s - loss: 0.2983 - acc: 0.9477 - mDice: 0.6790 - val_loss: 0.1147 - val_acc: 0.9521 - val_mDice: 0.3596

Epoch 00080: val_mDice did not improve from 0.36829
Epoch 81/300
 - 22s - loss: 0.2980 - acc: 0.9479 - mDice: 0.6792 - val_loss: 0.1282 - val_acc: 0.9528 - val_mDice: 0.3608

Epoch 00081: val_mDice did not improve from 0.36829
Epoch 82/300
 - 22s - loss: 0.2948 - acc: 0.9478 - mDice: 0.6827 - val_loss: 0.1281 - val_acc: 0.9525 - val_mDice: 0.3590

Epoch 00082: val_mDice did not improve from 0.36829
Epoch 83/300
 - 22s - loss: 0.2958 - acc: 0.9477 - mDice: 0.6816 - val_loss: 0.1299 - val_acc: 0.9520 - val_mDice: 0.3630

Epoch 00083: val_mDice did not improve from 0.36829
Epoch 84/300
 - 22s - loss: 0.2932 - acc: 0.9478 - mDice: 0.6844 - val_loss: 0.1177 - val_acc: 0.9530 - val_mDice: 0.3562

Epoch 00084: val_mDice did not improve from 0.36829
Epoch 85/300
 - 22s - loss: 0.2912 - acc: 0.9477 - mDice: 0.6866 - val_loss: 0.1160 - val_acc: 0.9525 - val_mDice: 0.3555

Epoch 00085: val_mDice did not improve from 0.36829
Epoch 86/300
 - 22s - loss: 0.2959 - acc: 0.9477 - mDice: 0.6816 - val_loss: 0.1199 - val_acc: 0.9529 - val_mDice: 0.3554

Epoch 00086: val_mDice did not improve from 0.36829
Epoch 87/300
 - 22s - loss: 0.2925 - acc: 0.9478 - mDice: 0.6853 - val_loss: 0.1060 - val_acc: 0.9530 - val_mDice: 0.3640

Epoch 00087: val_mDice did not improve from 0.36829
Epoch 88/300
 - 22s - loss: 0.2954 - acc: 0.9479 - mDice: 0.6821 - val_loss: 0.1078 - val_acc: 0.9527 - val_mDice: 0.3597

Epoch 00088: val_mDice did not improve from 0.36829
Epoch 89/300
 - 22s - loss: 0.2968 - acc: 0.9478 - mDice: 0.6805 - val_loss: 0.1030 - val_acc: 0.9532 - val_mDice: 0.3545

Epoch 00089: val_mDice did not improve from 0.36829
Epoch 90/300
 - 22s - loss: 0.2935 - acc: 0.9478 - mDice: 0.6841 - val_loss: 0.1337 - val_acc: 0.9508 - val_mDice: 0.3573

Epoch 00090: val_mDice did not improve from 0.36829
Epoch 91/300
 - 22s - loss: 0.2939 - acc: 0.9479 - mDice: 0.6837 - val_loss: 0.1028 - val_acc: 0.9526 - val_mDice: 0.3586

Epoch 00091: val_mDice did not improve from 0.36829
Epoch 92/300
 - 22s - loss: 0.2910 - acc: 0.9479 - mDice: 0.6868 - val_loss: 0.1164 - val_acc: 0.9522 - val_mDice: 0.3600

Epoch 00092: val_mDice did not improve from 0.36829
Epoch 93/300
 - 22s - loss: 0.2943 - acc: 0.9479 - mDice: 0.6832 - val_loss: 0.1358 - val_acc: 0.9514 - val_mDice: 0.3600

Epoch 00093: val_mDice did not improve from 0.36829

Epoch 00093: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 94/300
 - 22s - loss: 0.2938 - acc: 0.9478 - mDice: 0.6838 - val_loss: 0.1127 - val_acc: 0.9532 - val_mDice: 0.3515

Epoch 00094: val_mDice did not improve from 0.36829
Epoch 95/300
 - 22s - loss: 0.2953 - acc: 0.9479 - mDice: 0.6822 - val_loss: 0.1279 - val_acc: 0.9525 - val_mDice: 0.3618

Epoch 00095: val_mDice did not improve from 0.36829
Epoch 96/300
 - 22s - loss: 0.2918 - acc: 0.9479 - mDice: 0.6860 - val_loss: 0.1086 - val_acc: 0.9534 - val_mDice: 0.3587

Epoch 00096: val_mDice did not improve from 0.36829
Restoring model weights from the end of the best epoch
Epoch 00096: early stopping
{'val_loss': [0.6708160051157777, 0.5439995849951053, 0.4890647146483542, 0.4607677987553704, 0.3758022709631584, 0.3639594551543115, 0.31390242824252224, 0.3077442448743632, 0.34683174209695466, 0.26397813014476235, 0.25252123497111695, 0.2610781954627642, 0.24252877531337066, 0.29209957292801897, 0.2770388357498696, 0.2380275761718455, 0.2702156957501257, 0.23969356311668813, 0.22090427180043828, 0.2380012666649172, 0.24269142935813312, 0.18961829580628956, 0.17652432547068933, 0.2588990683024618, 0.2268877133946787, 0.22439250471332753, 0.19016154152406772, 0.205693756977857, 0.19858902128754366, 0.36944592901518647, 0.2742993968053603, 0.20315080526953852, 0.21377497990156563, 0.20226316038243683, 0.21497698475829732, 0.19784615457740048, 0.19119872020977274, 0.20758097633575154, 0.21873787227972136, 0.19986382043513823, 0.19730180234346592, 0.19931922402416527, 0.20089522959716932, 0.2242029395059381, 0.1790888703388857, 0.20467536162558786, 0.19825027715152418, 0.18046233576105933, 0.16597390313908247, 0.1496773824213781, 0.17738804760270974, 0.20238296695391345, 0.22426473386716886, 0.20953060822146552, 0.18951531380913417, 0.1845540581008708, 0.21562305826064146, 0.1946115409693038, 0.2150564590783816, 0.18931943155877606, 0.16478483203116437, 0.16460595613109394, 0.14006475608428598, 0.13846034803886859, 0.1695837228572075, 0.146767746591547, 0.17371482445373082, 0.15478445585607223, 0.14219123675999507, 0.15428956171733813, 0.14087735010232305, 0.1415447277397337, 0.1214722862834452, 0.1279813547316991, 0.14845493724438505, 0.139943241889418, 0.1169462398901372, 0.12748653503318488, 0.1282962658508143, 0.11470831221323723, 0.12822138271491293, 0.12807374597120452, 0.12988939073304057, 0.11771592005929897, 0.11597481073523072, 0.11991592671092548, 0.10595870751257934, 0.10776285508411451, 0.10300372624778005, 0.1337330499663949, 0.10281600265730496, 0.11639066152012265, 0.1358306687849928, 0.11267448234712889, 0.12788130712865944, 0.10857426461724329], 'val_acc': [0.9290550972374392, 0.937325871326554, 0.9456353548546912, 0.947718364252171, 0.9469444869269787, 0.9395007860492652, 0.9468848277145708, 0.9485293156664136, 0.9279781168615314, 0.950106080988763, 0.9502737589285407, 0.9515071217442902, 0.950651014354867, 0.9465011207150741, 0.9468445316166945, 0.948761479955324, 0.9451532917962947, 0.9498045906214647, 0.951089557627557, 0.9457981762751727, 0.9487824339262196, 0.9513571824825985, 0.9523261441311366, 0.9458239867653645, 0.9507735480724926, 0.9533724784851074, 0.9506993822648492, 0.9500673841422712, 0.9503527644654395, 0.9227077222206224, 0.9489291495000812, 0.9499416351318359, 0.9529371840853087, 0.9516135202327245, 0.9523857949485242, 0.952777561167596, 0.9533595795362768, 0.9490484553323665, 0.948892077090035, 0.95365623437183, 0.9524406060366564, 0.9520262698052635, 0.9519811213856012, 0.9528791367168158, 0.9540254347760913, 0.9522971099531147, 0.9520713930398645, 0.951937584809854, 0.9538932299949754, 0.9539948013466848, 0.9536046427740178, 0.9522664891162389, 0.9512233658575676, 0.949303193831108, 0.9532209399720313, 0.9534353571878352, 0.9502931052530316, 0.953290257655399, 0.9512136863990569, 0.9526969479842925, 0.9532999287188892, 0.9523019580773904, 0.9515264638712708, 0.9526518205521812, 0.9506284485400562, 0.9521600696402537, 0.9508638365167967, 0.9531919099915196, 0.9530129491443365, 0.9527969116895971, 0.9525115397614492, 0.952348701550927, 0.9527146824648682, 0.9528807443632207, 0.9519198545267884, 0.951333009021383, 0.9516731836426426, 0.9522680925651336, 0.9532773503115479, 0.9520568948396495, 0.9528194775044079, 0.9524583405172321, 0.9520165861492426, 0.9529710286100146, 0.9524696234246375, 0.9529226607000324, 0.9530000501955059, 0.9526614790231409, 0.9532289907965862, 0.950775168311428, 0.9526421452911806, 0.952166521213424, 0.9513555706386835, 0.9531693357816884, 0.9525131516053643, 0.9534369648342401], 'val_mDice': [0.25576036108631484, 0.25933887652108367, 0.30108982198674916, 0.34471324212114574, 0.3489256429840142, 0.26741417780728405, 0.34604407217301114, 0.31799243107228214, 0.3188621787957742, 0.35222727962782685, 0.3552141103526236, 0.3435161447441074, 0.36420109511261256, 0.351409128224346, 0.3591214162363133, 0.3603721640899148, 0.35036107079243994, 0.3619406151939446, 0.3285513015490183, 0.3475057176301177, 0.34692115976776877, 0.322320066707235, 0.35966307764321986, 0.35215554417858663, 0.35398798089631844, 0.35361361545576175, 0.3238392264490396, 0.3589376098882984, 0.36500345968024833, 0.30467050491084513, 0.3400704044271523, 0.3582627021930587, 0.3467547290132079, 0.32701701345578044, 0.35705486863431796, 0.36290655463514193, 0.35659131554650586, 0.3646938229950381, 0.3651725081071048, 0.33718756386931514, 0.3424997695944679, 0.36254266513065553, 0.34183127674418434, 0.3441228707071761, 0.3589238960978011, 0.35536400239232563, 0.3638522956572788, 0.36824189546242564, 0.36190155316406575, 0.3592110818750422, 0.3611574321985245, 0.34301502394004607, 0.3677919785111723, 0.36341054338804435, 0.36123191124536624, 0.3682850652807195, 0.367263284262637, 0.3591651417000193, 0.362774982628688, 0.3507062262842353, 0.36502093286581444, 0.36477353476302726, 0.3630394152772259, 0.36137957178371055, 0.364595835267658, 0.36518409113648914, 0.361840851395063, 0.35975504141878073, 0.3560771887571039, 0.36382223099050387, 0.36441962169089787, 0.3628906376764808, 0.3635882061551994, 0.3603304429373271, 0.35501753360452787, 0.3656012491441109, 0.36331461558879263, 0.36255090589254674, 0.3576068548669278, 0.3595961483431534, 0.3607660217184416, 0.35900324058364813, 0.3629798615272616, 0.3561984522032066, 0.35552715751486763, 0.35538443587195706, 0.3640024479426129, 0.35971336171660623, 0.3544566496996812, 0.35727736920538083, 0.3586077501236553, 0.36004350953538655, 0.3600292537413852, 0.3515474151977351, 0.36177136318784364, 0.3587142808336607], 'loss': [0.7256076920219511, 0.5473378806203179, 0.47819013552434453, 0.4475778162870531, 0.4298988949252705, 0.415063242588143, 0.40840086519052027, 0.40004424051811643, 0.3952355060646574, 0.3892709816814899, 0.38353924352572927, 0.37799781508339647, 0.3748709662265195, 0.3697721761983932, 0.3677349287455453, 0.3634787585036664, 0.36116275030576817, 0.36031466672680357, 0.3566915241709785, 0.35344031985437, 0.3533996698166497, 0.34939981124455605, 0.3503931559019549, 0.3453554754212956, 0.34746282917834986, 0.34403016436018113, 0.3401836304160787, 0.33741566910690424, 0.3293401688493167, 0.3275565426404398, 0.3273547431493214, 0.32590336872591896, 0.32443056105467327, 0.32297587035057496, 0.31902082839049306, 0.3190098572092693, 0.32008003354634823, 0.31890117081021035, 0.3186231997518384, 0.3175388288922057, 0.31548246019842147, 0.3161192421581664, 0.3170530597582776, 0.3124834121489762, 0.3110070892093146, 0.31025269791051496, 0.3120902772409591, 0.3081347439810519, 0.3081323256538328, 0.3085509726484898, 0.3102517923208233, 0.3085668282418157, 0.3083412510534928, 0.3062309243520639, 0.30368217487011545, 0.3034955994582697, 0.3015494984589417, 0.30221238161531655, 0.30062258857452573, 0.30081207996317755, 0.3029429874737471, 0.3019983159032685, 0.3010519117095538, 0.29486149706853226, 0.2967477066570901, 0.29822186349380153, 0.2996826404994981, 0.294023278791648, 0.2965063316222155, 0.29575278528785626, 0.295746338021556, 0.2964979916074564, 0.29611581371134393, 0.2970058894073072, 0.293379937396665, 0.2957755626225782, 0.2951049746109573, 0.29399371783048645, 0.29294592430258387, 0.2982735395046499, 0.2980252429789474, 0.2947912426022008, 0.2958216166191928, 0.29323896149067, 0.2912262072596662, 0.2958642823640156, 0.29245326135337724, 0.2953948524656338, 0.2968309538664848, 0.29346427680479736, 0.2939297511988782, 0.29102283382518446, 0.29434067530044344, 0.2937709087165798, 0.2953064125445663, 0.29175442540619934], 'acc': [0.8262668263175164, 0.9207514202877104, 0.9263374020316766, 0.929560158639691, 0.9317659474111002, 0.9332703908724527, 0.9343159745243846, 0.9357702804017554, 0.9367704647120043, 0.93759086656898, 0.9381973295410779, 0.9387935815157675, 0.9395579040863326, 0.9402571118306982, 0.9406065159267882, 0.9414333250661221, 0.9415640965750386, 0.9418987127605932, 0.9424067739216618, 0.9425246347455041, 0.94282378199332, 0.9430533064019188, 0.9431795703195052, 0.9435801715175152, 0.9436180097841916, 0.9438818300196932, 0.9441323636568938, 0.9441690987028929, 0.9447942000101325, 0.9448645514646432, 0.9450251110074558, 0.9452673589023312, 0.9452390950618127, 0.9453447492296458, 0.9455460362601595, 0.9455709414495317, 0.9456690369685906, 0.9457744812031569, 0.9458568730342575, 0.9457758179593915, 0.9459132094035133, 0.9459844287341648, 0.94564539944589, 0.9463393952398438, 0.9464394404474288, 0.9466072291053284, 0.9464584991869089, 0.9467103711081983, 0.9467973642919805, 0.9468057455129651, 0.9467149490052634, 0.9467866137787267, 0.9468239602536088, 0.9468578333905033, 0.9468097583851325, 0.9468412831980277, 0.9468503204245672, 0.9468415891966502, 0.9469525949240929, 0.9470395189510863, 0.9469615624377046, 0.946855016638655, 0.9472022376328899, 0.9473180076189314, 0.947483591253325, 0.9474678166301559, 0.9473605427475058, 0.9475725322310942, 0.9475749970631369, 0.9474272303100636, 0.9474528643958546, 0.9474778869183115, 0.9475749504574749, 0.9474703986913892, 0.947591733910494, 0.9477038439403347, 0.9475542466652451, 0.9476702764353776, 0.9476899007070032, 0.9476675991827215, 0.9478864206187751, 0.9478386757826788, 0.9477392888663182, 0.9478473378476957, 0.9477330912666591, 0.9477363071612281, 0.9477944510491288, 0.9478898015521459, 0.9478212814752498, 0.9477956028739012, 0.947938907241792, 0.9479356225265622, 0.9479381562098227, 0.9478356710375486, 0.9478887916894665, 0.9478928055577946], 'mDice': [0.2193619185928964, 0.4098655244276901, 0.48461966276303275, 0.517687296312454, 0.5367844770666783, 0.5528201646411479, 0.5600148192460288, 0.5690347354472949, 0.5742240820305783, 0.5806578994469558, 0.5868526795002591, 0.5928214514614875, 0.5961902810587806, 0.6016925712200382, 0.6038799435740604, 0.6084498908963922, 0.6109609247647371, 0.6118810775768179, 0.6158029604395707, 0.619300320309076, 0.6193164872485137, 0.6236399830176037, 0.6225604914609291, 0.628034701671134, 0.6257460491175433, 0.6294570098617047, 0.6336158096001941, 0.6366389029199351, 0.6454034735517699, 0.6473319868937304, 0.6475460136668885, 0.649116604392692, 0.6507261039219941, 0.6522958823366556, 0.6565922193896107, 0.6565851674964339, 0.6554292443592366, 0.65670430846646, 0.6569926991873883, 0.6581772297642163, 0.6603913912036828, 0.659694092598595, 0.6586870802044074, 0.6636310740518506, 0.6652299664050606, 0.6660371669729979, 0.6640535970748932, 0.668325240998516, 0.6683138756095404, 0.6678569477655381, 0.6660314222659671, 0.667845200480259, 0.6680442051350686, 0.670378892715437, 0.6731611615105872, 0.6733499519090757, 0.6754579457743626, 0.6747461721113153, 0.6764545509366149, 0.6762458226454144, 0.6739389554061829, 0.6749646452149648, 0.6759104581902508, 0.6826793636943723, 0.6806306634623491, 0.6790390247554519, 0.6774598253845228, 0.6835788342814121, 0.6808894450305414, 0.6817131711008316, 0.6817217100696981, 0.680909724623243, 0.6813105350455956, 0.6803534654523242, 0.6842764487061685, 0.681675497567386, 0.682406801807933, 0.683608234509265, 0.684744087198655, 0.6789784970728908, 0.6792335102469643, 0.6827396109179205, 0.6816258124543448, 0.6844181272478259, 0.6865989832490405, 0.6815834690409049, 0.6852767242170854, 0.6820796750440825, 0.6805139368623999, 0.6841053338548556, 0.683664511892657, 0.6868154335227317, 0.6832188306057208, 0.6838378498190723, 0.6821755165016298, 0.6860258976801792], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.54s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.34s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/sd0/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/sd1/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/sd2/vimp*': No such file or directory

  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.07it/s] 50%|█████     | 2/4 [00:00<00:00,  3.20it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.32it/s]100%|██████████| 4/4 [00:01<00:00,  3.22it/s]100%|██████████| 4/4 [00:01<00:00,  3.29it/s]

CrossVal ['a']
2020-01-21 03:54:51.597492: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 03:54:55.164195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 03:54:55.164261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 03:54:55.601163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 03:54:55.601225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 03:54:55.601236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 03:54:55.601702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 9
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
2020-01-21 03:54:59.056006: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 03:55:02.651143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 03:55:02.651203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 03:55:03.081499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 03:55:03.081561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 03:55:03.081573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 03:55:03.082039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 9
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
2020-01-21 03:55:06.372746: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 03:55:09.906983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 03:55:09.907039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 03:55:10.328371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 03:55:10.328433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 03:55:10.328443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 03:55:10.328902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 9
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
