2019-08-16 21:24:34.985053: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-16 21:24:35.377942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-08-16 21:24:35.378025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-16 21:24:36.031890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-16 21:24:36.034040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-16 21:24:36.035559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-16 21:24:36.037882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<06:23,  1.45s/it]Loading train:   1%|          | 2/266 [00:02<05:49,  1.32s/it]Loading train:   1%|          | 3/266 [00:03<05:32,  1.27s/it]Loading train:   2%|▏         | 4/266 [00:04<04:53,  1.12s/it]Loading train:   2%|▏         | 5/266 [00:05<04:22,  1.01s/it]Loading train:   2%|▏         | 6/266 [00:05<03:58,  1.09it/s]Loading train:   3%|▎         | 7/266 [00:06<04:03,  1.06it/s]Loading train:   3%|▎         | 8/266 [00:08<04:25,  1.03s/it]Loading train:   3%|▎         | 9/266 [00:09<04:36,  1.08s/it]Loading train:   4%|▍         | 10/266 [00:10<04:29,  1.05s/it]Loading train:   4%|▍         | 11/266 [00:11<04:26,  1.04s/it]Loading train:   5%|▍         | 12/266 [00:12<04:37,  1.09s/it]Loading train:   5%|▍         | 13/266 [00:13<04:44,  1.12s/it]Loading train:   5%|▌         | 14/266 [00:14<04:43,  1.12s/it]Loading train:   6%|▌         | 15/266 [00:15<04:41,  1.12s/it]Loading train:   6%|▌         | 16/266 [00:17<04:45,  1.14s/it]Loading train:   6%|▋         | 17/266 [00:18<04:50,  1.17s/it]Loading train:   7%|▋         | 18/266 [00:19<04:47,  1.16s/it]Loading train:   7%|▋         | 19/266 [00:20<04:36,  1.12s/it]Loading train:   8%|▊         | 20/266 [00:21<04:45,  1.16s/it]Loading train:   8%|▊         | 21/266 [00:22<04:47,  1.18s/it]Loading train:   8%|▊         | 22/266 [00:24<04:46,  1.17s/it]Loading train:   9%|▊         | 23/266 [00:24<04:07,  1.02s/it]Loading train:   9%|▉         | 24/266 [00:25<04:11,  1.04s/it]Loading train:   9%|▉         | 25/266 [00:26<03:37,  1.11it/s]Loading train:  10%|▉         | 26/266 [00:27<03:44,  1.07it/s]Loading train:  10%|█         | 27/266 [00:28<03:22,  1.18it/s]Loading train:  11%|█         | 28/266 [00:29<03:29,  1.13it/s]Loading train:  11%|█         | 29/266 [00:30<03:35,  1.10it/s]Loading train:  11%|█▏        | 30/266 [00:31<03:42,  1.06it/s]Loading train:  12%|█▏        | 31/266 [00:32<03:44,  1.05it/s]Loading train:  12%|█▏        | 32/266 [00:33<03:45,  1.04it/s]Loading train:  12%|█▏        | 33/266 [00:33<03:25,  1.13it/s]Loading train:  13%|█▎        | 34/266 [00:34<03:30,  1.10it/s]Loading train:  13%|█▎        | 35/266 [00:35<03:47,  1.01it/s]Loading train:  14%|█▎        | 36/266 [00:36<03:36,  1.06it/s]Loading train:  14%|█▍        | 37/266 [00:37<03:30,  1.09it/s]Loading train:  14%|█▍        | 38/266 [00:38<03:36,  1.05it/s]Loading train:  15%|█▍        | 39/266 [00:39<03:49,  1.01s/it]Loading train:  15%|█▌        | 40/266 [00:40<03:39,  1.03it/s]Loading train:  15%|█▌        | 41/266 [00:41<03:39,  1.03it/s]Loading train:  16%|█▌        | 42/266 [00:42<03:28,  1.08it/s]Loading train:  16%|█▌        | 43/266 [00:43<03:04,  1.21it/s]Loading train:  17%|█▋        | 44/266 [00:43<02:54,  1.27it/s]Loading train:  17%|█▋        | 45/266 [00:44<03:09,  1.17it/s]Loading train:  17%|█▋        | 46/266 [00:45<03:29,  1.05it/s]Loading train:  18%|█▊        | 47/266 [00:46<03:20,  1.09it/s]Loading train:  18%|█▊        | 48/266 [00:47<03:25,  1.06it/s]Loading train:  18%|█▊        | 49/266 [00:48<03:36,  1.00it/s]Loading train:  19%|█▉        | 50/266 [00:49<03:06,  1.16it/s]Loading train:  19%|█▉        | 51/266 [00:50<02:54,  1.23it/s]Loading train:  20%|█▉        | 52/266 [00:50<02:49,  1.26it/s]Loading train:  20%|█▉        | 53/266 [00:51<02:31,  1.41it/s]Loading train:  20%|██        | 54/266 [00:52<02:37,  1.34it/s]Loading train:  21%|██        | 55/266 [00:53<03:03,  1.15it/s]Loading train:  21%|██        | 56/266 [00:54<03:14,  1.08it/s]Loading train:  21%|██▏       | 57/266 [00:55<03:24,  1.02it/s]Loading train:  22%|██▏       | 58/266 [00:56<03:28,  1.00s/it]Loading train:  22%|██▏       | 59/266 [00:57<03:29,  1.01s/it]Loading train:  23%|██▎       | 60/266 [00:58<03:22,  1.02it/s]Loading train:  23%|██▎       | 61/266 [00:59<03:13,  1.06it/s]Loading train:  23%|██▎       | 62/266 [01:00<03:11,  1.06it/s]Loading train:  24%|██▎       | 63/266 [01:01<02:58,  1.14it/s]Loading train:  24%|██▍       | 64/266 [01:01<02:51,  1.17it/s]Loading train:  24%|██▍       | 65/266 [01:02<02:28,  1.35it/s]Loading train:  25%|██▍       | 66/266 [01:03<02:35,  1.28it/s]Loading train:  25%|██▌       | 67/266 [01:03<02:35,  1.28it/s]Loading train:  26%|██▌       | 68/266 [01:04<02:28,  1.33it/s]Loading train:  26%|██▌       | 69/266 [01:05<02:36,  1.26it/s]Loading train:  26%|██▋       | 70/266 [01:06<02:58,  1.10it/s]Loading train:  27%|██▋       | 71/266 [01:07<03:08,  1.03it/s]Loading train:  27%|██▋       | 72/266 [01:08<02:57,  1.09it/s]Loading train:  27%|██▋       | 73/266 [01:09<02:51,  1.12it/s]Loading train:  28%|██▊       | 74/266 [01:10<02:47,  1.15it/s]Loading train:  28%|██▊       | 75/266 [01:10<02:25,  1.31it/s]Loading train:  29%|██▊       | 76/266 [01:11<02:43,  1.16it/s]Loading train:  29%|██▉       | 77/266 [01:12<02:50,  1.11it/s]Loading train:  29%|██▉       | 78/266 [01:13<02:53,  1.09it/s]Loading train:  30%|██▉       | 79/266 [01:14<02:52,  1.09it/s]Loading train:  30%|███       | 80/266 [01:15<02:50,  1.09it/s]Loading train:  30%|███       | 81/266 [01:16<02:35,  1.19it/s]Loading train:  31%|███       | 82/266 [01:17<02:34,  1.19it/s]Loading train:  31%|███       | 83/266 [01:18<02:43,  1.12it/s]Loading train:  32%|███▏      | 84/266 [01:18<02:28,  1.22it/s]Loading train:  32%|███▏      | 85/266 [01:19<02:36,  1.16it/s]Loading train:  32%|███▏      | 86/266 [01:20<02:37,  1.14it/s]Loading train:  33%|███▎      | 87/266 [01:21<02:30,  1.19it/s]Loading train:  33%|███▎      | 88/266 [01:22<02:18,  1.29it/s]Loading train:  33%|███▎      | 89/266 [01:23<02:29,  1.19it/s]Loading train:  34%|███▍      | 90/266 [01:24<02:40,  1.10it/s]Loading train:  34%|███▍      | 91/266 [01:25<02:48,  1.04it/s]Loading train:  35%|███▍      | 92/266 [01:26<02:50,  1.02it/s]Loading train:  35%|███▍      | 93/266 [01:27<02:58,  1.03s/it]Loading train:  35%|███▌      | 94/266 [01:28<02:58,  1.04s/it]Loading train:  36%|███▌      | 95/266 [01:29<02:49,  1.01it/s]Loading train:  36%|███▌      | 96/266 [01:30<02:43,  1.04it/s]Loading train:  36%|███▋      | 97/266 [01:31<02:35,  1.09it/s]Loading train:  37%|███▋      | 98/266 [01:31<02:19,  1.21it/s]Loading train:  37%|███▋      | 99/266 [01:32<02:21,  1.18it/s]Loading train:  38%|███▊      | 100/266 [01:33<02:34,  1.07it/s]Loading train:  38%|███▊      | 101/266 [01:35<02:52,  1.04s/it]Loading train:  38%|███▊      | 102/266 [01:35<02:47,  1.02s/it]Loading train:  39%|███▊      | 103/266 [01:36<02:43,  1.00s/it]Loading train:  39%|███▉      | 104/266 [01:38<02:46,  1.03s/it]Loading train:  39%|███▉      | 105/266 [01:39<02:53,  1.07s/it]Loading train:  40%|███▉      | 106/266 [01:40<02:56,  1.11s/it]Loading train:  40%|████      | 107/266 [01:41<02:57,  1.12s/it]Loading train:  41%|████      | 108/266 [01:42<02:45,  1.05s/it]Loading train:  41%|████      | 109/266 [01:43<02:40,  1.02s/it]Loading train:  41%|████▏     | 110/266 [01:44<02:33,  1.01it/s]Loading train:  42%|████▏     | 111/266 [01:45<02:35,  1.00s/it]Loading train:  42%|████▏     | 112/266 [01:46<02:29,  1.03it/s]Loading train:  42%|████▏     | 113/266 [01:47<02:31,  1.01it/s]Loading train:  43%|████▎     | 114/266 [01:48<02:41,  1.06s/it]Loading train:  43%|████▎     | 115/266 [01:49<02:36,  1.04s/it]Loading train:  44%|████▎     | 116/266 [01:50<02:40,  1.07s/it]Loading train:  44%|████▍     | 117/266 [01:51<02:46,  1.12s/it]Loading train:  44%|████▍     | 118/266 [01:52<02:37,  1.07s/it]Loading train:  45%|████▍     | 119/266 [01:53<02:35,  1.06s/it]Loading train:  45%|████▌     | 120/266 [01:55<02:45,  1.13s/it]Loading train:  45%|████▌     | 121/266 [01:56<02:37,  1.09s/it]Loading train:  46%|████▌     | 122/266 [01:57<02:28,  1.03s/it]Loading train:  46%|████▌     | 123/266 [01:57<02:10,  1.10it/s]Loading train:  47%|████▋     | 124/266 [01:58<02:09,  1.10it/s]Loading train:  47%|████▋     | 125/266 [01:59<02:02,  1.15it/s]Loading train:  47%|████▋     | 126/266 [02:00<02:11,  1.06it/s]Loading train:  48%|████▊     | 127/266 [02:01<02:17,  1.01it/s]Loading train:  48%|████▊     | 128/266 [02:02<02:21,  1.03s/it]Loading train:  48%|████▊     | 129/266 [02:03<02:18,  1.01s/it]Loading train:  49%|████▉     | 130/266 [02:04<02:17,  1.01s/it]Loading train:  49%|████▉     | 131/266 [02:05<02:25,  1.08s/it]Loading train:  50%|████▉     | 132/266 [02:07<02:26,  1.09s/it]Loading train:  50%|█████     | 133/266 [02:07<02:18,  1.04s/it]Loading train:  50%|█████     | 134/266 [02:08<02:09,  1.02it/s]Loading train:  51%|█████     | 135/266 [02:09<02:09,  1.01it/s]Loading train:  51%|█████     | 136/266 [02:10<02:16,  1.05s/it]Loading train:  52%|█████▏    | 137/266 [02:12<02:18,  1.08s/it]Loading train:  52%|█████▏    | 138/266 [02:13<02:15,  1.06s/it]Loading train:  52%|█████▏    | 139/266 [02:14<02:18,  1.09s/it]Loading train:  53%|█████▎    | 140/266 [02:15<02:13,  1.06s/it]Loading train:  53%|█████▎    | 141/266 [02:15<01:59,  1.05it/s]Loading train:  53%|█████▎    | 142/266 [02:17<02:01,  1.02it/s]Loading train:  54%|█████▍    | 143/266 [02:18<02:01,  1.01it/s]Loading train:  54%|█████▍    | 144/266 [02:19<02:01,  1.00it/s]Loading train:  55%|█████▍    | 145/266 [02:20<02:05,  1.03s/it]Loading train:  55%|█████▍    | 146/266 [02:21<02:02,  1.02s/it]Loading train:  55%|█████▌    | 147/266 [02:22<02:02,  1.03s/it]Loading train:  56%|█████▌    | 148/266 [02:23<01:54,  1.03it/s]Loading train:  56%|█████▌    | 149/266 [02:24<01:59,  1.02s/it]Loading train:  56%|█████▋    | 150/266 [02:25<02:00,  1.04s/it]Loading train:  57%|█████▋    | 151/266 [02:26<01:58,  1.03s/it]Loading train:  57%|█████▋    | 152/266 [02:26<01:42,  1.11it/s]Loading train:  58%|█████▊    | 153/266 [02:27<01:40,  1.12it/s]Loading train:  58%|█████▊    | 154/266 [02:28<01:41,  1.11it/s]Loading train:  58%|█████▊    | 155/266 [02:29<01:40,  1.10it/s]Loading train:  59%|█████▊    | 156/266 [02:30<01:31,  1.20it/s]Loading train:  59%|█████▉    | 157/266 [02:31<01:29,  1.22it/s]Loading train:  59%|█████▉    | 158/266 [02:31<01:29,  1.21it/s]Loading train:  60%|█████▉    | 159/266 [02:32<01:22,  1.30it/s]Loading train:  60%|██████    | 160/266 [02:33<01:15,  1.40it/s]Loading train:  61%|██████    | 161/266 [02:33<01:20,  1.30it/s]Loading train:  61%|██████    | 162/266 [02:34<01:26,  1.20it/s]Loading train:  61%|██████▏   | 163/266 [02:35<01:29,  1.15it/s]Loading train:  62%|██████▏   | 164/266 [02:36<01:25,  1.19it/s]Loading train:  62%|██████▏   | 165/266 [02:37<01:23,  1.21it/s]Loading train:  62%|██████▏   | 166/266 [02:38<01:24,  1.18it/s]Loading train:  63%|██████▎   | 167/266 [02:39<01:31,  1.08it/s]Loading train:  63%|██████▎   | 168/266 [02:40<01:30,  1.08it/s]Loading train:  64%|██████▎   | 169/266 [02:41<01:35,  1.02it/s]Loading train:  64%|██████▍   | 170/266 [02:42<01:35,  1.00it/s]Loading train:  64%|██████▍   | 171/266 [02:43<01:36,  1.01s/it]Loading train:  65%|██████▍   | 172/266 [02:44<01:33,  1.00it/s]Loading train:  65%|██████▌   | 173/266 [02:45<01:33,  1.01s/it]Loading train:  65%|██████▌   | 174/266 [02:46<01:32,  1.01s/it]Loading train:  66%|██████▌   | 175/266 [02:47<01:39,  1.09s/it]Loading train:  66%|██████▌   | 176/266 [02:48<01:34,  1.05s/it]Loading train:  67%|██████▋   | 177/266 [02:49<01:19,  1.12it/s]Loading train:  67%|██████▋   | 178/266 [02:50<01:19,  1.11it/s]Loading train:  67%|██████▋   | 179/266 [02:51<01:23,  1.05it/s]Loading train:  68%|██████▊   | 180/266 [02:52<01:24,  1.01it/s]Loading train:  68%|██████▊   | 181/266 [02:53<01:26,  1.02s/it]Loading train:  68%|██████▊   | 182/266 [02:54<01:20,  1.04it/s]Loading train:  69%|██████▉   | 183/266 [02:55<01:24,  1.02s/it]Loading train:  69%|██████▉   | 184/266 [02:56<01:27,  1.07s/it]Loading train:  70%|██████▉   | 185/266 [02:57<01:30,  1.12s/it]Loading train:  70%|██████▉   | 186/266 [02:58<01:26,  1.08s/it]Loading train:  70%|███████   | 187/266 [03:00<01:32,  1.17s/it]Loading train:  71%|███████   | 188/266 [03:01<01:32,  1.18s/it]Loading train:  71%|███████   | 189/266 [03:02<01:31,  1.19s/it]Loading train:  71%|███████▏  | 190/266 [03:03<01:30,  1.19s/it]Loading train:  72%|███████▏  | 191/266 [03:05<01:31,  1.22s/it]Loading train:  72%|███████▏  | 192/266 [03:06<01:20,  1.08s/it]Loading train:  73%|███████▎  | 193/266 [03:06<01:11,  1.02it/s]Loading train:  73%|███████▎  | 194/266 [03:07<01:13,  1.01s/it]Loading train:  73%|███████▎  | 195/266 [03:08<01:06,  1.07it/s]Loading train:  74%|███████▎  | 196/266 [03:09<01:04,  1.09it/s]Loading train:  74%|███████▍  | 197/266 [03:10<01:04,  1.08it/s]Loading train:  74%|███████▍  | 198/266 [03:11<00:59,  1.14it/s]Loading train:  75%|███████▍  | 199/266 [03:11<00:51,  1.29it/s]Loading train:  75%|███████▌  | 200/266 [03:12<00:57,  1.14it/s]Loading train:  76%|███████▌  | 201/266 [03:13<00:58,  1.10it/s]Loading train:  76%|███████▌  | 202/266 [03:14<01:01,  1.04it/s]Loading train:  76%|███████▋  | 203/266 [03:15<01:00,  1.04it/s]Loading train:  77%|███████▋  | 204/266 [03:16<01:01,  1.01it/s]Loading train:  77%|███████▋  | 205/266 [03:17<00:59,  1.02it/s]Loading train:  77%|███████▋  | 206/266 [03:18<00:50,  1.19it/s]Loading train:  78%|███████▊  | 207/266 [03:19<00:49,  1.20it/s]Loading train:  78%|███████▊  | 208/266 [03:20<00:51,  1.12it/s]Loading train:  79%|███████▊  | 209/266 [03:21<00:53,  1.06it/s]Loading train:  79%|███████▉  | 210/266 [03:22<00:54,  1.03it/s]Loading train:  79%|███████▉  | 211/266 [03:22<00:47,  1.16it/s]Loading train:  80%|███████▉  | 212/266 [03:23<00:46,  1.16it/s]Loading train:  80%|████████  | 213/266 [03:24<00:43,  1.23it/s]Loading train:  80%|████████  | 214/266 [03:25<00:41,  1.25it/s]Loading train:  81%|████████  | 215/266 [03:26<00:43,  1.19it/s]Loading train:  81%|████████  | 216/266 [03:27<00:42,  1.18it/s]Loading train:  82%|████████▏ | 217/266 [03:27<00:41,  1.18it/s]Loading train:  82%|████████▏ | 218/266 [03:29<00:47,  1.01it/s]Loading train:  82%|████████▏ | 219/266 [03:30<00:46,  1.01it/s]Loading train:  83%|████████▎ | 220/266 [03:31<00:46,  1.01s/it]Loading train:  83%|████████▎ | 221/266 [03:32<00:45,  1.01s/it]Loading train:  83%|████████▎ | 222/266 [03:33<00:45,  1.04s/it]Loading train:  84%|████████▍ | 223/266 [03:34<00:45,  1.05s/it]Loading train:  84%|████████▍ | 224/266 [03:35<00:45,  1.07s/it]Loading train:  85%|████████▍ | 225/266 [03:36<00:46,  1.13s/it]Loading train:  85%|████████▍ | 226/266 [03:38<00:45,  1.14s/it]Loading train:  85%|████████▌ | 227/266 [03:39<00:44,  1.15s/it]Loading train:  86%|████████▌ | 228/266 [03:40<00:42,  1.12s/it]Loading train:  86%|████████▌ | 229/266 [03:41<00:41,  1.12s/it]Loading train:  86%|████████▋ | 230/266 [03:41<00:34,  1.04it/s]Loading train:  87%|████████▋ | 231/266 [03:42<00:30,  1.13it/s]Loading train:  87%|████████▋ | 232/266 [03:43<00:30,  1.13it/s]Loading train:  88%|████████▊ | 233/266 [03:44<00:28,  1.14it/s]Loading train:  88%|████████▊ | 234/266 [03:45<00:30,  1.06it/s]Loading train:  88%|████████▊ | 235/266 [03:46<00:30,  1.01it/s]Loading train:  89%|████████▊ | 236/266 [03:47<00:28,  1.07it/s]Loading train:  89%|████████▉ | 237/266 [03:48<00:25,  1.14it/s]Loading train:  89%|████████▉ | 238/266 [03:49<00:25,  1.10it/s]Loading train:  90%|████████▉ | 239/266 [03:50<00:26,  1.03it/s]Loading train:  90%|█████████ | 240/266 [03:51<00:24,  1.05it/s]Loading train:  91%|█████████ | 241/266 [03:52<00:23,  1.05it/s]Loading train:  91%|█████████ | 242/266 [03:53<00:23,  1.04it/s]Loading train:  91%|█████████▏| 243/266 [03:53<00:21,  1.07it/s]Loading train:  92%|█████████▏| 244/266 [03:54<00:20,  1.07it/s]Loading train:  92%|█████████▏| 245/266 [03:55<00:16,  1.27it/s]Loading train:  92%|█████████▏| 246/266 [03:55<00:14,  1.36it/s]Loading train:  93%|█████████▎| 247/266 [03:57<00:16,  1.17it/s]Loading train:  93%|█████████▎| 248/266 [03:58<00:16,  1.09it/s]Loading train:  94%|█████████▎| 249/266 [03:59<00:16,  1.00it/s]Loading train:  94%|█████████▍| 250/266 [04:00<00:16,  1.01s/it]Loading train:  94%|█████████▍| 251/266 [04:01<00:13,  1.07it/s]Loading train:  95%|█████████▍| 252/266 [04:01<00:11,  1.18it/s]Loading train:  95%|█████████▌| 253/266 [04:02<00:12,  1.07it/s]Loading train:  95%|█████████▌| 254/266 [04:04<00:11,  1.02it/s]Loading train:  96%|█████████▌| 255/266 [04:04<00:10,  1.03it/s]Loading train:  96%|█████████▌| 256/266 [04:05<00:09,  1.05it/s]Loading train:  97%|█████████▋| 257/266 [04:06<00:08,  1.00it/s]Loading train:  97%|█████████▋| 258/266 [04:08<00:08,  1.03s/it]Loading train:  97%|█████████▋| 259/266 [04:09<00:07,  1.00s/it]Loading train:  98%|█████████▊| 260/266 [04:10<00:06,  1.08s/it]Loading train:  98%|█████████▊| 261/266 [04:11<00:05,  1.01s/it]Loading train:  98%|█████████▊| 262/266 [04:11<00:03,  1.12it/s]Loading train:  99%|█████████▉| 263/266 [04:12<00:02,  1.12it/s]Loading train:  99%|█████████▉| 264/266 [04:13<00:01,  1.20it/s]Loading train: 100%|█████████▉| 265/266 [04:14<00:00,  1.06it/s]Loading train: 100%|██████████| 266/266 [04:15<00:00,  1.02s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/266 [00:00<00:23, 11.43it/s]concatenating: train:   2%|▏         | 4/266 [00:00<00:24, 10.75it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:27,  9.66it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:31,  8.38it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:31,  8.12it/s]concatenating: train:   3%|▎         | 9/266 [00:01<00:32,  7.99it/s]concatenating: train:  11%|█         | 28/266 [00:01<00:21, 11.22it/s]concatenating: train:  17%|█▋        | 45/266 [00:01<00:14, 15.51it/s]concatenating: train:  20%|██        | 54/266 [00:02<00:15, 13.83it/s]concatenating: train:  23%|██▎       | 61/266 [00:02<00:13, 15.38it/s]concatenating: train:  25%|██▌       | 67/266 [00:02<00:13, 14.73it/s]concatenating: train:  27%|██▋       | 71/266 [00:03<00:18, 10.81it/s]concatenating: train:  28%|██▊       | 74/266 [00:03<00:17, 11.22it/s]concatenating: train:  29%|██▉       | 77/266 [00:03<00:13, 13.80it/s]concatenating: train:  30%|███       | 80/266 [00:03<00:11, 16.47it/s]concatenating: train:  32%|███▏      | 84/266 [00:04<00:09, 19.30it/s]concatenating: train:  33%|███▎      | 88/266 [00:04<00:08, 21.26it/s]concatenating: train:  34%|███▍      | 91/266 [00:04<00:11, 15.22it/s]concatenating: train:  35%|███▌      | 94/266 [00:04<00:13, 12.82it/s]concatenating: train:  36%|███▌      | 96/266 [00:04<00:12, 13.11it/s]concatenating: train:  37%|███▋      | 99/266 [00:05<00:10, 15.40it/s]concatenating: train:  39%|███▊      | 103/266 [00:05<00:08, 18.25it/s]concatenating: train:  40%|████      | 107/266 [00:05<00:07, 21.78it/s]concatenating: train:  55%|█████▍    | 146/266 [00:05<00:04, 29.69it/s]concatenating: train:  58%|█████▊    | 154/266 [00:06<00:06, 16.34it/s]concatenating: train:  60%|██████    | 160/266 [00:06<00:05, 18.92it/s]concatenating: train:  63%|██████▎   | 168/266 [00:06<00:04, 24.45it/s]concatenating: train:  82%|████████▏ | 218/266 [00:06<00:01, 34.21it/s]concatenating: train:  89%|████████▉ | 237/266 [00:08<00:01, 26.22it/s]concatenating: train:  94%|█████████▍| 251/266 [00:09<00:00, 20.17it/s]concatenating: train:  98%|█████████▊| 262/266 [00:09<00:00, 22.57it/s]concatenating: train: 100%|██████████| 266/266 [00:09<00:00, 26.87it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:04,  1.13s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.09s/it]Loading test:  60%|██████    | 3/5 [00:02<00:01,  1.01it/s]Loading test:  80%|████████  | 4/5 [00:03<00:00,  1.16it/s]Loading test: 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  20%|██        | 1/5 [00:00<00:00,  9.64it/s]concatenating: validation:  40%|████      | 2/5 [00:00<00:00,  9.11it/s]concatenating: validation:  60%|██████    | 3/5 [00:00<00:00,  9.36it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 12.01it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<02:58,  1.48it/s]Loading trainS:   1%|          | 2/266 [00:01<03:18,  1.33it/s]Loading trainS:   1%|          | 3/266 [00:02<03:41,  1.19it/s]Loading trainS:   2%|▏         | 4/266 [00:03<03:59,  1.09it/s]Loading trainS:   2%|▏         | 5/266 [00:04<03:44,  1.16it/s]Loading trainS:   2%|▏         | 6/266 [00:05<03:58,  1.09it/s]Loading trainS:   3%|▎         | 7/266 [00:06<04:16,  1.01it/s]Loading trainS:   3%|▎         | 8/266 [00:07<04:36,  1.07s/it]Loading trainS:   3%|▎         | 9/266 [00:08<04:16,  1.00it/s]Loading trainS:   4%|▍         | 10/266 [00:09<04:26,  1.04s/it]Loading trainS:   4%|▍         | 11/266 [00:10<04:26,  1.04s/it]Loading trainS:   5%|▍         | 12/266 [00:11<04:03,  1.04it/s]Loading trainS:   5%|▍         | 13/266 [00:12<04:08,  1.02it/s]Loading trainS:   5%|▌         | 14/266 [00:13<03:57,  1.06it/s]Loading trainS:   6%|▌         | 15/266 [00:14<04:07,  1.01it/s]Loading trainS:   6%|▌         | 16/266 [00:15<04:12,  1.01s/it]Loading trainS:   6%|▋         | 17/266 [00:16<04:23,  1.06s/it]Loading trainS:   7%|▋         | 18/266 [00:18<04:26,  1.07s/it]Loading trainS:   7%|▋         | 19/266 [00:19<04:28,  1.09s/it]Loading trainS:   8%|▊         | 20/266 [00:20<04:37,  1.13s/it]Loading trainS:   8%|▊         | 21/266 [00:21<04:24,  1.08s/it]Loading trainS:   8%|▊         | 22/266 [00:21<03:49,  1.07it/s]Loading trainS:   9%|▊         | 23/266 [00:23<04:04,  1.01s/it]Loading trainS:   9%|▉         | 24/266 [00:23<03:50,  1.05it/s]Loading trainS:   9%|▉         | 25/266 [00:25<04:17,  1.07s/it]Loading trainS:  10%|▉         | 26/266 [00:26<04:26,  1.11s/it]Loading trainS:  10%|█         | 27/266 [00:27<04:11,  1.05s/it]Loading trainS:  11%|█         | 28/266 [00:28<03:46,  1.05it/s]Loading trainS:  11%|█         | 29/266 [00:29<03:58,  1.01s/it]Loading trainS:  11%|█▏        | 30/266 [00:30<04:14,  1.08s/it]Loading trainS:  12%|█▏        | 31/266 [00:31<04:02,  1.03s/it]Loading trainS:  12%|█▏        | 32/266 [00:32<04:08,  1.06s/it]Loading trainS:  12%|█▏        | 33/266 [00:33<04:12,  1.08s/it]Loading trainS:  13%|█▎        | 34/266 [00:34<04:12,  1.09s/it]Loading trainS:  13%|█▎        | 35/266 [00:35<03:49,  1.01it/s]Loading trainS:  14%|█▎        | 36/266 [00:36<03:55,  1.02s/it]Loading trainS:  14%|█▍        | 37/266 [00:37<03:53,  1.02s/it]Loading trainS:  14%|█▍        | 38/266 [00:38<04:03,  1.07s/it]Loading trainS:  15%|█▍        | 39/266 [00:39<04:01,  1.06s/it]Loading trainS:  15%|█▌        | 40/266 [00:40<03:33,  1.06it/s]Loading trainS:  15%|█▌        | 41/266 [00:41<03:03,  1.23it/s]Loading trainS:  16%|█▌        | 42/266 [00:41<02:59,  1.25it/s]Loading trainS:  16%|█▌        | 43/266 [00:42<02:53,  1.29it/s]Loading trainS:  17%|█▋        | 44/266 [00:43<02:56,  1.26it/s]Loading trainS:  17%|█▋        | 45/266 [00:44<03:06,  1.18it/s]Loading trainS:  17%|█▋        | 46/266 [00:44<02:33,  1.44it/s]Loading trainS:  18%|█▊        | 47/266 [00:45<02:44,  1.33it/s]Loading trainS:  18%|█▊        | 48/266 [00:46<02:50,  1.28it/s]Loading trainS:  18%|█▊        | 49/266 [00:46<02:26,  1.48it/s]Loading trainS:  19%|█▉        | 50/266 [00:47<02:27,  1.46it/s]Loading trainS:  19%|█▉        | 51/266 [00:48<02:19,  1.54it/s]Loading trainS:  20%|█▉        | 52/266 [00:49<02:32,  1.40it/s]Loading trainS:  20%|█▉        | 53/266 [00:49<02:25,  1.47it/s]Loading trainS:  20%|██        | 54/266 [00:50<02:16,  1.55it/s]Loading trainS:  21%|██        | 55/266 [00:50<02:08,  1.64it/s]Loading trainS:  21%|██        | 56/266 [00:51<02:27,  1.42it/s]Loading trainS:  21%|██▏       | 57/266 [00:52<02:39,  1.31it/s]Loading trainS:  22%|██▏       | 58/266 [00:53<02:59,  1.16it/s]Loading trainS:  22%|██▏       | 59/266 [00:54<03:19,  1.04it/s]Loading trainS:  23%|██▎       | 60/266 [00:55<03:28,  1.01s/it]Loading trainS:  23%|██▎       | 61/266 [00:56<03:14,  1.06it/s]Loading trainS:  23%|██▎       | 62/266 [00:57<02:52,  1.18it/s]Loading trainS:  24%|██▎       | 63/266 [00:58<02:49,  1.20it/s]Loading trainS:  24%|██▍       | 64/266 [00:58<02:37,  1.28it/s]Loading trainS:  24%|██▍       | 65/266 [00:59<02:34,  1.30it/s]Loading trainS:  25%|██▍       | 66/266 [01:00<02:24,  1.38it/s]Loading trainS:  25%|██▌       | 67/266 [01:00<02:11,  1.51it/s]Loading trainS:  26%|██▌       | 68/266 [01:01<02:10,  1.51it/s]Loading trainS:  26%|██▌       | 69/266 [01:02<02:18,  1.42it/s]Loading trainS:  26%|██▋       | 70/266 [01:02<02:13,  1.47it/s]Loading trainS:  27%|██▋       | 71/266 [01:03<02:13,  1.46it/s]Loading trainS:  27%|██▋       | 72/266 [01:04<02:23,  1.35it/s]Loading trainS:  27%|██▋       | 73/266 [01:05<02:31,  1.28it/s]Loading trainS:  28%|██▊       | 74/266 [01:05<02:15,  1.42it/s]Loading trainS:  28%|██▊       | 75/266 [01:06<02:27,  1.29it/s]Loading trainS:  29%|██▊       | 76/266 [01:07<02:47,  1.13it/s]Loading trainS:  29%|██▉       | 77/266 [01:08<02:50,  1.11it/s]Loading trainS:  29%|██▉       | 78/266 [01:10<03:09,  1.01s/it]Loading trainS:  30%|██▉       | 79/266 [01:11<03:15,  1.05s/it]Loading trainS:  30%|███       | 80/266 [01:12<03:16,  1.06s/it]Loading trainS:  30%|███       | 81/266 [01:12<02:49,  1.09it/s]Loading trainS:  31%|███       | 82/266 [01:13<02:44,  1.12it/s]Loading trainS:  31%|███       | 83/266 [01:14<02:37,  1.16it/s]Loading trainS:  32%|███▏      | 84/266 [01:15<02:40,  1.13it/s]Loading trainS:  32%|███▏      | 85/266 [01:16<02:41,  1.12it/s]Loading trainS:  32%|███▏      | 86/266 [01:17<02:36,  1.15it/s]Loading trainS:  33%|███▎      | 87/266 [01:18<02:43,  1.10it/s]Loading trainS:  33%|███▎      | 88/266 [01:18<02:33,  1.16it/s]Loading trainS:  33%|███▎      | 89/266 [01:19<02:39,  1.11it/s]Loading trainS:  34%|███▍      | 90/266 [01:20<02:43,  1.08it/s]Loading trainS:  34%|███▍      | 91/266 [01:21<02:29,  1.17it/s]Loading trainS:  35%|███▍      | 92/266 [01:22<02:15,  1.28it/s]Loading trainS:  35%|███▍      | 93/266 [01:22<02:13,  1.30it/s]Loading trainS:  35%|███▌      | 94/266 [01:23<02:18,  1.24it/s]Loading trainS:  36%|███▌      | 95/266 [01:24<02:20,  1.21it/s]Loading trainS:  36%|███▌      | 96/266 [01:25<02:18,  1.22it/s]Loading trainS:  36%|███▋      | 97/266 [01:26<02:06,  1.33it/s]Loading trainS:  37%|███▋      | 98/266 [01:26<01:55,  1.45it/s]Loading trainS:  37%|███▋      | 99/266 [01:27<01:47,  1.55it/s]Loading trainS:  38%|███▊      | 100/266 [01:27<01:37,  1.71it/s]Loading trainS:  38%|███▊      | 101/266 [01:28<01:51,  1.48it/s]Loading trainS:  38%|███▊      | 102/266 [01:29<01:47,  1.53it/s]Loading trainS:  39%|███▊      | 103/266 [01:29<01:43,  1.58it/s]Loading trainS:  39%|███▉      | 104/266 [01:30<01:54,  1.41it/s]Loading trainS:  39%|███▉      | 105/266 [01:31<01:56,  1.38it/s]Loading trainS:  40%|███▉      | 106/266 [01:32<02:01,  1.31it/s]Loading trainS:  40%|████      | 107/266 [01:33<02:13,  1.20it/s]Loading trainS:  41%|████      | 108/266 [01:33<02:10,  1.21it/s]Loading trainS:  41%|████      | 109/266 [01:34<02:07,  1.23it/s]Loading trainS:  41%|████▏     | 110/266 [01:35<02:02,  1.28it/s]Loading trainS:  42%|████▏     | 111/266 [01:36<02:10,  1.19it/s]Loading trainS:  42%|████▏     | 112/266 [01:37<02:15,  1.14it/s]Loading trainS:  42%|████▏     | 113/266 [01:38<02:22,  1.07it/s]Loading trainS:  43%|████▎     | 114/266 [01:39<02:27,  1.03it/s]Loading trainS:  43%|████▎     | 115/266 [01:40<02:31,  1.00s/it]Loading trainS:  44%|████▎     | 116/266 [01:41<02:36,  1.04s/it]Loading trainS:  44%|████▍     | 117/266 [01:42<02:30,  1.01s/it]Loading trainS:  44%|████▍     | 118/266 [01:43<02:36,  1.06s/it]Loading trainS:  45%|████▍     | 119/266 [01:44<02:24,  1.02it/s]Loading trainS:  45%|████▌     | 120/266 [01:45<02:29,  1.02s/it]Loading trainS:  45%|████▌     | 121/266 [01:46<02:22,  1.02it/s]Loading trainS:  46%|████▌     | 122/266 [01:47<02:11,  1.09it/s]Loading trainS:  46%|████▌     | 123/266 [01:48<02:12,  1.08it/s]Loading trainS:  47%|████▋     | 124/266 [01:48<01:58,  1.20it/s]Loading trainS:  47%|████▋     | 125/266 [01:49<02:01,  1.16it/s]Loading trainS:  47%|████▋     | 126/266 [01:50<02:09,  1.08it/s]Loading trainS:  48%|████▊     | 127/266 [01:52<02:16,  1.01it/s]Loading trainS:  48%|████▊     | 128/266 [01:53<02:22,  1.03s/it]Loading trainS:  48%|████▊     | 129/266 [01:54<02:21,  1.03s/it]Loading trainS:  49%|████▉     | 130/266 [01:55<02:27,  1.09s/it]Loading trainS:  49%|████▉     | 131/266 [01:56<02:30,  1.11s/it]Loading trainS:  50%|████▉     | 132/266 [01:57<02:27,  1.10s/it]Loading trainS:  50%|█████     | 133/266 [01:58<02:18,  1.04s/it]Loading trainS:  50%|█████     | 134/266 [01:59<02:18,  1.05s/it]Loading trainS:  51%|█████     | 135/266 [02:00<02:21,  1.08s/it]Loading trainS:  51%|█████     | 136/266 [02:02<02:23,  1.10s/it]Loading trainS:  52%|█████▏    | 137/266 [02:02<02:12,  1.03s/it]Loading trainS:  52%|█████▏    | 138/266 [02:03<02:01,  1.05it/s]Loading trainS:  52%|█████▏    | 139/266 [02:04<01:58,  1.07it/s]Loading trainS:  53%|█████▎    | 140/266 [02:05<02:04,  1.01it/s]Loading trainS:  53%|█████▎    | 141/266 [02:06<02:11,  1.05s/it]Loading trainS:  53%|█████▎    | 142/266 [02:08<02:16,  1.10s/it]Loading trainS:  54%|█████▍    | 143/266 [02:08<02:09,  1.05s/it]Loading trainS:  54%|█████▍    | 144/266 [02:10<02:08,  1.05s/it]Loading trainS:  55%|█████▍    | 145/266 [02:11<02:10,  1.08s/it]Loading trainS:  55%|█████▍    | 146/266 [02:11<01:55,  1.04it/s]Loading trainS:  55%|█████▌    | 147/266 [02:13<02:02,  1.03s/it]Loading trainS:  56%|█████▌    | 148/266 [02:14<01:59,  1.02s/it]Loading trainS:  56%|█████▌    | 149/266 [02:15<01:56,  1.00it/s]Loading trainS:  56%|█████▋    | 150/266 [02:16<02:02,  1.06s/it]Loading trainS:  57%|█████▋    | 151/266 [02:17<02:04,  1.08s/it]Loading trainS:  57%|█████▋    | 152/266 [02:18<01:59,  1.05s/it]Loading trainS:  58%|█████▊    | 153/266 [02:19<01:51,  1.01it/s]Loading trainS:  58%|█████▊    | 154/266 [02:20<01:50,  1.01it/s]Loading trainS:  58%|█████▊    | 155/266 [02:21<01:46,  1.04it/s]Loading trainS:  59%|█████▊    | 156/266 [02:22<01:48,  1.02it/s]Loading trainS:  59%|█████▉    | 157/266 [02:23<01:46,  1.02it/s]Loading trainS:  59%|█████▉    | 158/266 [02:23<01:44,  1.03it/s]Loading trainS:  60%|█████▉    | 159/266 [02:24<01:41,  1.05it/s]Loading trainS:  60%|██████    | 160/266 [02:25<01:25,  1.25it/s]Loading trainS:  61%|██████    | 161/266 [02:26<01:29,  1.18it/s]Loading trainS:  61%|██████    | 162/266 [02:27<01:38,  1.06it/s]Loading trainS:  61%|██████▏   | 163/266 [02:28<01:28,  1.16it/s]Loading trainS:  62%|██████▏   | 164/266 [02:29<01:28,  1.15it/s]Loading trainS:  62%|██████▏   | 165/266 [02:29<01:25,  1.18it/s]Loading trainS:  62%|██████▏   | 166/266 [02:30<01:14,  1.33it/s]Loading trainS:  63%|██████▎   | 167/266 [02:31<01:22,  1.20it/s]Loading trainS:  63%|██████▎   | 168/266 [02:32<01:24,  1.16it/s]Loading trainS:  64%|██████▎   | 169/266 [02:33<01:31,  1.06it/s]Loading trainS:  64%|██████▍   | 170/266 [02:34<01:24,  1.14it/s]Loading trainS:  64%|██████▍   | 171/266 [02:34<01:17,  1.22it/s]Loading trainS:  65%|██████▍   | 172/266 [02:35<01:20,  1.17it/s]Loading trainS:  65%|██████▌   | 173/266 [02:36<01:18,  1.19it/s]Loading trainS:  65%|██████▌   | 174/266 [02:37<01:06,  1.38it/s]Loading trainS:  66%|██████▌   | 175/266 [02:37<01:02,  1.45it/s]Loading trainS:  66%|██████▌   | 176/266 [02:38<01:02,  1.44it/s]Loading trainS:  67%|██████▋   | 177/266 [02:39<01:13,  1.22it/s]Loading trainS:  67%|██████▋   | 178/266 [02:40<01:20,  1.09it/s]Loading trainS:  67%|██████▋   | 179/266 [02:41<01:17,  1.12it/s]Loading trainS:  68%|██████▊   | 180/266 [02:42<01:23,  1.03it/s]Loading trainS:  68%|██████▊   | 181/266 [02:43<01:14,  1.14it/s]Loading trainS:  68%|██████▊   | 182/266 [02:43<01:04,  1.29it/s]Loading trainS:  69%|██████▉   | 183/266 [02:44<01:06,  1.24it/s]Loading trainS:  69%|██████▉   | 184/266 [02:45<01:13,  1.12it/s]Loading trainS:  70%|██████▉   | 185/266 [02:47<01:20,  1.01it/s]Loading trainS:  70%|██████▉   | 186/266 [02:48<01:19,  1.01it/s]Loading trainS:  70%|███████   | 187/266 [02:48<01:11,  1.11it/s]Loading trainS:  71%|███████   | 188/266 [02:49<01:05,  1.19it/s]Loading trainS:  71%|███████   | 189/266 [02:50<01:00,  1.28it/s]Loading trainS:  71%|███████▏  | 190/266 [02:50<00:55,  1.37it/s]Loading trainS:  72%|███████▏  | 191/266 [02:51<00:57,  1.30it/s]Loading trainS:  72%|███████▏  | 192/266 [02:52<00:59,  1.25it/s]Loading trainS:  73%|███████▎  | 193/266 [02:53<00:56,  1.29it/s]Loading trainS:  73%|███████▎  | 194/266 [02:54<00:59,  1.21it/s]Loading trainS:  73%|███████▎  | 195/266 [02:54<00:59,  1.19it/s]Loading trainS:  74%|███████▎  | 196/266 [02:55<00:57,  1.21it/s]Loading trainS:  74%|███████▍  | 197/266 [02:56<00:56,  1.23it/s]Loading trainS:  74%|███████▍  | 198/266 [02:56<00:48,  1.40it/s]Loading trainS:  75%|███████▍  | 199/266 [02:57<00:45,  1.49it/s]Loading trainS:  75%|███████▌  | 200/266 [02:58<00:46,  1.42it/s]Loading trainS:  76%|███████▌  | 201/266 [02:58<00:43,  1.49it/s]Loading trainS:  76%|███████▌  | 202/266 [02:59<00:39,  1.61it/s]Loading trainS:  76%|███████▋  | 203/266 [02:59<00:36,  1.71it/s]Loading trainS:  77%|███████▋  | 204/266 [03:00<00:37,  1.65it/s]Loading trainS:  77%|███████▋  | 205/266 [03:01<00:34,  1.77it/s]Loading trainS:  77%|███████▋  | 206/266 [03:01<00:38,  1.56it/s]Loading trainS:  78%|███████▊  | 207/266 [03:02<00:39,  1.50it/s]Loading trainS:  78%|███████▊  | 208/266 [03:03<00:35,  1.65it/s]Loading trainS:  79%|███████▊  | 209/266 [03:03<00:33,  1.72it/s]Loading trainS:  79%|███████▉  | 210/266 [03:04<00:35,  1.57it/s]Loading trainS:  79%|███████▉  | 211/266 [03:05<00:40,  1.35it/s]Loading trainS:  80%|███████▉  | 212/266 [03:06<00:42,  1.26it/s]Loading trainS:  80%|████████  | 213/266 [03:06<00:39,  1.33it/s]Loading trainS:  80%|████████  | 214/266 [03:07<00:36,  1.43it/s]Loading trainS:  81%|████████  | 215/266 [03:08<00:34,  1.48it/s]Loading trainS:  81%|████████  | 216/266 [03:08<00:31,  1.60it/s]Loading trainS:  82%|████████▏ | 217/266 [03:09<00:30,  1.60it/s]Loading trainS:  82%|████████▏ | 218/266 [03:09<00:31,  1.54it/s]Loading trainS:  82%|████████▏ | 219/266 [03:10<00:29,  1.60it/s]Loading trainS:  83%|████████▎ | 220/266 [03:11<00:29,  1.55it/s]Loading trainS:  83%|████████▎ | 221/266 [03:11<00:28,  1.59it/s]Loading trainS:  83%|████████▎ | 222/266 [03:12<00:31,  1.39it/s]Loading trainS:  84%|████████▍ | 223/266 [03:13<00:28,  1.53it/s]Loading trainS:  84%|████████▍ | 224/266 [03:13<00:26,  1.57it/s]Loading trainS:  85%|████████▍ | 225/266 [03:14<00:26,  1.56it/s]Loading trainS:  85%|████████▍ | 226/266 [03:14<00:23,  1.68it/s]Loading trainS:  85%|████████▌ | 227/266 [03:15<00:22,  1.75it/s]Loading trainS:  86%|████████▌ | 228/266 [03:16<00:21,  1.78it/s]Loading trainS:  86%|████████▌ | 229/266 [03:16<00:19,  1.91it/s]Loading trainS:  86%|████████▋ | 230/266 [03:16<00:18,  1.99it/s]Loading trainS:  87%|████████▋ | 231/266 [03:17<00:17,  1.95it/s]Loading trainS:  87%|████████▋ | 232/266 [03:17<00:17,  1.98it/s]Loading trainS:  88%|████████▊ | 233/266 [03:18<00:16,  1.99it/s]Loading trainS:  88%|████████▊ | 234/266 [03:18<00:15,  2.05it/s]Loading trainS:  88%|████████▊ | 235/266 [03:19<00:14,  2.12it/s]Loading trainS:  89%|████████▊ | 236/266 [03:19<00:14,  2.12it/s]Loading trainS:  89%|████████▉ | 237/266 [03:20<00:14,  2.00it/s]Loading trainS:  89%|████████▉ | 238/266 [03:20<00:13,  2.04it/s]Loading trainS:  90%|████████▉ | 239/266 [03:21<00:13,  2.07it/s]Loading trainS:  90%|█████████ | 240/266 [03:21<00:12,  2.11it/s]Loading trainS:  91%|█████████ | 241/266 [03:22<00:11,  2.11it/s]Loading trainS:  91%|█████████ | 242/266 [03:22<00:11,  2.08it/s]Loading trainS:  91%|█████████▏| 243/266 [03:23<00:10,  2.20it/s]Loading trainS:  92%|█████████▏| 244/266 [03:23<00:10,  2.15it/s]Loading trainS:  92%|█████████▏| 245/266 [03:24<00:09,  2.20it/s]Loading trainS:  92%|█████████▏| 246/266 [03:24<00:09,  2.20it/s]Loading trainS:  93%|█████████▎| 247/266 [03:24<00:08,  2.16it/s]Loading trainS:  93%|█████████▎| 248/266 [03:25<00:07,  2.27it/s]Loading trainS:  94%|█████████▎| 249/266 [03:25<00:07,  2.28it/s]Loading trainS:  94%|█████████▍| 250/266 [03:26<00:07,  2.20it/s]Loading trainS:  94%|█████████▍| 251/266 [03:26<00:06,  2.20it/s]Loading trainS:  95%|█████████▍| 252/266 [03:27<00:06,  2.31it/s]Loading trainS:  95%|█████████▌| 253/266 [03:27<00:05,  2.39it/s]Loading trainS:  95%|█████████▌| 254/266 [03:27<00:04,  2.45it/s]Loading trainS:  96%|█████████▌| 255/266 [03:28<00:04,  2.36it/s]Loading trainS:  96%|█████████▌| 256/266 [03:28<00:04,  2.31it/s]Loading trainS:  97%|█████████▋| 257/266 [03:29<00:03,  2.28it/s]Loading trainS:  97%|█████████▋| 258/266 [03:29<00:03,  2.36it/s]Loading trainS:  97%|█████████▋| 259/266 [03:30<00:02,  2.42it/s]Loading trainS:  98%|█████████▊| 260/266 [03:30<00:02,  2.48it/s]Loading trainS:  98%|█████████▊| 261/266 [03:30<00:01,  2.51it/s]Loading trainS:  98%|█████████▊| 262/266 [03:31<00:01,  2.54it/s]Loading trainS:  99%|█████████▉| 263/266 [03:31<00:01,  2.44it/s]Loading trainS:  99%|█████████▉| 264/266 [03:32<00:00,  2.46it/s]Loading trainS: 100%|█████████▉| 265/266 [03:32<00:00,  2.49it/s]Loading trainS: 100%|██████████| 266/266 [03:32<00:00,  2.52it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:01,  2.61it/s]Loading testS:  40%|████      | 2/5 [00:00<00:01,  2.64it/s]Loading testS:  60%|██████    | 3/5 [00:01<00:00,  2.66it/s]Loading testS:  80%|████████  | 4/5 [00:01<00:00,  2.69it/s]Loading testS: 100%|██████████| 5/5 [00:01<00:00,  2.68it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.69it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.80it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.40it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  8.20it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:03,  8.92it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  7.81it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02, 10.07it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02, 10.19it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:01<00:02,  8.59it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:01, 10.74it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 11.29it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 11.61it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  9.30it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:02<00:00, 11.41it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:02<00:00, 11.22it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 11.45it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.88it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 12.54it/s]---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 4,742
Non-trainable params: 218,420
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 320 samples
Epoch 1/300
 - 33s - loss: 0.6583 - acc: 0.9650 - mDice: 0.3231 - val_loss: 1.7966 - val_acc: 0.9764 - val_mDice: 0.2613

Epoch 00001: val_mDice improved from -inf to 0.26125, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 28s - loss: 0.2750 - acc: 0.9766 - mDice: 0.5900 - val_loss: 0.9586 - val_acc: 0.9844 - val_mDice: 0.5795

Epoch 00002: val_mDice improved from 0.26125 to 0.57953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 28s - loss: 0.2080 - acc: 0.9810 - mDice: 0.6721 - val_loss: 0.9024 - val_acc: 0.9854 - val_mDice: 0.6028

Epoch 00003: val_mDice improved from 0.57953 to 0.60283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 28s - loss: 0.1779 - acc: 0.9834 - mDice: 0.7126 - val_loss: 1.1595 - val_acc: 0.9573 - val_mDice: 0.4589

Epoch 00004: val_mDice did not improve from 0.60283
Epoch 5/300
 - 28s - loss: 0.1569 - acc: 0.9854 - mDice: 0.7411 - val_loss: 1.0994 - val_acc: 0.9841 - val_mDice: 0.5002

Epoch 00005: val_mDice did not improve from 0.60283
Epoch 6/300
 - 28s - loss: 0.1465 - acc: 0.9861 - mDice: 0.7543 - val_loss: 0.8450 - val_acc: 0.9884 - val_mDice: 0.7036

Epoch 00006: val_mDice improved from 0.60283 to 0.70365, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 28s - loss: 0.1350 - acc: 0.9870 - mDice: 0.7704 - val_loss: 0.8403 - val_acc: 0.9876 - val_mDice: 0.7134

Epoch 00007: val_mDice improved from 0.70365 to 0.71344, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 28s - loss: 0.1301 - acc: 0.9874 - mDice: 0.7777 - val_loss: 0.8417 - val_acc: 0.9877 - val_mDice: 0.6416

Epoch 00008: val_mDice did not improve from 0.71344
Epoch 9/300
 - 28s - loss: 0.1244 - acc: 0.9878 - mDice: 0.7860 - val_loss: 0.8767 - val_acc: 0.9874 - val_mDice: 0.7008

Epoch 00009: val_mDice did not improve from 0.71344
Epoch 10/300
 - 29s - loss: 0.1195 - acc: 0.9881 - mDice: 0.7933 - val_loss: 0.8537 - val_acc: 0.9875 - val_mDice: 0.7029

Epoch 00010: val_mDice did not improve from 0.71344
Epoch 11/300
 - 29s - loss: 0.1168 - acc: 0.9883 - mDice: 0.7974 - val_loss: 0.8398 - val_acc: 0.9884 - val_mDice: 0.7153

Epoch 00011: val_mDice improved from 0.71344 to 0.71533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 29s - loss: 0.1149 - acc: 0.9885 - mDice: 0.8005 - val_loss: 0.6711 - val_acc: 0.9866 - val_mDice: 0.5872

Epoch 00012: val_mDice did not improve from 0.71533
Epoch 13/300
 - 29s - loss: 0.1124 - acc: 0.9888 - mDice: 0.8044 - val_loss: 0.8548 - val_acc: 0.9878 - val_mDice: 0.6527

Epoch 00013: val_mDice did not improve from 0.71533
Epoch 14/300
 - 29s - loss: 0.1093 - acc: 0.9890 - mDice: 0.8091 - val_loss: 0.8916 - val_acc: 0.9848 - val_mDice: 0.6777

Epoch 00014: val_mDice did not improve from 0.71533
Epoch 15/300
 - 30s - loss: 0.1065 - acc: 0.9892 - mDice: 0.8134 - val_loss: 1.0096 - val_acc: 0.9730 - val_mDice: 0.5821

Epoch 00015: val_mDice did not improve from 0.71533
Epoch 16/300
 - 29s - loss: 0.1069 - acc: 0.9892 - mDice: 0.8129 - val_loss: 0.9930 - val_acc: 0.9753 - val_mDice: 0.5967

Epoch 00016: val_mDice did not improve from 0.71533
Epoch 17/300
 - 30s - loss: 0.1048 - acc: 0.9894 - mDice: 0.8163 - val_loss: 0.6631 - val_acc: 0.9856 - val_mDice: 0.5521

Epoch 00017: val_mDice did not improve from 0.71533
Epoch 18/300
 - 30s - loss: 0.1014 - acc: 0.9896 - mDice: 0.8215 - val_loss: 0.6823 - val_acc: 0.9895 - val_mDice: 0.6965

Epoch 00018: val_mDice did not improve from 0.71533
Epoch 19/300
 - 29s - loss: 0.1005 - acc: 0.9897 - mDice: 0.8229 - val_loss: 0.8969 - val_acc: 0.9838 - val_mDice: 0.6758

Epoch 00019: val_mDice did not improve from 0.71533
Epoch 20/300
 - 30s - loss: 0.0993 - acc: 0.9898 - mDice: 0.8248 - val_loss: 0.7556 - val_acc: 0.9899 - val_mDice: 0.7378

Epoch 00020: val_mDice improved from 0.71533 to 0.73783, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 29s - loss: 0.0996 - acc: 0.9897 - mDice: 0.8244 - val_loss: 0.4680 - val_acc: 0.9899 - val_mDice: 0.7172

Epoch 00021: val_mDice did not improve from 0.73783
Epoch 22/300
 - 29s - loss: 0.0968 - acc: 0.9900 - mDice: 0.8288 - val_loss: 0.3992 - val_acc: 0.9904 - val_mDice: 0.7314

Epoch 00022: val_mDice did not improve from 0.73783
Epoch 23/300
 - 30s - loss: 0.0960 - acc: 0.9901 - mDice: 0.8301 - val_loss: 0.4812 - val_acc: 0.9874 - val_mDice: 0.6121

Epoch 00023: val_mDice did not improve from 0.73783
Epoch 24/300
 - 30s - loss: 0.0944 - acc: 0.9902 - mDice: 0.8326 - val_loss: 0.7249 - val_acc: 0.9871 - val_mDice: 0.6043

Epoch 00024: val_mDice did not improve from 0.73783
Epoch 25/300
 - 29s - loss: 0.0938 - acc: 0.9903 - mDice: 0.8336 - val_loss: 0.6448 - val_acc: 0.9901 - val_mDice: 0.7224

Epoch 00025: val_mDice did not improve from 0.73783
Epoch 26/300
 - 29s - loss: 0.0925 - acc: 0.9904 - mDice: 0.8358 - val_loss: 0.8242 - val_acc: 0.9832 - val_mDice: 0.4595

Epoch 00026: val_mDice did not improve from 0.73783
Epoch 27/300
 - 29s - loss: 0.0948 - acc: 0.9902 - mDice: 0.8320 - val_loss: 0.7007 - val_acc: 0.9899 - val_mDice: 0.7120

Epoch 00027: val_mDice did not improve from 0.73783
Epoch 28/300
 - 29s - loss: 0.0925 - acc: 0.9904 - mDice: 0.8358 - val_loss: 0.7340 - val_acc: 0.9903 - val_mDice: 0.7450

Epoch 00028: val_mDice improved from 0.73783 to 0.74505, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 29s - loss: 0.0910 - acc: 0.9905 - mDice: 0.8381 - val_loss: 0.8532 - val_acc: 0.9868 - val_mDice: 0.7101

Epoch 00029: val_mDice did not improve from 0.74505
Epoch 30/300
 - 30s - loss: 0.0917 - acc: 0.9904 - mDice: 0.8371 - val_loss: 0.6709 - val_acc: 0.9909 - val_mDice: 0.7632

Epoch 00030: val_mDice improved from 0.74505 to 0.76323, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 29s - loss: 0.0922 - acc: 0.9904 - mDice: 0.8362 - val_loss: 0.4357 - val_acc: 0.9878 - val_mDice: 0.6326

Epoch 00031: val_mDice did not improve from 0.76323
Epoch 32/300
 - 29s - loss: 0.0909 - acc: 0.9905 - mDice: 0.8383 - val_loss: 0.7419 - val_acc: 0.9907 - val_mDice: 0.7620

Epoch 00032: val_mDice did not improve from 0.76323
Epoch 33/300
 - 29s - loss: 0.0904 - acc: 0.9906 - mDice: 0.8390 - val_loss: 0.9086 - val_acc: 0.9833 - val_mDice: 0.6702

Epoch 00033: val_mDice did not improve from 0.76323
Epoch 34/300
 - 30s - loss: 0.0891 - acc: 0.9907 - mDice: 0.8413 - val_loss: 0.7981 - val_acc: 0.9898 - val_mDice: 0.7345

Epoch 00034: val_mDice did not improve from 0.76323
Epoch 35/300
 - 30s - loss: 0.0890 - acc: 0.9907 - mDice: 0.8413 - val_loss: 0.7596 - val_acc: 0.9826 - val_mDice: 0.4333

Epoch 00035: val_mDice did not improve from 0.76323
Epoch 36/300
 - 29s - loss: 0.0875 - acc: 0.9908 - mDice: 0.8438 - val_loss: 0.8610 - val_acc: 0.9859 - val_mDice: 0.7035

Epoch 00036: val_mDice did not improve from 0.76323
Epoch 37/300
 - 30s - loss: 0.0883 - acc: 0.9908 - mDice: 0.8426 - val_loss: 0.9488 - val_acc: 0.9797 - val_mDice: 0.6363

Epoch 00037: val_mDice did not improve from 0.76323
Epoch 38/300
 - 30s - loss: 0.0877 - acc: 0.9908 - mDice: 0.8436 - val_loss: 0.8379 - val_acc: 0.9882 - val_mDice: 0.7223

Epoch 00038: val_mDice did not improve from 0.76323
Epoch 39/300
 - 29s - loss: 0.0876 - acc: 0.9908 - mDice: 0.8436 - val_loss: 0.5940 - val_acc: 0.9886 - val_mDice: 0.6555

Epoch 00039: val_mDice did not improve from 0.76323
Epoch 40/300
 - 30s - loss: 0.0866 - acc: 0.9909 - mDice: 0.8453 - val_loss: 0.7633 - val_acc: 0.9896 - val_mDice: 0.7007

Epoch 00040: val_mDice did not improve from 0.76323
Epoch 41/300
 - 30s - loss: 0.0858 - acc: 0.9909 - mDice: 0.8466 - val_loss: 0.8432 - val_acc: 0.9865 - val_mDice: 0.7083

Epoch 00041: val_mDice did not improve from 0.76323
Epoch 42/300
 - 29s - loss: 0.0860 - acc: 0.9909 - mDice: 0.8463 - val_loss: 0.8532 - val_acc: 0.9869 - val_mDice: 0.7085

Epoch 00042: val_mDice did not improve from 0.76323
Epoch 43/300
 - 29s - loss: 0.0844 - acc: 0.9911 - mDice: 0.8490 - val_loss: 0.7107 - val_acc: 0.9903 - val_mDice: 0.7161

Epoch 00043: val_mDice did not improve from 0.76323
Epoch 44/300
 - 29s - loss: 0.0842 - acc: 0.9911 - mDice: 0.8492 - val_loss: 0.7844 - val_acc: 0.9898 - val_mDice: 0.7516

Epoch 00044: val_mDice did not improve from 0.76323
Epoch 45/300
 - 29s - loss: 0.0860 - acc: 0.9909 - mDice: 0.8463 - val_loss: 1.0070 - val_acc: 0.9728 - val_mDice: 0.5860

Epoch 00045: val_mDice did not improve from 0.76323
Epoch 46/300
 - 29s - loss: 0.0850 - acc: 0.9911 - mDice: 0.8480 - val_loss: 0.1958 - val_acc: 0.9891 - val_mDice: 0.6749

Epoch 00046: val_mDice did not improve from 0.76323
Epoch 47/300
 - 29s - loss: 0.0844 - acc: 0.9911 - mDice: 0.8489 - val_loss: 0.9339 - val_acc: 0.9815 - val_mDice: 0.6591

Epoch 00047: val_mDice did not improve from 0.76323
Epoch 48/300
 - 29s - loss: 0.0841 - acc: 0.9911 - mDice: 0.8495 - val_loss: 0.7826 - val_acc: 0.9894 - val_mDice: 0.6903

Epoch 00048: val_mDice did not improve from 0.76323
Epoch 49/300
 - 29s - loss: 0.0830 - acc: 0.9912 - mDice: 0.8512 - val_loss: 0.8992 - val_acc: 0.9840 - val_mDice: 0.6819

Epoch 00049: val_mDice did not improve from 0.76323
Epoch 50/300
 - 29s - loss: 0.0831 - acc: 0.9912 - mDice: 0.8511 - val_loss: 0.7570 - val_acc: 0.9906 - val_mDice: 0.7612

Epoch 00050: val_mDice did not improve from 0.76323
Epoch 51/300
 - 29s - loss: 0.0827 - acc: 0.9912 - mDice: 0.8517 - val_loss: 0.7586 - val_acc: 0.9909 - val_mDice: 0.7588

Epoch 00051: val_mDice did not improve from 0.76323
Epoch 52/300
 - 30s - loss: 0.0825 - acc: 0.9913 - mDice: 0.8521 - val_loss: 0.6833 - val_acc: 0.9909 - val_mDice: 0.7377

Epoch 00052: val_mDice did not improve from 0.76323
Epoch 53/300
 - 29s - loss: 0.0813 - acc: 0.9914 - mDice: 0.8540 - val_loss: 0.8704 - val_acc: 0.9877 - val_mDice: 0.6152

Epoch 00053: val_mDice did not improve from 0.76323
Epoch 54/300
 - 29s - loss: 0.0819 - acc: 0.9913 - mDice: 0.8531 - val_loss: 0.8025 - val_acc: 0.9880 - val_mDice: 0.7255

Epoch 00054: val_mDice did not improve from 0.76323
Epoch 55/300
 - 29s - loss: 0.0819 - acc: 0.9913 - mDice: 0.8531 - val_loss: 0.8317 - val_acc: 0.9873 - val_mDice: 0.7172

Epoch 00055: val_mDice did not improve from 0.76323
Epoch 56/300
 - 30s - loss: 0.0813 - acc: 0.9913 - mDice: 0.8541 - val_loss: 0.7557 - val_acc: 0.9910 - val_mDice: 0.7562

Epoch 00056: val_mDice did not improve from 0.76323
Epoch 57/300
 - 29s - loss: 0.0812 - acc: 0.9914 - mDice: 0.8542 - val_loss: 0.9134 - val_acc: 0.9822 - val_mDice: 0.6571

Epoch 00057: val_mDice did not improve from 0.76323
Epoch 58/300
 - 29s - loss: 0.0811 - acc: 0.9914 - mDice: 0.8545 - val_loss: 0.7257 - val_acc: 0.9911 - val_mDice: 0.7459

Epoch 00058: val_mDice did not improve from 0.76323
Epoch 59/300
 - 29s - loss: 0.0801 - acc: 0.9915 - mDice: 0.8560 - val_loss: 0.8401 - val_acc: 0.9867 - val_mDice: 0.7070

Epoch 00059: val_mDice did not improve from 0.76323
Epoch 60/300
 - 29s - loss: 0.0810 - acc: 0.9914 - mDice: 0.8546 - val_loss: 0.9215 - val_acc: 0.9868 - val_mDice: 0.5878

Epoch 00060: val_mDice did not improve from 0.76323
Epoch 61/300
 - 30s - loss: 0.0799 - acc: 0.9915 - mDice: 0.8563 - val_loss: 0.8287 - val_acc: 0.9895 - val_mDice: 0.7483

Epoch 00061: val_mDice did not improve from 0.76323
Epoch 62/300
 - 30s - loss: 0.0808 - acc: 0.9914 - mDice: 0.8548 - val_loss: 0.7570 - val_acc: 0.9911 - val_mDice: 0.7658

Epoch 00062: val_mDice improved from 0.76323 to 0.76578, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 29s - loss: 0.0802 - acc: 0.9915 - mDice: 0.8559 - val_loss: 0.7280 - val_acc: 0.9912 - val_mDice: 0.7614

Epoch 00063: val_mDice did not improve from 0.76578
Epoch 64/300
 - 29s - loss: 0.0804 - acc: 0.9914 - mDice: 0.8555 - val_loss: 0.7353 - val_acc: 0.9897 - val_mDice: 0.7028

Epoch 00064: val_mDice did not improve from 0.76578
Epoch 65/300
 - 29s - loss: 0.0804 - acc: 0.9915 - mDice: 0.8555 - val_loss: 0.3987 - val_acc: 0.9858 - val_mDice: 0.5425

Epoch 00065: val_mDice did not improve from 0.76578
Epoch 66/300
 - 29s - loss: 0.0790 - acc: 0.9916 - mDice: 0.8579 - val_loss: 0.7434 - val_acc: 0.9913 - val_mDice: 0.7686

Epoch 00066: val_mDice improved from 0.76578 to 0.76861, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 29s - loss: 0.0797 - acc: 0.9915 - mDice: 0.8567 - val_loss: 0.6875 - val_acc: 0.9914 - val_mDice: 0.7652

Epoch 00067: val_mDice did not improve from 0.76861
Epoch 68/300
 - 30s - loss: 0.0787 - acc: 0.9916 - mDice: 0.8583 - val_loss: 0.7466 - val_acc: 0.9904 - val_mDice: 0.7324

Epoch 00068: val_mDice did not improve from 0.76861
Epoch 69/300
 - 30s - loss: 0.0792 - acc: 0.9915 - mDice: 0.8575 - val_loss: 0.3176 - val_acc: 0.9863 - val_mDice: 0.5672

Epoch 00069: val_mDice did not improve from 0.76861
Epoch 70/300
 - 29s - loss: 0.0794 - acc: 0.9915 - mDice: 0.8573 - val_loss: 0.7763 - val_acc: 0.9904 - val_mDice: 0.7643

Epoch 00070: val_mDice did not improve from 0.76861
Epoch 71/300
 - 30s - loss: 0.0784 - acc: 0.9916 - mDice: 0.8588 - val_loss: 0.6283 - val_acc: 0.9915 - val_mDice: 0.7653

Epoch 00071: val_mDice did not improve from 0.76861
Epoch 72/300
 - 29s - loss: 0.0788 - acc: 0.9916 - mDice: 0.8583 - val_loss: 0.7135 - val_acc: 0.9908 - val_mDice: 0.7514

Epoch 00072: val_mDice did not improve from 0.76861
Epoch 73/300
 - 29s - loss: 0.0784 - acc: 0.9916 - mDice: 0.8589 - val_loss: 0.7424 - val_acc: 0.9915 - val_mDice: 0.7707

Epoch 00073: val_mDice improved from 0.76861 to 0.77066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 29s - loss: 0.0780 - acc: 0.9917 - mDice: 0.8595 - val_loss: 0.7713 - val_acc: 0.9899 - val_mDice: 0.7609

Epoch 00074: val_mDice did not improve from 0.77066
Epoch 75/300
 - 30s - loss: 0.0781 - acc: 0.9917 - mDice: 0.8594 - val_loss: 0.2577 - val_acc: 0.9874 - val_mDice: 0.6074

Epoch 00075: val_mDice did not improve from 0.77066
Epoch 76/300
 - 29s - loss: 0.0780 - acc: 0.9917 - mDice: 0.8595 - val_loss: 0.8262 - val_acc: 0.9882 - val_mDice: 0.7315

Epoch 00076: val_mDice did not improve from 0.77066
Epoch 77/300
 - 29s - loss: 0.0783 - acc: 0.9916 - mDice: 0.8591 - val_loss: 0.3025 - val_acc: 0.9868 - val_mDice: 0.5835

Epoch 00077: val_mDice did not improve from 0.77066
Epoch 78/300
 - 29s - loss: 0.0775 - acc: 0.9917 - mDice: 0.8603 - val_loss: 0.7257 - val_acc: 0.9904 - val_mDice: 0.7197

Epoch 00078: val_mDice did not improve from 0.77066
Epoch 79/300
 - 29s - loss: 0.0786 - acc: 0.9916 - mDice: 0.8585 - val_loss: 0.6309 - val_acc: 0.9913 - val_mDice: 0.7593

Epoch 00079: val_mDice did not improve from 0.77066
Epoch 80/300
 - 29s - loss: 0.0773 - acc: 0.9917 - mDice: 0.8607 - val_loss: 0.8062 - val_acc: 0.9899 - val_mDice: 0.7514

Epoch 00080: val_mDice did not improve from 0.77066
Epoch 81/300
 - 29s - loss: 0.0768 - acc: 0.9917 - mDice: 0.8615 - val_loss: 0.7877 - val_acc: 0.9911 - val_mDice: 0.7601

Epoch 00081: val_mDice did not improve from 0.77066
Epoch 82/300
 - 30s - loss: 0.0781 - acc: 0.9916 - mDice: 0.8593 - val_loss: 0.8365 - val_acc: 0.9874 - val_mDice: 0.7246

Epoch 00082: val_mDice did not improve from 0.77066
Epoch 83/300
 - 29s - loss: 0.0777 - acc: 0.9917 - mDice: 0.8601 - val_loss: 0.7075 - val_acc: 0.9915 - val_mDice: 0.7779

Epoch 00083: val_mDice improved from 0.77066 to 0.77793, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 29s - loss: 0.0764 - acc: 0.9918 - mDice: 0.8622 - val_loss: 0.7283 - val_acc: 0.9912 - val_mDice: 0.7629

Epoch 00084: val_mDice did not improve from 0.77793
Epoch 85/300
 - 29s - loss: 0.0768 - acc: 0.9918 - mDice: 0.8615 - val_loss: 0.0914 - val_acc: 0.9909 - val_mDice: 0.7462

Epoch 00085: val_mDice did not improve from 0.77793
Epoch 86/300
 - 29s - loss: 0.0769 - acc: 0.9918 - mDice: 0.8614 - val_loss: 0.7744 - val_acc: 0.9908 - val_mDice: 0.7692

Epoch 00086: val_mDice did not improve from 0.77793
Epoch 87/300
 - 29s - loss: 0.0767 - acc: 0.9918 - mDice: 0.8618 - val_loss: 0.6272 - val_acc: 0.9917 - val_mDice: 0.7710

Epoch 00087: val_mDice did not improve from 0.77793
Epoch 88/300
 - 29s - loss: 0.0756 - acc: 0.9918 - mDice: 0.8635 - val_loss: 0.7335 - val_acc: 0.9914 - val_mDice: 0.7679

Epoch 00088: val_mDice did not improve from 0.77793
Epoch 89/300
 - 29s - loss: 0.0762 - acc: 0.9918 - mDice: 0.8626 - val_loss: 0.7502 - val_acc: 0.9893 - val_mDice: 0.6877

Epoch 00089: val_mDice did not improve from 0.77793
Epoch 90/300
 - 29s - loss: 0.0755 - acc: 0.9919 - mDice: 0.8637 - val_loss: 0.2256 - val_acc: 0.9884 - val_mDice: 0.6471

Epoch 00090: val_mDice did not improve from 0.77793
Epoch 91/300
 - 30s - loss: 0.0769 - acc: 0.9918 - mDice: 0.8614 - val_loss: 0.6863 - val_acc: 0.9916 - val_mDice: 0.7766

Epoch 00091: val_mDice did not improve from 0.77793
Epoch 92/300
 - 29s - loss: 0.0754 - acc: 0.9919 - mDice: 0.8638 - val_loss: 0.7509 - val_acc: 0.9912 - val_mDice: 0.7647

Epoch 00092: val_mDice did not improve from 0.77793
Epoch 93/300
 - 29s - loss: 0.0757 - acc: 0.9919 - mDice: 0.8634 - val_loss: 0.6796 - val_acc: 0.9895 - val_mDice: 0.6862

Epoch 00093: val_mDice did not improve from 0.77793
Epoch 94/300
 - 29s - loss: 0.0752 - acc: 0.9919 - mDice: 0.8642 - val_loss: 0.1299 - val_acc: 0.9902 - val_mDice: 0.7086

Epoch 00094: val_mDice did not improve from 0.77793
Epoch 95/300
 - 30s - loss: 0.0752 - acc: 0.9919 - mDice: 0.8642 - val_loss: 0.1717 - val_acc: 0.9890 - val_mDice: 0.6684

Epoch 00095: val_mDice did not improve from 0.77793
Epoch 96/300
 - 29s - loss: 0.0762 - acc: 0.9918 - mDice: 0.8626 - val_loss: 0.6423 - val_acc: 0.9915 - val_mDice: 0.7646

Epoch 00096: val_mDice did not improve from 0.77793
Epoch 97/300
 - 29s - loss: 0.0751 - acc: 0.9919 - mDice: 0.8644 - val_loss: 0.7431 - val_acc: 0.9910 - val_mDice: 0.7626

Epoch 00097: val_mDice did not improve from 0.77793
Epoch 98/300
 - 30s - loss: 0.0757 - acc: 0.9919 - mDice: 0.8634 - val_loss: 0.7544 - val_acc: 0.9899 - val_mDice: 0.7084

Epoch 00098: val_mDice did not improve from 0.77793
Epoch 99/300
 - 29s - loss: 0.0755 - acc: 0.9919 - mDice: 0.8638 - val_loss: 0.7802 - val_acc: 0.9913 - val_mDice: 0.7684

Epoch 00099: val_mDice did not improve from 0.77793
Epoch 100/300
 - 29s - loss: 0.0752 - acc: 0.9919 - mDice: 0.8642 - val_loss: 0.8875 - val_acc: 0.9850 - val_mDice: 0.6915

Epoch 00100: val_mDice did not improve from 0.77793
Epoch 101/300
 - 30s - loss: 0.0750 - acc: 0.9919 - mDice: 0.8646 - val_loss: 0.3257 - val_acc: 0.9868 - val_mDice: 0.5791

Epoch 00101: val_mDice did not improve from 0.77793
Epoch 102/300
 - 29s - loss: 0.0754 - acc: 0.9919 - mDice: 0.8639 - val_loss: 0.8688 - val_acc: 0.9861 - val_mDice: 0.7064

Epoch 00102: val_mDice did not improve from 0.77793
Epoch 103/300
 - 29s - loss: 0.0747 - acc: 0.9919 - mDice: 0.8650 - val_loss: 0.3095 - val_acc: 0.9892 - val_mDice: 0.6826

Epoch 00103: val_mDice did not improve from 0.77793
Epoch 104/300
 - 30s - loss: 0.0756 - acc: 0.9919 - mDice: 0.8636 - val_loss: 0.6822 - val_acc: 0.9914 - val_mDice: 0.7712

Epoch 00104: val_mDice did not improve from 0.77793
Epoch 105/300
 - 29s - loss: 0.0745 - acc: 0.9920 - mDice: 0.8653 - val_loss: 0.7692 - val_acc: 0.9895 - val_mDice: 0.6906

Epoch 00105: val_mDice did not improve from 0.77793
Epoch 106/300
 - 29s - loss: 0.0748 - acc: 0.9919 - mDice: 0.8649 - val_loss: 0.8945 - val_acc: 0.9845 - val_mDice: 0.6891

Epoch 00106: val_mDice did not improve from 0.77793
Epoch 107/300
 - 28s - loss: 0.0743 - acc: 0.9920 - mDice: 0.8658 - val_loss: 0.7201 - val_acc: 0.9910 - val_mDice: 0.7539

Epoch 00107: val_mDice did not improve from 0.77793
Epoch 108/300
 - 28s - loss: 0.0743 - acc: 0.9920 - mDice: 0.8657 - val_loss: 0.2994 - val_acc: 0.9902 - val_mDice: 0.7165

Epoch 00108: val_mDice did not improve from 0.77793
Epoch 109/300
 - 28s - loss: 0.0744 - acc: 0.9920 - mDice: 0.8657 - val_loss: 0.7598 - val_acc: 0.9901 - val_mDice: 0.7577

Epoch 00109: val_mDice did not improve from 0.77793
Epoch 110/300
 - 28s - loss: 0.0744 - acc: 0.9920 - mDice: 0.8656 - val_loss: 0.9174 - val_acc: 0.9828 - val_mDice: 0.6719

Epoch 00110: val_mDice did not improve from 0.77793
Epoch 111/300
 - 28s - loss: 0.0736 - acc: 0.9921 - mDice: 0.8669 - val_loss: 0.7811 - val_acc: 0.9905 - val_mDice: 0.7648

Epoch 00111: val_mDice did not improve from 0.77793
Epoch 112/300
 - 28s - loss: 0.0739 - acc: 0.9920 - mDice: 0.8664 - val_loss: 0.6735 - val_acc: 0.9915 - val_mDice: 0.7714

Epoch 00112: val_mDice did not improve from 0.77793
Epoch 113/300
 - 28s - loss: 0.0741 - acc: 0.9920 - mDice: 0.8661 - val_loss: 0.6584 - val_acc: 0.9913 - val_mDice: 0.7602

Epoch 00113: val_mDice did not improve from 0.77793
Epoch 114/300
 - 28s - loss: 0.0734 - acc: 0.9921 - mDice: 0.8672 - val_loss: 0.7719 - val_acc: 0.9912 - val_mDice: 0.7746

Epoch 00114: val_mDice did not improve from 0.77793
Epoch 115/300
 - 28s - loss: 0.0730 - acc: 0.9921 - mDice: 0.8680 - val_loss: 0.7473 - val_acc: 0.9897 - val_mDice: 0.6956

Epoch 00115: val_mDice did not improve from 0.77793
Epoch 116/300
 - 28s - loss: 0.0741 - acc: 0.9920 - mDice: 0.8662 - val_loss: 0.5175 - val_acc: 0.9915 - val_mDice: 0.7629

Epoch 00116: val_mDice did not improve from 0.77793
Epoch 117/300
 - 28s - loss: 0.0734 - acc: 0.9920 - mDice: 0.8672 - val_loss: 0.7741 - val_acc: 0.9904 - val_mDice: 0.7648

Epoch 00117: val_mDice did not improve from 0.77793
Epoch 118/300
 - 28s - loss: 0.0737 - acc: 0.9921 - mDice: 0.8668 - val_loss: 0.6986 - val_acc: 0.9904 - val_mDice: 0.7257

Epoch 00118: val_mDice did not improve from 0.77793
Epoch 119/300
 - 28s - loss: 0.0740 - acc: 0.9920 - mDice: 0.8663 - val_loss: 0.7381 - val_acc: 0.9909 - val_mDice: 0.7669

Epoch 00119: val_mDice did not improve from 0.77793
Epoch 120/300
 - 28s - loss: 0.0735 - acc: 0.9921 - mDice: 0.8672 - val_loss: 0.2922 - val_acc: 0.9875 - val_mDice: 0.6121

Epoch 00120: val_mDice did not improve from 0.77793
Epoch 121/300
 - 29s - loss: 0.0731 - acc: 0.9921 - mDice: 0.8677 - val_loss: 0.2487 - val_acc: 0.9877 - val_mDice: 0.6171

Epoch 00121: val_mDice did not improve from 0.77793
Epoch 122/300
 - 29s - loss: 0.0736 - acc: 0.9920 - mDice: 0.8669 - val_loss: 0.7914 - val_acc: 0.9903 - val_mDice: 0.7647

Epoch 00122: val_mDice did not improve from 0.77793
Epoch 123/300
 - 29s - loss: 0.0732 - acc: 0.9921 - mDice: 0.8676 - val_loss: 0.8313 - val_acc: 0.9872 - val_mDice: 0.7231

Epoch 00123: val_mDice did not improve from 0.77793
Restoring model weights from the end of the best epoch
Epoch 00123: early stopping
{'val_loss': [1.7965911589562893, 0.9585933433845639, 0.9023877137806267, 1.159480266738683, 1.0994476771447808, 0.8449914851225913, 0.8403240912593901, 0.8417330044321716, 0.8766618710942566, 0.8536800148431212, 0.8398046584334224, 0.6711073787882924, 0.8548264752607793, 0.891647269949317, 1.0095784054137766, 0.9929514192044735, 0.6630663147661835, 0.6822949526831508, 0.8969351621344686, 0.7556318859569728, 0.46795875346288085, 0.39920960809104145, 0.48116436740383506, 0.7248512206133455, 0.6448338446207345, 0.8241921400185674, 0.7006711408030242, 0.7340308732818812, 0.8532213459257036, 0.6709393276832998, 0.43565467768348753, 0.7419279736932367, 0.9086286791134626, 0.7980737020261586, 0.759592923335731, 0.8609673730097711, 0.9488452312070876, 0.8378851860761642, 0.593958706362173, 0.7633245796896517, 0.8432296414393932, 0.8531807682011276, 0.7106737268622965, 0.7844200110994279, 1.0070284293033183, 0.19575980794616044, 0.9339490965940058, 0.7825913296546787, 0.8991581723093987, 0.75700269988738, 0.7586021402385086, 0.6833208496682346, 0.8703911404591054, 0.8024580287747085, 0.831749468576163, 0.7557181583251804, 0.9133896825369447, 0.7256960773374885, 0.8401305950246751, 0.9214865178801119, 0.8287002595607191, 0.7569601451978087, 0.728026078781113, 0.7353267059661448, 0.39867795328609645, 0.7434027907438576, 0.6874924427829683, 0.7466198860201985, 0.3175763660110533, 0.7763135377317667, 0.6282802254427224, 0.7135164607316256, 0.7423609711695462, 0.7712785673793405, 0.25773123279213905, 0.8262000344693661, 0.3024545556399971, 0.7257000650279224, 0.6308774631470442, 0.8061873770784587, 0.7877351390197873, 0.8364691694732755, 0.7074821314308792, 0.7282556283753365, 0.09140545758418739, 0.7743825255893171, 0.6272276644594967, 0.7335199608933181, 0.750158186769113, 0.2255964153446257, 0.686274545500055, 0.7508593217935413, 0.6796310315839946, 0.12987509509548545, 0.17171134240925312, 0.6422933721914887, 0.7430872817058116, 0.7544342526234686, 0.780161052942276, 0.8874502775724977, 0.3256819078233093, 0.8687678563874215, 0.3095128848217428, 0.6821833555586636, 0.7692059790715575, 0.8945129073690623, 0.7201330817770213, 0.2994045387022197, 0.759756722021848, 0.9173694830387831, 0.7810799139551818, 0.6734679241199046, 0.6584186980035156, 0.7719039379153401, 0.747283429838717, 0.5174847361631691, 0.7741032235790044, 0.6985808147583157, 0.7381370440125465, 0.29216909664683044, 0.24865788989700377, 0.791379886912182, 0.8313388640526682], 'val_acc': [0.9764011148363352, 0.9843739960342646, 0.9854234047234058, 0.957292415201664, 0.984061187133193, 0.9883912578225136, 0.9876187425106764, 0.9876606445759535, 0.9874082077294588, 0.9874770529568195, 0.9883930031210184, 0.9865800514817238, 0.9878472127020359, 0.9848130159080029, 0.973013948649168, 0.9752576854079962, 0.9856334459036589, 0.989474331960082, 0.9838107600808144, 0.9898754190653563, 0.9898846540600061, 0.990374306216836, 0.9874129425734282, 0.9871218390762806, 0.9900717437267303, 0.9831851571798325, 0.9898524861782789, 0.9903181828558445, 0.9868030566722155, 0.990943793207407, 0.9878157954663038, 0.990723030641675, 0.9833014085888863, 0.989752197638154, 0.9826007336378098, 0.9858629107475281, 0.9796830117702484, 0.9881555382162333, 0.988646199926734, 0.9896167479455471, 0.986535158008337, 0.9868706651031971, 0.9902595710009336, 0.9897621870040894, 0.972846832126379, 0.9891138914972544, 0.9815423432737589, 0.9894281923770905, 0.9839733988046646, 0.9905628822743893, 0.9909048825502396, 0.9908846765756607, 0.9876953177154064, 0.9880405254662037, 0.9873386118561029, 0.9909527767449617, 0.9821537211537361, 0.9910954460501671, 0.9866783507168293, 0.9868422169238329, 0.9894536212086678, 0.9911211412400007, 0.9912049621343613, 0.9897210393100977, 0.9857940711081028, 0.9912822768092155, 0.9913785606622696, 0.9904339239001274, 0.9862530454993248, 0.9903603475540876, 0.9915299694985151, 0.9907906223088503, 0.9915155004709959, 0.9899143539369106, 0.9873593170195818, 0.9881545472890139, 0.9867651332169771, 0.9903551042079926, 0.991273045539856, 0.9898622203618288, 0.9911016710102558, 0.9874161835759878, 0.9914508853107691, 0.9912246633321047, 0.990914361551404, 0.9908070880919695, 0.9916611723601818, 0.9913847912102938, 0.9893052130937576, 0.988433662801981, 0.9915983285754919, 0.9912219103425741, 0.9895154815167189, 0.9901560600847006, 0.9890128616243601, 0.9915282242000103, 0.9909522738307714, 0.9899495057761669, 0.9913089703768492, 0.9849654324352741, 0.9868000764399767, 0.9860539864748716, 0.9892186559736729, 0.9913900382816792, 0.9895267132669687, 0.9844957310706377, 0.9910438116639853, 0.990167785435915, 0.9901121519505978, 0.9828489124774933, 0.9905244782567024, 0.9914870709180832, 0.9913396518677473, 0.9911553077399731, 0.9897409733384848, 0.9915364403277636, 0.9904027506709099, 0.990380797535181, 0.9908953998237848, 0.9875149689614773, 0.9877023044973612, 0.9903234355151653, 0.9871911928057671], 'val_mDice': [0.2612549497644068, 0.5795293743722141, 0.6028294912539423, 0.4588902834802866, 0.5002255609724671, 0.7036469224840403, 0.7134379539638758, 0.6416189623996615, 0.7008226215839386, 0.7028686907142401, 0.7153318542987108, 0.587155045941472, 0.6527255289256573, 0.6777136214077473, 0.5821255408227444, 0.5966904712840915, 0.5521266756113619, 0.6965434504672885, 0.675809714011848, 0.7378268335014582, 0.7171535398811102, 0.7314342632889748, 0.6121369893662632, 0.6043199705891311, 0.7224023994058371, 0.45953676832141355, 0.7119869440793991, 0.745046129450202, 0.7100736815482378, 0.7632285449653864, 0.632625580765307, 0.7620330527424812, 0.6702371034771204, 0.7345386315137148, 0.4333281615981832, 0.7035409659147263, 0.6362651940435171, 0.7222517672926188, 0.6554789212532341, 0.7006708160042763, 0.708322087302804, 0.7084891758859158, 0.7161000883206725, 0.7516452763229609, 0.5859542172402143, 0.6749188946560025, 0.6591406185179949, 0.6902742851525545, 0.6818779651075602, 0.7611538842320442, 0.7588251307606697, 0.7376617565751076, 0.6151515687815845, 0.7255311030894518, 0.7171656377613544, 0.7562433555722237, 0.6570608587935567, 0.7458972278982401, 0.7069588080048561, 0.5877872742712498, 0.7483157515525818, 0.7657785527408123, 0.7613964565098286, 0.7028028927743435, 0.5425298155751079, 0.7686065882444382, 0.7651559077203274, 0.732352128252387, 0.5672287871129811, 0.7643444836139679, 0.7652596291154623, 0.751379057765007, 0.7706620078533888, 0.7608568388968706, 0.6073908740654588, 0.7314588315784931, 0.5835161590948701, 0.719682895578444, 0.7592667732387781, 0.751411747187376, 0.7600528467446566, 0.7245636451989412, 0.7779338862746954, 0.762922503054142, 0.7461860869079828, 0.7691836636513472, 0.7709969952702522, 0.7678721379488707, 0.6877114111557603, 0.6470569595694542, 0.7765985000878572, 0.76472582295537, 0.6861692387610674, 0.7086095726117492, 0.6684093549847603, 0.764628779143095, 0.7626241724938154, 0.7084035314619541, 0.7683579530566931, 0.6914791231974959, 0.5790659680496901, 0.7063822355121374, 0.6826219521462917, 0.7712356001138687, 0.6906204437837005, 0.6890974566340446, 0.7538608089089394, 0.7164827780798078, 0.7577165961265564, 0.6719063827767968, 0.7647633645683527, 0.7713922392576933, 0.7602375280112028, 0.7746183220297098, 0.6956095062196255, 0.7628612406551838, 0.7648217771202326, 0.7257137540727854, 0.7668840698897839, 0.6121153035201132, 0.6170801888220012, 0.7647078204900026, 0.7231122348457575], 'loss': [0.658289465839001, 0.27504481540035675, 0.20797643433696678, 0.17791317128042344, 0.15689484395049544, 0.146463480165705, 0.13504339432562631, 0.1300947257152373, 0.12436986471356118, 0.11951860981379096, 0.11679608355789937, 0.11488363806833937, 0.1124024514450392, 0.1092813341564884, 0.10653577781664005, 0.10685658057565346, 0.10477617248742162, 0.10137774335551347, 0.10052459421761847, 0.09933708261460866, 0.09955577299334702, 0.09677672119292008, 0.09602004685485156, 0.09443271705047364, 0.09381080018768705, 0.09248477031998453, 0.09484515802038176, 0.09247111298677965, 0.09104080163128522, 0.09166726050676612, 0.0922213706167453, 0.09088485171442541, 0.0904470325980097, 0.08905847582057598, 0.089016086670238, 0.08752687570204078, 0.08828772527553194, 0.08766918348117518, 0.08763911863078054, 0.0866301924784909, 0.08581903430384266, 0.08600589896489397, 0.0843772997124638, 0.08420365158716492, 0.08597841230840568, 0.0849737833691513, 0.08442858862460692, 0.08406818514546756, 0.08301510443871786, 0.0830573032690082, 0.0827404297230391, 0.08250048418999051, 0.08130206341475536, 0.08185540473293562, 0.08185002499996778, 0.08125789712521968, 0.08124397775872225, 0.08111980683609031, 0.08013270331101507, 0.08096109030449503, 0.07993289473041004, 0.08082244176414559, 0.08016825154245391, 0.08044832937685166, 0.08041792744892919, 0.07898523809049164, 0.07967963710209143, 0.07872983880676057, 0.07920240899010154, 0.07935493287862047, 0.07841180977546283, 0.07875838601665033, 0.0784048246289222, 0.0780437776414833, 0.0781037261331069, 0.07801816404641429, 0.07829625827864209, 0.07752472452799437, 0.07862804678716809, 0.07730623439432188, 0.07678679158439118, 0.07812850058435411, 0.07768264861239212, 0.07638249959200673, 0.07684454704604872, 0.07687093385504876, 0.07665712490549718, 0.0756333669052662, 0.07619190537607169, 0.07548970199036753, 0.07687749215092783, 0.07541403737154187, 0.07567739969073334, 0.07524695533766097, 0.07521691386782363, 0.07620300353612165, 0.07512682308720438, 0.0757326910964105, 0.07548990785864375, 0.07521849037847606, 0.07499533753607959, 0.0753886054214893, 0.0747218717907182, 0.07560521245575802, 0.07454465826741977, 0.07477397233138686, 0.07427714912836957, 0.07430448189751743, 0.07436767005125404, 0.07437118329735352, 0.07364352961821023, 0.0738810449687151, 0.07407286398712663, 0.07340875126796274, 0.07297387648474689, 0.07405228448413426, 0.073435673212512, 0.07368865600339458, 0.07395515003048668, 0.07346237272169583, 0.07314950507253225, 0.07361238292351657, 0.07322458088513976], 'acc': [0.965036821445688, 0.9765912194250352, 0.9809648833084261, 0.9834410964410812, 0.9853510339251514, 0.9861127101019089, 0.9869528616764162, 0.9873782621729702, 0.9877653954349579, 0.9881356464967476, 0.9883418154866075, 0.9885273229595232, 0.9887513311964385, 0.9890239258060143, 0.9892119245827232, 0.9891954311458051, 0.9893611482215479, 0.9896293330707796, 0.9897039867699513, 0.9898191795845515, 0.9897195347526894, 0.9899934345772559, 0.9900687843980143, 0.9901801939485407, 0.9902561848222053, 0.9903660206148208, 0.9901803882747795, 0.9903530999870961, 0.9905049871924697, 0.9904458015184169, 0.9904414002413643, 0.9905248004841419, 0.9905581087258353, 0.9906889223409776, 0.9906843354772078, 0.9908080965824156, 0.9907766356553518, 0.990795790823658, 0.9908248251415482, 0.9908846412176481, 0.9909482193876812, 0.9909249164576978, 0.9910770985699842, 0.9910759190808359, 0.9909390656045329, 0.9910557841512264, 0.9911115659478512, 0.991129322107126, 0.9912325683282174, 0.99121058005911, 0.9912424618197578, 0.9912556431857747, 0.991365028477524, 0.9913232822832059, 0.991320239219209, 0.9913422344134524, 0.9913698840642676, 0.9914084617803774, 0.9914702243044274, 0.9913817585117105, 0.9914681126573602, 0.9913706942843936, 0.9914789546293651, 0.9914433752408742, 0.9914663832431695, 0.9915833009636968, 0.9915130829179524, 0.9915804579807267, 0.9915309819081554, 0.9915175514796255, 0.9916255902410233, 0.9916088473884102, 0.9916262773530082, 0.9916504172410674, 0.9916535659895633, 0.9916611747402645, 0.9916399344135358, 0.9917006958537891, 0.991622283675605, 0.9917213540051171, 0.9917449652906272, 0.9916455906083169, 0.9917213575507867, 0.9917870725476813, 0.9917924416055521, 0.9917625290211505, 0.9917784120073382, 0.9918468777097669, 0.9918365505385153, 0.9918697316416384, 0.9917628093091012, 0.9918890591835026, 0.9918525518891259, 0.9918951160389469, 0.9918745869790773, 0.9918072767632756, 0.9919095838666863, 0.9918570398550298, 0.9918955002524122, 0.9918973911815478, 0.9919384530339352, 0.9919002337767103, 0.9919371969527839, 0.9918760981960857, 0.9919892552146322, 0.9919357436576122, 0.9919758728049948, 0.9919679780877314, 0.9920043672185139, 0.991978331041264, 0.9920534071668011, 0.992029662274644, 0.9920041538273761, 0.9920537786449416, 0.9920782114177737, 0.9920014172491192, 0.9920485306245962, 0.9920628130179714, 0.9920068309256407, 0.9920524901749296, 0.9920786343704495, 0.9920474694236945, 0.9920766855818034], 'mDice': [0.3231368299941125, 0.5900342446407042, 0.6720690720135623, 0.7125960130357459, 0.741053096616782, 0.7543491750446584, 0.7703695410674937, 0.7776865407649657, 0.7860369786050547, 0.7932999717714175, 0.7974364088846718, 0.8004779701824594, 0.8044059228035608, 0.8091130083870691, 0.8134447644573202, 0.8129495679743547, 0.8162539238518118, 0.8215427935379673, 0.8228657523403188, 0.8248390515769998, 0.8243656537113697, 0.8288311487195993, 0.8300524294909609, 0.8325654843120303, 0.8335962272402273, 0.8357886615164593, 0.8319888100871173, 0.8357685117532085, 0.838139488546915, 0.8371199734315953, 0.8362492718624018, 0.8383308268111384, 0.839045505338365, 0.8413359978216457, 0.8413302521610337, 0.843788204333557, 0.8426267976016828, 0.8435900828779346, 0.8436290543018712, 0.8452742585906504, 0.8465882271884888, 0.8463025717033359, 0.8489612592655814, 0.8492077407250217, 0.8463362813605323, 0.8480174275409667, 0.8489045294040003, 0.8494629039572716, 0.8512458980353546, 0.8511383584377532, 0.8517004497599433, 0.8520683735951848, 0.8540438211267524, 0.8531369167821837, 0.8531358055125494, 0.8541217637530212, 0.8541537880398912, 0.8545121609077396, 0.8559762684119153, 0.8546262354721684, 0.8562996779721387, 0.8548334032259046, 0.855922813447106, 0.8554831322469099, 0.8555123994709709, 0.8578723770089857, 0.8567054631666563, 0.8582776128255464, 0.8575106703460846, 0.8572732730929522, 0.8588315536298028, 0.85829434347607, 0.8588747083067382, 0.8594633286196249, 0.8593686327045632, 0.859539155651499, 0.8590726346397312, 0.8603359505844071, 0.8585488058637906, 0.8607194414049598, 0.8615353045195421, 0.8593453576205513, 0.8600923838107943, 0.8622182322335157, 0.8614935390387205, 0.8614087791909348, 0.8617807482819919, 0.8634595080398164, 0.862556214966646, 0.8637386336342665, 0.8614050589386727, 0.8638490688527961, 0.8634160044883843, 0.8641597111285793, 0.8641705270274475, 0.8625589014069189, 0.8643555860839628, 0.8633503318317707, 0.863790303486017, 0.8641843665445309, 0.8646106632298483, 0.8639454651657291, 0.865003154150933, 0.8635746306127631, 0.8653482587353837, 0.8649397441786009, 0.8657665030527187, 0.8657194056673805, 0.8656663304285483, 0.8656244786051324, 0.8668528424539784, 0.8664258171335613, 0.8660924734797255, 0.8671934402223053, 0.8679572811619225, 0.8661802603321179, 0.8671527544463485, 0.8668044893475959, 0.8663385528311491, 0.8671648087187955, 0.8676649248667873, 0.8669242793928944, 0.8675704302654265]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.21it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:01,  1.51it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.84it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.20it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  2.62it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:16,  3.47it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:18,  3.36it/s]predicting train subjects:   1%|          | 3/266 [00:00<01:16,  3.44it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:17,  3.38it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:16,  3.41it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<01:13,  3.52it/s]predicting train subjects:   3%|▎         | 7/266 [00:02<01:15,  3.43it/s]predicting train subjects:   3%|▎         | 8/266 [00:02<01:12,  3.58it/s]predicting train subjects:   3%|▎         | 9/266 [00:02<01:09,  3.69it/s]predicting train subjects:   4%|▍         | 10/266 [00:02<01:08,  3.76it/s]predicting train subjects:   4%|▍         | 11/266 [00:03<01:10,  3.60it/s]predicting train subjects:   5%|▍         | 12/266 [00:03<01:09,  3.65it/s]predicting train subjects:   5%|▍         | 13/266 [00:03<01:08,  3.68it/s]predicting train subjects:   5%|▌         | 14/266 [00:03<01:11,  3.53it/s]predicting train subjects:   6%|▌         | 15/266 [00:04<01:14,  3.36it/s]predicting train subjects:   6%|▌         | 16/266 [00:04<01:12,  3.46it/s]predicting train subjects:   6%|▋         | 17/266 [00:04<01:09,  3.58it/s]predicting train subjects:   7%|▋         | 18/266 [00:05<01:11,  3.45it/s]predicting train subjects:   7%|▋         | 19/266 [00:05<01:14,  3.30it/s]predicting train subjects:   8%|▊         | 20/266 [00:05<01:12,  3.41it/s]predicting train subjects:   8%|▊         | 21/266 [00:06<01:12,  3.36it/s]predicting train subjects:   8%|▊         | 22/266 [00:06<01:09,  3.52it/s]predicting train subjects:   9%|▊         | 23/266 [00:06<01:09,  3.50it/s]predicting train subjects:   9%|▉         | 24/266 [00:06<01:08,  3.54it/s]predicting train subjects:   9%|▉         | 25/266 [00:07<01:05,  3.70it/s]predicting train subjects:  10%|▉         | 26/266 [00:07<01:03,  3.75it/s]predicting train subjects:  10%|█         | 27/266 [00:07<01:01,  3.88it/s]predicting train subjects:  11%|█         | 28/266 [00:07<01:00,  3.91it/s]predicting train subjects:  11%|█         | 29/266 [00:08<01:01,  3.85it/s]predicting train subjects:  11%|█▏        | 30/266 [00:08<01:00,  3.88it/s]predicting train subjects:  12%|█▏        | 31/266 [00:08<01:01,  3.81it/s]predicting train subjects:  12%|█▏        | 32/266 [00:08<01:01,  3.80it/s]predicting train subjects:  12%|█▏        | 33/266 [00:09<01:01,  3.77it/s]predicting train subjects:  13%|█▎        | 34/266 [00:09<01:01,  3.76it/s]predicting train subjects:  13%|█▎        | 35/266 [00:09<01:03,  3.62it/s]predicting train subjects:  14%|█▎        | 36/266 [00:09<01:01,  3.74it/s]predicting train subjects:  14%|█▍        | 37/266 [00:10<00:59,  3.84it/s]predicting train subjects:  14%|█▍        | 38/266 [00:10<01:00,  3.76it/s]predicting train subjects:  15%|█▍        | 39/266 [00:10<01:00,  3.73it/s]predicting train subjects:  15%|█▌        | 40/266 [00:11<01:01,  3.68it/s]predicting train subjects:  15%|█▌        | 41/266 [00:11<00:59,  3.76it/s]predicting train subjects:  16%|█▌        | 42/266 [00:11<00:55,  4.01it/s]predicting train subjects:  16%|█▌        | 43/266 [00:11<00:53,  4.17it/s]predicting train subjects:  17%|█▋        | 44/266 [00:11<00:50,  4.36it/s]predicting train subjects:  17%|█▋        | 45/266 [00:12<00:51,  4.32it/s]predicting train subjects:  17%|█▋        | 46/266 [00:12<00:51,  4.25it/s]predicting train subjects:  18%|█▊        | 47/266 [00:12<00:54,  4.03it/s]predicting train subjects:  18%|█▊        | 48/266 [00:12<00:53,  4.04it/s]predicting train subjects:  18%|█▊        | 49/266 [00:13<00:51,  4.24it/s]predicting train subjects:  19%|█▉        | 50/266 [00:13<00:48,  4.44it/s]predicting train subjects:  19%|█▉        | 51/266 [00:13<00:47,  4.57it/s]predicting train subjects:  20%|█▉        | 52/266 [00:13<00:46,  4.60it/s]predicting train subjects:  20%|█▉        | 53/266 [00:13<00:45,  4.68it/s]predicting train subjects:  20%|██        | 54/266 [00:14<00:44,  4.72it/s]predicting train subjects:  21%|██        | 55/266 [00:14<00:45,  4.68it/s]predicting train subjects:  21%|██        | 56/266 [00:14<00:44,  4.71it/s]predicting train subjects:  21%|██▏       | 57/266 [00:14<00:45,  4.64it/s]predicting train subjects:  22%|██▏       | 58/266 [00:15<00:44,  4.70it/s]predicting train subjects:  22%|██▏       | 59/266 [00:15<00:43,  4.78it/s]predicting train subjects:  23%|██▎       | 60/266 [00:15<00:46,  4.43it/s]predicting train subjects:  23%|██▎       | 61/266 [00:15<00:46,  4.40it/s]predicting train subjects:  23%|██▎       | 62/266 [00:15<00:44,  4.58it/s]predicting train subjects:  24%|██▎       | 63/266 [00:16<00:42,  4.75it/s]predicting train subjects:  24%|██▍       | 64/266 [00:16<00:43,  4.64it/s]predicting train subjects:  24%|██▍       | 65/266 [00:16<00:42,  4.70it/s]predicting train subjects:  25%|██▍       | 66/266 [00:16<00:42,  4.75it/s]predicting train subjects:  25%|██▌       | 67/266 [00:17<00:44,  4.49it/s]predicting train subjects:  26%|██▌       | 68/266 [00:17<00:45,  4.35it/s]predicting train subjects:  26%|██▌       | 69/266 [00:17<00:46,  4.25it/s]predicting train subjects:  26%|██▋       | 70/266 [00:17<00:46,  4.21it/s]predicting train subjects:  27%|██▋       | 71/266 [00:17<00:44,  4.39it/s]predicting train subjects:  27%|██▋       | 72/266 [00:18<00:43,  4.48it/s]predicting train subjects:  27%|██▋       | 73/266 [00:18<00:41,  4.65it/s]predicting train subjects:  28%|██▊       | 74/266 [00:18<00:42,  4.53it/s]predicting train subjects:  28%|██▊       | 75/266 [00:18<00:41,  4.61it/s]predicting train subjects:  29%|██▊       | 76/266 [00:19<00:40,  4.67it/s]predicting train subjects:  29%|██▉       | 77/266 [00:19<00:41,  4.51it/s]predicting train subjects:  29%|██▉       | 78/266 [00:19<00:45,  4.17it/s]predicting train subjects:  30%|██▉       | 79/266 [00:19<00:46,  4.05it/s]predicting train subjects:  30%|███       | 80/266 [00:20<00:46,  3.97it/s]predicting train subjects:  30%|███       | 81/266 [00:20<00:46,  3.99it/s]predicting train subjects:  31%|███       | 82/266 [00:20<00:45,  4.02it/s]predicting train subjects:  31%|███       | 83/266 [00:20<00:48,  3.78it/s]predicting train subjects:  32%|███▏      | 84/266 [00:21<00:46,  3.88it/s]predicting train subjects:  32%|███▏      | 85/266 [00:21<00:45,  3.94it/s]predicting train subjects:  32%|███▏      | 86/266 [00:21<00:45,  3.95it/s]predicting train subjects:  33%|███▎      | 87/266 [00:21<00:44,  4.02it/s]predicting train subjects:  33%|███▎      | 88/266 [00:22<00:45,  3.95it/s]predicting train subjects:  33%|███▎      | 89/266 [00:22<00:47,  3.76it/s]predicting train subjects:  34%|███▍      | 90/266 [00:22<00:46,  3.76it/s]predicting train subjects:  34%|███▍      | 91/266 [00:22<00:44,  3.90it/s]predicting train subjects:  35%|███▍      | 92/266 [00:23<00:44,  3.91it/s]predicting train subjects:  35%|███▍      | 93/266 [00:23<00:44,  3.92it/s]predicting train subjects:  35%|███▌      | 94/266 [00:23<00:46,  3.73it/s]predicting train subjects:  36%|███▌      | 95/266 [00:23<00:45,  3.80it/s]predicting train subjects:  36%|███▌      | 96/266 [00:24<00:46,  3.68it/s]predicting train subjects:  36%|███▋      | 97/266 [00:24<00:47,  3.53it/s]predicting train subjects:  37%|███▋      | 98/266 [00:24<00:48,  3.45it/s]predicting train subjects:  37%|███▋      | 99/266 [00:25<00:48,  3.46it/s]predicting train subjects:  38%|███▊      | 100/266 [00:25<00:43,  3.80it/s]predicting train subjects:  38%|███▊      | 101/266 [00:25<00:40,  4.06it/s]predicting train subjects:  38%|███▊      | 102/266 [00:25<00:39,  4.19it/s]predicting train subjects:  39%|███▊      | 103/266 [00:26<00:38,  4.24it/s]predicting train subjects:  39%|███▉      | 104/266 [00:26<00:36,  4.38it/s]predicting train subjects:  39%|███▉      | 105/266 [00:26<00:36,  4.44it/s]predicting train subjects:  40%|███▉      | 106/266 [00:26<00:35,  4.45it/s]predicting train subjects:  40%|████      | 107/266 [00:26<00:34,  4.56it/s]predicting train subjects:  41%|████      | 108/266 [00:27<00:34,  4.62it/s]predicting train subjects:  41%|████      | 109/266 [00:27<00:34,  4.54it/s]predicting train subjects:  41%|████▏     | 110/266 [00:27<00:34,  4.58it/s]predicting train subjects:  42%|████▏     | 111/266 [00:27<00:34,  4.47it/s]predicting train subjects:  42%|████▏     | 112/266 [00:28<00:37,  4.16it/s]predicting train subjects:  42%|████▏     | 113/266 [00:28<00:36,  4.24it/s]predicting train subjects:  43%|████▎     | 114/266 [00:28<00:35,  4.29it/s]predicting train subjects:  43%|████▎     | 115/266 [00:28<00:34,  4.43it/s]predicting train subjects:  44%|████▎     | 116/266 [00:28<00:33,  4.53it/s]predicting train subjects:  44%|████▍     | 117/266 [00:29<00:32,  4.55it/s]predicting train subjects:  44%|████▍     | 118/266 [00:29<00:32,  4.61it/s]predicting train subjects:  45%|████▍     | 119/266 [00:29<00:35,  4.11it/s]predicting train subjects:  45%|████▌     | 120/266 [00:29<00:36,  4.02it/s]predicting train subjects:  45%|████▌     | 121/266 [00:30<00:39,  3.67it/s]predicting train subjects:  46%|████▌     | 122/266 [00:30<00:37,  3.79it/s]predicting train subjects:  46%|████▌     | 123/266 [00:30<00:36,  3.87it/s]predicting train subjects:  47%|████▋     | 124/266 [00:30<00:36,  3.94it/s]predicting train subjects:  47%|████▋     | 125/266 [00:31<00:37,  3.78it/s]predicting train subjects:  47%|████▋     | 126/266 [00:31<00:36,  3.78it/s]predicting train subjects:  48%|████▊     | 127/266 [00:31<00:35,  3.86it/s]predicting train subjects:  48%|████▊     | 128/266 [00:32<00:35,  3.93it/s]predicting train subjects:  48%|████▊     | 129/266 [00:32<00:36,  3.73it/s]predicting train subjects:  49%|████▉     | 130/266 [00:32<00:36,  3.72it/s]predicting train subjects:  49%|████▉     | 131/266 [00:32<00:35,  3.80it/s]predicting train subjects:  50%|████▉     | 132/266 [00:33<00:37,  3.61it/s]predicting train subjects:  50%|█████     | 133/266 [00:33<00:35,  3.75it/s]predicting train subjects:  50%|█████     | 134/266 [00:33<00:35,  3.71it/s]predicting train subjects:  51%|█████     | 135/266 [00:33<00:34,  3.83it/s]predicting train subjects:  51%|█████     | 136/266 [00:34<00:33,  3.92it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:34<00:32,  3.93it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:34<00:34,  3.73it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:34<00:33,  3.84it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:35<00:31,  3.97it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:35<00:30,  4.03it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:35<00:31,  3.95it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:35<00:30,  3.97it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:36<00:30,  4.00it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:36<00:29,  4.10it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:36<00:29,  4.12it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:36<00:28,  4.19it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:37<00:28,  4.19it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:37<00:28,  4.08it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:37<00:29,  3.90it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:37<00:28,  4.06it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:38<00:28,  4.00it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:38<00:27,  4.07it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:38<00:26,  4.18it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:38<00:24,  4.48it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:38<00:23,  4.78it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:39<00:22,  4.78it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:39<00:21,  5.01it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:39<00:20,  5.17it/s]predicting train subjects:  60%|██████    | 160/266 [00:39<00:19,  5.33it/s]predicting train subjects:  61%|██████    | 161/266 [00:39<00:19,  5.43it/s]predicting train subjects:  61%|██████    | 162/266 [00:40<00:18,  5.52it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:40<00:18,  5.48it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:40<00:18,  5.52it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:40<00:18,  5.58it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:40<00:17,  5.61it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:40<00:17,  5.60it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:41<00:17,  5.64it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:41<00:17,  5.67it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:41<00:16,  5.69it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:41<00:16,  5.73it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:41<00:16,  5.54it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:42<00:18,  5.06it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:42<00:18,  4.98it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:42<00:18,  5.01it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:42<00:18,  5.00it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:42<00:18,  4.71it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:43<00:19,  4.44it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:43<00:18,  4.59it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:43<00:18,  4.72it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:43<00:17,  4.79it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:43<00:17,  4.73it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:44<00:17,  4.79it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:44<00:17,  4.73it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:44<00:16,  4.81it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:44<00:16,  4.89it/s]predicting train subjects:  70%|███████   | 187/266 [00:44<00:16,  4.89it/s]predicting train subjects:  71%|███████   | 188/266 [00:45<00:15,  4.89it/s]predicting train subjects:  71%|███████   | 189/266 [00:45<00:15,  4.95it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:45<00:15,  4.93it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:45<00:15,  4.92it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:46<00:16,  4.46it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:46<00:16,  4.47it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:46<00:17,  4.14it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:46<00:16,  4.33it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:47<00:15,  4.38it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:47<00:15,  4.54it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:47<00:14,  4.67it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:47<00:14,  4.75it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:47<00:13,  4.73it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:48<00:14,  4.42it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:48<00:14,  4.53it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:48<00:13,  4.53it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:48<00:13,  4.63it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:48<00:12,  4.72it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:49<00:12,  4.63it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:49<00:12,  4.75it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:49<00:12,  4.70it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:49<00:12,  4.66it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:49<00:11,  4.72it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:50<00:11,  4.66it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:50<00:11,  4.66it/s]predicting train subjects:  80%|████████  | 213/266 [00:50<00:11,  4.76it/s]predicting train subjects:  80%|████████  | 214/266 [00:50<00:10,  4.85it/s]predicting train subjects:  81%|████████  | 215/266 [00:51<00:11,  4.62it/s]predicting train subjects:  81%|████████  | 216/266 [00:51<00:11,  4.48it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:51<00:11,  4.38it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:51<00:11,  4.31it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:52<00:11,  4.26it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:52<00:10,  4.23it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:52<00:10,  4.48it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:52<00:09,  4.74it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:52<00:08,  4.89it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:53<00:08,  4.82it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:53<00:08,  4.82it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:53<00:08,  4.98it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:53<00:07,  5.06it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:53<00:07,  5.08it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:54<00:07,  5.02it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:54<00:07,  5.00it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:54<00:07,  4.98it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:54<00:06,  5.03it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:54<00:06,  4.92it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:55<00:06,  4.75it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:55<00:06,  4.84it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:55<00:06,  4.87it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:55<00:05,  4.97it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:55<00:05,  5.01it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:56<00:05,  4.92it/s]predicting train subjects:  90%|█████████ | 240/266 [00:56<00:05,  4.82it/s]predicting train subjects:  91%|█████████ | 241/266 [00:56<00:05,  4.58it/s]predicting train subjects:  91%|█████████ | 242/266 [00:56<00:05,  4.68it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:56<00:04,  4.66it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:57<00:04,  4.88it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:57<00:04,  4.61it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:57<00:04,  4.43it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:57<00:04,  4.32it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:58<00:04,  4.23it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:58<00:04,  4.01it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:58<00:03,  4.11it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:58<00:03,  4.18it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:59<00:03,  4.21it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:59<00:03,  4.15it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:59<00:02,  4.20it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:59<00:02,  4.21it/s]predicting train subjects:  96%|█████████▌| 256/266 [01:00<00:02,  3.96it/s]predicting train subjects:  97%|█████████▋| 257/266 [01:00<00:02,  3.59it/s]predicting train subjects:  97%|█████████▋| 258/266 [01:00<00:02,  3.79it/s]predicting train subjects:  97%|█████████▋| 259/266 [01:00<00:01,  3.92it/s]predicting train subjects:  98%|█████████▊| 260/266 [01:01<00:01,  4.04it/s]predicting train subjects:  98%|█████████▊| 261/266 [01:01<00:01,  4.00it/s]predicting train subjects:  98%|█████████▊| 262/266 [01:01<00:01,  3.93it/s]predicting train subjects:  99%|█████████▉| 263/266 [01:01<00:00,  4.05it/s]predicting train subjects:  99%|█████████▉| 264/266 [01:02<00:00,  4.07it/s]predicting train subjects: 100%|█████████▉| 265/266 [01:02<00:00,  3.90it/s]predicting train subjects: 100%|██████████| 266/266 [01:02<00:00,  4.01it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  4.49it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  4.56it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  4.61it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  4.78it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:01<00:00,  4.74it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<01:09,  3.81it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<01:12,  3.64it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<01:07,  3.87it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<01:02,  4.19it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:01<01:02,  4.15it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<01:05,  3.99it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<01:09,  3.71it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:02<01:09,  3.73it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:02<01:07,  3.79it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:02<01:08,  3.75it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:02<01:08,  3.74it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:03<01:11,  3.57it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:03<01:08,  3.70it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:03<01:06,  3.77it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:03<01:09,  3.59it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:04<01:08,  3.67it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:04<01:10,  3.51it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:04<01:12,  3.43it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:05<01:09,  3.56it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:05<01:07,  3.64it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:05<01:05,  3.75it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:05<01:05,  3.73it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:06<01:04,  3.75it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:06<01:06,  3.63it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:06<01:05,  3.70it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:06<01:02,  3.84it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:07<01:00,  3.92it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:07<01:01,  3.84it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:07<01:03,  3.75it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:08<01:01,  3.82it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:08<01:04,  3.63it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:08<01:02,  3.73it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:08<01:01,  3.78it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:09<01:03,  3.64it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:09<01:02,  3.72it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:09<01:02,  3.70it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:09<01:01,  3.71it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:10<01:00,  3.77it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:10<01:02,  3.64it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:10<01:00,  3.72it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:11<01:02,  3.61it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:11<01:00,  3.69it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:11<00:55,  4.03it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:11<00:51,  4.33it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:11<00:48,  4.55it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:12<00:46,  4.72it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:12<00:45,  4.86it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:12<00:45,  4.77it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:12<00:46,  4.68it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:12<00:45,  4.78it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:13<00:45,  4.68it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:13<00:45,  4.69it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:13<00:44,  4.82it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:13<00:42,  4.93it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:13<00:43,  4.91it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:14<00:42,  4.98it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:14<00:41,  5.04it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:14<00:43,  4.81it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:14<00:43,  4.79it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:14<00:42,  4.90it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:15<00:41,  4.94it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:15<00:40,  5.05it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:15<00:39,  5.14it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:15<00:38,  5.18it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:15<00:38,  5.22it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:16<00:38,  5.26it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:16<00:37,  5.28it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:16<00:37,  5.25it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:16<00:37,  5.28it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:16<00:38,  5.11it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:17<00:38,  5.09it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:17<00:37,  5.15it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:17<00:37,  5.18it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:17<00:36,  5.20it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:17<00:36,  5.17it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:18<00:36,  5.18it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:18<00:36,  5.11it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:18<00:42,  4.41it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:18<00:43,  4.34it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:19<00:45,  4.09it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:19<00:46,  4.02it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:19<00:45,  4.05it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:19<00:44,  4.07it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:20<00:47,  3.85it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:20<00:49,  3.69it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:20<00:50,  3.58it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:20<00:51,  3.47it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:21<00:49,  3.63it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:21<00:50,  3.54it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:21<00:47,  3.70it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:21<00:45,  3.82it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:22<00:44,  3.91it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:22<00:43,  4.00it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:22<00:44,  3.89it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:22<00:43,  3.97it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:23<00:40,  4.19it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:23<00:41,  4.09it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:23<00:41,  4.05it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:23<00:37,  4.42it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:24<00:36,  4.59it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:24<00:35,  4.64it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:24<00:34,  4.71it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:24<00:34,  4.72it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:24<00:34,  4.76it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:25<00:33,  4.75it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:25<00:34,  4.68it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:25<00:34,  4.65it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:25<00:34,  4.54it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:26<00:34,  4.50it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:26<00:34,  4.58it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:26<00:35,  4.40it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:26<00:36,  4.18it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:27<00:38,  3.99it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:27<00:36,  4.15it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:27<00:35,  4.23it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:27<00:34,  4.34it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:27<00:34,  4.28it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:28<00:35,  4.22it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:28<00:35,  4.11it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:28<00:36,  4.02it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:28<00:36,  4.02it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:29<00:36,  3.93it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:29<00:36,  3.96it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:29<00:35,  3.99it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:30<00:37,  3.76it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:30<00:37,  3.73it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:30<00:36,  3.83it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:30<00:35,  3.87it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:31<00:36,  3.80it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:31<00:35,  3.88it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:31<00:36,  3.68it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:31<00:35,  3.74it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:32<00:37,  3.58it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:32<00:38,  3.47it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:32<00:38,  3.40it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:33<00:38,  3.37it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:33<00:37,  3.40it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:33<00:37,  3.44it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:33<00:36,  3.47it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:34<00:34,  3.65it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:34<00:35,  3.53it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:34<00:33,  3.71it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:34<00:32,  3.78it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:35<00:33,  3.69it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:35<00:31,  3.79it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:35<00:30,  3.92it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:35<00:30,  3.93it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:36<00:29,  3.98it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:36<00:28,  4.06it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:36<00:28,  4.13it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:36<00:27,  4.20it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:37<00:26,  4.27it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:37<00:26,  4.33it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:37<00:27,  4.09it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:37<00:26,  4.12it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:38<00:24,  4.48it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:38<00:22,  4.78it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:38<00:21,  5.02it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:38<00:20,  5.18it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:38<00:20,  5.30it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:38<00:20,  5.25it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:39<00:19,  5.34it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:39<00:19,  5.38it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:39<00:19,  5.37it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:39<00:18,  5.43it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:39<00:18,  5.37it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:40<00:18,  5.47it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:40<00:17,  5.46it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:40<00:17,  5.45it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:40<00:17,  5.49it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:40<00:17,  5.43it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:41<00:17,  5.34it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:41<00:19,  4.81it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:41<00:18,  4.87it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:41<00:18,  4.92it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:41<00:18,  4.92it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:42<00:17,  4.96it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:42<00:17,  4.99it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:42<00:17,  4.95it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:42<00:17,  4.96it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:42<00:17,  4.89it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:43<00:17,  4.93it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:43<00:17,  4.82it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:43<00:16,  4.84it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:43<00:17,  4.69it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:43<00:16,  4.75it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:44<00:16,  4.82it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:44<00:16,  4.76it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:44<00:16,  4.63it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:44<00:16,  4.48it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:45<00:17,  4.24it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:45<00:16,  4.45it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:45<00:15,  4.62it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:45<00:17,  4.10it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:46<00:17,  3.99it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:46<00:17,  3.93it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:46<00:17,  3.90it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:46<00:16,  4.13it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:47<00:15,  4.22it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:47<00:15,  4.38it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:47<00:14,  4.40it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:47<00:14,  4.54it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:47<00:13,  4.61it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:48<00:13,  4.69it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:48<00:13,  4.58it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:48<00:12,  4.65it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:48<00:13,  4.52it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:48<00:12,  4.58it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:49<00:12,  4.52it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:49<00:12,  4.35it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:49<00:12,  4.33it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:49<00:12,  4.27it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:50<00:12,  4.21it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:50<00:11,  4.50it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:50<00:10,  4.73it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:50<00:10,  4.87it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:50<00:09,  4.92it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:51<00:10,  4.71it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:51<00:10,  4.46it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:51<00:10,  4.42it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:51<00:09,  4.60it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:52<00:09,  4.43it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:52<00:09,  4.60it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:52<00:08,  4.71it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:52<00:09,  4.51it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:52<00:08,  4.69it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:53<00:08,  4.87it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:53<00:07,  4.94it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:53<00:07,  5.07it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:53<00:07,  5.02it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:53<00:07,  4.79it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:54<00:07,  4.73it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:54<00:06,  4.83it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:54<00:06,  4.91it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:54<00:06,  4.82it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:54<00:06,  4.76it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:55<00:06,  4.72it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:55<00:05,  4.80it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:55<00:05,  4.72it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:55<00:05,  4.78it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:56<00:05,  4.70it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:56<00:05,  4.74it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:56<00:05,  4.59it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:56<00:04,  4.71it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:56<00:04,  4.85it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:57<00:04,  4.95it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:57<00:04,  4.71it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:57<00:03,  4.79it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:57<00:03,  4.55it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:57<00:03,  4.34it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:58<00:03,  4.31it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:58<00:03,  4.24it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:58<00:03,  4.16it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:58<00:03,  3.89it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:59<00:02,  3.73it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:59<00:02,  3.61it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:59<00:02,  3.55it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [01:00<00:02,  3.51it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [01:00<00:02,  3.48it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [01:00<00:01,  3.47it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [01:01<00:01,  3.46it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [01:01<00:01,  3.52it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [01:01<00:00,  3.68it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [01:01<00:00,  3.76it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [01:02<00:00,  3.81it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [01:02<00:00,  3.69it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 75.46it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:04, 62.28it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:03, 63.15it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 63.27it/s]saving BB  train1-THALAMUS:  11%|█         | 29/266 [00:00<00:03, 65.34it/s]saving BB  train1-THALAMUS:  14%|█▍        | 37/266 [00:00<00:03, 67.10it/s]saving BB  train1-THALAMUS:  17%|█▋        | 45/266 [00:00<00:03, 68.80it/s]saving BB  train1-THALAMUS:  20%|█▉        | 53/266 [00:00<00:03, 70.68it/s]saving BB  train1-THALAMUS:  23%|██▎       | 61/266 [00:00<00:02, 72.46it/s]saving BB  train1-THALAMUS:  26%|██▋       | 70/266 [00:00<00:02, 74.88it/s]saving BB  train1-THALAMUS:  29%|██▉       | 78/266 [00:01<00:02, 75.62it/s]saving BB  train1-THALAMUS:  32%|███▏      | 86/266 [00:01<00:02, 74.46it/s]saving BB  train1-THALAMUS:  35%|███▌      | 94/266 [00:01<00:02, 73.76it/s]saving BB  train1-THALAMUS:  38%|███▊      | 102/266 [00:01<00:02, 74.91it/s]saving BB  train1-THALAMUS:  41%|████▏     | 110/266 [00:01<00:02, 75.47it/s]saving BB  train1-THALAMUS:  44%|████▍     | 118/266 [00:01<00:01, 76.00it/s]saving BB  train1-THALAMUS:  47%|████▋     | 126/266 [00:01<00:01, 74.71it/s]saving BB  train1-THALAMUS:  50%|█████     | 134/266 [00:01<00:01, 73.63it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 142/266 [00:01<00:01, 69.52it/s]saving BB  train1-THALAMUS:  56%|█████▋    | 150/266 [00:02<00:01, 69.82it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 158/266 [00:02<00:01, 71.48it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 167/266 [00:02<00:01, 75.48it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 176/266 [00:02<00:01, 78.13it/s]saving BB  train1-THALAMUS:  70%|██████▉   | 185/266 [00:02<00:01, 80.29it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 194/266 [00:02<00:00, 79.22it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 202/266 [00:02<00:00, 77.37it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 210/266 [00:02<00:00, 75.76it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 218/266 [00:02<00:00, 76.60it/s]saving BB  train1-THALAMUS:  85%|████████▌ | 227/266 [00:03<00:00, 78.10it/s]saving BB  train1-THALAMUS:  89%|████████▊ | 236/266 [00:03<00:00, 79.02it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 245/266 [00:03<00:00, 79.98it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 254/266 [00:03<00:00, 78.47it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 262/266 [00:03<00:00, 76.75it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 74.73it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 65.02it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 7/266 [00:00<00:03, 69.66it/s]saving BB  train1-THALAMUS Sagittal:   5%|▌         | 14/266 [00:00<00:03, 68.02it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 21/266 [00:00<00:03, 67.25it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 29/266 [00:00<00:03, 68.53it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 37/266 [00:00<00:03, 69.63it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 45/266 [00:00<00:03, 71.05it/s]saving BB  train1-THALAMUS Sagittal:  20%|█▉        | 53/266 [00:00<00:02, 72.33it/s]saving BB  train1-THALAMUS Sagittal:  23%|██▎       | 61/266 [00:00<00:02, 73.69it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▋       | 70/266 [00:00<00:02, 76.10it/s]saving BB  train1-THALAMUS Sagittal:  30%|██▉       | 79/266 [00:01<00:02, 77.28it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 87/266 [00:01<00:02, 75.84it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 95/266 [00:01<00:02, 74.23it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▊      | 103/266 [00:01<00:02, 74.57it/s]saving BB  train1-THALAMUS Sagittal:  42%|████▏     | 111/266 [00:01<00:02, 74.98it/s]saving BB  train1-THALAMUS Sagittal:  45%|████▍     | 119/266 [00:01<00:01, 75.06it/s]saving BB  train1-THALAMUS Sagittal:  48%|████▊     | 127/266 [00:01<00:01, 73.51it/s]saving BB  train1-THALAMUS Sagittal:  51%|█████     | 135/266 [00:01<00:01, 72.84it/s]saving BB  train1-THALAMUS Sagittal:  54%|█████▍    | 143/266 [00:01<00:01, 71.89it/s]saving BB  train1-THALAMUS Sagittal:  57%|█████▋    | 151/266 [00:02<00:01, 69.58it/s]saving BB  train1-THALAMUS Sagittal:  60%|█████▉    | 159/266 [00:02<00:01, 70.69it/s]saving BB  train1-THALAMUS Sagittal:  63%|██████▎   | 168/266 [00:02<00:01, 74.44it/s]saving BB  train1-THALAMUS Sagittal:  67%|██████▋   | 177/266 [00:02<00:01, 77.48it/s]saving BB  train1-THALAMUS Sagittal:  70%|██████▉   | 186/266 [00:02<00:00, 80.08it/s]saving BB  train1-THALAMUS Sagittal:  73%|███████▎  | 195/266 [00:02<00:00, 80.37it/s]saving BB  train1-THALAMUS Sagittal:  77%|███████▋  | 204/266 [00:02<00:00, 77.96it/s]saving BB  train1-THALAMUS Sagittal:  80%|███████▉  | 212/266 [00:02<00:00, 76.60it/s]saving BB  train1-THALAMUS Sagittal:  83%|████████▎ | 221/266 [00:02<00:00, 77.67it/s]saving BB  train1-THALAMUS Sagittal:  86%|████████▋ | 230/266 [00:03<00:00, 78.59it/s]saving BB  train1-THALAMUS Sagittal:  90%|████████▉ | 239/266 [00:03<00:00, 79.81it/s]saving BB  train1-THALAMUS Sagittal:  93%|█████████▎| 248/266 [00:03<00:00, 80.24it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 257/266 [00:03<00:00, 78.10it/s]saving BB  train1-THALAMUS Sagittal: 100%|█████████▉| 265/266 [00:03<00:00, 75.50it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 75.09it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:39,  1.74s/it]Loading train:   1%|          | 2/266 [00:03<07:18,  1.66s/it]Loading train:   1%|          | 3/266 [00:04<06:24,  1.46s/it]Loading train:   2%|▏         | 4/266 [00:05<05:43,  1.31s/it]Loading train:   2%|▏         | 5/266 [00:06<05:40,  1.30s/it]Loading train:   2%|▏         | 6/266 [00:07<05:12,  1.20s/it]Loading train:   3%|▎         | 7/266 [00:08<04:44,  1.10s/it]Loading train:   3%|▎         | 8/266 [00:09<04:28,  1.04s/it]Loading train:   3%|▎         | 9/266 [00:10<04:16,  1.00it/s]Loading train:   4%|▍         | 10/266 [00:11<04:08,  1.03it/s]Loading train:   4%|▍         | 11/266 [00:11<03:54,  1.09it/s]Loading train:   5%|▍         | 12/266 [00:12<03:47,  1.11it/s]Loading train:   5%|▍         | 13/266 [00:13<03:36,  1.17it/s]Loading train:   5%|▌         | 14/266 [00:14<03:28,  1.21it/s]Loading train:   6%|▌         | 15/266 [00:14<03:24,  1.23it/s]Loading train:   6%|▌         | 16/266 [00:15<03:21,  1.24it/s]Loading train:   6%|▋         | 17/266 [00:16<03:24,  1.22it/s]Loading train:   7%|▋         | 18/266 [00:17<03:25,  1.21it/s]Loading train:   7%|▋         | 19/266 [00:18<03:26,  1.20it/s]Loading train:   8%|▊         | 20/266 [00:19<03:32,  1.16it/s]Loading train:   8%|▊         | 21/266 [00:20<03:31,  1.16it/s]Loading train:   8%|▊         | 22/266 [00:20<03:30,  1.16it/s]Loading train:   9%|▊         | 23/266 [00:21<03:25,  1.18it/s]Loading train:   9%|▉         | 24/266 [00:22<03:23,  1.19it/s]Loading train:   9%|▉         | 25/266 [00:23<03:21,  1.20it/s]Loading train:  10%|▉         | 26/266 [00:24<03:14,  1.23it/s]Loading train:  10%|█         | 27/266 [00:24<03:09,  1.26it/s]Loading train:  11%|█         | 28/266 [00:25<03:05,  1.28it/s]Loading train:  11%|█         | 29/266 [00:26<03:03,  1.29it/s]Loading train:  11%|█▏        | 30/266 [00:27<03:02,  1.29it/s]Loading train:  12%|█▏        | 31/266 [00:27<02:59,  1.31it/s]Loading train:  12%|█▏        | 32/266 [00:28<02:56,  1.33it/s]Loading train:  12%|█▏        | 33/266 [00:29<02:53,  1.34it/s]Loading train:  13%|█▎        | 34/266 [00:30<02:54,  1.33it/s]Loading train:  13%|█▎        | 35/266 [00:30<02:58,  1.29it/s]Loading train:  14%|█▎        | 36/266 [00:31<03:04,  1.25it/s]Loading train:  14%|█▍        | 37/266 [00:32<03:08,  1.21it/s]Loading train:  14%|█▍        | 38/266 [00:33<03:02,  1.25it/s]Loading train:  15%|█▍        | 39/266 [00:34<02:59,  1.26it/s]Loading train:  15%|█▌        | 40/266 [00:35<03:01,  1.25it/s]Loading train:  15%|█▌        | 41/266 [00:35<03:06,  1.21it/s]Loading train:  16%|█▌        | 42/266 [00:36<02:59,  1.25it/s]Loading train:  16%|█▌        | 43/266 [00:37<02:53,  1.28it/s]Loading train:  17%|█▋        | 44/266 [00:38<02:46,  1.33it/s]Loading train:  17%|█▋        | 45/266 [00:38<02:42,  1.36it/s]Loading train:  17%|█▋        | 46/266 [00:39<02:38,  1.39it/s]Loading train:  18%|█▊        | 47/266 [00:40<02:34,  1.42it/s]Loading train:  18%|█▊        | 48/266 [00:40<02:28,  1.47it/s]Loading train:  18%|█▊        | 49/266 [00:41<02:26,  1.48it/s]Loading train:  19%|█▉        | 50/266 [00:42<02:30,  1.44it/s]Loading train:  19%|█▉        | 51/266 [00:42<02:28,  1.45it/s]Loading train:  20%|█▉        | 52/266 [00:43<02:25,  1.47it/s]Loading train:  20%|█▉        | 53/266 [00:44<02:24,  1.48it/s]Loading train:  20%|██        | 54/266 [00:44<02:20,  1.51it/s]Loading train:  21%|██        | 55/266 [00:45<02:21,  1.49it/s]Loading train:  21%|██        | 56/266 [00:46<02:20,  1.50it/s]Loading train:  21%|██▏       | 57/266 [00:46<02:16,  1.53it/s]Loading train:  22%|██▏       | 58/266 [00:47<02:15,  1.53it/s]Loading train:  22%|██▏       | 59/266 [00:48<02:17,  1.51it/s]Loading train:  23%|██▎       | 60/266 [00:48<02:20,  1.46it/s]Loading train:  23%|██▎       | 61/266 [00:49<02:19,  1.47it/s]Loading train:  23%|██▎       | 62/266 [00:50<02:17,  1.49it/s]Loading train:  24%|██▎       | 63/266 [00:50<02:17,  1.48it/s]Loading train:  24%|██▍       | 64/266 [00:51<02:15,  1.49it/s]Loading train:  24%|██▍       | 65/266 [00:52<02:16,  1.47it/s]Loading train:  25%|██▍       | 66/266 [00:52<02:14,  1.49it/s]Loading train:  25%|██▌       | 67/266 [00:53<02:11,  1.51it/s]Loading train:  26%|██▌       | 68/266 [00:54<02:12,  1.50it/s]Loading train:  26%|██▌       | 69/266 [00:54<02:11,  1.49it/s]Loading train:  26%|██▋       | 70/266 [00:55<02:11,  1.49it/s]Loading train:  27%|██▋       | 71/266 [00:56<02:11,  1.49it/s]Loading train:  27%|██▋       | 72/266 [00:56<02:09,  1.50it/s]Loading train:  27%|██▋       | 73/266 [00:57<02:09,  1.50it/s]Loading train:  28%|██▊       | 74/266 [00:58<02:05,  1.53it/s]Loading train:  28%|██▊       | 75/266 [00:58<02:02,  1.55it/s]Loading train:  29%|██▊       | 76/266 [00:59<02:06,  1.51it/s]Loading train:  29%|██▉       | 77/266 [01:00<02:03,  1.53it/s]Loading train:  29%|██▉       | 78/266 [01:01<02:18,  1.36it/s]Loading train:  30%|██▉       | 79/266 [01:01<02:24,  1.29it/s]Loading train:  30%|███       | 80/266 [01:02<02:30,  1.24it/s]Loading train:  30%|███       | 81/266 [01:03<02:34,  1.19it/s]Loading train:  31%|███       | 82/266 [01:04<02:36,  1.18it/s]Loading train:  31%|███       | 83/266 [01:05<02:39,  1.15it/s]Loading train:  32%|███▏      | 84/266 [01:06<02:39,  1.14it/s]Loading train:  32%|███▏      | 85/266 [01:07<02:41,  1.12it/s]Loading train:  32%|███▏      | 86/266 [01:08<02:38,  1.13it/s]Loading train:  33%|███▎      | 87/266 [01:09<02:34,  1.16it/s]Loading train:  33%|███▎      | 88/266 [01:09<02:34,  1.15it/s]Loading train:  33%|███▎      | 89/266 [01:10<02:33,  1.15it/s]Loading train:  34%|███▍      | 90/266 [01:11<02:32,  1.15it/s]Loading train:  34%|███▍      | 91/266 [01:12<02:32,  1.15it/s]Loading train:  35%|███▍      | 92/266 [01:13<02:31,  1.15it/s]Loading train:  35%|███▍      | 93/266 [01:14<02:29,  1.16it/s]Loading train:  35%|███▌      | 94/266 [01:15<02:27,  1.16it/s]Loading train:  36%|███▌      | 95/266 [01:15<02:28,  1.15it/s]Loading train:  36%|███▌      | 96/266 [01:17<02:43,  1.04it/s]Loading train:  36%|███▋      | 97/266 [01:18<03:02,  1.08s/it]Loading train:  37%|███▋      | 98/266 [01:19<03:04,  1.10s/it]Loading train:  37%|███▋      | 99/266 [01:20<02:54,  1.04s/it]Loading train:  38%|███▊      | 100/266 [01:21<02:50,  1.03s/it]Loading train:  38%|███▊      | 101/266 [01:22<02:31,  1.09it/s]Loading train:  38%|███▊      | 102/266 [01:22<02:20,  1.17it/s]Loading train:  39%|███▊      | 103/266 [01:23<02:10,  1.25it/s]Loading train:  39%|███▉      | 104/266 [01:24<02:03,  1.31it/s]Loading train:  39%|███▉      | 105/266 [01:24<01:59,  1.34it/s]Loading train:  40%|███▉      | 106/266 [01:25<01:53,  1.41it/s]Loading train:  40%|████      | 107/266 [01:26<01:48,  1.47it/s]Loading train:  41%|████      | 108/266 [01:26<01:47,  1.47it/s]Loading train:  41%|████      | 109/266 [01:27<01:47,  1.46it/s]Loading train:  41%|████▏     | 110/266 [01:28<01:49,  1.43it/s]Loading train:  42%|████▏     | 111/266 [01:29<01:49,  1.41it/s]Loading train:  42%|████▏     | 112/266 [01:29<01:53,  1.36it/s]Loading train:  42%|████▏     | 113/266 [01:30<01:50,  1.38it/s]Loading train:  43%|████▎     | 114/266 [01:31<01:46,  1.42it/s]Loading train:  43%|████▎     | 115/266 [01:31<01:48,  1.40it/s]Loading train:  44%|████▎     | 116/266 [01:32<01:46,  1.41it/s]Loading train:  44%|████▍     | 117/266 [01:33<01:45,  1.41it/s]Loading train:  44%|████▍     | 118/266 [01:34<01:46,  1.39it/s]Loading train:  45%|████▍     | 119/266 [01:35<01:54,  1.28it/s]Loading train:  45%|████▌     | 120/266 [01:35<02:00,  1.21it/s]Loading train:  45%|████▌     | 121/266 [01:36<02:04,  1.17it/s]Loading train:  46%|████▌     | 122/266 [01:37<02:05,  1.15it/s]Loading train:  46%|████▌     | 123/266 [01:38<02:05,  1.14it/s]Loading train:  47%|████▋     | 124/266 [01:39<02:04,  1.14it/s]Loading train:  47%|████▋     | 125/266 [01:40<02:02,  1.15it/s]Loading train:  47%|████▋     | 126/266 [01:41<02:04,  1.12it/s]Loading train:  48%|████▊     | 127/266 [01:42<02:04,  1.11it/s]Loading train:  48%|████▊     | 128/266 [01:43<02:02,  1.12it/s]Loading train:  48%|████▊     | 129/266 [01:43<01:58,  1.16it/s]Loading train:  49%|████▉     | 130/266 [01:44<01:58,  1.14it/s]Loading train:  49%|████▉     | 131/266 [01:45<01:59,  1.13it/s]Loading train:  50%|████▉     | 132/266 [01:46<01:56,  1.15it/s]Loading train:  50%|█████     | 133/266 [01:47<01:56,  1.14it/s]Loading train:  50%|█████     | 134/266 [01:48<01:57,  1.12it/s]Loading train:  51%|█████     | 135/266 [01:49<01:54,  1.14it/s]Loading train:  51%|█████     | 136/266 [01:50<01:53,  1.14it/s]Loading train:  52%|█████▏    | 137/266 [01:50<01:49,  1.18it/s]Loading train:  52%|█████▏    | 138/266 [01:51<01:46,  1.20it/s]Loading train:  52%|█████▏    | 139/266 [01:52<01:45,  1.20it/s]Loading train:  53%|█████▎    | 140/266 [01:53<01:44,  1.21it/s]Loading train:  53%|█████▎    | 141/266 [01:54<01:41,  1.23it/s]Loading train:  53%|█████▎    | 142/266 [01:54<01:40,  1.23it/s]Loading train:  54%|█████▍    | 143/266 [01:55<01:40,  1.23it/s]Loading train:  54%|█████▍    | 144/266 [01:56<01:39,  1.23it/s]Loading train:  55%|█████▍    | 145/266 [01:57<01:37,  1.24it/s]Loading train:  55%|█████▍    | 146/266 [01:58<01:36,  1.24it/s]Loading train:  55%|█████▌    | 147/266 [01:58<01:35,  1.24it/s]Loading train:  56%|█████▌    | 148/266 [01:59<01:35,  1.24it/s]Loading train:  56%|█████▌    | 149/266 [02:00<01:33,  1.25it/s]Loading train:  56%|█████▋    | 150/266 [02:01<01:30,  1.28it/s]Loading train:  57%|█████▋    | 151/266 [02:02<01:28,  1.29it/s]Loading train:  57%|█████▋    | 152/266 [02:02<01:28,  1.29it/s]Loading train:  58%|█████▊    | 153/266 [02:03<01:26,  1.30it/s]Loading train:  58%|█████▊    | 154/266 [02:04<01:26,  1.29it/s]Loading train:  58%|█████▊    | 155/266 [02:05<01:21,  1.37it/s]Loading train:  59%|█████▊    | 156/266 [02:05<01:16,  1.43it/s]Loading train:  59%|█████▉    | 157/266 [02:06<01:13,  1.48it/s]Loading train:  59%|█████▉    | 158/266 [02:06<01:12,  1.49it/s]Loading train:  60%|█████▉    | 159/266 [02:07<01:10,  1.52it/s]Loading train:  60%|██████    | 160/266 [02:08<01:07,  1.57it/s]Loading train:  61%|██████    | 161/266 [02:08<01:06,  1.57it/s]Loading train:  61%|██████    | 162/266 [02:09<01:07,  1.54it/s]Loading train:  61%|██████▏   | 163/266 [02:10<01:06,  1.54it/s]Loading train:  62%|██████▏   | 164/266 [02:10<01:05,  1.56it/s]Loading train:  62%|██████▏   | 165/266 [02:11<01:04,  1.55it/s]Loading train:  62%|██████▏   | 166/266 [02:12<01:05,  1.52it/s]Loading train:  63%|██████▎   | 167/266 [02:12<01:03,  1.56it/s]Loading train:  63%|██████▎   | 168/266 [02:13<01:02,  1.57it/s]Loading train:  64%|██████▎   | 169/266 [02:13<01:01,  1.58it/s]Loading train:  64%|██████▍   | 170/266 [02:14<01:02,  1.54it/s]Loading train:  64%|██████▍   | 171/266 [02:15<01:00,  1.56it/s]Loading train:  65%|██████▍   | 172/266 [02:15<00:59,  1.58it/s]Loading train:  65%|██████▌   | 173/266 [02:16<01:00,  1.55it/s]Loading train:  65%|██████▌   | 174/266 [02:17<00:59,  1.56it/s]Loading train:  66%|██████▌   | 175/266 [02:17<00:59,  1.54it/s]Loading train:  66%|██████▌   | 176/266 [02:18<01:01,  1.46it/s]Loading train:  67%|██████▋   | 177/266 [02:19<00:58,  1.51it/s]Loading train:  67%|██████▋   | 178/266 [02:19<00:57,  1.52it/s]Loading train:  67%|██████▋   | 179/266 [02:20<00:56,  1.53it/s]Loading train:  68%|██████▊   | 180/266 [02:21<00:55,  1.56it/s]Loading train:  68%|██████▊   | 181/266 [02:21<00:54,  1.55it/s]Loading train:  68%|██████▊   | 182/266 [02:22<00:53,  1.58it/s]Loading train:  69%|██████▉   | 183/266 [02:23<00:53,  1.55it/s]Loading train:  69%|██████▉   | 184/266 [02:23<00:51,  1.60it/s]Loading train:  70%|██████▉   | 185/266 [02:24<00:50,  1.61it/s]Loading train:  70%|██████▉   | 186/266 [02:24<00:50,  1.59it/s]Loading train:  70%|███████   | 187/266 [02:25<00:48,  1.62it/s]Loading train:  71%|███████   | 188/266 [02:26<00:47,  1.63it/s]Loading train:  71%|███████   | 189/266 [02:26<00:47,  1.64it/s]Loading train:  71%|███████▏  | 190/266 [02:27<00:47,  1.60it/s]Loading train:  72%|███████▏  | 191/266 [02:28<00:58,  1.28it/s]Loading train:  72%|███████▏  | 192/266 [02:29<01:00,  1.22it/s]Loading train:  73%|███████▎  | 193/266 [02:30<01:02,  1.17it/s]Loading train:  73%|███████▎  | 194/266 [02:31<01:10,  1.02it/s]Loading train:  73%|███████▎  | 195/266 [02:32<01:03,  1.12it/s]Loading train:  74%|███████▎  | 196/266 [02:32<00:57,  1.21it/s]Loading train:  74%|███████▍  | 197/266 [02:33<00:53,  1.29it/s]Loading train:  74%|███████▍  | 198/266 [02:34<00:49,  1.36it/s]Loading train:  75%|███████▍  | 199/266 [02:34<00:48,  1.38it/s]Loading train:  75%|███████▌  | 200/266 [02:35<00:47,  1.38it/s]Loading train:  76%|███████▌  | 201/266 [02:36<00:47,  1.37it/s]Loading train:  76%|███████▌  | 202/266 [02:37<00:46,  1.39it/s]Loading train:  76%|███████▋  | 203/266 [02:37<00:43,  1.44it/s]Loading train:  77%|███████▋  | 204/266 [02:38<00:43,  1.44it/s]Loading train:  77%|███████▋  | 205/266 [02:39<00:42,  1.44it/s]Loading train:  77%|███████▋  | 206/266 [02:39<00:41,  1.45it/s]Loading train:  78%|███████▊  | 207/266 [02:40<00:40,  1.45it/s]Loading train:  78%|███████▊  | 208/266 [02:41<00:40,  1.42it/s]Loading train:  79%|███████▊  | 209/266 [02:41<00:40,  1.41it/s]Loading train:  79%|███████▉  | 210/266 [02:42<00:39,  1.42it/s]Loading train:  79%|███████▉  | 211/266 [02:43<00:38,  1.43it/s]Loading train:  80%|███████▉  | 212/266 [02:44<00:37,  1.43it/s]Loading train:  80%|████████  | 213/266 [02:44<00:36,  1.47it/s]Loading train:  80%|████████  | 214/266 [02:45<00:34,  1.51it/s]Loading train:  81%|████████  | 215/266 [02:45<00:32,  1.57it/s]Loading train:  81%|████████  | 216/266 [02:46<00:31,  1.58it/s]Loading train:  82%|████████▏ | 217/266 [02:47<00:30,  1.59it/s]Loading train:  82%|████████▏ | 218/266 [02:47<00:30,  1.57it/s]Loading train:  82%|████████▏ | 219/266 [02:48<00:30,  1.55it/s]Loading train:  83%|████████▎ | 220/266 [02:49<00:29,  1.55it/s]Loading train:  83%|████████▎ | 221/266 [02:49<00:29,  1.54it/s]Loading train:  83%|████████▎ | 222/266 [02:50<00:28,  1.57it/s]Loading train:  84%|████████▍ | 223/266 [02:50<00:26,  1.60it/s]Loading train:  84%|████████▍ | 224/266 [02:51<00:26,  1.61it/s]Loading train:  85%|████████▍ | 225/266 [02:52<00:25,  1.62it/s]Loading train:  85%|████████▍ | 226/266 [02:52<00:24,  1.66it/s]Loading train:  85%|████████▌ | 227/266 [02:53<00:23,  1.66it/s]Loading train:  86%|████████▌ | 228/266 [02:53<00:23,  1.64it/s]Loading train:  86%|████████▌ | 229/266 [02:54<00:23,  1.55it/s]Loading train:  86%|████████▋ | 230/266 [02:55<00:23,  1.55it/s]Loading train:  87%|████████▋ | 231/266 [02:55<00:22,  1.53it/s]Loading train:  87%|████████▋ | 232/266 [02:56<00:21,  1.55it/s]Loading train:  88%|████████▊ | 233/266 [02:57<00:20,  1.61it/s]Loading train:  88%|████████▊ | 234/266 [02:57<00:19,  1.63it/s]Loading train:  88%|████████▊ | 235/266 [02:58<00:19,  1.62it/s]Loading train:  89%|████████▊ | 236/266 [02:58<00:18,  1.64it/s]Loading train:  89%|████████▉ | 237/266 [02:59<00:17,  1.65it/s]Loading train:  89%|████████▉ | 238/266 [03:00<00:16,  1.66it/s]Loading train:  90%|████████▉ | 239/266 [03:00<00:16,  1.66it/s]Loading train:  90%|█████████ | 240/266 [03:01<00:15,  1.64it/s]Loading train:  91%|█████████ | 241/266 [03:01<00:14,  1.67it/s]Loading train:  91%|█████████ | 242/266 [03:02<00:14,  1.67it/s]Loading train:  91%|█████████▏| 243/266 [03:03<00:13,  1.70it/s]Loading train:  92%|█████████▏| 244/266 [03:03<00:12,  1.70it/s]Loading train:  92%|█████████▏| 245/266 [03:04<00:12,  1.72it/s]Loading train:  92%|█████████▏| 246/266 [03:04<00:11,  1.75it/s]Loading train:  93%|█████████▎| 247/266 [03:05<00:10,  1.76it/s]Loading train:  93%|█████████▎| 248/266 [03:05<00:10,  1.76it/s]Loading train:  94%|█████████▎| 249/266 [03:06<00:10,  1.58it/s]Loading train:  94%|█████████▍| 250/266 [03:07<00:10,  1.50it/s]Loading train:  94%|█████████▍| 251/266 [03:08<00:10,  1.47it/s]Loading train:  95%|█████████▍| 252/266 [03:08<00:09,  1.45it/s]Loading train:  95%|█████████▌| 253/266 [03:09<00:09,  1.43it/s]Loading train:  95%|█████████▌| 254/266 [03:10<00:08,  1.39it/s]Loading train:  96%|█████████▌| 255/266 [03:11<00:07,  1.39it/s]Loading train:  96%|█████████▌| 256/266 [03:11<00:07,  1.39it/s]Loading train:  97%|█████████▋| 257/266 [03:12<00:06,  1.40it/s]Loading train:  97%|█████████▋| 258/266 [03:13<00:05,  1.41it/s]Loading train:  97%|█████████▋| 259/266 [03:13<00:04,  1.41it/s]Loading train:  98%|█████████▊| 260/266 [03:14<00:04,  1.41it/s]Loading train:  98%|█████████▊| 261/266 [03:15<00:03,  1.40it/s]Loading train:  98%|█████████▊| 262/266 [03:16<00:02,  1.34it/s]Loading train:  99%|█████████▉| 263/266 [03:17<00:02,  1.29it/s]Loading train:  99%|█████████▉| 264/266 [03:17<00:01,  1.28it/s]Loading train: 100%|█████████▉| 265/266 [03:18<00:00,  1.28it/s]Loading train: 100%|██████████| 266/266 [03:19<00:00,  1.28it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:02, 126.42it/s]concatenating: train:  13%|█▎        | 34/266 [00:00<00:01, 141.90it/s]concatenating: train:  19%|█▉        | 51/266 [00:00<00:01, 149.24it/s]concatenating: train:  26%|██▌       | 68/266 [00:00<00:01, 153.48it/s]concatenating: train:  32%|███▏      | 84/266 [00:00<00:01, 152.96it/s]concatenating: train:  37%|███▋      | 99/266 [00:00<00:01, 149.80it/s]concatenating: train:  43%|████▎     | 115/266 [00:00<00:00, 151.75it/s]concatenating: train:  49%|████▉     | 130/266 [00:00<00:00, 145.63it/s]concatenating: train:  55%|█████▍    | 146/266 [00:00<00:00, 148.33it/s]concatenating: train:  62%|██████▏   | 166/266 [00:01<00:00, 158.83it/s]concatenating: train:  68%|██████▊   | 182/266 [00:01<00:00, 155.50it/s]concatenating: train:  75%|███████▍  | 199/266 [00:01<00:00, 158.21it/s]concatenating: train:  82%|████████▏ | 219/266 [00:01<00:00, 167.38it/s]concatenating: train:  89%|████████▊ | 236/266 [00:01<00:00, 168.15it/s]concatenating: train:  96%|█████████▌| 255/266 [00:01<00:00, 171.62it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 165.05it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:04,  1.13s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.12s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.09s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.06s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.09s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 546.69it/s]2019-08-16 22:38:46.247327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-16 22:38:46.247420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-16 22:38:46.247434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-16 22:38:46.247442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-16 22:38:46.247841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.84it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:05,  6.86it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.53it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  8.29it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:03,  9.04it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  7.76it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02, 10.01it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02, 10.27it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:01<00:02,  8.32it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:01, 10.36it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 10.92it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 11.07it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  8.60it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:02<00:00, 10.55it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00, 10.85it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 10.64it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.46it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 12.04it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 88, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 88, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 88, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 88, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 88, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 88, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 88, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 44, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 44, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 44, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 44, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 44, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 44, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 44, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 44, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 22, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 22, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 22, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 22, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 22, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 22, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 22, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 22, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 22, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 22, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 44, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 44, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 44, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 44, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 44, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 44, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 44, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 44, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 44, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 44, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 88, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 88, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 88, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 88, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 88, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 88, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 88, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 88, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 88, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 88, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 5,413
Non-trainable params: 218,420
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.08646624e-02 3.15453095e-02 7.37643676e-02 9.16559358e-03
 2.65273149e-02 6.94012941e-03 1.02420670e-01 1.15816666e-01
 8.60879324e-02 1.30797527e-02 2.79114009e-01 1.94415323e-01
 2.58269274e-04]
Train on 8975 samples, validate on 168 samples
Epoch 1/300
 - 13s - loss: 3.0501 - acc: 0.6283 - mDice: 0.0875 - val_loss: 2.9869 - val_acc: 0.9068 - val_mDice: 0.1630

Epoch 00001: val_mDice improved from -inf to 0.16305, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 8s - loss: 1.8065 - acc: 0.8724 - mDice: 0.2091 - val_loss: 2.2079 - val_acc: 0.8882 - val_mDice: 0.2488

Epoch 00002: val_mDice improved from 0.16305 to 0.24879, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 9s - loss: 1.3728 - acc: 0.8858 - mDice: 0.2870 - val_loss: 1.9688 - val_acc: 0.9207 - val_mDice: 0.3170

Epoch 00003: val_mDice improved from 0.24879 to 0.31695, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 8s - loss: 1.1603 - acc: 0.8919 - mDice: 0.3382 - val_loss: 1.6901 - val_acc: 0.9210 - val_mDice: 0.3819

Epoch 00004: val_mDice improved from 0.31695 to 0.38194, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 8s - loss: 1.0402 - acc: 0.8971 - mDice: 0.3736 - val_loss: 1.4698 - val_acc: 0.9058 - val_mDice: 0.4298

Epoch 00005: val_mDice improved from 0.38194 to 0.42978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 8s - loss: 0.9421 - acc: 0.9022 - mDice: 0.4046 - val_loss: 1.6731 - val_acc: 0.9253 - val_mDice: 0.3652

Epoch 00006: val_mDice did not improve from 0.42978
Epoch 7/300
 - 8s - loss: 0.8796 - acc: 0.9056 - mDice: 0.4252 - val_loss: 1.2281 - val_acc: 0.9294 - val_mDice: 0.4885

Epoch 00007: val_mDice improved from 0.42978 to 0.48850, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 8s - loss: 0.8142 - acc: 0.9105 - mDice: 0.4494 - val_loss: 1.1384 - val_acc: 0.9335 - val_mDice: 0.4934

Epoch 00008: val_mDice improved from 0.48850 to 0.49338, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 8s - loss: 0.7376 - acc: 0.9141 - mDice: 0.4774 - val_loss: 1.1088 - val_acc: 0.9346 - val_mDice: 0.5058

Epoch 00009: val_mDice improved from 0.49338 to 0.50575, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 8s - loss: 0.6877 - acc: 0.9174 - mDice: 0.4988 - val_loss: 1.1436 - val_acc: 0.9350 - val_mDice: 0.5141

Epoch 00010: val_mDice improved from 0.50575 to 0.51406, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 8s - loss: 0.6462 - acc: 0.9199 - mDice: 0.5172 - val_loss: 0.9582 - val_acc: 0.9378 - val_mDice: 0.5500

Epoch 00011: val_mDice improved from 0.51406 to 0.55004, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 8s - loss: 0.6087 - acc: 0.9220 - mDice: 0.5339 - val_loss: 1.1589 - val_acc: 0.9140 - val_mDice: 0.5181

Epoch 00012: val_mDice did not improve from 0.55004
Epoch 13/300
 - 8s - loss: 0.5774 - acc: 0.9243 - mDice: 0.5499 - val_loss: 1.1263 - val_acc: 0.9351 - val_mDice: 0.5132

Epoch 00013: val_mDice did not improve from 0.55004
Epoch 14/300
 - 8s - loss: 0.5529 - acc: 0.9260 - mDice: 0.5631 - val_loss: 0.7892 - val_acc: 0.9441 - val_mDice: 0.5892

Epoch 00014: val_mDice improved from 0.55004 to 0.58923, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 8s - loss: 0.5432 - acc: 0.9268 - mDice: 0.5704 - val_loss: 2.4560 - val_acc: 0.8109 - val_mDice: 0.2652

Epoch 00015: val_mDice did not improve from 0.58923
Epoch 16/300
 - 9s - loss: 0.5316 - acc: 0.9273 - mDice: 0.5750 - val_loss: 0.9529 - val_acc: 0.9429 - val_mDice: 0.5917

Epoch 00016: val_mDice improved from 0.58923 to 0.59174, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 8s - loss: 0.5102 - acc: 0.9290 - mDice: 0.5872 - val_loss: 0.8083 - val_acc: 0.9458 - val_mDice: 0.6033

Epoch 00017: val_mDice improved from 0.59174 to 0.60326, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 8s - loss: 0.4958 - acc: 0.9302 - mDice: 0.5959 - val_loss: 0.6966 - val_acc: 0.9377 - val_mDice: 0.5949

Epoch 00018: val_mDice did not improve from 0.60326
Epoch 19/300
 - 9s - loss: 0.4850 - acc: 0.9309 - mDice: 0.6029 - val_loss: 0.7596 - val_acc: 0.9383 - val_mDice: 0.5900

Epoch 00019: val_mDice did not improve from 0.60326
Epoch 20/300
 - 8s - loss: 0.4793 - acc: 0.9314 - mDice: 0.6066 - val_loss: 0.7917 - val_acc: 0.9439 - val_mDice: 0.6076

Epoch 00020: val_mDice improved from 0.60326 to 0.60761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 8s - loss: 0.4708 - acc: 0.9318 - mDice: 0.6114 - val_loss: 0.9281 - val_acc: 0.9464 - val_mDice: 0.5932

Epoch 00021: val_mDice did not improve from 0.60761
Epoch 22/300
 - 8s - loss: 0.4690 - acc: 0.9318 - mDice: 0.6131 - val_loss: 0.7776 - val_acc: 0.9427 - val_mDice: 0.5970

Epoch 00022: val_mDice did not improve from 0.60761
Epoch 23/300
 - 8s - loss: 0.4588 - acc: 0.9325 - mDice: 0.6188 - val_loss: 0.7932 - val_acc: 0.9426 - val_mDice: 0.5972

Epoch 00023: val_mDice did not improve from 0.60761
Epoch 24/300
 - 8s - loss: 0.4588 - acc: 0.9326 - mDice: 0.6192 - val_loss: 0.6931 - val_acc: 0.9469 - val_mDice: 0.6147

Epoch 00024: val_mDice improved from 0.60761 to 0.61467, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 8s - loss: 0.4544 - acc: 0.9330 - mDice: 0.6217 - val_loss: 0.9009 - val_acc: 0.9431 - val_mDice: 0.5986

Epoch 00025: val_mDice did not improve from 0.61467
Epoch 26/300
 - 8s - loss: 0.4537 - acc: 0.9329 - mDice: 0.6224 - val_loss: 0.9273 - val_acc: 0.9440 - val_mDice: 0.5890

Epoch 00026: val_mDice did not improve from 0.61467
Epoch 27/300
 - 8s - loss: 0.4481 - acc: 0.9332 - mDice: 0.6259 - val_loss: 0.6459 - val_acc: 0.9329 - val_mDice: 0.5816

Epoch 00027: val_mDice did not improve from 0.61467
Epoch 28/300
 - 8s - loss: 0.4462 - acc: 0.9336 - mDice: 0.6272 - val_loss: 1.1288 - val_acc: 0.9416 - val_mDice: 0.5531

Epoch 00028: val_mDice did not improve from 0.61467
Epoch 29/300
 - 8s - loss: 0.4460 - acc: 0.9334 - mDice: 0.6272 - val_loss: 0.5591 - val_acc: 0.9379 - val_mDice: 0.6021

Epoch 00029: val_mDice did not improve from 0.61467
Epoch 30/300
 - 8s - loss: 0.4424 - acc: 0.9338 - mDice: 0.6296 - val_loss: 0.7172 - val_acc: 0.9459 - val_mDice: 0.5965

Epoch 00030: val_mDice did not improve from 0.61467
Epoch 31/300
 - 8s - loss: 0.4381 - acc: 0.9340 - mDice: 0.6319 - val_loss: 0.9532 - val_acc: 0.9364 - val_mDice: 0.5179

Epoch 00031: val_mDice did not improve from 0.61467
Epoch 32/300
 - 8s - loss: 0.4469 - acc: 0.9338 - mDice: 0.6312 - val_loss: 0.6334 - val_acc: 0.9424 - val_mDice: 0.6032

Epoch 00032: val_mDice did not improve from 0.61467
Epoch 33/300
 - 8s - loss: 0.4359 - acc: 0.9343 - mDice: 0.6336 - val_loss: 0.6176 - val_acc: 0.9452 - val_mDice: 0.6076

Epoch 00033: val_mDice did not improve from 0.61467
Epoch 34/300
 - 8s - loss: 0.4330 - acc: 0.9343 - mDice: 0.6352 - val_loss: 0.6466 - val_acc: 0.9467 - val_mDice: 0.6125

Epoch 00034: val_mDice did not improve from 0.61467
Epoch 35/300
 - 8s - loss: 0.4324 - acc: 0.9345 - mDice: 0.6357 - val_loss: 0.7869 - val_acc: 0.9475 - val_mDice: 0.6071

Epoch 00035: val_mDice did not improve from 0.61467
Epoch 36/300
 - 8s - loss: 0.4326 - acc: 0.9345 - mDice: 0.6358 - val_loss: 0.7852 - val_acc: 0.9474 - val_mDice: 0.6008

Epoch 00036: val_mDice did not improve from 0.61467
Epoch 37/300
 - 8s - loss: 0.4319 - acc: 0.9344 - mDice: 0.6362 - val_loss: 0.7758 - val_acc: 0.9477 - val_mDice: 0.6060

Epoch 00037: val_mDice did not improve from 0.61467
Epoch 38/300
 - 8s - loss: 0.4361 - acc: 0.9346 - mDice: 0.6361 - val_loss: 0.5619 - val_acc: 0.9485 - val_mDice: 0.6054

Epoch 00038: val_mDice did not improve from 0.61467
Epoch 39/300
 - 8s - loss: 0.4269 - acc: 0.9349 - mDice: 0.6395 - val_loss: 0.6507 - val_acc: 0.9469 - val_mDice: 0.6156

Epoch 00039: val_mDice improved from 0.61467 to 0.61556, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 8s - loss: 0.4262 - acc: 0.9349 - mDice: 0.6398 - val_loss: 0.7574 - val_acc: 0.9464 - val_mDice: 0.6055

Epoch 00040: val_mDice did not improve from 0.61556
Epoch 41/300
 - 8s - loss: 0.4225 - acc: 0.9352 - mDice: 0.6420 - val_loss: 0.5756 - val_acc: 0.9503 - val_mDice: 0.6177

Epoch 00041: val_mDice improved from 0.61556 to 0.61770, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 8s - loss: 0.4236 - acc: 0.9350 - mDice: 0.6412 - val_loss: 0.5833 - val_acc: 0.9469 - val_mDice: 0.6186

Epoch 00042: val_mDice improved from 0.61770 to 0.61864, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 8s - loss: 0.4206 - acc: 0.9354 - mDice: 0.6431 - val_loss: 0.6932 - val_acc: 0.9465 - val_mDice: 0.6137

Epoch 00043: val_mDice did not improve from 0.61864
Epoch 44/300
 - 8s - loss: 0.4227 - acc: 0.9352 - mDice: 0.6420 - val_loss: 0.8594 - val_acc: 0.9397 - val_mDice: 0.5399

Epoch 00044: val_mDice did not improve from 0.61864
Epoch 45/300
 - 8s - loss: 0.4192 - acc: 0.9356 - mDice: 0.6443 - val_loss: 0.7002 - val_acc: 0.9466 - val_mDice: 0.6006

Epoch 00045: val_mDice did not improve from 0.61864
Epoch 46/300
 - 8s - loss: 0.4216 - acc: 0.9352 - mDice: 0.6428 - val_loss: 0.6954 - val_acc: 0.9453 - val_mDice: 0.6120

Epoch 00046: val_mDice did not improve from 0.61864
Epoch 47/300
 - 8s - loss: 0.4158 - acc: 0.9357 - mDice: 0.6462 - val_loss: 0.7101 - val_acc: 0.9456 - val_mDice: 0.6061

Epoch 00047: val_mDice did not improve from 0.61864
Epoch 48/300
 - 8s - loss: 0.4183 - acc: 0.9356 - mDice: 0.6447 - val_loss: 0.7892 - val_acc: 0.9421 - val_mDice: 0.6028

Epoch 00048: val_mDice did not improve from 0.61864
Epoch 49/300
 - 8s - loss: 0.4161 - acc: 0.9356 - mDice: 0.6462 - val_loss: 0.6884 - val_acc: 0.9493 - val_mDice: 0.6180

Epoch 00049: val_mDice did not improve from 0.61864
Epoch 50/300
 - 8s - loss: 0.4254 - acc: 0.9356 - mDice: 0.6454 - val_loss: 0.6459 - val_acc: 0.9447 - val_mDice: 0.6044

Epoch 00050: val_mDice did not improve from 0.61864
Epoch 51/300
 - 8s - loss: 0.4152 - acc: 0.9357 - mDice: 0.6468 - val_loss: 0.7208 - val_acc: 0.9429 - val_mDice: 0.6044

Epoch 00051: val_mDice did not improve from 0.61864
Epoch 52/300
 - 8s - loss: 0.4159 - acc: 0.9355 - mDice: 0.6463 - val_loss: 0.8039 - val_acc: 0.9466 - val_mDice: 0.5842

Epoch 00052: val_mDice did not improve from 0.61864
Epoch 53/300
 - 8s - loss: 0.4180 - acc: 0.9357 - mDice: 0.6452 - val_loss: 0.7115 - val_acc: 0.9423 - val_mDice: 0.5982

Epoch 00053: val_mDice did not improve from 0.61864
Epoch 54/300
 - 8s - loss: 0.4126 - acc: 0.9358 - mDice: 0.6483 - val_loss: 0.5425 - val_acc: 0.9500 - val_mDice: 0.6209

Epoch 00054: val_mDice improved from 0.61864 to 0.62086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 8s - loss: 0.4128 - acc: 0.9357 - mDice: 0.6483 - val_loss: 0.6953 - val_acc: 0.9476 - val_mDice: 0.6173

Epoch 00055: val_mDice did not improve from 0.62086
Epoch 56/300
 - 8s - loss: 0.4159 - acc: 0.9355 - mDice: 0.6462 - val_loss: 0.5594 - val_acc: 0.9381 - val_mDice: 0.5965

Epoch 00056: val_mDice did not improve from 0.62086
Epoch 57/300
 - 8s - loss: 0.4150 - acc: 0.9359 - mDice: 0.6470 - val_loss: 0.6849 - val_acc: 0.9428 - val_mDice: 0.6119

Epoch 00057: val_mDice did not improve from 0.62086
Epoch 58/300
 - 8s - loss: 0.4142 - acc: 0.9358 - mDice: 0.6474 - val_loss: 0.7503 - val_acc: 0.9496 - val_mDice: 0.6136

Epoch 00058: val_mDice did not improve from 0.62086
Epoch 59/300
 - 8s - loss: 0.4111 - acc: 0.9360 - mDice: 0.6494 - val_loss: 0.6604 - val_acc: 0.9434 - val_mDice: 0.6090

Epoch 00059: val_mDice did not improve from 0.62086
Epoch 60/300
 - 8s - loss: 0.4128 - acc: 0.9359 - mDice: 0.6483 - val_loss: 0.5951 - val_acc: 0.9480 - val_mDice: 0.6207

Epoch 00060: val_mDice did not improve from 0.62086
Epoch 61/300
 - 8s - loss: 0.4111 - acc: 0.9360 - mDice: 0.6496 - val_loss: 0.6415 - val_acc: 0.9478 - val_mDice: 0.6119

Epoch 00061: val_mDice did not improve from 0.62086
Epoch 62/300
 - 8s - loss: 0.4110 - acc: 0.9361 - mDice: 0.6496 - val_loss: 0.9649 - val_acc: 0.9372 - val_mDice: 0.5688

Epoch 00062: val_mDice did not improve from 0.62086
Epoch 63/300
 - 8s - loss: 0.4109 - acc: 0.9361 - mDice: 0.6496 - val_loss: 0.8190 - val_acc: 0.9452 - val_mDice: 0.5908

Epoch 00063: val_mDice did not improve from 0.62086
Epoch 64/300
 - 8s - loss: 0.4076 - acc: 0.9362 - mDice: 0.6516 - val_loss: 0.6802 - val_acc: 0.9465 - val_mDice: 0.6210

Epoch 00064: val_mDice improved from 0.62086 to 0.62101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 8s - loss: 0.4065 - acc: 0.9363 - mDice: 0.6523 - val_loss: 0.8684 - val_acc: 0.9390 - val_mDice: 0.5356

Epoch 00065: val_mDice did not improve from 0.62101
Epoch 66/300
 - 8s - loss: 0.4088 - acc: 0.9363 - mDice: 0.6510 - val_loss: 0.6755 - val_acc: 0.9498 - val_mDice: 0.6145

Epoch 00066: val_mDice did not improve from 0.62101
Epoch 67/300
 - 8s - loss: 0.4055 - acc: 0.9365 - mDice: 0.6531 - val_loss: 0.8216 - val_acc: 0.9480 - val_mDice: 0.6047

Epoch 00067: val_mDice did not improve from 0.62101
Epoch 68/300
 - 8s - loss: 0.4152 - acc: 0.9364 - mDice: 0.6513 - val_loss: 0.6595 - val_acc: 0.9311 - val_mDice: 0.5447

Epoch 00068: val_mDice did not improve from 0.62101
Epoch 69/300
 - 8s - loss: 0.4148 - acc: 0.9358 - mDice: 0.6470 - val_loss: 0.6946 - val_acc: 0.9477 - val_mDice: 0.6021

Epoch 00069: val_mDice did not improve from 0.62101
Epoch 70/300
 - 9s - loss: 0.4074 - acc: 0.9365 - mDice: 0.6522 - val_loss: 0.8329 - val_acc: 0.9435 - val_mDice: 0.5756

Epoch 00070: val_mDice did not improve from 0.62101
Epoch 71/300
 - 8s - loss: 0.4074 - acc: 0.9365 - mDice: 0.6519 - val_loss: 0.7718 - val_acc: 0.9454 - val_mDice: 0.5897

Epoch 00071: val_mDice did not improve from 0.62101
Epoch 72/300
 - 8s - loss: 0.4066 - acc: 0.9362 - mDice: 0.6524 - val_loss: 0.6983 - val_acc: 0.9466 - val_mDice: 0.6139

Epoch 00072: val_mDice did not improve from 0.62101
Epoch 73/300
 - 9s - loss: 0.4066 - acc: 0.9366 - mDice: 0.6526 - val_loss: 0.7377 - val_acc: 0.9470 - val_mDice: 0.5972

Epoch 00073: val_mDice did not improve from 0.62101
Epoch 74/300
 - 8s - loss: 0.4053 - acc: 0.9364 - mDice: 0.6532 - val_loss: 0.8273 - val_acc: 0.9491 - val_mDice: 0.6149

Epoch 00074: val_mDice did not improve from 0.62101
Epoch 75/300
 - 9s - loss: 0.4107 - acc: 0.9362 - mDice: 0.6500 - val_loss: 0.4879 - val_acc: 0.9454 - val_mDice: 0.5776

Epoch 00075: val_mDice did not improve from 0.62101
Epoch 76/300
 - 8s - loss: 0.4064 - acc: 0.9365 - mDice: 0.6529 - val_loss: 0.7553 - val_acc: 0.9450 - val_mDice: 0.5901

Epoch 00076: val_mDice did not improve from 0.62101
Epoch 77/300
 - 8s - loss: 0.4046 - acc: 0.9366 - mDice: 0.6537 - val_loss: 0.7669 - val_acc: 0.9368 - val_mDice: 0.5925

Epoch 00077: val_mDice did not improve from 0.62101
Epoch 78/300
 - 8s - loss: 0.4028 - acc: 0.9367 - mDice: 0.6549 - val_loss: 0.7118 - val_acc: 0.9476 - val_mDice: 0.6128

Epoch 00078: val_mDice did not improve from 0.62101
Epoch 79/300
 - 8s - loss: 0.4058 - acc: 0.9364 - mDice: 0.6528 - val_loss: 0.5472 - val_acc: 0.9465 - val_mDice: 0.5896

Epoch 00079: val_mDice did not improve from 0.62101
Epoch 80/300
 - 8s - loss: 0.4048 - acc: 0.9365 - mDice: 0.6537 - val_loss: 0.6860 - val_acc: 0.9477 - val_mDice: 0.6021

Epoch 00080: val_mDice did not improve from 0.62101
Epoch 81/300
 - 8s - loss: 0.4026 - acc: 0.9368 - mDice: 0.6549 - val_loss: 0.6426 - val_acc: 0.9494 - val_mDice: 0.6185

Epoch 00081: val_mDice did not improve from 0.62101
Epoch 82/300
 - 8s - loss: 0.4030 - acc: 0.9368 - mDice: 0.6546 - val_loss: 0.6995 - val_acc: 0.9478 - val_mDice: 0.6103

Epoch 00082: val_mDice did not improve from 0.62101
Epoch 83/300
 - 8s - loss: 0.4069 - acc: 0.9366 - mDice: 0.6524 - val_loss: 0.5446 - val_acc: 0.9487 - val_mDice: 0.6123

Epoch 00083: val_mDice did not improve from 0.62101
Epoch 84/300
 - 8s - loss: 0.4008 - acc: 0.9369 - mDice: 0.6563 - val_loss: 0.7247 - val_acc: 0.9473 - val_mDice: 0.6021

Epoch 00084: val_mDice did not improve from 0.62101
Epoch 85/300
 - 8s - loss: 0.4014 - acc: 0.9369 - mDice: 0.6557 - val_loss: 0.7757 - val_acc: 0.9439 - val_mDice: 0.5771

Epoch 00085: val_mDice did not improve from 0.62101
Epoch 86/300
 - 8s - loss: 0.4022 - acc: 0.9368 - mDice: 0.6553 - val_loss: 0.8372 - val_acc: 0.9480 - val_mDice: 0.6023

Epoch 00086: val_mDice did not improve from 0.62101
Epoch 87/300
 - 8s - loss: 0.4027 - acc: 0.9367 - mDice: 0.6549 - val_loss: 0.5584 - val_acc: 0.9396 - val_mDice: 0.6025

Epoch 00087: val_mDice did not improve from 0.62101
Epoch 88/300
 - 8s - loss: 0.4017 - acc: 0.9368 - mDice: 0.6554 - val_loss: 0.4706 - val_acc: 0.9483 - val_mDice: 0.6154

Epoch 00088: val_mDice did not improve from 0.62101
Epoch 89/300
 - 8s - loss: 0.4012 - acc: 0.9369 - mDice: 0.6559 - val_loss: 0.5843 - val_acc: 0.9488 - val_mDice: 0.6117

Epoch 00089: val_mDice did not improve from 0.62101
Epoch 90/300
 - 8s - loss: 0.4011 - acc: 0.9368 - mDice: 0.6560 - val_loss: 0.5528 - val_acc: 0.9394 - val_mDice: 0.6005

Epoch 00090: val_mDice did not improve from 0.62101
Epoch 91/300
 - 8s - loss: 0.4024 - acc: 0.9367 - mDice: 0.6551 - val_loss: 0.6861 - val_acc: 0.9443 - val_mDice: 0.5755

Epoch 00091: val_mDice did not improve from 0.62101
Epoch 92/300
 - 8s - loss: 0.4035 - acc: 0.9367 - mDice: 0.6546 - val_loss: 0.8363 - val_acc: 0.9479 - val_mDice: 0.6077

Epoch 00092: val_mDice did not improve from 0.62101
Epoch 93/300
 - 8s - loss: 0.4037 - acc: 0.9370 - mDice: 0.6563 - val_loss: 0.7470 - val_acc: 0.9430 - val_mDice: 0.5960

Epoch 00093: val_mDice did not improve from 0.62101
Epoch 94/300
 - 8s - loss: 0.4005 - acc: 0.9369 - mDice: 0.6562 - val_loss: 0.6811 - val_acc: 0.9482 - val_mDice: 0.6068

Epoch 00094: val_mDice did not improve from 0.62101
Epoch 95/300
 - 8s - loss: 0.4011 - acc: 0.9369 - mDice: 0.6561 - val_loss: 0.6285 - val_acc: 0.9493 - val_mDice: 0.6137

Epoch 00095: val_mDice did not improve from 0.62101
Epoch 96/300
 - 8s - loss: 0.3979 - acc: 0.9370 - mDice: 0.6580 - val_loss: 0.5476 - val_acc: 0.9489 - val_mDice: 0.6220

Epoch 00096: val_mDice improved from 0.62101 to 0.62202, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 8s - loss: 0.3996 - acc: 0.9370 - mDice: 0.6570 - val_loss: 0.7265 - val_acc: 0.9479 - val_mDice: 0.6088

Epoch 00097: val_mDice did not improve from 0.62202
Epoch 98/300
 - 8s - loss: 0.3988 - acc: 0.9370 - mDice: 0.6575 - val_loss: 0.6197 - val_acc: 0.9441 - val_mDice: 0.6093

Epoch 00098: val_mDice did not improve from 0.62202
Epoch 99/300
 - 8s - loss: 0.4071 - acc: 0.9371 - mDice: 0.6573 - val_loss: 0.5440 - val_acc: 0.9500 - val_mDice: 0.6150

Epoch 00099: val_mDice did not improve from 0.62202
Epoch 100/300
 - 8s - loss: 0.3980 - acc: 0.9372 - mDice: 0.6580 - val_loss: 0.6486 - val_acc: 0.9497 - val_mDice: 0.6173

Epoch 00100: val_mDice did not improve from 0.62202
Epoch 101/300
 - 8s - loss: 0.4001 - acc: 0.9371 - mDice: 0.6570 - val_loss: 0.5069 - val_acc: 0.9491 - val_mDice: 0.6232

Epoch 00101: val_mDice improved from 0.62202 to 0.62318, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 8s - loss: 0.3959 - acc: 0.9371 - mDice: 0.6592 - val_loss: 0.5173 - val_acc: 0.9507 - val_mDice: 0.6175

Epoch 00102: val_mDice did not improve from 0.62318
Epoch 103/300
 - 8s - loss: 0.3996 - acc: 0.9369 - mDice: 0.6570 - val_loss: 0.6938 - val_acc: 0.9453 - val_mDice: 0.5885

Epoch 00103: val_mDice did not improve from 0.62318
Epoch 104/300
 - 8s - loss: 0.3965 - acc: 0.9370 - mDice: 0.6589 - val_loss: 0.8219 - val_acc: 0.9460 - val_mDice: 0.6100

Epoch 00104: val_mDice did not improve from 0.62318
Epoch 105/300
 - 8s - loss: 0.3967 - acc: 0.9373 - mDice: 0.6589 - val_loss: 0.7251 - val_acc: 0.9446 - val_mDice: 0.6082

Epoch 00105: val_mDice did not improve from 0.62318
Epoch 106/300
 - 8s - loss: 0.4040 - acc: 0.9373 - mDice: 0.6595 - val_loss: 0.5290 - val_acc: 0.9471 - val_mDice: 0.6157

Epoch 00106: val_mDice did not improve from 0.62318
Epoch 107/300
 - 8s - loss: 0.3971 - acc: 0.9370 - mDice: 0.6586 - val_loss: 0.7031 - val_acc: 0.9407 - val_mDice: 0.5969

Epoch 00107: val_mDice did not improve from 0.62318
Epoch 108/300
 - 8s - loss: 0.3964 - acc: 0.9372 - mDice: 0.6590 - val_loss: 0.6878 - val_acc: 0.9481 - val_mDice: 0.6154

Epoch 00108: val_mDice did not improve from 0.62318
Epoch 109/300
 - 8s - loss: 0.3984 - acc: 0.9372 - mDice: 0.6580 - val_loss: 0.5749 - val_acc: 0.9457 - val_mDice: 0.6081

Epoch 00109: val_mDice did not improve from 0.62318
Epoch 110/300
 - 8s - loss: 0.3951 - acc: 0.9374 - mDice: 0.6600 - val_loss: 0.5599 - val_acc: 0.9466 - val_mDice: 0.6114

Epoch 00110: val_mDice did not improve from 0.62318
Epoch 111/300
 - 8s - loss: 0.3948 - acc: 0.9375 - mDice: 0.6602 - val_loss: 0.6644 - val_acc: 0.9507 - val_mDice: 0.6246

Epoch 00111: val_mDice improved from 0.62318 to 0.62458, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 8s - loss: 0.3966 - acc: 0.9373 - mDice: 0.6591 - val_loss: 0.6022 - val_acc: 0.9471 - val_mDice: 0.6205

Epoch 00112: val_mDice did not improve from 0.62458
Epoch 113/300
 - 8s - loss: 0.3946 - acc: 0.9374 - mDice: 0.6604 - val_loss: 0.5577 - val_acc: 0.9467 - val_mDice: 0.6139

Epoch 00113: val_mDice did not improve from 0.62458
Epoch 114/300
 - 8s - loss: 0.3976 - acc: 0.9373 - mDice: 0.6587 - val_loss: 0.5259 - val_acc: 0.9495 - val_mDice: 0.6112

Epoch 00114: val_mDice did not improve from 0.62458
Epoch 115/300
 - 8s - loss: 0.3942 - acc: 0.9374 - mDice: 0.6606 - val_loss: 0.5560 - val_acc: 0.9452 - val_mDice: 0.6152

Epoch 00115: val_mDice did not improve from 0.62458
Epoch 116/300
 - 8s - loss: 0.3967 - acc: 0.9371 - mDice: 0.6589 - val_loss: 0.5024 - val_acc: 0.9505 - val_mDice: 0.6228

Epoch 00116: val_mDice did not improve from 0.62458
Epoch 117/300
 - 8s - loss: 0.3958 - acc: 0.9372 - mDice: 0.6595 - val_loss: 0.7568 - val_acc: 0.9469 - val_mDice: 0.6010

Epoch 00117: val_mDice did not improve from 0.62458
Epoch 118/300
 - 8s - loss: 0.4041 - acc: 0.9372 - mDice: 0.6596 - val_loss: 0.7409 - val_acc: 0.9346 - val_mDice: 0.5863

Epoch 00118: val_mDice did not improve from 0.62458
Epoch 119/300
 - 8s - loss: 0.4240 - acc: 0.9354 - mDice: 0.6413 - val_loss: 0.7452 - val_acc: 0.9459 - val_mDice: 0.5926

Epoch 00119: val_mDice did not improve from 0.62458
Epoch 120/300
 - 8s - loss: 0.4019 - acc: 0.9368 - mDice: 0.6556 - val_loss: 0.7020 - val_acc: 0.9468 - val_mDice: 0.6057

Epoch 00120: val_mDice did not improve from 0.62458
Epoch 121/300
 - 8s - loss: 0.4183 - acc: 0.9358 - mDice: 0.6481 - val_loss: 0.5933 - val_acc: 0.9384 - val_mDice: 0.6005

Epoch 00121: val_mDice did not improve from 0.62458
Epoch 122/300
 - 8s - loss: 0.4045 - acc: 0.9368 - mDice: 0.6539 - val_loss: 0.7538 - val_acc: 0.9438 - val_mDice: 0.5735

Epoch 00122: val_mDice did not improve from 0.62458
Epoch 123/300
 - 8s - loss: 0.3953 - acc: 0.9373 - mDice: 0.6597 - val_loss: 0.7694 - val_acc: 0.9440 - val_mDice: 0.5927

Epoch 00123: val_mDice did not improve from 0.62458
Epoch 124/300
 - 8s - loss: 0.3951 - acc: 0.9375 - mDice: 0.6601 - val_loss: 0.6406 - val_acc: 0.9468 - val_mDice: 0.6167

Epoch 00124: val_mDice did not improve from 0.62458
Epoch 125/300
 - 8s - loss: 0.3988 - acc: 0.9374 - mDice: 0.6580 - val_loss: 0.7589 - val_acc: 0.9472 - val_mDice: 0.6016

Epoch 00125: val_mDice did not improve from 0.62458
Epoch 126/300
 - 8s - loss: 0.4020 - acc: 0.9374 - mDice: 0.6603 - val_loss: 0.5733 - val_acc: 0.9494 - val_mDice: 0.6158

Epoch 00126: val_mDice did not improve from 0.62458
Epoch 127/300
 - 8s - loss: 0.3951 - acc: 0.9375 - mDice: 0.6603 - val_loss: 0.6288 - val_acc: 0.9479 - val_mDice: 0.6202

Epoch 00127: val_mDice did not improve from 0.62458
Epoch 128/300
 - 8s - loss: 0.3934 - acc: 0.9375 - mDice: 0.6614 - val_loss: 0.6915 - val_acc: 0.9488 - val_mDice: 0.6114

Epoch 00128: val_mDice did not improve from 0.62458
Epoch 129/300
 - 8s - loss: 0.3922 - acc: 0.9377 - mDice: 0.6618 - val_loss: 0.5541 - val_acc: 0.9500 - val_mDice: 0.6171

Epoch 00129: val_mDice did not improve from 0.62458
Epoch 130/300
 - 8s - loss: 0.3954 - acc: 0.9376 - mDice: 0.6607 - val_loss: 0.6808 - val_acc: 0.9480 - val_mDice: 0.6159

Epoch 00130: val_mDice did not improve from 0.62458
Epoch 131/300
 - 8s - loss: 0.3921 - acc: 0.9377 - mDice: 0.6619 - val_loss: 0.9040 - val_acc: 0.9399 - val_mDice: 0.5481

Epoch 00131: val_mDice did not improve from 0.62458
Epoch 132/300
 - 8s - loss: 0.3926 - acc: 0.9377 - mDice: 0.6618 - val_loss: 0.6700 - val_acc: 0.9499 - val_mDice: 0.6209

Epoch 00132: val_mDice did not improve from 0.62458
Epoch 133/300
 - 8s - loss: 0.3914 - acc: 0.9376 - mDice: 0.6624 - val_loss: 0.6390 - val_acc: 0.9487 - val_mDice: 0.6150

Epoch 00133: val_mDice did not improve from 0.62458
Epoch 134/300
 - 8s - loss: 0.3916 - acc: 0.9377 - mDice: 0.6622 - val_loss: 0.6585 - val_acc: 0.9460 - val_mDice: 0.6149

Epoch 00134: val_mDice did not improve from 0.62458
Epoch 135/300
 - 8s - loss: 0.3921 - acc: 0.9377 - mDice: 0.6621 - val_loss: 0.5511 - val_acc: 0.9412 - val_mDice: 0.6047

Epoch 00135: val_mDice did not improve from 0.62458
Epoch 136/300
 - 8s - loss: 0.3934 - acc: 0.9375 - mDice: 0.6612 - val_loss: 0.5349 - val_acc: 0.9459 - val_mDice: 0.6183

Epoch 00136: val_mDice did not improve from 0.62458
Epoch 137/300
 - 8s - loss: 0.3997 - acc: 0.9376 - mDice: 0.6619 - val_loss: 0.5340 - val_acc: 0.9404 - val_mDice: 0.6102

Epoch 00137: val_mDice did not improve from 0.62458
Epoch 138/300
 - 8s - loss: 0.3932 - acc: 0.9376 - mDice: 0.6613 - val_loss: 0.5010 - val_acc: 0.9492 - val_mDice: 0.6177

Epoch 00138: val_mDice did not improve from 0.62458
Epoch 139/300
 - 8s - loss: 0.3930 - acc: 0.9378 - mDice: 0.6618 - val_loss: 0.6992 - val_acc: 0.9440 - val_mDice: 0.5955

Epoch 00139: val_mDice did not improve from 0.62458
Epoch 140/300
 - 8s - loss: 0.3925 - acc: 0.9377 - mDice: 0.6617 - val_loss: 0.7052 - val_acc: 0.9458 - val_mDice: 0.5994

Epoch 00140: val_mDice did not improve from 0.62458
Epoch 141/300
 - 8s - loss: 0.3949 - acc: 0.9375 - mDice: 0.6602 - val_loss: 0.5314 - val_acc: 0.9493 - val_mDice: 0.6191

Epoch 00141: val_mDice did not improve from 0.62458
Epoch 142/300
 - 10s - loss: 0.3942 - acc: 0.9375 - mDice: 0.6608 - val_loss: 0.6917 - val_acc: 0.9487 - val_mDice: 0.6141

Epoch 00142: val_mDice did not improve from 0.62458
Epoch 143/300
 - 8s - loss: 0.3911 - acc: 0.9377 - mDice: 0.6625 - val_loss: 0.5139 - val_acc: 0.9507 - val_mDice: 0.6236

Epoch 00143: val_mDice did not improve from 0.62458
Epoch 144/300
 - 9s - loss: 0.3931 - acc: 0.9376 - mDice: 0.6614 - val_loss: 0.6789 - val_acc: 0.9412 - val_mDice: 0.6029

Epoch 00144: val_mDice did not improve from 0.62458
Epoch 145/300
 - 8s - loss: 0.3927 - acc: 0.9376 - mDice: 0.6617 - val_loss: 0.7283 - val_acc: 0.9489 - val_mDice: 0.6142

Epoch 00145: val_mDice did not improve from 0.62458
Epoch 146/300
 - 8s - loss: 0.3918 - acc: 0.9377 - mDice: 0.6623 - val_loss: 0.6668 - val_acc: 0.9493 - val_mDice: 0.6137

Epoch 00146: val_mDice did not improve from 0.62458
Epoch 147/300
 - 9s - loss: 0.3913 - acc: 0.9377 - mDice: 0.6627 - val_loss: 0.9238 - val_acc: 0.9439 - val_mDice: 0.5705

Epoch 00147: val_mDice did not improve from 0.62458
Epoch 148/300
 - 8s - loss: 0.3906 - acc: 0.9378 - mDice: 0.6632 - val_loss: 0.5986 - val_acc: 0.9460 - val_mDice: 0.5804

Epoch 00148: val_mDice did not improve from 0.62458
Epoch 149/300
 - 8s - loss: 0.3919 - acc: 0.9376 - mDice: 0.6620 - val_loss: 0.7061 - val_acc: 0.9434 - val_mDice: 0.5970

Epoch 00149: val_mDice did not improve from 0.62458
Epoch 150/300
 - 8s - loss: 0.3924 - acc: 0.9376 - mDice: 0.6620 - val_loss: 0.5243 - val_acc: 0.9483 - val_mDice: 0.6185

Epoch 00150: val_mDice did not improve from 0.62458
Epoch 151/300
 - 8s - loss: 0.3901 - acc: 0.9378 - mDice: 0.6633 - val_loss: 0.8670 - val_acc: 0.9471 - val_mDice: 0.5964

Epoch 00151: val_mDice did not improve from 0.62458
Restoring model weights from the end of the best epoch
Epoch 00151: early stopping
{'val_loss': [2.9868889309111095, 2.2079265997523354, 1.96883311016219, 1.690131588351159, 1.4698495609419686, 1.6731111747877938, 1.2281009405851364, 1.138403547661645, 1.108802123793534, 1.1436266324349813, 0.9581923789921261, 1.1588540169454755, 1.1262588202953339, 0.7891582243499302, 2.4560326508113315, 0.9528566613083794, 0.8083498094763074, 0.6966109754783767, 0.7596230283379555, 0.7916641643359548, 0.9281405196303413, 0.7776450206126485, 0.7932379873735564, 0.6931025673236165, 0.900949895027138, 0.9272973232326054, 0.6459058977308727, 1.1288312822580338, 0.5590552038380078, 0.7171722572474253, 0.953237727639221, 0.6334016883657092, 0.6176351390424228, 0.6465979548437255, 0.7869107411021278, 0.7851717188244774, 0.7758395129016468, 0.5618528574705124, 0.6506917590186709, 0.757449087642488, 0.5756400667485737, 0.5833392299356914, 0.6932218163495972, 0.8593746583376612, 0.7001907836113658, 0.6953840536021051, 0.7100632034596943, 0.7891737442641031, 0.6883983033753577, 0.6458539022576242, 0.7208467771609625, 0.8038743095738548, 0.7115359455347061, 0.5425304138944262, 0.6953353378034773, 0.5593755571615129, 0.6849235413329942, 0.75031280624015, 0.6603731931675048, 0.5951362770228159, 0.64154594710895, 0.9648637278448968, 0.8189990438875698, 0.680209877945128, 0.8683585638091678, 0.6755299351754642, 0.8215911831884157, 0.6595332636719659, 0.6945644166497957, 0.8328542421971049, 0.7718102211753527, 0.6983271954315049, 0.7377011946269444, 0.8273308191980634, 0.48792916749204907, 0.7552634307316372, 0.7669114374688694, 0.7118352999289831, 0.5472048258497602, 0.6860330016130493, 0.6426123430331548, 0.6994746118074372, 0.5446123821394784, 0.7247236505860374, 0.7757333981848898, 0.8372292185113543, 0.5584037421005112, 0.4705976290362222, 0.5843295717523211, 0.552832027276357, 0.686062216049149, 0.8362654440459751, 0.7469563555149805, 0.6810851313528561, 0.6285260961878867, 0.5475787704899198, 0.726463007075446, 0.6196806054739725, 0.5439972547548158, 0.6486299023741767, 0.5069037725528082, 0.517325469780536, 0.6938497278661955, 0.8219243983427683, 0.7251026538156328, 0.5289556284745535, 0.7031479697851908, 0.6877554726032984, 0.5748513277087893, 0.5598879222358976, 0.6644054456126123, 0.6021770345313209, 0.5576536886039234, 0.5258841780679566, 0.5560135192104748, 0.5023708435751143, 0.756821789202236, 0.7409177152883439, 0.7451721014721053, 0.7019855142349288, 0.593251899949142, 0.7537847672189985, 0.7694350952903429, 0.640622788596721, 0.7588812928824198, 0.5732653708685012, 0.6288032843953087, 0.6914503116692815, 0.5541207836497397, 0.6807836088396254, 0.9040456407126927, 0.6699514271957534, 0.6390172544689405, 0.6584566586783954, 0.5511062954153333, 0.5349267185443923, 0.5340421061430659, 0.5009697012248493, 0.6992338466502371, 0.7052375237856593, 0.5313769390895253, 0.6917058868067605, 0.5139460829751832, 0.6788958094659305, 0.728292666375637, 0.6668023789922396, 0.9238153784757569, 0.5985928408446766, 0.7060777591097922, 0.5243327564426831, 0.8670092966584932], 'val_acc': [0.9067846146367845, 0.8882198532422384, 0.9207290226504916, 0.9210151795830045, 0.9058337467057365, 0.9253103491805849, 0.9294364530415762, 0.9335104993411473, 0.9345602428629285, 0.9350454246714002, 0.9377874781688055, 0.914042999347051, 0.9351325553087961, 0.9441079681827909, 0.8109221614542461, 0.9429073660146623, 0.945823691430546, 0.937663890066601, 0.9383364007586524, 0.943938883287566, 0.9463973662682942, 0.9427044299386796, 0.942615977355412, 0.9468500266472498, 0.9431141785212925, 0.9440260132153829, 0.9328939339944294, 0.9416000793377558, 0.9378733216297059, 0.945916052375521, 0.936369620618366, 0.9423688281150091, 0.9452331640890667, 0.9466887279635384, 0.9475121349096298, 0.9473677376906077, 0.9477449712299165, 0.9485306356634412, 0.9468877336808613, 0.9464428758337384, 0.9502801845471064, 0.9468630325226557, 0.9464740916377022, 0.9396931067818687, 0.9466002654461634, 0.9452747702598572, 0.9456311904248738, 0.9420657597836994, 0.9492551648900622, 0.9446920106808344, 0.9428540305012748, 0.9466236808470317, 0.9422855873902639, 0.949997916108086, 0.9475693525302977, 0.938095775388536, 0.9427941704080218, 0.9496167820124399, 0.9433964434124175, 0.9479803982235137, 0.9478321181876319, 0.9371800138836816, 0.9452292550177801, 0.946468900356974, 0.9389829025382087, 0.9497663804462978, 0.9479895212820598, 0.9310923417409261, 0.9476838069302695, 0.9435408420505977, 0.9453788314546857, 0.9466171995514915, 0.946960584748359, 0.9491029899744761, 0.9454425835893268, 0.9450484542619615, 0.9367637683947881, 0.9475732502483186, 0.9465209217298598, 0.9476929214738664, 0.9494424838395346, 0.947754082935197, 0.9487387580530984, 0.947255878221421, 0.9439115737165723, 0.948009023354167, 0.9396189798911413, 0.9483238338004976, 0.9487556651944206, 0.9394095433609826, 0.9442666918039322, 0.9478659473714375, 0.9429658814555123, 0.9482119324661437, 0.9492928932110468, 0.9489338596661886, 0.9478555172681808, 0.9440507377896991, 0.9500265462057931, 0.949735165351913, 0.9490600639865512, 0.9507159640391668, 0.9452565610408783, 0.9459902211314156, 0.9446386957452411, 0.947124475524539, 0.9407363342387336, 0.9480805801493781, 0.9457469610940843, 0.9466458083618254, 0.9507445614962351, 0.9471192721809659, 0.9466770305520013, 0.9495062090101696, 0.9452318641401473, 0.9504974278665724, 0.9468812502565838, 0.9345628250212896, 0.9459004487310138, 0.9467758912415731, 0.9384079497484934, 0.9437957973707289, 0.9439726918935776, 0.9467654675245285, 0.9472012306962695, 0.9493839393059412, 0.9479244621027083, 0.9487621848072324, 0.9500018308560053, 0.9480402512209756, 0.9398934422504335, 0.9499250742651167, 0.9486776313611439, 0.9460201348577227, 0.9412176325207665, 0.945875740477017, 0.9403617275612695, 0.949166714435532, 0.9439648950383777, 0.945831515959331, 0.9492733719803038, 0.9486711096195948, 0.9507016447328386, 0.9412254286663873, 0.9489299853642782, 0.9493228253864107, 0.9438920602911994, 0.9460305238053912, 0.9433860409827459, 0.9482938895622889, 0.9470659572453726], 'val_mDice': [0.16304883697912806, 0.2487902758376939, 0.3169519542938187, 0.3819352246466137, 0.429778217559769, 0.36523881802956265, 0.4885013337646212, 0.4933848242674555, 0.5057523239936147, 0.5140606367162296, 0.5500413796731404, 0.5180580860802105, 0.5131540844837824, 0.5892272836395672, 0.2651536830124401, 0.5917399880431947, 0.6032632447424389, 0.5949230584360304, 0.5899576601527986, 0.6076079305438769, 0.5931571225325266, 0.5970495804434731, 0.597231727980432, 0.6146680354362443, 0.5985710631523814, 0.5889960243588402, 0.5816274051155362, 0.5531054316532045, 0.6021014004945755, 0.5965068758953185, 0.5179415419697762, 0.603150391862506, 0.6075718672502608, 0.612463195054304, 0.6071414379846483, 0.6008051264853704, 0.6060316555556797, 0.6053517411152521, 0.6155583546275184, 0.6055370086715335, 0.6177041828632355, 0.6186388606826464, 0.6136560340722402, 0.5398611937250409, 0.6006420992669605, 0.6119791240919203, 0.6060932484411058, 0.6027530570115361, 0.6180015664015498, 0.604438060096332, 0.6044476734740394, 0.5841955269376436, 0.5982331743552571, 0.6208568009592238, 0.6173353496761549, 0.596534625405357, 0.611893626905623, 0.6135794328791755, 0.6089893455306689, 0.6207055403363138, 0.6119215640993345, 0.5688456364330792, 0.5907938984178361, 0.6210051292464847, 0.5355677558552652, 0.6144770696049645, 0.6046918140990394, 0.5447443759157544, 0.6020777608667102, 0.5756404385680244, 0.5896776474657512, 0.6139173571552549, 0.5972104022900263, 0.614945772148314, 0.5775723918562844, 0.5901481338909694, 0.5925400853157043, 0.612776942551136, 0.5895832571245375, 0.6021217952171961, 0.6184863348801931, 0.610323422011875, 0.6123203291069894, 0.6020659034450849, 0.5771050190641767, 0.6023045558305014, 0.6024518910618055, 0.6153666845389775, 0.6116913833788463, 0.6004715590249925, 0.5754666009119579, 0.6076968351290339, 0.59601377589362, 0.6068285483689535, 0.6137025097296351, 0.622019170650414, 0.6088272656003634, 0.6093332292068572, 0.6149809108603568, 0.6172769254162198, 0.6231803979192462, 0.6175471547813642, 0.5884862676972434, 0.6099715633761316, 0.6082148197151366, 0.6157487098659787, 0.5968750431424096, 0.6153966940584636, 0.608065997560819, 0.6114373824426106, 0.6245840823366529, 0.6205321117526009, 0.6138595974161511, 0.6111630513554528, 0.6152485147828147, 0.6227641098556065, 0.6009713347469058, 0.5862518112574305, 0.5925788003064337, 0.6056644834932827, 0.60046943738347, 0.5734620580361003, 0.5927056386357262, 0.616685980842227, 0.6015774469290461, 0.6157772867452531, 0.6202198884316853, 0.6113763139361427, 0.6171039286113921, 0.6158736766803832, 0.5481481520192963, 0.6209126494470096, 0.6149864948931194, 0.6148763242222014, 0.6046974545433408, 0.6182942582028252, 0.6102388068324044, 0.6177078666431564, 0.5954755963314147, 0.5993709361978939, 0.6191304272839001, 0.6140996647023019, 0.623579414117904, 0.6029183289834431, 0.6141641477034205, 0.613674269545646, 0.5704897528602964, 0.580376015887374, 0.5969729473193487, 0.6185382249809447, 0.5964263535681225], 'loss': [3.0501291917891225, 1.8064918833522743, 1.3728179715802078, 1.1602936724102264, 1.0402079465329481, 0.9421071581189678, 0.8795920188712543, 0.8142485162009768, 0.7375957852950668, 0.6877144531951309, 0.6461976879486466, 0.6086651022055687, 0.5773641634784369, 0.5529119276070661, 0.5431969049581246, 0.5316473150983826, 0.5102027677891978, 0.49578720082147537, 0.48496274114651267, 0.47933168638715506, 0.47082743662977616, 0.4689695278914194, 0.4588126389927187, 0.45876596241276246, 0.4544080127248525, 0.45370978764504777, 0.448052379746291, 0.4461592113739269, 0.44600062964686443, 0.4424012036874766, 0.4380744565330176, 0.4468906623075267, 0.43590678453113374, 0.43301925370288097, 0.43240882394041524, 0.432609161958721, 0.4318786872794701, 0.4361071752638538, 0.4269364091845276, 0.426150260197419, 0.42253468644320136, 0.42358671215915417, 0.4206277330439735, 0.4227361777864791, 0.41918127757593115, 0.42155905487145556, 0.41583813938589814, 0.4183386448034005, 0.41612663225875257, 0.4253899108898673, 0.41520808516770685, 0.4158704535708786, 0.41799828360339725, 0.4126332960088937, 0.41277404806075985, 0.4159045317378881, 0.4150251971977999, 0.41421125102840095, 0.4111496710677665, 0.4128201553250422, 0.411062009238267, 0.41100733974185827, 0.410853062250488, 0.40760899320618355, 0.4065361756800277, 0.4088021212965665, 0.4055421503473457, 0.4152082757697464, 0.41477174114716087, 0.40737121707881724, 0.4073755277896658, 0.40663364554514125, 0.406551867225376, 0.40526291653306373, 0.4107341588705695, 0.4063720001649724, 0.4045859794430746, 0.40278565344040107, 0.405801593178162, 0.4047948059763417, 0.4025957422502194, 0.40304218635253586, 0.406912707186675, 0.4007838277099524, 0.4014067238917922, 0.40222830609690846, 0.40271256087882273, 0.4017204987637512, 0.4012074710432865, 0.4010994247573332, 0.402379679480635, 0.403529864001739, 0.4037472487824209, 0.4005075406729345, 0.4010986852612668, 0.3979444511255514, 0.39960455089226404, 0.398833810120904, 0.4070983563957108, 0.39798807292595545, 0.4000736184438955, 0.3959005308184451, 0.3995957734857097, 0.3965265082615664, 0.39673486791945434, 0.4039951551259394, 0.39705450156272953, 0.3963874868197693, 0.39838800133104774, 0.39508323001994394, 0.39484467272306883, 0.3965640429998839, 0.394580854927929, 0.397564145598903, 0.3941654797882091, 0.3966883484035481, 0.39577042666318357, 0.4040734439175109, 0.4240291650746858, 0.4018787856221531, 0.41834892735175766, 0.4045423710910723, 0.3952604498205743, 0.39506477217156244, 0.3988382553987848, 0.40195554320526655, 0.39507458908976284, 0.3934091177657454, 0.39224566415491874, 0.39540130032802356, 0.39206862740197884, 0.39257526704859935, 0.3914230308990957, 0.39162447658421934, 0.39206682357283357, 0.3933869579542314, 0.39974639392496814, 0.39318674229977857, 0.39298214387760855, 0.3925036220165348, 0.39487820448649624, 0.39424155673276745, 0.3911180861481053, 0.3930796457864448, 0.3926593548408126, 0.39179950462742436, 0.3912732771346164, 0.3906340933609806, 0.39192982927006265, 0.3923937125458359, 0.3900980521875504], 'acc': [0.6282676728364485, 0.8724306534591824, 0.8858239683932249, 0.8919008707933771, 0.8970537818241916, 0.9021676863468457, 0.9056363309990397, 0.910478602046754, 0.9140908834329887, 0.9174049669321533, 0.9198651795267727, 0.9219877465853784, 0.9242977773910778, 0.9260486586844356, 0.9267688732293323, 0.9273403459604737, 0.9289875722861224, 0.9301890227788006, 0.9309254796724133, 0.9314005755116348, 0.9317766660435286, 0.9317869421167294, 0.9325385206589127, 0.9325813735760022, 0.9330225771184087, 0.9329043610182313, 0.9332104521209483, 0.9336028839552303, 0.9334078002772955, 0.933778414321145, 0.9339574992158619, 0.933770381640591, 0.9342680719569533, 0.9343213689692506, 0.9345380026317904, 0.934473794316847, 0.9343784692227675, 0.9346061573055129, 0.9349181115793318, 0.9348815890407828, 0.935205261853412, 0.9350290474453344, 0.9354006865562503, 0.9351548349956949, 0.9355662549770642, 0.935201096501523, 0.9356941159057086, 0.9355662830360753, 0.9356278373670445, 0.9356451484486253, 0.9357382331385918, 0.9355341923270053, 0.9357111610077882, 0.935832196125413, 0.9357283226626829, 0.9355489208173619, 0.9358602959797575, 0.935836458106559, 0.9360335880999445, 0.9359229693505757, 0.9360413063203392, 0.9361408902410013, 0.9361476605981174, 0.9362191757119798, 0.9362923170862756, 0.9362900547662485, 0.9364518755657759, 0.9363542388408629, 0.9357677462373272, 0.9364838691474999, 0.9364501256132525, 0.9362202945857991, 0.9366232676426348, 0.9364262377651289, 0.9361782676662244, 0.9364618589287017, 0.9366155016057006, 0.9366828756080032, 0.9364463001240595, 0.9365073432497327, 0.9368278975274238, 0.9367761558808988, 0.9365927845986772, 0.9369067636040924, 0.936850784050721, 0.9367653451922212, 0.9366941003746309, 0.936787135753791, 0.9368650303909706, 0.936832549180161, 0.9367088077792218, 0.9367121145917845, 0.9370270949220258, 0.9369128733292263, 0.9369417047102139, 0.9370474287394361, 0.936990034447407, 0.9370352537851148, 0.937074601650238, 0.9372085425847088, 0.9370515152272407, 0.9370686348766337, 0.9369284832710011, 0.936994932322117, 0.9372577766854119, 0.9372854603034209, 0.9370272184480863, 0.9372137042805345, 0.9371651288194577, 0.9373857299265423, 0.9375291699154463, 0.9373378116108249, 0.9373977114563201, 0.9372741878862832, 0.9373527629129734, 0.9370813721733837, 0.9371557537891738, 0.937221741443889, 0.9354095002402835, 0.9368115854462541, 0.9357633617262986, 0.9368358561587533, 0.9373323819431422, 0.9374537624024415, 0.9373877511715158, 0.937384293105941, 0.9374653740181564, 0.9375162146217644, 0.937665671691257, 0.9375888237049985, 0.9376866852340592, 0.9376954255329866, 0.9376420274086318, 0.9377076740384434, 0.937702119516463, 0.937518480926503, 0.9376367445111606, 0.9376288295456294, 0.9377801336949915, 0.9376559539757732, 0.9375364752200984, 0.9375340651336819, 0.9377157294650595, 0.9376250073770295, 0.9375915044197465, 0.9376975684776944, 0.9377232049501042, 0.9378493084548908, 0.9376334083113498, 0.9376185300622478, 0.937813053722169], 'mDice': [0.08753122505537314, 0.20909398312688207, 0.2869780453798831, 0.3381822747606421, 0.373634970619818, 0.40457535254922083, 0.42517207342934143, 0.44941531316815647, 0.4774222868730763, 0.49878270015079, 0.5172171821833321, 0.5339183564969756, 0.549929947408129, 0.563110001239936, 0.5703525989832652, 0.5750011006437636, 0.5871764657225117, 0.5959108797288538, 0.6029268944827959, 0.6066167181912903, 0.6113867817814967, 0.613095796041834, 0.6188203838542311, 0.6191893942176797, 0.6217475541786895, 0.6224486580466162, 0.6259001984901748, 0.6271721528101101, 0.6272098666114063, 0.6295924975347386, 0.6319046342605336, 0.6311910622299216, 0.6336361628056901, 0.6352193501666396, 0.6357478906185182, 0.6357856714957936, 0.6362124480576901, 0.6360582496794486, 0.6394550516412782, 0.6397646016397184, 0.6419691186097339, 0.6412312483057007, 0.643091787036747, 0.6420287704069302, 0.6442528362061652, 0.6427593948449265, 0.6461965257410883, 0.644731207644375, 0.6462227600862721, 0.645436021610887, 0.6468096055028166, 0.6462917626734231, 0.6451981960896994, 0.6482594647447378, 0.648251388730445, 0.646231118849061, 0.6470458587232074, 0.6473528668076879, 0.6494363031679541, 0.6482640079801129, 0.6495929587850332, 0.6496023225253007, 0.6496135927840527, 0.6516148831518912, 0.6522766172387806, 0.6509616648918407, 0.6530810061603536, 0.6512694704167358, 0.647045759437476, 0.6521993845618203, 0.6519091001460147, 0.6524038783022952, 0.6526128548433522, 0.6531808851157058, 0.6500374888975308, 0.6529393118072021, 0.6536556747962505, 0.6548903391885891, 0.6527948857681997, 0.653737474948915, 0.6548563389725007, 0.6546109745429419, 0.6524481650514523, 0.6562560640669799, 0.6557094793465809, 0.6552570165033792, 0.654886660801667, 0.65540249998523, 0.6559282846769583, 0.6560372535896832, 0.6550879458531031, 0.6545914388300649, 0.6563295187392275, 0.6562291961526472, 0.6561087398143864, 0.6580023921631837, 0.6570004028862233, 0.657500504783269, 0.6573058861543873, 0.6579988666895704, 0.6570056275072869, 0.6591922805169831, 0.6570071762318731, 0.658925190941537, 0.6589092444576593, 0.6594748575043213, 0.6585748712000409, 0.6589947802774754, 0.6579713806468465, 0.6600327216151033, 0.6601619564391112, 0.6591265189946528, 0.6603712235320577, 0.658713622677625, 0.6606345311180795, 0.6589397869402319, 0.6595252244253345, 0.6595807905648744, 0.6413300269825545, 0.6556093211293552, 0.6480747061187511, 0.6539253956096086, 0.6597432937462682, 0.660102801070572, 0.6580248915053344, 0.6602951560511895, 0.6603339173335551, 0.6613861974899483, 0.6617998250015599, 0.6607252136910526, 0.6619281225881869, 0.6617878220539571, 0.6623796266101529, 0.6622379161521252, 0.6620926729151797, 0.6611694396372293, 0.6619078043775638, 0.6613011406656759, 0.6617792424717321, 0.6617175317408315, 0.6602356465414042, 0.6608217375192138, 0.6624714846730564, 0.6614481263838107, 0.6617127104057907, 0.6623265339803562, 0.6627093440642928, 0.6631679113196796, 0.6620353878706611, 0.662013346604318, 0.6632993540059889]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:03<00:14,  3.60s/it]predicting test subjects:  40%|████      | 2/5 [00:06<00:10,  3.39s/it]predicting test subjects:  60%|██████    | 3/5 [00:08<00:06,  3.12s/it]predicting test subjects:  80%|████████  | 4/5 [00:11<00:02,  2.94s/it]predicting test subjects: 100%|██████████| 5/5 [00:14<00:00,  3.01s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:03<15:29,  3.51s/it]predicting train subjects:   1%|          | 2/266 [00:06<15:08,  3.44s/it]predicting train subjects:   1%|          | 3/266 [00:09<14:17,  3.26s/it]predicting train subjects:   2%|▏         | 4/266 [00:12<13:45,  3.15s/it]predicting train subjects:   2%|▏         | 5/266 [00:15<13:54,  3.20s/it]predicting train subjects:   2%|▏         | 6/266 [00:19<14:07,  3.26s/it]predicting train subjects:   3%|▎         | 7/266 [00:22<14:16,  3.31s/it]predicting train subjects:   3%|▎         | 8/266 [00:26<14:18,  3.33s/it]predicting train subjects:   3%|▎         | 9/266 [00:29<14:17,  3.34s/it]predicting train subjects:   4%|▍         | 10/266 [00:32<14:21,  3.37s/it]predicting train subjects:   4%|▍         | 11/266 [00:36<14:19,  3.37s/it]predicting train subjects:   5%|▍         | 12/266 [00:39<14:19,  3.38s/it]predicting train subjects:   5%|▍         | 13/266 [00:43<14:21,  3.41s/it]predicting train subjects:   5%|▌         | 14/266 [00:46<14:22,  3.42s/it]predicting train subjects:   6%|▌         | 15/266 [00:49<14:14,  3.40s/it]predicting train subjects:   6%|▌         | 16/266 [00:53<14:14,  3.42s/it]predicting train subjects:   6%|▋         | 17/266 [00:56<14:08,  3.41s/it]predicting train subjects:   7%|▋         | 18/266 [01:00<14:03,  3.40s/it]predicting train subjects:   7%|▋         | 19/266 [01:03<14:08,  3.43s/it]predicting train subjects:   8%|▊         | 20/266 [01:07<14:07,  3.44s/it]predicting train subjects:   8%|▊         | 21/266 [01:10<13:58,  3.42s/it]predicting train subjects:   8%|▊         | 22/266 [01:13<13:51,  3.41s/it]predicting train subjects:   9%|▊         | 23/266 [01:17<13:48,  3.41s/it]predicting train subjects:   9%|▉         | 24/266 [01:20<13:34,  3.37s/it]predicting train subjects:   9%|▉         | 25/266 [01:23<13:25,  3.34s/it]predicting train subjects:  10%|▉         | 26/266 [01:27<13:19,  3.33s/it]predicting train subjects:  10%|█         | 27/266 [01:30<13:13,  3.32s/it]predicting train subjects:  11%|█         | 28/266 [01:33<13:05,  3.30s/it]predicting train subjects:  11%|█         | 29/266 [01:36<12:56,  3.27s/it]predicting train subjects:  11%|█▏        | 30/266 [01:40<12:50,  3.27s/it]predicting train subjects:  12%|█▏        | 31/266 [01:43<12:47,  3.27s/it]predicting train subjects:  12%|█▏        | 32/266 [01:46<12:40,  3.25s/it]predicting train subjects:  12%|█▏        | 33/266 [01:49<12:36,  3.25s/it]predicting train subjects:  13%|█▎        | 34/266 [01:53<12:31,  3.24s/it]predicting train subjects:  13%|█▎        | 35/266 [01:56<12:27,  3.24s/it]predicting train subjects:  14%|█▎        | 36/266 [01:59<12:27,  3.25s/it]predicting train subjects:  14%|█▍        | 37/266 [02:02<12:28,  3.27s/it]predicting train subjects:  14%|█▍        | 38/266 [02:06<12:27,  3.28s/it]predicting train subjects:  15%|█▍        | 39/266 [02:09<12:28,  3.30s/it]predicting train subjects:  15%|█▌        | 40/266 [02:12<12:24,  3.29s/it]predicting train subjects:  15%|█▌        | 41/266 [02:16<12:15,  3.27s/it]predicting train subjects:  16%|█▌        | 42/266 [02:18<11:41,  3.13s/it]predicting train subjects:  16%|█▌        | 43/266 [02:21<11:07,  2.99s/it]predicting train subjects:  17%|█▋        | 44/266 [02:24<10:48,  2.92s/it]predicting train subjects:  17%|█▋        | 45/266 [02:27<10:43,  2.91s/it]predicting train subjects:  17%|█▋        | 46/266 [02:30<10:37,  2.90s/it]predicting train subjects:  18%|█▊        | 47/266 [02:32<10:37,  2.91s/it]predicting train subjects:  18%|█▊        | 48/266 [02:35<10:31,  2.90s/it]predicting train subjects:  18%|█▊        | 49/266 [02:38<10:27,  2.89s/it]predicting train subjects:  19%|█▉        | 50/266 [02:41<10:20,  2.87s/it]predicting train subjects:  19%|█▉        | 51/266 [02:44<10:19,  2.88s/it]predicting train subjects:  20%|█▉        | 52/266 [02:47<10:15,  2.88s/it]predicting train subjects:  20%|█▉        | 53/266 [02:50<10:15,  2.89s/it]predicting train subjects:  20%|██        | 54/266 [02:53<10:09,  2.88s/it]predicting train subjects:  21%|██        | 55/266 [02:55<10:03,  2.86s/it]predicting train subjects:  21%|██        | 56/266 [02:58<10:03,  2.87s/it]predicting train subjects:  21%|██▏       | 57/266 [03:01<09:58,  2.87s/it]predicting train subjects:  22%|██▏       | 58/266 [03:04<09:55,  2.86s/it]predicting train subjects:  22%|██▏       | 59/266 [03:07<09:52,  2.86s/it]predicting train subjects:  23%|██▎       | 60/266 [03:10<09:40,  2.82s/it]predicting train subjects:  23%|██▎       | 61/266 [03:12<09:34,  2.80s/it]predicting train subjects:  23%|██▎       | 62/266 [03:15<09:19,  2.74s/it]predicting train subjects:  24%|██▎       | 63/266 [03:18<09:17,  2.75s/it]predicting train subjects:  24%|██▍       | 64/266 [03:20<09:15,  2.75s/it]predicting train subjects:  24%|██▍       | 65/266 [03:23<09:03,  2.71s/it]predicting train subjects:  25%|██▍       | 66/266 [03:26<08:55,  2.68s/it]predicting train subjects:  25%|██▌       | 67/266 [03:28<08:50,  2.67s/it]predicting train subjects:  26%|██▌       | 68/266 [03:31<08:38,  2.62s/it]predicting train subjects:  26%|██▌       | 69/266 [03:33<08:36,  2.62s/it]predicting train subjects:  26%|██▋       | 70/266 [03:36<08:31,  2.61s/it]predicting train subjects:  27%|██▋       | 71/266 [03:39<08:30,  2.62s/it]predicting train subjects:  27%|██▋       | 72/266 [03:41<08:23,  2.59s/it]predicting train subjects:  27%|██▋       | 73/266 [03:44<08:17,  2.58s/it]predicting train subjects:  28%|██▊       | 74/266 [03:46<08:15,  2.58s/it]predicting train subjects:  28%|██▊       | 75/266 [03:49<08:10,  2.57s/it]predicting train subjects:  29%|██▊       | 76/266 [03:51<08:06,  2.56s/it]predicting train subjects:  29%|██▉       | 77/266 [03:54<08:01,  2.55s/it]predicting train subjects:  29%|██▉       | 78/266 [03:57<08:43,  2.78s/it]predicting train subjects:  30%|██▉       | 79/266 [04:01<09:10,  2.95s/it]predicting train subjects:  30%|███       | 80/266 [04:04<09:27,  3.05s/it]predicting train subjects:  30%|███       | 81/266 [04:07<09:34,  3.11s/it]predicting train subjects:  31%|███       | 82/266 [04:10<09:44,  3.18s/it]predicting train subjects:  31%|███       | 83/266 [04:14<09:48,  3.22s/it]predicting train subjects:  32%|███▏      | 84/266 [04:17<09:50,  3.25s/it]predicting train subjects:  32%|███▏      | 85/266 [04:20<09:50,  3.26s/it]predicting train subjects:  32%|███▏      | 86/266 [04:24<09:49,  3.28s/it]predicting train subjects:  33%|███▎      | 87/266 [04:27<09:50,  3.30s/it]predicting train subjects:  33%|███▎      | 88/266 [04:30<09:44,  3.28s/it]predicting train subjects:  33%|███▎      | 89/266 [04:34<09:43,  3.30s/it]predicting train subjects:  34%|███▍      | 90/266 [04:37<09:39,  3.29s/it]predicting train subjects:  34%|███▍      | 91/266 [04:40<09:34,  3.28s/it]predicting train subjects:  35%|███▍      | 92/266 [04:43<09:32,  3.29s/it]predicting train subjects:  35%|███▍      | 93/266 [04:47<09:28,  3.29s/it]predicting train subjects:  35%|███▌      | 94/266 [04:50<09:26,  3.29s/it]predicting train subjects:  36%|███▌      | 95/266 [04:53<09:20,  3.28s/it]predicting train subjects:  36%|███▌      | 96/266 [04:56<09:02,  3.19s/it]predicting train subjects:  36%|███▋      | 97/266 [05:00<09:06,  3.23s/it]predicting train subjects:  37%|███▋      | 98/266 [05:03<09:00,  3.22s/it]predicting train subjects:  37%|███▋      | 99/266 [05:05<08:17,  2.98s/it]predicting train subjects:  38%|███▊      | 100/266 [05:08<08:04,  2.92s/it]predicting train subjects:  38%|███▊      | 101/266 [05:11<08:02,  2.93s/it]predicting train subjects:  38%|███▊      | 102/266 [05:14<08:05,  2.96s/it]predicting train subjects:  39%|███▊      | 103/266 [05:17<08:01,  2.95s/it]predicting train subjects:  39%|███▉      | 104/266 [05:20<07:55,  2.94s/it]predicting train subjects:  39%|███▉      | 105/266 [05:23<07:52,  2.93s/it]predicting train subjects:  40%|███▉      | 106/266 [05:26<07:47,  2.92s/it]predicting train subjects:  40%|████      | 107/266 [05:29<07:47,  2.94s/it]predicting train subjects:  41%|████      | 108/266 [05:32<07:47,  2.96s/it]predicting train subjects:  41%|████      | 109/266 [05:35<07:46,  2.97s/it]predicting train subjects:  41%|████▏     | 110/266 [05:38<07:42,  2.97s/it]predicting train subjects:  42%|████▏     | 111/266 [05:41<07:44,  3.00s/it]predicting train subjects:  42%|████▏     | 112/266 [05:44<07:43,  3.01s/it]predicting train subjects:  42%|████▏     | 113/266 [05:47<07:38,  3.00s/it]predicting train subjects:  43%|████▎     | 114/266 [05:50<07:33,  2.98s/it]predicting train subjects:  43%|████▎     | 115/266 [05:53<07:35,  3.01s/it]predicting train subjects:  44%|████▎     | 116/266 [05:56<07:30,  3.00s/it]predicting train subjects:  44%|████▍     | 117/266 [05:59<07:24,  2.98s/it]predicting train subjects:  44%|████▍     | 118/266 [06:02<07:21,  2.98s/it]predicting train subjects:  45%|████▍     | 119/266 [06:05<07:36,  3.11s/it]predicting train subjects:  45%|████▌     | 120/266 [06:08<07:40,  3.16s/it]predicting train subjects:  45%|████▌     | 121/266 [06:12<07:43,  3.20s/it]predicting train subjects:  46%|████▌     | 122/266 [06:15<07:45,  3.23s/it]predicting train subjects:  46%|████▌     | 123/266 [06:18<07:45,  3.26s/it]predicting train subjects:  47%|████▋     | 124/266 [06:21<07:45,  3.28s/it]predicting train subjects:  47%|████▋     | 125/266 [06:25<07:41,  3.27s/it]predicting train subjects:  47%|████▋     | 126/266 [06:28<07:38,  3.28s/it]predicting train subjects:  48%|████▊     | 127/266 [06:31<07:39,  3.31s/it]predicting train subjects:  48%|████▊     | 128/266 [06:35<07:35,  3.30s/it]predicting train subjects:  48%|████▊     | 129/266 [06:38<07:34,  3.32s/it]predicting train subjects:  49%|████▉     | 130/266 [06:41<07:29,  3.30s/it]predicting train subjects:  49%|████▉     | 131/266 [06:45<07:27,  3.31s/it]predicting train subjects:  50%|████▉     | 132/266 [06:48<07:22,  3.30s/it]predicting train subjects:  50%|█████     | 133/266 [06:51<07:18,  3.30s/it]predicting train subjects:  50%|█████     | 134/266 [06:54<07:12,  3.27s/it]predicting train subjects:  51%|█████     | 135/266 [06:58<07:11,  3.30s/it]predicting train subjects:  51%|█████     | 136/266 [07:01<07:08,  3.30s/it]predicting train subjects:  52%|█████▏    | 137/266 [07:04<07:01,  3.27s/it]predicting train subjects:  52%|█████▏    | 138/266 [07:07<06:55,  3.24s/it]predicting train subjects:  52%|█████▏    | 139/266 [07:11<06:47,  3.21s/it]predicting train subjects:  53%|█████▎    | 140/266 [07:14<06:42,  3.20s/it]predicting train subjects:  53%|█████▎    | 141/266 [07:17<06:38,  3.19s/it]predicting train subjects:  53%|█████▎    | 142/266 [07:20<06:36,  3.20s/it]predicting train subjects:  54%|█████▍    | 143/266 [07:23<06:31,  3.18s/it]predicting train subjects:  54%|█████▍    | 144/266 [07:27<06:28,  3.18s/it]predicting train subjects:  55%|█████▍    | 145/266 [07:30<06:24,  3.17s/it]predicting train subjects:  55%|█████▍    | 146/266 [07:33<06:20,  3.17s/it]predicting train subjects:  55%|█████▌    | 147/266 [07:36<06:19,  3.19s/it]predicting train subjects:  56%|█████▌    | 148/266 [07:39<06:14,  3.18s/it]predicting train subjects:  56%|█████▌    | 149/266 [07:42<06:10,  3.17s/it]predicting train subjects:  56%|█████▋    | 150/266 [07:46<06:10,  3.20s/it]predicting train subjects:  57%|█████▋    | 151/266 [07:49<06:07,  3.20s/it]predicting train subjects:  57%|█████▋    | 152/266 [07:52<06:06,  3.21s/it]predicting train subjects:  58%|█████▊    | 153/266 [07:55<06:00,  3.19s/it]predicting train subjects:  58%|█████▊    | 154/266 [07:58<05:57,  3.19s/it]predicting train subjects:  58%|█████▊    | 155/266 [08:01<05:31,  2.99s/it]predicting train subjects:  59%|█████▊    | 156/266 [08:03<05:15,  2.87s/it]predicting train subjects:  59%|█████▉    | 157/266 [08:06<04:57,  2.73s/it]predicting train subjects:  59%|█████▉    | 158/266 [08:08<04:41,  2.61s/it]predicting train subjects:  60%|█████▉    | 159/266 [08:11<04:33,  2.56s/it]predicting train subjects:  60%|██████    | 160/266 [08:13<04:25,  2.51s/it]predicting train subjects:  61%|██████    | 161/266 [08:16<04:27,  2.55s/it]predicting train subjects:  61%|██████    | 162/266 [08:18<04:28,  2.58s/it]predicting train subjects:  61%|██████▏   | 163/266 [08:21<04:27,  2.60s/it]predicting train subjects:  62%|██████▏   | 164/266 [08:24<04:22,  2.57s/it]predicting train subjects:  62%|██████▏   | 165/266 [08:26<04:16,  2.54s/it]predicting train subjects:  62%|██████▏   | 166/266 [08:29<04:13,  2.54s/it]predicting train subjects:  63%|██████▎   | 167/266 [08:31<04:11,  2.54s/it]predicting train subjects:  63%|██████▎   | 168/266 [08:33<04:00,  2.46s/it]predicting train subjects:  64%|██████▎   | 169/266 [08:36<03:58,  2.46s/it]predicting train subjects:  64%|██████▍   | 170/266 [08:38<03:59,  2.49s/it]predicting train subjects:  64%|██████▍   | 171/266 [08:41<03:56,  2.49s/it]predicting train subjects:  65%|██████▍   | 172/266 [08:43<03:50,  2.45s/it]predicting train subjects:  65%|██████▌   | 173/266 [08:46<03:55,  2.54s/it]predicting train subjects:  65%|██████▌   | 174/266 [08:49<04:03,  2.64s/it]predicting train subjects:  66%|██████▌   | 175/266 [08:52<04:06,  2.70s/it]predicting train subjects:  66%|██████▌   | 176/266 [08:54<04:06,  2.74s/it]predicting train subjects:  67%|██████▋   | 177/266 [08:57<04:06,  2.77s/it]predicting train subjects:  67%|██████▋   | 178/266 [09:00<04:07,  2.81s/it]predicting train subjects:  67%|██████▋   | 179/266 [09:03<04:05,  2.82s/it]predicting train subjects:  68%|██████▊   | 180/266 [09:06<04:03,  2.83s/it]predicting train subjects:  68%|██████▊   | 181/266 [09:09<03:59,  2.81s/it]predicting train subjects:  68%|██████▊   | 182/266 [09:12<03:57,  2.83s/it]predicting train subjects:  69%|██████▉   | 183/266 [09:14<03:53,  2.81s/it]predicting train subjects:  69%|██████▉   | 184/266 [09:17<03:50,  2.81s/it]predicting train subjects:  70%|██████▉   | 185/266 [09:20<03:48,  2.82s/it]predicting train subjects:  70%|██████▉   | 186/266 [09:23<03:46,  2.83s/it]predicting train subjects:  70%|███████   | 187/266 [09:26<03:43,  2.84s/it]predicting train subjects:  71%|███████   | 188/266 [09:29<03:40,  2.83s/it]predicting train subjects:  71%|███████   | 189/266 [09:31<03:37,  2.83s/it]predicting train subjects:  71%|███████▏  | 190/266 [09:34<03:35,  2.84s/it]predicting train subjects:  72%|███████▏  | 191/266 [09:37<03:36,  2.88s/it]predicting train subjects:  72%|███████▏  | 192/266 [09:40<03:26,  2.79s/it]predicting train subjects:  73%|███████▎  | 193/266 [09:42<03:20,  2.74s/it]predicting train subjects:  73%|███████▎  | 194/266 [09:46<03:28,  2.89s/it]predicting train subjects:  73%|███████▎  | 195/266 [09:49<03:26,  2.90s/it]predicting train subjects:  74%|███████▎  | 196/266 [09:51<03:23,  2.91s/it]predicting train subjects:  74%|███████▍  | 197/266 [09:54<03:22,  2.93s/it]predicting train subjects:  74%|███████▍  | 198/266 [09:57<03:20,  2.95s/it]predicting train subjects:  75%|███████▍  | 199/266 [10:00<03:17,  2.95s/it]predicting train subjects:  75%|███████▌  | 200/266 [10:03<03:14,  2.94s/it]predicting train subjects:  76%|███████▌  | 201/266 [10:06<03:12,  2.96s/it]predicting train subjects:  76%|███████▌  | 202/266 [10:09<03:09,  2.96s/it]predicting train subjects:  76%|███████▋  | 203/266 [10:12<03:07,  2.98s/it]predicting train subjects:  77%|███████▋  | 204/266 [10:15<02:58,  2.88s/it]predicting train subjects:  77%|███████▋  | 205/266 [10:18<02:57,  2.91s/it]predicting train subjects:  77%|███████▋  | 206/266 [10:21<02:56,  2.93s/it]predicting train subjects:  78%|███████▊  | 207/266 [10:24<02:53,  2.95s/it]predicting train subjects:  78%|███████▊  | 208/266 [10:27<02:47,  2.89s/it]predicting train subjects:  79%|███████▊  | 209/266 [10:29<02:40,  2.81s/it]predicting train subjects:  79%|███████▉  | 210/266 [10:32<02:38,  2.82s/it]predicting train subjects:  79%|███████▉  | 211/266 [10:35<02:36,  2.84s/it]predicting train subjects:  80%|███████▉  | 212/266 [10:38<02:34,  2.85s/it]predicting train subjects:  80%|████████  | 213/266 [10:40<02:22,  2.70s/it]predicting train subjects:  80%|████████  | 214/266 [10:43<02:14,  2.59s/it]predicting train subjects:  81%|████████  | 215/266 [10:45<02:09,  2.53s/it]predicting train subjects:  81%|████████  | 216/266 [10:47<02:05,  2.50s/it]predicting train subjects:  82%|████████▏ | 217/266 [10:50<02:02,  2.49s/it]predicting train subjects:  82%|████████▏ | 218/266 [10:53<02:01,  2.53s/it]predicting train subjects:  82%|████████▏ | 219/266 [10:55<01:58,  2.52s/it]predicting train subjects:  83%|████████▎ | 220/266 [10:57<01:54,  2.50s/it]predicting train subjects:  83%|████████▎ | 221/266 [11:00<01:52,  2.51s/it]predicting train subjects:  83%|████████▎ | 222/266 [11:02<01:47,  2.45s/it]predicting train subjects:  84%|████████▍ | 223/266 [11:05<01:45,  2.44s/it]predicting train subjects:  84%|████████▍ | 224/266 [11:07<01:43,  2.46s/it]predicting train subjects:  85%|████████▍ | 225/266 [11:10<01:40,  2.46s/it]predicting train subjects:  85%|████████▍ | 226/266 [11:12<01:38,  2.47s/it]predicting train subjects:  85%|████████▌ | 227/266 [11:15<01:36,  2.47s/it]predicting train subjects:  86%|████████▌ | 228/266 [11:17<01:32,  2.44s/it]predicting train subjects:  86%|████████▌ | 229/266 [11:20<01:32,  2.50s/it]predicting train subjects:  86%|████████▋ | 230/266 [11:22<01:27,  2.42s/it]predicting train subjects:  87%|████████▋ | 231/266 [11:24<01:25,  2.43s/it]predicting train subjects:  87%|████████▋ | 232/266 [11:27<01:24,  2.48s/it]predicting train subjects:  88%|████████▊ | 233/266 [11:29<01:21,  2.46s/it]predicting train subjects:  88%|████████▊ | 234/266 [11:32<01:19,  2.48s/it]predicting train subjects:  88%|████████▊ | 235/266 [11:34<01:16,  2.48s/it]predicting train subjects:  89%|████████▊ | 236/266 [11:37<01:15,  2.51s/it]predicting train subjects:  89%|████████▉ | 237/266 [11:39<01:12,  2.51s/it]predicting train subjects:  89%|████████▉ | 238/266 [11:42<01:09,  2.49s/it]predicting train subjects:  90%|████████▉ | 239/266 [11:44<01:06,  2.46s/it]predicting train subjects:  90%|█████████ | 240/266 [11:47<01:07,  2.61s/it]predicting train subjects:  91%|█████████ | 241/266 [11:50<01:07,  2.70s/it]predicting train subjects:  91%|█████████ | 242/266 [11:53<01:07,  2.80s/it]predicting train subjects:  91%|█████████▏| 243/266 [11:56<01:06,  2.89s/it]predicting train subjects:  92%|█████████▏| 244/266 [11:59<01:05,  2.96s/it]predicting train subjects:  92%|█████████▏| 245/266 [12:03<01:03,  3.03s/it]predicting train subjects:  92%|█████████▏| 246/266 [12:06<01:01,  3.08s/it]predicting train subjects:  93%|█████████▎| 247/266 [12:09<00:59,  3.13s/it]predicting train subjects:  93%|█████████▎| 248/266 [12:12<00:56,  3.13s/it]predicting train subjects:  94%|█████████▎| 249/266 [12:16<00:56,  3.35s/it]predicting train subjects:  94%|█████████▍| 250/266 [12:20<00:57,  3.59s/it]predicting train subjects:  94%|█████████▍| 251/266 [12:24<00:53,  3.59s/it]predicting train subjects:  95%|█████████▍| 252/266 [12:28<00:51,  3.64s/it]predicting train subjects:  95%|█████████▌| 253/266 [12:31<00:47,  3.66s/it]predicting train subjects:  95%|█████████▌| 254/266 [12:35<00:43,  3.64s/it]predicting train subjects:  96%|█████████▌| 255/266 [12:39<00:40,  3.65s/it]predicting train subjects:  96%|█████████▌| 256/266 [12:42<00:37,  3.74s/it]predicting train subjects:  97%|█████████▋| 257/266 [12:46<00:33,  3.71s/it]predicting train subjects:  97%|█████████▋| 258/266 [12:50<00:30,  3.76s/it]predicting train subjects:  97%|█████████▋| 259/266 [12:54<00:26,  3.72s/it]predicting train subjects:  98%|█████████▊| 260/266 [12:58<00:22,  3.82s/it]predicting train subjects:  98%|█████████▊| 261/266 [13:01<00:18,  3.73s/it]predicting train subjects:  98%|█████████▊| 262/266 [13:05<00:14,  3.72s/it]predicting train subjects:  99%|█████████▉| 263/266 [13:09<00:11,  3.79s/it]predicting train subjects:  99%|█████████▉| 264/266 [13:12<00:07,  3.72s/it]predicting train subjects: 100%|█████████▉| 265/266 [13:16<00:03,  3.72s/it]predicting train subjects: 100%|██████████| 266/266 [13:20<00:00,  3.66s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<02:56,  1.50it/s]Loading train:   1%|          | 2/266 [00:01<02:56,  1.50it/s]Loading train:   1%|          | 3/266 [00:01<02:50,  1.54it/s]Loading train:   2%|▏         | 4/266 [00:02<03:11,  1.37it/s]Loading train:   2%|▏         | 5/266 [00:03<03:13,  1.35it/s]Loading train:   2%|▏         | 6/266 [00:04<02:59,  1.45it/s]Loading train:   3%|▎         | 7/266 [00:04<02:53,  1.49it/s]Loading train:   3%|▎         | 8/266 [00:05<03:08,  1.37it/s]Loading train:   3%|▎         | 9/266 [00:06<02:57,  1.44it/s]Loading train:   4%|▍         | 10/266 [00:07<03:01,  1.41it/s]Loading train:   4%|▍         | 11/266 [00:07<02:59,  1.42it/s]Loading train:   5%|▍         | 12/266 [00:08<02:45,  1.54it/s]Loading train:   5%|▍         | 13/266 [00:08<02:43,  1.55it/s]Loading train:   5%|▌         | 14/266 [00:09<02:46,  1.52it/s]Loading train:   6%|▌         | 15/266 [00:10<02:45,  1.51it/s]Loading train:   6%|▌         | 16/266 [00:10<02:45,  1.51it/s]Loading train:   6%|▋         | 17/266 [00:11<02:42,  1.53it/s]Loading train:   7%|▋         | 18/266 [00:12<02:49,  1.46it/s]Loading train:   7%|▋         | 19/266 [00:12<02:43,  1.51it/s]Loading train:   8%|▊         | 20/266 [00:13<02:46,  1.48it/s]Loading train:   8%|▊         | 21/266 [00:14<02:54,  1.41it/s]Loading train:   8%|▊         | 22/266 [00:15<03:01,  1.35it/s]Loading train:   9%|▊         | 23/266 [00:15<02:54,  1.39it/s]Loading train:   9%|▉         | 24/266 [00:16<02:53,  1.39it/s]Loading train:   9%|▉         | 25/266 [00:17<02:54,  1.38it/s]Loading train:  10%|▉         | 26/266 [00:17<02:45,  1.45it/s]Loading train:  10%|█         | 27/266 [00:18<02:33,  1.55it/s]Loading train:  11%|█         | 28/266 [00:19<02:31,  1.57it/s]Loading train:  11%|█         | 29/266 [00:19<02:39,  1.48it/s]Loading train:  11%|█▏        | 30/266 [00:20<02:35,  1.52it/s]Loading train:  12%|█▏        | 31/266 [00:21<02:30,  1.56it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:26,  1.60it/s]Loading train:  12%|█▏        | 33/266 [00:22<02:20,  1.66it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:18,  1.68it/s]Loading train:  13%|█▎        | 35/266 [00:23<02:12,  1.74it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:07,  1.81it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:16,  1.67it/s]Loading train:  14%|█▍        | 38/266 [00:25<02:17,  1.66it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:11,  1.72it/s]Loading train:  15%|█▌        | 40/266 [00:26<02:08,  1.76it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:12,  1.70it/s]Loading train:  16%|█▌        | 42/266 [00:27<02:16,  1.64it/s]Loading train:  16%|█▌        | 43/266 [00:28<02:15,  1.65it/s]Loading train:  17%|█▋        | 44/266 [00:28<02:05,  1.77it/s]Loading train:  17%|█▋        | 45/266 [00:29<02:02,  1.80it/s]Loading train:  17%|█▋        | 46/266 [00:29<02:02,  1.79it/s]Loading train:  18%|█▊        | 47/266 [00:30<02:00,  1.82it/s]Loading train:  18%|█▊        | 48/266 [00:30<02:02,  1.78it/s]Loading train:  18%|█▊        | 49/266 [00:31<02:05,  1.72it/s]Loading train:  19%|█▉        | 50/266 [00:31<01:53,  1.91it/s]Loading train:  19%|█▉        | 51/266 [00:32<01:51,  1.92it/s]Loading train:  20%|█▉        | 52/266 [00:32<01:50,  1.94it/s]Loading train:  20%|█▉        | 53/266 [00:33<01:50,  1.93it/s]Loading train:  20%|██        | 54/266 [00:33<01:54,  1.85it/s]Loading train:  21%|██        | 55/266 [00:34<01:50,  1.91it/s]Loading train:  21%|██        | 56/266 [00:34<01:48,  1.93it/s]Loading train:  21%|██▏       | 57/266 [00:35<01:46,  1.97it/s]Loading train:  22%|██▏       | 58/266 [00:36<01:53,  1.84it/s]Loading train:  22%|██▏       | 59/266 [00:36<01:54,  1.80it/s]Loading train:  23%|██▎       | 60/266 [00:37<02:05,  1.65it/s]Loading train:  23%|██▎       | 61/266 [00:37<01:58,  1.73it/s]Loading train:  23%|██▎       | 62/266 [00:38<01:56,  1.76it/s]Loading train:  24%|██▎       | 63/266 [00:38<01:49,  1.85it/s]Loading train:  24%|██▍       | 64/266 [00:39<01:52,  1.80it/s]Loading train:  24%|██▍       | 65/266 [00:40<01:52,  1.79it/s]Loading train:  25%|██▍       | 66/266 [00:40<01:54,  1.75it/s]Loading train:  25%|██▌       | 67/266 [00:41<01:51,  1.79it/s]Loading train:  26%|██▌       | 68/266 [00:41<01:48,  1.83it/s]Loading train:  26%|██▌       | 69/266 [00:42<01:47,  1.83it/s]Loading train:  26%|██▋       | 70/266 [00:42<01:46,  1.83it/s]Loading train:  27%|██▋       | 71/266 [00:43<01:53,  1.72it/s]Loading train:  27%|██▋       | 72/266 [00:44<01:52,  1.72it/s]Loading train:  27%|██▋       | 73/266 [00:44<01:49,  1.77it/s]Loading train:  28%|██▊       | 74/266 [00:45<01:45,  1.82it/s]Loading train:  28%|██▊       | 75/266 [00:45<01:48,  1.76it/s]Loading train:  29%|██▊       | 76/266 [00:46<01:48,  1.75it/s]Loading train:  29%|██▉       | 77/266 [00:46<01:49,  1.72it/s]Loading train:  29%|██▉       | 78/266 [00:47<01:54,  1.64it/s]Loading train:  30%|██▉       | 79/266 [00:48<01:52,  1.66it/s]Loading train:  30%|███       | 80/266 [00:48<01:51,  1.67it/s]Loading train:  30%|███       | 81/266 [00:49<01:53,  1.63it/s]Loading train:  31%|███       | 82/266 [00:50<01:56,  1.58it/s]Loading train:  31%|███       | 83/266 [00:50<01:51,  1.64it/s]Loading train:  32%|███▏      | 84/266 [00:51<01:56,  1.56it/s]Loading train:  32%|███▏      | 85/266 [00:51<01:53,  1.59it/s]Loading train:  32%|███▏      | 86/266 [00:52<01:53,  1.58it/s]Loading train:  33%|███▎      | 87/266 [00:53<01:54,  1.57it/s]Loading train:  33%|███▎      | 88/266 [00:53<01:52,  1.58it/s]Loading train:  33%|███▎      | 89/266 [00:54<01:47,  1.65it/s]Loading train:  34%|███▍      | 90/266 [00:55<01:50,  1.60it/s]Loading train:  34%|███▍      | 91/266 [00:55<01:45,  1.65it/s]Loading train:  35%|███▍      | 92/266 [00:56<01:47,  1.62it/s]Loading train:  35%|███▍      | 93/266 [00:56<01:49,  1.58it/s]Loading train:  35%|███▌      | 94/266 [00:57<01:50,  1.56it/s]Loading train:  36%|███▌      | 95/266 [00:58<01:53,  1.51it/s]Loading train:  36%|███▌      | 96/266 [00:58<01:53,  1.50it/s]Loading train:  36%|███▋      | 97/266 [00:59<01:54,  1.47it/s]Loading train:  37%|███▋      | 98/266 [01:00<01:52,  1.49it/s]Loading train:  37%|███▋      | 99/266 [01:00<01:46,  1.57it/s]Loading train:  38%|███▊      | 100/266 [01:01<01:44,  1.58it/s]Loading train:  38%|███▊      | 101/266 [01:02<01:39,  1.67it/s]Loading train:  38%|███▊      | 102/266 [01:02<01:37,  1.69it/s]Loading train:  39%|███▊      | 103/266 [01:03<01:38,  1.65it/s]Loading train:  39%|███▉      | 104/266 [01:03<01:35,  1.70it/s]Loading train:  39%|███▉      | 105/266 [01:04<01:32,  1.74it/s]Loading train:  40%|███▉      | 106/266 [01:04<01:33,  1.72it/s]Loading train:  40%|████      | 107/266 [01:05<01:33,  1.70it/s]Loading train:  41%|████      | 108/266 [01:06<01:30,  1.75it/s]Loading train:  41%|████      | 109/266 [01:06<01:26,  1.81it/s]Loading train:  41%|████▏     | 110/266 [01:07<01:28,  1.76it/s]Loading train:  42%|████▏     | 111/266 [01:07<01:35,  1.62it/s]Loading train:  42%|████▏     | 112/266 [01:08<01:30,  1.70it/s]Loading train:  42%|████▏     | 113/266 [01:08<01:26,  1.77it/s]Loading train:  43%|████▎     | 114/266 [01:09<01:23,  1.81it/s]Loading train:  43%|████▎     | 115/266 [01:09<01:21,  1.85it/s]Loading train:  44%|████▎     | 116/266 [01:10<01:24,  1.77it/s]Loading train:  44%|████▍     | 117/266 [01:11<01:23,  1.79it/s]Loading train:  44%|████▍     | 118/266 [01:11<01:20,  1.83it/s]Loading train:  45%|████▍     | 119/266 [01:12<01:29,  1.63it/s]Loading train:  45%|████▌     | 120/266 [01:12<01:26,  1.69it/s]Loading train:  45%|████▌     | 121/266 [01:13<01:30,  1.60it/s]Loading train:  46%|████▌     | 122/266 [01:14<01:29,  1.61it/s]Loading train:  46%|████▌     | 123/266 [01:14<01:28,  1.61it/s]Loading train:  47%|████▋     | 124/266 [01:15<01:27,  1.62it/s]Loading train:  47%|████▋     | 125/266 [01:16<01:27,  1.61it/s]Loading train:  47%|████▋     | 126/266 [01:16<01:31,  1.54it/s]Loading train:  48%|████▊     | 127/266 [01:17<01:30,  1.54it/s]Loading train:  48%|████▊     | 128/266 [01:18<01:26,  1.60it/s]Loading train:  48%|████▊     | 129/266 [01:18<01:22,  1.66it/s]Loading train:  49%|████▉     | 130/266 [01:19<01:22,  1.64it/s]Loading train:  49%|████▉     | 131/266 [01:19<01:24,  1.61it/s]Loading train:  50%|████▉     | 132/266 [01:20<01:24,  1.59it/s]Loading train:  50%|█████     | 133/266 [01:21<01:23,  1.59it/s]Loading train:  50%|█████     | 134/266 [01:21<01:24,  1.56it/s]Loading train:  51%|█████     | 135/266 [01:22<01:27,  1.50it/s]Loading train:  51%|█████     | 136/266 [01:23<01:26,  1.51it/s]Loading train:  52%|█████▏    | 137/266 [01:23<01:22,  1.56it/s]Loading train:  52%|█████▏    | 138/266 [01:24<01:19,  1.61it/s]Loading train:  52%|█████▏    | 139/266 [01:25<01:17,  1.63it/s]Loading train:  53%|█████▎    | 140/266 [01:25<01:21,  1.54it/s]Loading train:  53%|█████▎    | 141/266 [01:26<01:17,  1.61it/s]Loading train:  53%|█████▎    | 142/266 [01:26<01:17,  1.59it/s]Loading train:  54%|█████▍    | 143/266 [01:27<01:19,  1.54it/s]Loading train:  54%|█████▍    | 144/266 [01:28<01:18,  1.55it/s]Loading train:  55%|█████▍    | 145/266 [01:28<01:16,  1.58it/s]Loading train:  55%|█████▍    | 146/266 [01:29<01:17,  1.56it/s]Loading train:  55%|█████▌    | 147/266 [01:30<01:18,  1.51it/s]Loading train:  56%|█████▌    | 148/266 [01:31<01:21,  1.45it/s]Loading train:  56%|█████▌    | 149/266 [01:31<01:17,  1.51it/s]Loading train:  56%|█████▋    | 150/266 [01:32<01:16,  1.52it/s]Loading train:  57%|█████▋    | 151/266 [01:32<01:12,  1.60it/s]Loading train:  57%|█████▋    | 152/266 [01:33<01:13,  1.55it/s]Loading train:  58%|█████▊    | 153/266 [01:34<01:12,  1.55it/s]Loading train:  58%|█████▊    | 154/266 [01:34<01:16,  1.47it/s]Loading train:  58%|█████▊    | 155/266 [01:35<01:12,  1.54it/s]Loading train:  59%|█████▊    | 156/266 [01:35<01:06,  1.66it/s]Loading train:  59%|█████▉    | 157/266 [01:36<01:06,  1.64it/s]Loading train:  59%|█████▉    | 158/266 [01:37<01:02,  1.72it/s]Loading train:  60%|█████▉    | 159/266 [01:37<00:57,  1.85it/s]Loading train:  60%|██████    | 160/266 [01:38<00:58,  1.83it/s]Loading train:  61%|██████    | 161/266 [01:38<00:55,  1.89it/s]Loading train:  61%|██████    | 162/266 [01:39<00:56,  1.84it/s]Loading train:  61%|██████▏   | 163/266 [01:39<00:54,  1.87it/s]Loading train:  62%|██████▏   | 164/266 [01:40<00:55,  1.84it/s]Loading train:  62%|██████▏   | 165/266 [01:40<00:53,  1.89it/s]Loading train:  62%|██████▏   | 166/266 [01:41<00:56,  1.78it/s]Loading train:  63%|██████▎   | 167/266 [01:42<00:57,  1.73it/s]Loading train:  63%|██████▎   | 168/266 [01:42<00:57,  1.71it/s]Loading train:  64%|██████▎   | 169/266 [01:43<00:59,  1.62it/s]Loading train:  64%|██████▍   | 170/266 [01:43<00:56,  1.68it/s]Loading train:  64%|██████▍   | 171/266 [01:44<00:52,  1.81it/s]Loading train:  65%|██████▍   | 172/266 [01:44<00:52,  1.80it/s]Loading train:  65%|██████▌   | 173/266 [01:45<00:53,  1.74it/s]Loading train:  65%|██████▌   | 174/266 [01:46<00:53,  1.72it/s]Loading train:  66%|██████▌   | 175/266 [01:46<00:50,  1.80it/s]Loading train:  66%|██████▌   | 176/266 [01:47<00:53,  1.68it/s]Loading train:  67%|██████▋   | 177/266 [01:47<00:51,  1.72it/s]Loading train:  67%|██████▋   | 178/266 [01:48<00:50,  1.73it/s]Loading train:  67%|██████▋   | 179/266 [01:48<00:50,  1.74it/s]Loading train:  68%|██████▊   | 180/266 [01:49<00:50,  1.71it/s]Loading train:  68%|██████▊   | 181/266 [01:50<00:48,  1.77it/s]Loading train:  68%|██████▊   | 182/266 [01:50<00:46,  1.79it/s]Loading train:  69%|██████▉   | 183/266 [01:51<00:48,  1.73it/s]Loading train:  69%|██████▉   | 184/266 [01:51<00:47,  1.72it/s]Loading train:  70%|██████▉   | 185/266 [01:52<00:49,  1.64it/s]Loading train:  70%|██████▉   | 186/266 [01:53<00:48,  1.64it/s]Loading train:  70%|███████   | 187/266 [01:53<00:47,  1.65it/s]Loading train:  71%|███████   | 188/266 [01:54<00:47,  1.64it/s]Loading train:  71%|███████   | 189/266 [01:54<00:47,  1.62it/s]Loading train:  71%|███████▏  | 190/266 [01:55<00:47,  1.61it/s]Loading train:  72%|███████▏  | 191/266 [01:56<00:48,  1.53it/s]Loading train:  72%|███████▏  | 192/266 [01:56<00:48,  1.54it/s]Loading train:  73%|███████▎  | 193/266 [01:57<00:46,  1.59it/s]Loading train:  73%|███████▎  | 194/266 [01:58<00:46,  1.55it/s]Loading train:  73%|███████▎  | 195/266 [01:58<00:45,  1.56it/s]Loading train:  74%|███████▎  | 196/266 [01:59<00:45,  1.54it/s]Loading train:  74%|███████▍  | 197/266 [02:00<00:43,  1.60it/s]Loading train:  74%|███████▍  | 198/266 [02:00<00:42,  1.59it/s]Loading train:  75%|███████▍  | 199/266 [02:01<00:41,  1.62it/s]Loading train:  75%|███████▌  | 200/266 [02:01<00:40,  1.64it/s]Loading train:  76%|███████▌  | 201/266 [02:02<00:39,  1.63it/s]Loading train:  76%|███████▌  | 202/266 [02:03<00:39,  1.63it/s]Loading train:  76%|███████▋  | 203/266 [02:03<00:37,  1.68it/s]Loading train:  77%|███████▋  | 204/266 [02:04<00:38,  1.62it/s]Loading train:  77%|███████▋  | 205/266 [02:05<00:39,  1.55it/s]Loading train:  77%|███████▋  | 206/266 [02:05<00:37,  1.60it/s]Loading train:  78%|███████▊  | 207/266 [02:06<00:36,  1.61it/s]Loading train:  78%|███████▊  | 208/266 [02:07<00:38,  1.52it/s]Loading train:  79%|███████▊  | 209/266 [02:07<00:39,  1.43it/s]Loading train:  79%|███████▉  | 210/266 [02:08<00:37,  1.49it/s]Loading train:  79%|███████▉  | 211/266 [02:09<00:36,  1.53it/s]Loading train:  80%|███████▉  | 212/266 [02:09<00:33,  1.59it/s]Loading train:  80%|████████  | 213/266 [02:10<00:30,  1.72it/s]Loading train:  80%|████████  | 214/266 [02:10<00:29,  1.76it/s]Loading train:  81%|████████  | 215/266 [02:11<00:27,  1.83it/s]Loading train:  81%|████████  | 216/266 [02:11<00:27,  1.80it/s]Loading train:  82%|████████▏ | 217/266 [02:12<00:26,  1.85it/s]Loading train:  82%|████████▏ | 218/266 [02:12<00:26,  1.80it/s]Loading train:  82%|████████▏ | 219/266 [02:13<00:27,  1.71it/s]Loading train:  83%|████████▎ | 220/266 [02:14<00:26,  1.73it/s]Loading train:  83%|████████▎ | 221/266 [02:14<00:25,  1.75it/s]Loading train:  83%|████████▎ | 222/266 [02:15<00:25,  1.75it/s]Loading train:  84%|████████▍ | 223/266 [02:15<00:24,  1.75it/s]Loading train:  84%|████████▍ | 224/266 [02:16<00:24,  1.69it/s]Loading train:  85%|████████▍ | 225/266 [02:16<00:24,  1.65it/s]Loading train:  85%|████████▍ | 226/266 [02:17<00:25,  1.60it/s]Loading train:  85%|████████▌ | 227/266 [02:18<00:23,  1.66it/s]Loading train:  86%|████████▌ | 228/266 [02:18<00:20,  1.84it/s]Loading train:  86%|████████▌ | 229/266 [02:19<00:22,  1.63it/s]Loading train:  86%|████████▋ | 230/266 [02:20<00:23,  1.56it/s]Loading train:  87%|████████▋ | 231/266 [02:20<00:22,  1.58it/s]Loading train:  87%|████████▋ | 232/266 [02:21<00:21,  1.60it/s]Loading train:  88%|████████▊ | 233/266 [02:21<00:20,  1.61it/s]Loading train:  88%|████████▊ | 234/266 [02:22<00:20,  1.56it/s]Loading train:  88%|████████▊ | 235/266 [02:23<00:20,  1.53it/s]Loading train:  89%|████████▊ | 236/266 [02:23<00:19,  1.53it/s]Loading train:  89%|████████▉ | 237/266 [02:24<00:18,  1.54it/s]Loading train:  89%|████████▉ | 238/266 [02:25<00:17,  1.58it/s]Loading train:  90%|████████▉ | 239/266 [02:25<00:17,  1.58it/s]Loading train:  90%|█████████ | 240/266 [02:26<00:16,  1.55it/s]Loading train:  91%|█████████ | 241/266 [02:27<00:16,  1.53it/s]Loading train:  91%|█████████ | 242/266 [02:27<00:15,  1.54it/s]Loading train:  91%|█████████▏| 243/266 [02:28<00:13,  1.66it/s]Loading train:  92%|█████████▏| 244/266 [02:29<00:14,  1.56it/s]Loading train:  92%|█████████▏| 245/266 [02:29<00:13,  1.53it/s]Loading train:  92%|█████████▏| 246/266 [02:30<00:12,  1.58it/s]Loading train:  93%|█████████▎| 247/266 [02:30<00:11,  1.60it/s]Loading train:  93%|█████████▎| 248/266 [02:31<00:11,  1.60it/s]Loading train:  94%|█████████▎| 249/266 [02:32<00:10,  1.55it/s]Loading train:  94%|█████████▍| 250/266 [02:32<00:10,  1.57it/s]Loading train:  94%|█████████▍| 251/266 [02:33<00:09,  1.55it/s]Loading train:  95%|█████████▍| 252/266 [02:34<00:08,  1.59it/s]Loading train:  95%|█████████▌| 253/266 [02:34<00:09,  1.43it/s]Loading train:  95%|█████████▌| 254/266 [02:35<00:08,  1.43it/s]Loading train:  96%|█████████▌| 255/266 [02:36<00:08,  1.37it/s]Loading train:  96%|█████████▌| 256/266 [02:37<00:07,  1.38it/s]Loading train:  97%|█████████▋| 257/266 [02:37<00:06,  1.33it/s]Loading train:  97%|█████████▋| 258/266 [02:38<00:05,  1.34it/s]Loading train:  97%|█████████▋| 259/266 [02:39<00:05,  1.28it/s]Loading train:  98%|█████████▊| 260/266 [02:40<00:04,  1.32it/s]Loading train:  98%|█████████▊| 261/266 [02:40<00:03,  1.42it/s]Loading train:  98%|█████████▊| 262/266 [02:41<00:02,  1.36it/s]Loading train:  99%|█████████▉| 263/266 [02:42<00:02,  1.43it/s]Loading train:  99%|█████████▉| 264/266 [02:43<00:01,  1.40it/s]Loading train: 100%|█████████▉| 265/266 [02:43<00:00,  1.48it/s]Loading train: 100%|██████████| 266/266 [02:44<00:00,  1.46it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   0%|          | 1/266 [00:00<00:27,  9.78it/s]concatenating: train:   1%|          | 3/266 [00:00<00:23, 11.21it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:19, 13.61it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:14, 17.72it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:10, 23.73it/s]concatenating: train:  14%|█▍        | 37/266 [00:00<00:07, 30.40it/s]concatenating: train:  17%|█▋        | 44/266 [00:00<00:07, 31.12it/s]concatenating: train:  21%|██▏       | 57/266 [00:01<00:05, 37.85it/s]concatenating: train:  27%|██▋       | 73/266 [00:01<00:03, 48.95it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:03, 57.88it/s]concatenating: train:  37%|███▋      | 98/266 [00:01<00:02, 58.44it/s]concatenating: train:  42%|████▏     | 112/266 [00:01<00:02, 70.33it/s]concatenating: train:  46%|████▌     | 122/266 [00:01<00:02, 68.02it/s]concatenating: train:  54%|█████▍    | 143/266 [00:01<00:01, 85.04it/s]concatenating: train:  59%|█████▊    | 156/266 [00:02<00:01, 86.28it/s]concatenating: train:  71%|███████   | 189/266 [00:02<00:00, 110.41it/s]concatenating: train:  79%|███████▉  | 210/266 [00:02<00:00, 121.29it/s]concatenating: train:  88%|████████▊ | 233/266 [00:02<00:00, 141.13it/s]concatenating: train:  97%|█████████▋| 259/266 [00:02<00:00, 161.68it/s]concatenating: train: 100%|██████████| 266/266 [00:02<00:00, 106.95it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:02,  1.61it/s]Loading test:  40%|████      | 2/5 [00:01<00:01,  1.62it/s]Loading test:  60%|██████    | 3/5 [00:01<00:01,  1.73it/s]Loading test:  80%|████████  | 4/5 [00:02<00:00,  1.76it/s]Loading test: 100%|██████████| 5/5 [00:02<00:00,  1.69it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  40%|████      | 2/5 [00:00<00:00, 12.31it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 19.73it/s]2019-08-16 23:15:56.714706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-16 23:15:56.714810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-16 23:15:56.714825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-16 23:15:56.714834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-16 23:15:56.715264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:12,  3.53it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:09,  4.30it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:08,  4.62it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:05,  6.09it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:01<00:05,  5.87it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  6.79it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:05,  6.07it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  7.93it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  8.48it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  7.03it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  8.53it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01,  9.21it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01,  9.48it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:03<00:01,  8.05it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.80it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00,  9.64it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00,  9.65it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:04<00:00,  7.74it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:04<00:00, 10.58it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 9,812
Non-trainable params: 490,530
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 500 samples
Epoch 1/300
 - 48s - loss: 0.1922 - acc: 0.9795 - mDice: 0.7573 - val_loss: 0.0902 - val_acc: 0.9930 - val_mDice: 0.8463

Epoch 00001: val_mDice improved from -inf to 0.84630, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 44s - loss: 0.0839 - acc: 0.9887 - mDice: 0.8590 - val_loss: 0.0780 - val_acc: 0.9934 - val_mDice: 0.8672

Epoch 00002: val_mDice improved from 0.84630 to 0.86718, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 44s - loss: 0.0739 - acc: 0.9915 - mDice: 0.8756 - val_loss: 0.0678 - val_acc: 0.9945 - val_mDice: 0.8847

Epoch 00003: val_mDice improved from 0.86718 to 0.88468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 44s - loss: 0.0659 - acc: 0.9930 - mDice: 0.8822 - val_loss: 0.0629 - val_acc: 0.9944 - val_mDice: 0.8853

Epoch 00004: val_mDice improved from 0.88468 to 0.88534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 44s - loss: 0.0617 - acc: 0.9934 - mDice: 0.8872 - val_loss: 0.0613 - val_acc: 0.9943 - val_mDice: 0.8879

Epoch 00005: val_mDice improved from 0.88534 to 0.88793, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 45s - loss: 0.0595 - acc: 0.9936 - mDice: 0.8910 - val_loss: 0.0601 - val_acc: 0.9946 - val_mDice: 0.8900

Epoch 00006: val_mDice improved from 0.88793 to 0.89003, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 45s - loss: 0.0580 - acc: 0.9938 - mDice: 0.8935 - val_loss: 0.0638 - val_acc: 0.9944 - val_mDice: 0.8834

Epoch 00007: val_mDice did not improve from 0.89003
Epoch 8/300
 - 45s - loss: 0.0564 - acc: 0.9939 - mDice: 0.8963 - val_loss: 0.0636 - val_acc: 0.9946 - val_mDice: 0.8838

Epoch 00008: val_mDice did not improve from 0.89003
Epoch 9/300
 - 44s - loss: 0.0555 - acc: 0.9940 - mDice: 0.8977 - val_loss: 0.0605 - val_acc: 0.9948 - val_mDice: 0.8894

Epoch 00009: val_mDice did not improve from 0.89003
Epoch 10/300
 - 44s - loss: 0.0548 - acc: 0.9941 - mDice: 0.8991 - val_loss: 0.0722 - val_acc: 0.9943 - val_mDice: 0.8694

Epoch 00010: val_mDice did not improve from 0.89003
Epoch 11/300
 - 44s - loss: 0.0538 - acc: 0.9942 - mDice: 0.9008 - val_loss: 0.0614 - val_acc: 0.9941 - val_mDice: 0.8878

Epoch 00011: val_mDice did not improve from 0.89003
Epoch 12/300
 - 44s - loss: 0.0536 - acc: 0.9942 - mDice: 0.9011 - val_loss: 0.0807 - val_acc: 0.9936 - val_mDice: 0.8557

Epoch 00012: val_mDice did not improve from 0.89003
Epoch 13/300
 - 48s - loss: 0.0528 - acc: 0.9943 - mDice: 0.9025 - val_loss: 0.0612 - val_acc: 0.9946 - val_mDice: 0.8880

Epoch 00013: val_mDice did not improve from 0.89003
Epoch 14/300
 - 54s - loss: 0.0522 - acc: 0.9943 - mDice: 0.9034 - val_loss: 0.0592 - val_acc: 0.9944 - val_mDice: 0.8916

Epoch 00014: val_mDice improved from 0.89003 to 0.89164, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 57s - loss: 0.0514 - acc: 0.9944 - mDice: 0.9049 - val_loss: 0.0580 - val_acc: 0.9947 - val_mDice: 0.8937

Epoch 00015: val_mDice improved from 0.89164 to 0.89367, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 55s - loss: 0.0511 - acc: 0.9944 - mDice: 0.9054 - val_loss: 0.0586 - val_acc: 0.9949 - val_mDice: 0.8925

Epoch 00016: val_mDice did not improve from 0.89367
Epoch 17/300
 - 56s - loss: 0.0506 - acc: 0.9945 - mDice: 0.9063 - val_loss: 0.0573 - val_acc: 0.9946 - val_mDice: 0.8951

Epoch 00017: val_mDice improved from 0.89367 to 0.89506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 56s - loss: 0.0503 - acc: 0.9945 - mDice: 0.9068 - val_loss: 0.0615 - val_acc: 0.9944 - val_mDice: 0.8877

Epoch 00018: val_mDice did not improve from 0.89506
Epoch 19/300
 - 58s - loss: 0.0502 - acc: 0.9945 - mDice: 0.9071 - val_loss: 0.0878 - val_acc: 0.9934 - val_mDice: 0.8445

Epoch 00019: val_mDice did not improve from 0.89506
Epoch 20/300
 - 57s - loss: 0.0499 - acc: 0.9946 - mDice: 0.9076 - val_loss: 0.0567 - val_acc: 0.9946 - val_mDice: 0.8959

Epoch 00020: val_mDice improved from 0.89506 to 0.89590, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 57s - loss: 0.0492 - acc: 0.9946 - mDice: 0.9088 - val_loss: 0.0655 - val_acc: 0.9945 - val_mDice: 0.8808

Epoch 00021: val_mDice did not improve from 0.89590
Epoch 22/300
 - 57s - loss: 0.0489 - acc: 0.9947 - mDice: 0.9093 - val_loss: 0.0577 - val_acc: 0.9946 - val_mDice: 0.8942

Epoch 00022: val_mDice did not improve from 0.89590
Epoch 23/300
 - 58s - loss: 0.0489 - acc: 0.9947 - mDice: 0.9093 - val_loss: 0.0573 - val_acc: 0.9944 - val_mDice: 0.8949

Epoch 00023: val_mDice did not improve from 0.89590
Epoch 24/300
 - 56s - loss: 0.0484 - acc: 0.9947 - mDice: 0.9102 - val_loss: 0.0601 - val_acc: 0.9943 - val_mDice: 0.8903

Epoch 00024: val_mDice did not improve from 0.89590
Epoch 25/300
 - 56s - loss: 0.0484 - acc: 0.9947 - mDice: 0.9102 - val_loss: 0.0654 - val_acc: 0.9943 - val_mDice: 0.8811

Epoch 00025: val_mDice did not improve from 0.89590
Epoch 26/300
 - 57s - loss: 0.0481 - acc: 0.9947 - mDice: 0.9108 - val_loss: 0.0616 - val_acc: 0.9941 - val_mDice: 0.8877

Epoch 00026: val_mDice did not improve from 0.89590
Epoch 27/300
 - 58s - loss: 0.0480 - acc: 0.9948 - mDice: 0.9110 - val_loss: 0.0587 - val_acc: 0.9946 - val_mDice: 0.8927

Epoch 00027: val_mDice did not improve from 0.89590
Epoch 28/300
 - 57s - loss: 0.0477 - acc: 0.9948 - mDice: 0.9115 - val_loss: 0.0580 - val_acc: 0.9945 - val_mDice: 0.8937

Epoch 00028: val_mDice did not improve from 0.89590
Epoch 29/300
 - 60s - loss: 0.0475 - acc: 0.9948 - mDice: 0.9117 - val_loss: 0.0626 - val_acc: 0.9944 - val_mDice: 0.8858

Epoch 00029: val_mDice did not improve from 0.89590
Epoch 30/300
 - 61s - loss: 0.0473 - acc: 0.9948 - mDice: 0.9121 - val_loss: 0.0618 - val_acc: 0.9946 - val_mDice: 0.8870

Epoch 00030: val_mDice did not improve from 0.89590
Epoch 31/300
 - 61s - loss: 0.0471 - acc: 0.9948 - mDice: 0.9125 - val_loss: 0.0571 - val_acc: 0.9945 - val_mDice: 0.8954

Epoch 00031: val_mDice did not improve from 0.89590
Epoch 32/300
 - 62s - loss: 0.0469 - acc: 0.9949 - mDice: 0.9129 - val_loss: 0.0582 - val_acc: 0.9946 - val_mDice: 0.8935

Epoch 00032: val_mDice did not improve from 0.89590
Epoch 33/300
 - 61s - loss: 0.0467 - acc: 0.9949 - mDice: 0.9132 - val_loss: 0.0609 - val_acc: 0.9940 - val_mDice: 0.8888

Epoch 00033: val_mDice did not improve from 0.89590
Epoch 34/300
 - 60s - loss: 0.0467 - acc: 0.9949 - mDice: 0.9132 - val_loss: 0.0584 - val_acc: 0.9946 - val_mDice: 0.8930

Epoch 00034: val_mDice did not improve from 0.89590
Epoch 35/300
 - 58s - loss: 0.0463 - acc: 0.9949 - mDice: 0.9139 - val_loss: 0.0571 - val_acc: 0.9946 - val_mDice: 0.8955

Epoch 00035: val_mDice did not improve from 0.89590
Epoch 36/300
 - 56s - loss: 0.0465 - acc: 0.9949 - mDice: 0.9136 - val_loss: 0.0634 - val_acc: 0.9943 - val_mDice: 0.8844

Epoch 00036: val_mDice did not improve from 0.89590
Epoch 37/300
 - 59s - loss: 0.0462 - acc: 0.9949 - mDice: 0.9142 - val_loss: 0.0574 - val_acc: 0.9944 - val_mDice: 0.8947

Epoch 00037: val_mDice did not improve from 0.89590
Epoch 38/300
 - 56s - loss: 0.0462 - acc: 0.9949 - mDice: 0.9141 - val_loss: 0.0606 - val_acc: 0.9943 - val_mDice: 0.8895

Epoch 00038: val_mDice did not improve from 0.89590
Epoch 39/300
 - 58s - loss: 0.0460 - acc: 0.9950 - mDice: 0.9144 - val_loss: 0.0585 - val_acc: 0.9943 - val_mDice: 0.8931

Epoch 00039: val_mDice did not improve from 0.89590
Epoch 40/300
 - 52s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9148 - val_loss: 0.0597 - val_acc: 0.9948 - val_mDice: 0.8907

Epoch 00040: val_mDice did not improve from 0.89590
Epoch 41/300
 - 46s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9147 - val_loss: 0.0606 - val_acc: 0.9939 - val_mDice: 0.8895

Epoch 00041: val_mDice did not improve from 0.89590
Epoch 42/300
 - 45s - loss: 0.0457 - acc: 0.9950 - mDice: 0.9150 - val_loss: 0.0571 - val_acc: 0.9945 - val_mDice: 0.8955

Epoch 00042: val_mDice did not improve from 0.89590
Epoch 43/300
 - 44s - loss: 0.0456 - acc: 0.9950 - mDice: 0.9152 - val_loss: 0.0580 - val_acc: 0.9943 - val_mDice: 0.8940

Epoch 00043: val_mDice did not improve from 0.89590
Epoch 44/300
 - 45s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9155 - val_loss: 0.0584 - val_acc: 0.9945 - val_mDice: 0.8933

Epoch 00044: val_mDice did not improve from 0.89590
Epoch 45/300
 - 44s - loss: 0.0452 - acc: 0.9950 - mDice: 0.9159 - val_loss: 0.0610 - val_acc: 0.9946 - val_mDice: 0.8886

Epoch 00045: val_mDice did not improve from 0.89590
Epoch 46/300
 - 44s - loss: 0.0453 - acc: 0.9950 - mDice: 0.9158 - val_loss: 0.0609 - val_acc: 0.9944 - val_mDice: 0.8889

Epoch 00046: val_mDice did not improve from 0.89590
Epoch 47/300
 - 45s - loss: 0.0452 - acc: 0.9950 - mDice: 0.9159 - val_loss: 0.0615 - val_acc: 0.9941 - val_mDice: 0.8879

Epoch 00047: val_mDice did not improve from 0.89590
Epoch 48/300
 - 48s - loss: 0.0451 - acc: 0.9950 - mDice: 0.9160 - val_loss: 0.0588 - val_acc: 0.9944 - val_mDice: 0.8925

Epoch 00048: val_mDice did not improve from 0.89590
Epoch 49/300
 - 53s - loss: 0.0447 - acc: 0.9951 - mDice: 0.9168 - val_loss: 0.0579 - val_acc: 0.9945 - val_mDice: 0.8940

Epoch 00049: val_mDice did not improve from 0.89590
Epoch 50/300
 - 56s - loss: 0.0449 - acc: 0.9951 - mDice: 0.9163 - val_loss: 0.0568 - val_acc: 0.9944 - val_mDice: 0.8959

Epoch 00050: val_mDice did not improve from 0.89590
Epoch 51/300
 - 57s - loss: 0.0446 - acc: 0.9951 - mDice: 0.9169 - val_loss: 0.0583 - val_acc: 0.9943 - val_mDice: 0.8936

Epoch 00051: val_mDice did not improve from 0.89590
Epoch 52/300
 - 60s - loss: 0.0446 - acc: 0.9951 - mDice: 0.9169 - val_loss: 0.0569 - val_acc: 0.9945 - val_mDice: 0.8957

Epoch 00052: val_mDice did not improve from 0.89590
Epoch 53/300
 - 59s - loss: 0.0448 - acc: 0.9951 - mDice: 0.9167 - val_loss: 0.0587 - val_acc: 0.9948 - val_mDice: 0.8927

Epoch 00053: val_mDice did not improve from 0.89590
Epoch 54/300
 - 57s - loss: 0.0445 - acc: 0.9951 - mDice: 0.9171 - val_loss: 0.0625 - val_acc: 0.9943 - val_mDice: 0.8861

Epoch 00054: val_mDice did not improve from 0.89590
Epoch 55/300
 - 59s - loss: 0.0444 - acc: 0.9951 - mDice: 0.9174 - val_loss: 0.0567 - val_acc: 0.9947 - val_mDice: 0.8960

Epoch 00055: val_mDice improved from 0.89590 to 0.89601, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 57s - loss: 0.0445 - acc: 0.9951 - mDice: 0.9172 - val_loss: 0.0594 - val_acc: 0.9944 - val_mDice: 0.8914

Epoch 00056: val_mDice did not improve from 0.89601
Epoch 57/300
 - 56s - loss: 0.0445 - acc: 0.9951 - mDice: 0.9171 - val_loss: 0.0586 - val_acc: 0.9943 - val_mDice: 0.8928

Epoch 00057: val_mDice did not improve from 0.89601
Epoch 58/300
 - 52s - loss: 0.0443 - acc: 0.9951 - mDice: 0.9174 - val_loss: 0.0579 - val_acc: 0.9945 - val_mDice: 0.8940

Epoch 00058: val_mDice did not improve from 0.89601
Epoch 59/300
 - 45s - loss: 0.0440 - acc: 0.9951 - mDice: 0.9180 - val_loss: 0.0595 - val_acc: 0.9945 - val_mDice: 0.8913

Epoch 00059: val_mDice did not improve from 0.89601
Epoch 60/300
 - 45s - loss: 0.0440 - acc: 0.9951 - mDice: 0.9180 - val_loss: 0.0606 - val_acc: 0.9944 - val_mDice: 0.8895

Epoch 00060: val_mDice did not improve from 0.89601
Epoch 61/300
 - 45s - loss: 0.0440 - acc: 0.9951 - mDice: 0.9180 - val_loss: 0.0614 - val_acc: 0.9946 - val_mDice: 0.8880

Epoch 00061: val_mDice did not improve from 0.89601
Epoch 62/300
 - 45s - loss: 0.0439 - acc: 0.9951 - mDice: 0.9182 - val_loss: 0.0672 - val_acc: 0.9940 - val_mDice: 0.8779

Epoch 00062: val_mDice did not improve from 0.89601
Epoch 63/300
 - 45s - loss: 0.0438 - acc: 0.9951 - mDice: 0.9183 - val_loss: 0.0601 - val_acc: 0.9945 - val_mDice: 0.8904

Epoch 00063: val_mDice did not improve from 0.89601
Epoch 64/300
 - 45s - loss: 0.0438 - acc: 0.9952 - mDice: 0.9184 - val_loss: 0.0590 - val_acc: 0.9944 - val_mDice: 0.8921

Epoch 00064: val_mDice did not improve from 0.89601
Epoch 65/300
 - 45s - loss: 0.0439 - acc: 0.9952 - mDice: 0.9182 - val_loss: 0.0596 - val_acc: 0.9946 - val_mDice: 0.8911

Epoch 00065: val_mDice did not improve from 0.89601
Epoch 66/300
 - 44s - loss: 0.0437 - acc: 0.9952 - mDice: 0.9186 - val_loss: 0.0593 - val_acc: 0.9943 - val_mDice: 0.8919

Epoch 00066: val_mDice did not improve from 0.89601
Epoch 67/300
 - 45s - loss: 0.0437 - acc: 0.9952 - mDice: 0.9186 - val_loss: 0.0584 - val_acc: 0.9945 - val_mDice: 0.8930

Epoch 00067: val_mDice did not improve from 0.89601
Epoch 68/300
 - 44s - loss: 0.0436 - acc: 0.9952 - mDice: 0.9188 - val_loss: 0.0560 - val_acc: 0.9947 - val_mDice: 0.8973

Epoch 00068: val_mDice improved from 0.89601 to 0.89730, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 44s - loss: 0.0436 - acc: 0.9952 - mDice: 0.9187 - val_loss: 0.0602 - val_acc: 0.9943 - val_mDice: 0.8900

Epoch 00069: val_mDice did not improve from 0.89730
Epoch 70/300
 - 45s - loss: 0.0434 - acc: 0.9952 - mDice: 0.9190 - val_loss: 0.0567 - val_acc: 0.9945 - val_mDice: 0.8960

Epoch 00070: val_mDice did not improve from 0.89730
Epoch 71/300
 - 44s - loss: 0.0433 - acc: 0.9952 - mDice: 0.9193 - val_loss: 0.0604 - val_acc: 0.9942 - val_mDice: 0.8899

Epoch 00071: val_mDice did not improve from 0.89730
Epoch 72/300
 - 43s - loss: 0.0433 - acc: 0.9952 - mDice: 0.9192 - val_loss: 0.0578 - val_acc: 0.9941 - val_mDice: 0.8944

Epoch 00072: val_mDice did not improve from 0.89730
Epoch 73/300
 - 43s - loss: 0.0434 - acc: 0.9952 - mDice: 0.9191 - val_loss: 0.0594 - val_acc: 0.9941 - val_mDice: 0.8916

Epoch 00073: val_mDice did not improve from 0.89730
Epoch 74/300
 - 43s - loss: 0.0434 - acc: 0.9952 - mDice: 0.9190 - val_loss: 0.0567 - val_acc: 0.9946 - val_mDice: 0.8961

Epoch 00074: val_mDice did not improve from 0.89730
Epoch 75/300
 - 43s - loss: 0.0432 - acc: 0.9952 - mDice: 0.9195 - val_loss: 0.0579 - val_acc: 0.9943 - val_mDice: 0.8942

Epoch 00075: val_mDice did not improve from 0.89730
Epoch 76/300
 - 43s - loss: 0.0433 - acc: 0.9952 - mDice: 0.9193 - val_loss: 0.0590 - val_acc: 0.9946 - val_mDice: 0.8922

Epoch 00076: val_mDice did not improve from 0.89730
Epoch 77/300
 - 43s - loss: 0.0433 - acc: 0.9952 - mDice: 0.9192 - val_loss: 0.0590 - val_acc: 0.9940 - val_mDice: 0.8921

Epoch 00077: val_mDice did not improve from 0.89730
Epoch 78/300
 - 42s - loss: 0.0433 - acc: 0.9952 - mDice: 0.9193 - val_loss: 0.0593 - val_acc: 0.9940 - val_mDice: 0.8919

Epoch 00078: val_mDice did not improve from 0.89730
Epoch 79/300
 - 43s - loss: 0.0431 - acc: 0.9952 - mDice: 0.9196 - val_loss: 0.0594 - val_acc: 0.9944 - val_mDice: 0.8916

Epoch 00079: val_mDice did not improve from 0.89730
Epoch 80/300
 - 43s - loss: 0.0431 - acc: 0.9952 - mDice: 0.9197 - val_loss: 0.0591 - val_acc: 0.9944 - val_mDice: 0.8921

Epoch 00080: val_mDice did not improve from 0.89730
Epoch 81/300
 - 44s - loss: 0.0432 - acc: 0.9952 - mDice: 0.9195 - val_loss: 0.0560 - val_acc: 0.9947 - val_mDice: 0.8973

Epoch 00081: val_mDice improved from 0.89730 to 0.89731, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 43s - loss: 0.0431 - acc: 0.9952 - mDice: 0.9197 - val_loss: 0.0590 - val_acc: 0.9945 - val_mDice: 0.8923

Epoch 00082: val_mDice did not improve from 0.89731
Epoch 83/300
 - 44s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9201 - val_loss: 0.0567 - val_acc: 0.9946 - val_mDice: 0.8962

Epoch 00083: val_mDice did not improve from 0.89731
Epoch 84/300
 - 44s - loss: 0.0429 - acc: 0.9952 - mDice: 0.9201 - val_loss: 0.0584 - val_acc: 0.9945 - val_mDice: 0.8932

Epoch 00084: val_mDice did not improve from 0.89731
Epoch 85/300
 - 44s - loss: 0.0429 - acc: 0.9952 - mDice: 0.9200 - val_loss: 0.0579 - val_acc: 0.9945 - val_mDice: 0.8940

Epoch 00085: val_mDice did not improve from 0.89731
Epoch 86/300
 - 43s - loss: 0.0430 - acc: 0.9952 - mDice: 0.9198 - val_loss: 0.0728 - val_acc: 0.9942 - val_mDice: 0.8686

Epoch 00086: val_mDice did not improve from 0.89731
Epoch 87/300
 - 42s - loss: 0.0429 - acc: 0.9952 - mDice: 0.9199 - val_loss: 0.0590 - val_acc: 0.9945 - val_mDice: 0.8921

Epoch 00087: val_mDice did not improve from 0.89731
Epoch 88/300
 - 43s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9202 - val_loss: 0.0608 - val_acc: 0.9939 - val_mDice: 0.8889

Epoch 00088: val_mDice did not improve from 0.89731
Epoch 89/300
 - 42s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9201 - val_loss: 0.0598 - val_acc: 0.9943 - val_mDice: 0.8909

Epoch 00089: val_mDice did not improve from 0.89731
Epoch 90/300
 - 43s - loss: 0.0427 - acc: 0.9953 - mDice: 0.9204 - val_loss: 0.0567 - val_acc: 0.9946 - val_mDice: 0.8961

Epoch 00090: val_mDice did not improve from 0.89731
Epoch 91/300
 - 43s - loss: 0.0426 - acc: 0.9953 - mDice: 0.9205 - val_loss: 0.0578 - val_acc: 0.9945 - val_mDice: 0.8943

Epoch 00091: val_mDice did not improve from 0.89731
Epoch 92/300
 - 42s - loss: 0.0427 - acc: 0.9953 - mDice: 0.9203 - val_loss: 0.0577 - val_acc: 0.9944 - val_mDice: 0.8944

Epoch 00092: val_mDice did not improve from 0.89731
Epoch 93/300
 - 43s - loss: 0.0427 - acc: 0.9953 - mDice: 0.9204 - val_loss: 0.0584 - val_acc: 0.9944 - val_mDice: 0.8933

Epoch 00093: val_mDice did not improve from 0.89731
Epoch 94/300
 - 42s - loss: 0.0424 - acc: 0.9953 - mDice: 0.9208 - val_loss: 0.0578 - val_acc: 0.9946 - val_mDice: 0.8943

Epoch 00094: val_mDice did not improve from 0.89731
Epoch 95/300
 - 43s - loss: 0.0423 - acc: 0.9953 - mDice: 0.9211 - val_loss: 0.0562 - val_acc: 0.9945 - val_mDice: 0.8972

Epoch 00095: val_mDice did not improve from 0.89731
Epoch 96/300
 - 43s - loss: 0.0424 - acc: 0.9953 - mDice: 0.9209 - val_loss: 0.0618 - val_acc: 0.9942 - val_mDice: 0.8875

Epoch 00096: val_mDice did not improve from 0.89731
Epoch 97/300
 - 42s - loss: 0.0425 - acc: 0.9953 - mDice: 0.9207 - val_loss: 0.0584 - val_acc: 0.9944 - val_mDice: 0.8932

Epoch 00097: val_mDice did not improve from 0.89731
Epoch 98/300
 - 43s - loss: 0.0425 - acc: 0.9953 - mDice: 0.9208 - val_loss: 0.0582 - val_acc: 0.9946 - val_mDice: 0.8935

Epoch 00098: val_mDice did not improve from 0.89731
Epoch 99/300
 - 43s - loss: 0.0423 - acc: 0.9953 - mDice: 0.9210 - val_loss: 0.0599 - val_acc: 0.9943 - val_mDice: 0.8906

Epoch 00099: val_mDice did not improve from 0.89731
Epoch 100/300
 - 43s - loss: 0.0424 - acc: 0.9953 - mDice: 0.9209 - val_loss: 0.0597 - val_acc: 0.9946 - val_mDice: 0.8909

Epoch 00100: val_mDice did not improve from 0.89731
Epoch 101/300
 - 43s - loss: 0.0420 - acc: 0.9953 - mDice: 0.9216 - val_loss: 0.0569 - val_acc: 0.9945 - val_mDice: 0.8957

Epoch 00101: val_mDice did not improve from 0.89731
Epoch 102/300
 - 42s - loss: 0.0423 - acc: 0.9953 - mDice: 0.9211 - val_loss: 0.0554 - val_acc: 0.9947 - val_mDice: 0.8984

Epoch 00102: val_mDice improved from 0.89731 to 0.89838, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 43s - loss: 0.0424 - acc: 0.9953 - mDice: 0.9210 - val_loss: 0.0558 - val_acc: 0.9946 - val_mDice: 0.8976

Epoch 00103: val_mDice did not improve from 0.89838
Epoch 104/300
 - 43s - loss: 0.0423 - acc: 0.9953 - mDice: 0.9211 - val_loss: 0.0576 - val_acc: 0.9946 - val_mDice: 0.8946

Epoch 00104: val_mDice did not improve from 0.89838
Epoch 105/300
 - 44s - loss: 0.0421 - acc: 0.9953 - mDice: 0.9214 - val_loss: 0.0587 - val_acc: 0.9947 - val_mDice: 0.8926

Epoch 00105: val_mDice did not improve from 0.89838
Epoch 106/300
 - 44s - loss: 0.0424 - acc: 0.9953 - mDice: 0.9209 - val_loss: 0.0561 - val_acc: 0.9946 - val_mDice: 0.8970

Epoch 00106: val_mDice did not improve from 0.89838
Epoch 107/300
 - 44s - loss: 0.0424 - acc: 0.9953 - mDice: 0.9208 - val_loss: 0.0588 - val_acc: 0.9944 - val_mDice: 0.8924

Epoch 00107: val_mDice did not improve from 0.89838
Epoch 108/300
 - 45s - loss: 0.0423 - acc: 0.9953 - mDice: 0.9211 - val_loss: 0.0605 - val_acc: 0.9945 - val_mDice: 0.8896

Epoch 00108: val_mDice did not improve from 0.89838
Epoch 109/300
 - 45s - loss: 0.0421 - acc: 0.9953 - mDice: 0.9215 - val_loss: 0.0579 - val_acc: 0.9943 - val_mDice: 0.8941

Epoch 00109: val_mDice did not improve from 0.89838
Epoch 110/300
 - 45s - loss: 0.0420 - acc: 0.9953 - mDice: 0.9215 - val_loss: 0.0571 - val_acc: 0.9944 - val_mDice: 0.8955

Epoch 00110: val_mDice did not improve from 0.89838
Epoch 111/300
 - 45s - loss: 0.0421 - acc: 0.9953 - mDice: 0.9214 - val_loss: 0.0593 - val_acc: 0.9946 - val_mDice: 0.8916

Epoch 00111: val_mDice did not improve from 0.89838
Epoch 112/300
 - 45s - loss: 0.0421 - acc: 0.9953 - mDice: 0.9215 - val_loss: 0.0570 - val_acc: 0.9945 - val_mDice: 0.8956

Epoch 00112: val_mDice did not improve from 0.89838
Epoch 113/300
 - 45s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9219 - val_loss: 0.0578 - val_acc: 0.9946 - val_mDice: 0.8942

Epoch 00113: val_mDice did not improve from 0.89838
Epoch 114/300
 - 44s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9220 - val_loss: 0.0624 - val_acc: 0.9942 - val_mDice: 0.8864

Epoch 00114: val_mDice did not improve from 0.89838
Epoch 115/300
 - 43s - loss: 0.0419 - acc: 0.9953 - mDice: 0.9219 - val_loss: 0.0593 - val_acc: 0.9946 - val_mDice: 0.8916

Epoch 00115: val_mDice did not improve from 0.89838
Epoch 116/300
 - 43s - loss: 0.0420 - acc: 0.9953 - mDice: 0.9216 - val_loss: 0.0586 - val_acc: 0.9943 - val_mDice: 0.8929

Epoch 00116: val_mDice did not improve from 0.89838
Epoch 117/300
 - 43s - loss: 0.0417 - acc: 0.9954 - mDice: 0.9221 - val_loss: 0.0584 - val_acc: 0.9946 - val_mDice: 0.8933

Epoch 00117: val_mDice did not improve from 0.89838
Epoch 118/300
 - 44s - loss: 0.0419 - acc: 0.9953 - mDice: 0.9217 - val_loss: 0.0578 - val_acc: 0.9945 - val_mDice: 0.8943

Epoch 00118: val_mDice did not improve from 0.89838
Epoch 119/300
 - 44s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9220 - val_loss: 0.0594 - val_acc: 0.9942 - val_mDice: 0.8915

Epoch 00119: val_mDice did not improve from 0.89838
Epoch 120/300
 - 44s - loss: 0.0419 - acc: 0.9953 - mDice: 0.9218 - val_loss: 0.0569 - val_acc: 0.9947 - val_mDice: 0.8958

Epoch 00120: val_mDice did not improve from 0.89838
Epoch 121/300
 - 45s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9220 - val_loss: 0.0578 - val_acc: 0.9945 - val_mDice: 0.8945

Epoch 00121: val_mDice did not improve from 0.89838
Epoch 122/300
 - 46s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9219 - val_loss: 0.0574 - val_acc: 0.9943 - val_mDice: 0.8950

Epoch 00122: val_mDice did not improve from 0.89838
Epoch 123/300
 - 45s - loss: 0.0417 - acc: 0.9953 - mDice: 0.9221 - val_loss: 0.0577 - val_acc: 0.9945 - val_mDice: 0.8944

Epoch 00123: val_mDice did not improve from 0.89838
Epoch 124/300
 - 45s - loss: 0.0417 - acc: 0.9953 - mDice: 0.9221 - val_loss: 0.0587 - val_acc: 0.9942 - val_mDice: 0.8926

Epoch 00124: val_mDice did not improve from 0.89838
Epoch 125/300
 - 45s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9220 - val_loss: 0.0601 - val_acc: 0.9941 - val_mDice: 0.8904

Epoch 00125: val_mDice did not improve from 0.89838
Epoch 126/300
 - 45s - loss: 0.0416 - acc: 0.9954 - mDice: 0.9223 - val_loss: 0.0578 - val_acc: 0.9946 - val_mDice: 0.8943

Epoch 00126: val_mDice did not improve from 0.89838
Epoch 127/300
 - 45s - loss: 0.0417 - acc: 0.9953 - mDice: 0.9222 - val_loss: 0.0571 - val_acc: 0.9948 - val_mDice: 0.8955

Epoch 00127: val_mDice did not improve from 0.89838
Epoch 128/300
 - 45s - loss: 0.0415 - acc: 0.9954 - mDice: 0.9225 - val_loss: 0.0602 - val_acc: 0.9945 - val_mDice: 0.8902

Epoch 00128: val_mDice did not improve from 0.89838
Epoch 129/300
 - 44s - loss: 0.0416 - acc: 0.9954 - mDice: 0.9223 - val_loss: 0.0579 - val_acc: 0.9944 - val_mDice: 0.8942

Epoch 00129: val_mDice did not improve from 0.89838
Epoch 130/300
 - 44s - loss: 0.0416 - acc: 0.9954 - mDice: 0.9223 - val_loss: 0.0591 - val_acc: 0.9943 - val_mDice: 0.8921

Epoch 00130: val_mDice did not improve from 0.89838
Epoch 131/300
 - 44s - loss: 0.0416 - acc: 0.9953 - mDice: 0.9223 - val_loss: 0.0592 - val_acc: 0.9942 - val_mDice: 0.8920

Epoch 00131: val_mDice did not improve from 0.89838
Epoch 132/300
 - 44s - loss: 0.0416 - acc: 0.9953 - mDice: 0.9224 - val_loss: 0.0577 - val_acc: 0.9947 - val_mDice: 0.8944

Epoch 00132: val_mDice did not improve from 0.89838
Epoch 133/300
 - 44s - loss: 0.0415 - acc: 0.9954 - mDice: 0.9226 - val_loss: 0.0591 - val_acc: 0.9946 - val_mDice: 0.8920

Epoch 00133: val_mDice did not improve from 0.89838
Epoch 134/300
 - 44s - loss: 0.0415 - acc: 0.9954 - mDice: 0.9224 - val_loss: 0.0609 - val_acc: 0.9945 - val_mDice: 0.8888

Epoch 00134: val_mDice did not improve from 0.89838
Epoch 135/300
 - 44s - loss: 0.0414 - acc: 0.9954 - mDice: 0.9227 - val_loss: 0.0599 - val_acc: 0.9946 - val_mDice: 0.8906

Epoch 00135: val_mDice did not improve from 0.89838
Epoch 136/300
 - 44s - loss: 0.0414 - acc: 0.9954 - mDice: 0.9228 - val_loss: 0.0587 - val_acc: 0.9945 - val_mDice: 0.8928

Epoch 00136: val_mDice did not improve from 0.89838
Epoch 137/300
 - 44s - loss: 0.0414 - acc: 0.9954 - mDice: 0.9227 - val_loss: 0.0582 - val_acc: 0.9945 - val_mDice: 0.8936

Epoch 00137: val_mDice did not improve from 0.89838
Epoch 138/300
 - 44s - loss: 0.0415 - acc: 0.9954 - mDice: 0.9225 - val_loss: 0.0588 - val_acc: 0.9945 - val_mDice: 0.8925

Epoch 00138: val_mDice did not improve from 0.89838
Epoch 139/300
 - 44s - loss: 0.0413 - acc: 0.9954 - mDice: 0.9228 - val_loss: 0.0599 - val_acc: 0.9943 - val_mDice: 0.8907

Epoch 00139: val_mDice did not improve from 0.89838
Epoch 140/300
 - 44s - loss: 0.0413 - acc: 0.9954 - mDice: 0.9229 - val_loss: 0.0589 - val_acc: 0.9946 - val_mDice: 0.8924

Epoch 00140: val_mDice did not improve from 0.89838
Epoch 141/300
 - 44s - loss: 0.0413 - acc: 0.9954 - mDice: 0.9228 - val_loss: 0.0586 - val_acc: 0.9945 - val_mDice: 0.8929

Epoch 00141: val_mDice did not improve from 0.89838
Epoch 142/300
 - 44s - loss: 0.0412 - acc: 0.9954 - mDice: 0.9230 - val_loss: 0.0604 - val_acc: 0.9945 - val_mDice: 0.8898

Epoch 00142: val_mDice did not improve from 0.89838
Restoring model weights from the end of the best epoch
Epoch 00142: early stopping
{'val_loss': [0.09018574878573418, 0.07803550213575364, 0.06777646355330944, 0.0628511618822813, 0.061274001374840736, 0.06006796769797802, 0.0637967899441719, 0.06363823935389519, 0.06045227833092213, 0.07221869789063931, 0.061401602998375895, 0.08071811720728875, 0.06115894988179207, 0.059166524186730386, 0.05803622715175152, 0.05861199796199799, 0.05731665529310703, 0.061524121463298796, 0.08782656416296959, 0.056732652336359025, 0.06551658250391483, 0.057701971381902695, 0.05729138106107712, 0.06006234139204025, 0.06535097546875476, 0.06156882271170616, 0.05872916057705879, 0.05803850032389164, 0.06262198649346828, 0.06176638938486576, 0.05708067342638969, 0.058153829723596576, 0.06091355159878731, 0.058445129171013833, 0.05705244950950146, 0.06343013308942318, 0.05742439068853855, 0.060551978275179864, 0.05848358981311321, 0.05971578843891621, 0.06064492389559746, 0.05709417536854744, 0.05797785632312298, 0.05835002437233925, 0.0609980221837759, 0.060850226879119874, 0.06148383282124996, 0.05878991633653641, 0.0579123567789793, 0.05679824724793434, 0.058264696039259434, 0.056907357275485994, 0.05866644885390997, 0.06251609288156032, 0.05671266876161098, 0.05943508446216583, 0.05864710547029972, 0.05791793018579483, 0.05950187258422375, 0.060552100837230685, 0.06137449592351914, 0.06723393462598323, 0.06007500104606152, 0.05903931930661201, 0.05957602486014366, 0.05927683636546135, 0.05843614377081394, 0.055955206230282786, 0.0602169793099165, 0.056723398715257646, 0.06036629974842071, 0.05781795419752598, 0.05940834134817123, 0.056701092049479485, 0.05789225958287716, 0.058984629437327386, 0.05904153846204281, 0.059275222942233086, 0.059396881610155106, 0.05906597524881363, 0.05600504875183106, 0.05895852744579315, 0.05665538758039475, 0.05841280333697796, 0.05788412988185883, 0.07284007444977761, 0.0590177197009325, 0.06084314212203026, 0.05980537198483944, 0.056734079495072365, 0.057762179896235465, 0.05769150443375111, 0.05835178829729557, 0.05777634792029858, 0.056157979741692544, 0.06183178722858429, 0.05843790844082832, 0.05822550095617771, 0.05989721231162548, 0.05972857549786568, 0.05688367113471031, 0.05544586665928364, 0.05581427030265331, 0.05756451338529587, 0.05869133286178112, 0.05611346140503883, 0.058838390931487086, 0.06051866188645363, 0.05790247060358524, 0.05709696896374226, 0.059310702979564665, 0.05697678737342358, 0.05779955983161926, 0.062386364489793775, 0.0593096062541008, 0.05859529748558998, 0.0583625178784132, 0.057784004136919975, 0.05938245430588722, 0.05688120536506176, 0.05776403322815895, 0.05742191188037395, 0.057748615741729736, 0.05874149650335312, 0.060127108730375765, 0.057780345901846886, 0.057099102437496184, 0.06015502437949181, 0.05788982547819614, 0.05909399352967739, 0.059221716970205306, 0.05769071634858847, 0.05912326760590077, 0.06094833761453629, 0.05989536643028259, 0.05865955762565136, 0.058213666826486585, 0.05879671163856983, 0.059914001077413556, 0.058858980983495714, 0.05858571920543909, 0.060423987731337545], 'val_acc': [0.9930043756961823, 0.9934364020824432, 0.9944563865661621, 0.9944329917430877, 0.9942838728427887, 0.994581139087677, 0.9943825602531433, 0.9945694446563721, 0.9947658360004425, 0.9942524313926697, 0.994100147485733, 0.9936418175697327, 0.9946476638317108, 0.9944249451160431, 0.9946807861328125, 0.9948706090450287, 0.9945772409439086, 0.9943871855735779, 0.9933520913124084, 0.9946325540542602, 0.9944624781608582, 0.9946481466293335, 0.9943737804889679, 0.9942582726478577, 0.9942590236663819, 0.9941201210021973, 0.9946418166160583, 0.9945465445518493, 0.9943598926067352, 0.9945994079113006, 0.9945331335067749, 0.9946074604988098, 0.9939637005329132, 0.9945760250091553, 0.9946327924728393, 0.9942958056926727, 0.9944134950637817, 0.9943355321884155, 0.9942670524120331, 0.9948338150978089, 0.9939473688602447, 0.9944705128669739, 0.9942555963993073, 0.9944892764091492, 0.9945567727088929, 0.9943854808807373, 0.9940818727016449, 0.9944478571414948, 0.9945289969444275, 0.994414484500885, 0.9943099439144134, 0.9945160865783691, 0.9948040843009949, 0.9943245708942413, 0.994715404510498, 0.9943757355213165, 0.9943335711956024, 0.9945358157157898, 0.9945031702518463, 0.9943859577178955, 0.9945782124996185, 0.9940229058265686, 0.9944980442523956, 0.9943784177303314, 0.9946486413478851, 0.994304096698761, 0.99454385638237, 0.9947268486022949, 0.9943452656269074, 0.99454385638237, 0.9942173540592194, 0.9941454648971557, 0.9941447257995606, 0.994639128446579, 0.9942768096923829, 0.9946096479892731, 0.9940226554870606, 0.9939744174480438, 0.9944298207759857, 0.9944356799125671, 0.9946527779102325, 0.9944788098335267, 0.9945575058460235, 0.9945092558860779, 0.9945280194282532, 0.994189327955246, 0.9944824576377869, 0.9939439475536347, 0.9943318724632263, 0.9945748090744019, 0.9945146143436432, 0.9944493174552917, 0.9943584263324737, 0.9946111083030701, 0.9945402085781098, 0.9942302584648133, 0.994437861442566, 0.9945635795593262, 0.9942539036273956, 0.9945974588394165, 0.9945353329181671, 0.9946583807468414, 0.9946140348911285, 0.994564813375473, 0.9947175979614258, 0.9945923566818238, 0.9944113075733185, 0.9945175468921661, 0.9942989826202393, 0.9944388449192048, 0.9946493625640869, 0.9945346057415009, 0.9946461975574493, 0.9942100405693054, 0.9945752918720245, 0.9943075060844422, 0.9945674955844879, 0.9944897592067719, 0.9942468345165253, 0.9946620404720307, 0.9945221722126008, 0.9943438172340393, 0.9944724798202514, 0.9941856741905213, 0.9941271960735321, 0.9946172058582305, 0.9947972655296325, 0.9945277750492096, 0.9944332361221313, 0.9943403899669647, 0.9941500961780548, 0.9946798205375671, 0.9946118414402008, 0.9945243716239929, 0.9945679843425751, 0.9944729626178741, 0.9945248544216156, 0.9944809913635254, 0.9943070113658905, 0.9945996522903442, 0.9945270359516144, 0.9945177793502807], 'val_mDice': [0.8462953746318818, 0.8671798884868622, 0.8846806228160858, 0.885335648059845, 0.8879310965538025, 0.8900275826454163, 0.8834451675415039, 0.8837646842002869, 0.88939528465271, 0.8694177865982056, 0.8878018200397492, 0.8556730628013611, 0.8880474150180817, 0.8916439771652221, 0.8936677396297454, 0.892520397901535, 0.8950562059879303, 0.8877469003200531, 0.8445388078689575, 0.8959030032157898, 0.8807896435260772, 0.8941746652126312, 0.8949113547801971, 0.8902985453605652, 0.8811251163482666, 0.8876615285873413, 0.8926654040813446, 0.8937334716320038, 0.885791540145874, 0.8870343267917633, 0.8953546941280365, 0.893549108505249, 0.8888439536094666, 0.8930018663406372, 0.8955432593822479, 0.8844158113002777, 0.8947268545627594, 0.8894570589065551, 0.8930826485157013, 0.8907354652881623, 0.8895167112350464, 0.8954645991325378, 0.8939536571502685, 0.8933091819286346, 0.8885697007179261, 0.8888928532600403, 0.8879293620586395, 0.8925040066242218, 0.8939591646194458, 0.8958884537220001, 0.8935990691184997, 0.8957122445106507, 0.8926842331886291, 0.8860586524009705, 0.8960070431232452, 0.8914346039295197, 0.8928344249725342, 0.8939906477928161, 0.8913111448287964, 0.8894724071025848, 0.8879836022853851, 0.8778917372226716, 0.8903701484203339, 0.8921093881130219, 0.8911277234554291, 0.891886568069458, 0.8930454909801483, 0.8972962915897369, 0.8899962723255157, 0.8959897637367249, 0.889886099100113, 0.8943727016448975, 0.8915595412254333, 0.8960930645465851, 0.8941578984260559, 0.892166942358017, 0.8921343505382537, 0.8919140219688415, 0.8915752589702606, 0.8921220004558563, 0.8973134338855744, 0.8922526359558105, 0.8962201178073883, 0.8932314991950989, 0.8939641833305358, 0.8685881793498993, 0.8920958399772644, 0.8889309346675873, 0.890898060798645, 0.896118825674057, 0.8943489491939545, 0.8944293916225433, 0.8933166205883026, 0.8942663133144378, 0.8971551835536957, 0.8875047862529755, 0.8931837260723114, 0.8934806764125824, 0.8906454086303711, 0.8909201920032501, 0.8957420706748962, 0.8983797311782837, 0.8975766181945801, 0.8945684194564819, 0.8925530910491943, 0.8970467984676361, 0.8924301564693451, 0.8895910084247589, 0.8941365838050842, 0.8955264747142792, 0.8915912985801697, 0.8955838143825531, 0.8941837906837463, 0.886387723684311, 0.8916126966476441, 0.892905992269516, 0.8932521045207977, 0.8942871391773224, 0.8914509415626526, 0.89582559466362, 0.8944605946540832, 0.894979053735733, 0.8943720102310181, 0.8926075339317322, 0.8903808414936065, 0.8942982375621795, 0.8955477118492127, 0.8902368009090423, 0.8941571414470673, 0.8920911133289338, 0.8920017540454864, 0.8944371342658997, 0.8920163094997406, 0.8888059914112091, 0.8906166434288025, 0.8927699565887451, 0.8935820639133454, 0.8924719572067261, 0.8907032251358032, 0.8924046158790588, 0.8929492712020874, 0.889781528711319], 'loss': [0.19216739732240878, 0.08391250171760026, 0.0739306334332795, 0.06589214302665718, 0.061694590032407355, 0.05949899491625366, 0.0580138232120109, 0.05640074472703892, 0.05554137389073866, 0.054767528393167036, 0.053783401844622845, 0.053609761462138944, 0.05278606538846144, 0.05224748298951223, 0.05142897365480799, 0.05114654431922002, 0.05060713323332402, 0.05032491890503462, 0.05018010333951921, 0.04986395896443213, 0.049205916266944684, 0.048910453561395936, 0.04892387645852933, 0.04840298704985348, 0.048391197758737126, 0.04806072022467048, 0.047968981879340254, 0.0476639902893168, 0.047534981174064045, 0.04731511035959714, 0.04711450860607099, 0.04689573058721745, 0.04671980605497341, 0.046681658448058105, 0.046328804771529265, 0.04650150354592628, 0.04616865321264929, 0.046227071524905884, 0.04600639525241491, 0.04581029106607842, 0.04583787712519007, 0.04566796706714473, 0.04560695652334079, 0.045425158000554196, 0.045206319756373826, 0.04526076739501603, 0.04516766530752084, 0.045138313105143456, 0.04467745415628644, 0.04494725513645455, 0.04463928288838021, 0.04461284094063267, 0.04475649420279661, 0.04453778099137563, 0.04435220084743416, 0.04446576989626252, 0.044509269240019675, 0.04434787885260257, 0.0440034925998616, 0.04403670040116086, 0.04403239075048805, 0.043927617698793914, 0.04383869909633095, 0.043783444215353524, 0.0439191645843696, 0.04365132620538376, 0.043681551433875254, 0.04357068033659791, 0.043647624836568345, 0.04344968794530477, 0.0433048331767228, 0.043348134411626355, 0.043374048080411794, 0.043447456544867265, 0.04318581168682015, 0.04326014395738123, 0.043324648339904195, 0.043310441914403426, 0.043138052602122355, 0.043070174807441505, 0.04317744754370089, 0.043063927342536924, 0.042831645035738816, 0.04285688534835299, 0.04291851442372057, 0.04298630108595678, 0.042935081850281435, 0.04276060440330685, 0.042834748525477154, 0.04268220823782474, 0.04261852612285474, 0.042736989073049354, 0.04268886462145754, 0.04242573812147124, 0.04227247258766942, 0.042390110890342135, 0.04248446419405951, 0.04245191936545183, 0.042340564236269036, 0.04238941364702967, 0.04202102438000343, 0.0423054254330663, 0.04236618886328495, 0.04227032440960755, 0.04211411958874948, 0.042387417980917705, 0.04244063995379503, 0.04230325889889504, 0.042051191632957514, 0.04204713274582123, 0.04214163687487854, 0.04205420168122878, 0.041831243085484034, 0.04180116875634046, 0.04185763612713283, 0.04198602727093991, 0.04172541944522176, 0.041934393722472534, 0.041794824989613524, 0.04191608180823351, 0.041792146484724776, 0.04184966176076347, 0.04171029256926509, 0.041749135186341935, 0.041767482292308394, 0.0415905982943571, 0.04165265562144103, 0.04152351097614196, 0.04159305289408468, 0.041610565397656944, 0.041603995734075604, 0.04157718448835895, 0.0414793535314417, 0.041543935370772375, 0.041396074273352686, 0.0413575038822604, 0.041366916439630516, 0.04150432010303741, 0.04134778438849639, 0.04131089831765737, 0.04134677969725645, 0.04121839585628021], 'acc': [0.9794962712178793, 0.9887396888845019, 0.9914981371639889, 0.9929718523711796, 0.9934003518087481, 0.9936181511143616, 0.993765511147125, 0.9939141163391695, 0.9939970437551902, 0.9940647654059395, 0.9941809563028189, 0.9942038673958765, 0.9942902167748888, 0.9943369651256482, 0.9944156051729893, 0.9944280122566543, 0.9944922092447592, 0.9945377825212643, 0.9945487563375892, 0.9945682102867094, 0.9946269530654017, 0.994655743797701, 0.9946633972581375, 0.994701749342586, 0.9947085292979861, 0.9947489375627176, 0.9947579742802656, 0.9947796398093941, 0.9947933521979354, 0.9948211073785979, 0.9948336831216631, 0.9948569084813657, 0.9948858346685053, 0.9948754842869061, 0.9949105615724375, 0.9949079410999233, 0.9949304158759917, 0.9949292926073253, 0.9949528043001851, 0.99496689644019, 0.9949664734169512, 0.9949884741117271, 0.9949869994105652, 0.9950040898876107, 0.9950182590363821, 0.995015830684331, 0.9950221464233298, 0.9950241194505862, 0.9950716720412994, 0.9950506595330516, 0.9950773271775925, 0.9950827909920698, 0.9950630624871778, 0.995085450430053, 0.9951061152243071, 0.9951007306910142, 0.9950946549551107, 0.9951012665407534, 0.9951253647495365, 0.995133453189494, 0.995129195575218, 0.9951488428715235, 0.9951458635039535, 0.9951538739128759, 0.9951557759350637, 0.995172610225225, 0.9951611494960076, 0.9951737935883731, 0.9951591763452271, 0.9951828824096705, 0.9951998434739084, 0.9951902710173671, 0.9952025558078992, 0.9951950502910172, 0.9951950329252305, 0.9952028935803313, 0.9951923953105507, 0.9952060462948287, 0.9952060980493029, 0.995219250422103, 0.9952121113799719, 0.9952124764086387, 0.9952450805667572, 0.9952359448446314, 0.9952322004176789, 0.9952235250912257, 0.9952410272592371, 0.9952416515586315, 0.9952393753052303, 0.9952507418281427, 0.9952513930643129, 0.9952537473146736, 0.995257889359362, 0.9952834858155589, 0.9952836895281212, 0.9952845429498403, 0.9952737557060419, 0.9952751049982814, 0.9952885668479724, 0.9952775219456492, 0.995316512756257, 0.9952803424420402, 0.9952803335589526, 0.9952890873232186, 0.9953097266842835, 0.9952824604525318, 0.9952817835680698, 0.995286727931699, 0.9953042725089466, 0.9953167384071281, 0.9952968147192268, 0.9953181184632612, 0.9953446337173797, 0.9953306084273478, 0.9953301718909968, 0.995324596557671, 0.9953523560829748, 0.9953278439746995, 0.9953359101079057, 0.9953257405847779, 0.995328806295704, 0.9953291284806706, 0.995343309962686, 0.995347634520643, 0.9953343469506994, 0.9953536270630553, 0.9953396529141366, 0.9953577455103783, 0.9953542723913653, 0.9953567878130093, 0.9953469381129818, 0.9953437987539969, 0.995358780340038, 0.9953582075352914, 0.9953764650362261, 0.9953777781678459, 0.9953746014130002, 0.9953572151297758, 0.9953691468862584, 0.9953735478012858, 0.9953801047211185, 0.9953865522262721], 'mDice': [0.7572858526307228, 0.859033256538105, 0.8756115120779464, 0.8822371035023092, 0.8872461089193576, 0.8909515992435786, 0.893477552430007, 0.8962524154074055, 0.8977251099625231, 0.8990650923556145, 0.900760890318402, 0.9010552239579037, 0.9024886210516692, 0.9034306184354521, 0.9048686690766162, 0.9053635435348522, 0.906309103579853, 0.9068002968746507, 0.9070628114625283, 0.9076164953151598, 0.9087794287439779, 0.9092988498120521, 0.9092708658114862, 0.9101986694375125, 0.9102178915105722, 0.9107942661573919, 0.9109617902730215, 0.9114997974097863, 0.9117294925221056, 0.9121189872355656, 0.9124753918747267, 0.9128660820524388, 0.9131733061339851, 0.9132466416192488, 0.9138710869573834, 0.913566178735755, 0.9141572271358733, 0.9140504045825844, 0.9144437887232697, 0.9147893117227819, 0.9147422621797663, 0.915048601475117, 0.9151547873403949, 0.9154762175555363, 0.9158723825413719, 0.9157747388414219, 0.9159405519350909, 0.915994583761747, 0.9168114635357159, 0.916333769742037, 0.916879395775913, 0.9169311249000993, 0.9166708068137691, 0.9170666597840802, 0.9173943922847406, 0.9171846251346829, 0.9171114721584283, 0.9174024217206361, 0.9180162378917635, 0.9179571820901863, 0.9179689445675184, 0.9181505466796418, 0.9183120076290114, 0.9184109417257549, 0.9181616929228523, 0.9186449218768332, 0.9185939856603282, 0.9187912273544137, 0.9186595689467936, 0.9190087055314046, 0.9192710801728495, 0.9191921165051063, 0.9191398070795375, 0.9190118363118527, 0.9194864408590021, 0.9193488709067132, 0.9192345939691364, 0.9192565225769428, 0.9195682325261614, 0.9196931564565529, 0.9194966775728591, 0.9197058956837839, 0.9201166599570446, 0.9200731451140726, 0.9199586362131165, 0.9198455896247054, 0.9199255648463316, 0.9202456635630952, 0.920111981387494, 0.920383881978651, 0.9205044858490671, 0.920286568727005, 0.9203733428245181, 0.9208464329851893, 0.9211251279201761, 0.9209092376478089, 0.9207383884632068, 0.9208051446942002, 0.9209995413851362, 0.9209095604759527, 0.9215721502970937, 0.9210676581171175, 0.9209539397491981, 0.9211287756575629, 0.9214032559092926, 0.9209125974526141, 0.9208189530141246, 0.921069029198585, 0.9215167689630958, 0.9215251698083176, 0.9213583249816106, 0.9215113839868198, 0.9219108906279347, 0.9219714506337696, 0.9218654092469817, 0.9216344405264999, 0.9221009217147025, 0.9217283786882963, 0.9219786681371406, 0.9217644235360881, 0.9219850695465021, 0.9218804491939075, 0.9221307613435025, 0.9220597266482145, 0.9220295781431063, 0.9223450339099886, 0.9222349866446811, 0.9224678331063433, 0.922340864191639, 0.9223101860215164, 0.9223262101659933, 0.9223741823553455, 0.9225506044091802, 0.9224320718294539, 0.9226921741745286, 0.922766188458878, 0.9227499225819682, 0.9225022487349546, 0.9227858520007344, 0.9228527563030656, 0.9227865989680936, 0.923014746440391]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.04it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:02,  1.28it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.56it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  1.81it/s]predicting test subjects: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:55,  2.30it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:53,  2.32it/s]predicting train subjects:   1%|          | 3/266 [00:01<01:45,  2.50it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:38,  2.67it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:40,  2.60it/s]predicting train subjects:   2%|▏         | 6/266 [00:02<01:44,  2.49it/s]predicting train subjects:   3%|▎         | 7/266 [00:02<01:53,  2.28it/s]predicting train subjects:   3%|▎         | 8/266 [00:03<01:50,  2.34it/s]predicting train subjects:   3%|▎         | 9/266 [00:03<01:47,  2.40it/s]predicting train subjects:   4%|▍         | 10/266 [00:04<01:40,  2.54it/s]predicting train subjects:   4%|▍         | 11/266 [00:04<01:37,  2.62it/s]predicting train subjects:   5%|▍         | 12/266 [00:04<01:38,  2.58it/s]predicting train subjects:   5%|▍         | 13/266 [00:05<01:39,  2.55it/s]predicting train subjects:   5%|▌         | 14/266 [00:05<01:36,  2.61it/s]predicting train subjects:   6%|▌         | 15/266 [00:05<01:34,  2.66it/s]predicting train subjects:   6%|▌         | 16/266 [00:06<01:34,  2.63it/s]predicting train subjects:   6%|▋         | 17/266 [00:06<01:36,  2.57it/s]predicting train subjects:   7%|▋         | 18/266 [00:07<01:37,  2.53it/s]predicting train subjects:   7%|▋         | 19/266 [00:07<01:38,  2.51it/s]predicting train subjects:   8%|▊         | 20/266 [00:07<01:38,  2.50it/s]predicting train subjects:   8%|▊         | 21/266 [00:08<01:37,  2.51it/s]predicting train subjects:   8%|▊         | 22/266 [00:08<01:36,  2.54it/s]predicting train subjects:   9%|▊         | 23/266 [00:09<01:38,  2.46it/s]predicting train subjects:   9%|▉         | 24/266 [00:09<01:36,  2.51it/s]predicting train subjects:   9%|▉         | 25/266 [00:09<01:32,  2.61it/s]predicting train subjects:  10%|▉         | 26/266 [00:10<01:28,  2.73it/s]predicting train subjects:  10%|█         | 27/266 [00:10<01:26,  2.77it/s]predicting train subjects:  11%|█         | 28/266 [00:10<01:33,  2.54it/s]predicting train subjects:  11%|█         | 29/266 [00:11<01:33,  2.55it/s]predicting train subjects:  11%|█▏        | 30/266 [00:11<01:27,  2.69it/s]predicting train subjects:  12%|█▏        | 31/266 [00:12<01:23,  2.81it/s]predicting train subjects:  12%|█▏        | 32/266 [00:12<01:23,  2.81it/s]predicting train subjects:  12%|█▏        | 33/266 [00:12<01:24,  2.76it/s]predicting train subjects:  13%|█▎        | 34/266 [00:13<01:29,  2.60it/s]predicting train subjects:  13%|█▎        | 35/266 [00:13<01:27,  2.65it/s]predicting train subjects:  14%|█▎        | 36/266 [00:13<01:25,  2.70it/s]predicting train subjects:  14%|█▍        | 37/266 [00:14<01:21,  2.80it/s]predicting train subjects:  14%|█▍        | 38/266 [00:14<01:20,  2.82it/s]predicting train subjects:  15%|█▍        | 39/266 [00:14<01:19,  2.85it/s]predicting train subjects:  15%|█▌        | 40/266 [00:15<01:18,  2.86it/s]predicting train subjects:  15%|█▌        | 41/266 [00:15<01:19,  2.85it/s]predicting train subjects:  16%|█▌        | 42/266 [00:15<01:14,  3.00it/s]predicting train subjects:  16%|█▌        | 43/266 [00:16<01:10,  3.15it/s]predicting train subjects:  17%|█▋        | 44/266 [00:16<01:07,  3.29it/s]predicting train subjects:  17%|█▋        | 45/266 [00:16<01:05,  3.37it/s]predicting train subjects:  17%|█▋        | 46/266 [00:17<01:14,  2.94it/s]predicting train subjects:  18%|█▊        | 47/266 [00:17<01:12,  3.01it/s]predicting train subjects:  18%|█▊        | 48/266 [00:17<01:10,  3.10it/s]predicting train subjects:  18%|█▊        | 49/266 [00:18<01:08,  3.19it/s]predicting train subjects:  19%|█▉        | 50/266 [00:18<01:07,  3.18it/s]predicting train subjects:  19%|█▉        | 51/266 [00:18<01:04,  3.32it/s]predicting train subjects:  20%|█▉        | 52/266 [00:19<01:06,  3.22it/s]predicting train subjects:  20%|█▉        | 53/266 [00:19<01:09,  3.09it/s]predicting train subjects:  20%|██        | 54/266 [00:19<01:09,  3.06it/s]predicting train subjects:  21%|██        | 55/266 [00:20<01:09,  3.04it/s]predicting train subjects:  21%|██        | 56/266 [00:20<01:06,  3.15it/s]predicting train subjects:  21%|██▏       | 57/266 [00:20<01:07,  3.08it/s]predicting train subjects:  22%|██▏       | 58/266 [00:20<01:05,  3.19it/s]predicting train subjects:  22%|██▏       | 59/266 [00:21<01:09,  2.99it/s]predicting train subjects:  23%|██▎       | 60/266 [00:21<01:11,  2.88it/s]predicting train subjects:  23%|██▎       | 61/266 [00:22<01:07,  3.05it/s]predicting train subjects:  23%|██▎       | 62/266 [00:22<01:04,  3.17it/s]predicting train subjects:  24%|██▎       | 63/266 [00:22<01:03,  3.21it/s]predicting train subjects:  24%|██▍       | 64/266 [00:22<01:00,  3.37it/s]predicting train subjects:  24%|██▍       | 65/266 [00:23<01:00,  3.30it/s]predicting train subjects:  25%|██▍       | 66/266 [00:23<00:58,  3.44it/s]predicting train subjects:  25%|██▌       | 67/266 [00:23<00:58,  3.39it/s]predicting train subjects:  26%|██▌       | 68/266 [00:24<00:58,  3.41it/s]predicting train subjects:  26%|██▌       | 69/266 [00:24<00:55,  3.53it/s]predicting train subjects:  26%|██▋       | 70/266 [00:24<00:56,  3.50it/s]predicting train subjects:  27%|██▋       | 71/266 [00:24<00:55,  3.49it/s]predicting train subjects:  27%|██▋       | 72/266 [00:25<00:56,  3.42it/s]predicting train subjects:  27%|██▋       | 73/266 [00:25<00:57,  3.38it/s]predicting train subjects:  28%|██▊       | 74/266 [00:25<00:57,  3.36it/s]predicting train subjects:  28%|██▊       | 75/266 [00:26<00:54,  3.52it/s]predicting train subjects:  29%|██▊       | 76/266 [00:26<00:52,  3.62it/s]predicting train subjects:  29%|██▉       | 77/266 [00:26<00:52,  3.59it/s]predicting train subjects:  29%|██▉       | 78/266 [00:26<00:56,  3.30it/s]predicting train subjects:  30%|██▉       | 79/266 [00:27<00:59,  3.15it/s]predicting train subjects:  30%|███       | 80/266 [00:27<01:02,  2.99it/s]predicting train subjects:  30%|███       | 81/266 [00:28<01:04,  2.87it/s]predicting train subjects:  31%|███       | 82/266 [00:28<01:07,  2.72it/s]predicting train subjects:  31%|███       | 83/266 [00:28<01:06,  2.77it/s]predicting train subjects:  32%|███▏      | 84/266 [00:29<01:04,  2.83it/s]predicting train subjects:  32%|███▏      | 85/266 [00:29<01:09,  2.61it/s]predicting train subjects:  32%|███▏      | 86/266 [00:29<01:07,  2.67it/s]predicting train subjects:  33%|███▎      | 87/266 [00:30<01:05,  2.71it/s]predicting train subjects:  33%|███▎      | 88/266 [00:30<01:04,  2.78it/s]predicting train subjects:  33%|███▎      | 89/266 [00:31<01:04,  2.75it/s]predicting train subjects:  34%|███▍      | 90/266 [00:31<01:05,  2.68it/s]predicting train subjects:  34%|███▍      | 91/266 [00:31<01:03,  2.76it/s]predicting train subjects:  35%|███▍      | 92/266 [00:32<01:01,  2.82it/s]predicting train subjects:  35%|███▍      | 93/266 [00:32<00:59,  2.88it/s]predicting train subjects:  35%|███▌      | 94/266 [00:32<00:58,  2.92it/s]predicting train subjects:  36%|███▌      | 95/266 [00:33<00:58,  2.91it/s]predicting train subjects:  36%|███▌      | 96/266 [00:33<01:01,  2.79it/s]predicting train subjects:  36%|███▋      | 97/266 [00:33<01:02,  2.70it/s]predicting train subjects:  37%|███▋      | 98/266 [00:34<01:02,  2.68it/s]predicting train subjects:  37%|███▋      | 99/266 [00:34<00:57,  2.90it/s]predicting train subjects:  38%|███▊      | 100/266 [00:34<00:57,  2.90it/s]predicting train subjects:  38%|███▊      | 101/266 [00:35<00:54,  3.01it/s]predicting train subjects:  38%|███▊      | 102/266 [00:35<00:55,  2.94it/s]predicting train subjects:  39%|███▊      | 103/266 [00:35<00:55,  2.94it/s]predicting train subjects:  39%|███▉      | 104/266 [00:36<00:54,  2.98it/s]predicting train subjects:  39%|███▉      | 105/266 [00:36<00:54,  2.94it/s]predicting train subjects:  40%|███▉      | 106/266 [00:36<00:52,  3.02it/s]predicting train subjects:  40%|████      | 107/266 [00:37<00:53,  2.99it/s]predicting train subjects:  41%|████      | 108/266 [00:37<00:54,  2.91it/s]predicting train subjects:  41%|████      | 109/266 [00:37<00:53,  2.95it/s]predicting train subjects:  41%|████▏     | 110/266 [00:38<00:51,  3.01it/s]predicting train subjects:  42%|████▏     | 111/266 [00:38<00:50,  3.10it/s]predicting train subjects:  42%|████▏     | 112/266 [00:38<00:48,  3.16it/s]predicting train subjects:  42%|████▏     | 113/266 [00:39<00:48,  3.18it/s]predicting train subjects:  43%|████▎     | 114/266 [00:39<00:50,  3.02it/s]predicting train subjects:  43%|████▎     | 115/266 [00:39<00:53,  2.84it/s]predicting train subjects:  44%|████▎     | 116/266 [00:40<00:51,  2.94it/s]predicting train subjects:  44%|████▍     | 117/266 [00:40<00:49,  3.00it/s]predicting train subjects:  44%|████▍     | 118/266 [00:40<00:48,  3.06it/s]predicting train subjects:  45%|████▍     | 119/266 [00:41<00:48,  3.00it/s]predicting train subjects:  45%|████▌     | 120/266 [00:41<00:49,  2.92it/s]predicting train subjects:  45%|████▌     | 121/266 [00:41<00:50,  2.88it/s]predicting train subjects:  46%|████▌     | 122/266 [00:42<00:52,  2.76it/s]predicting train subjects:  46%|████▌     | 123/266 [00:42<00:50,  2.82it/s]predicting train subjects:  47%|████▋     | 124/266 [00:42<00:50,  2.83it/s]predicting train subjects:  47%|████▋     | 125/266 [00:43<00:48,  2.89it/s]predicting train subjects:  47%|████▋     | 126/266 [00:43<00:49,  2.85it/s]predicting train subjects:  48%|████▊     | 127/266 [00:44<00:51,  2.69it/s]predicting train subjects:  48%|████▊     | 128/266 [00:44<00:51,  2.70it/s]predicting train subjects:  48%|████▊     | 129/266 [00:44<00:50,  2.69it/s]predicting train subjects:  49%|████▉     | 130/266 [00:45<00:49,  2.75it/s]predicting train subjects:  49%|████▉     | 131/266 [00:45<00:47,  2.82it/s]predicting train subjects:  50%|████▉     | 132/266 [00:45<00:48,  2.76it/s]predicting train subjects:  50%|█████     | 133/266 [00:46<00:48,  2.77it/s]predicting train subjects:  50%|█████     | 134/266 [00:46<00:50,  2.64it/s]predicting train subjects:  51%|█████     | 135/266 [00:47<00:47,  2.75it/s]predicting train subjects:  51%|█████     | 136/266 [00:47<00:47,  2.73it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:47<00:46,  2.77it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:48<00:45,  2.81it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:48<00:44,  2.83it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:48<00:44,  2.83it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:49<00:43,  2.86it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:49<00:41,  2.96it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:49<00:41,  2.93it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:50<00:41,  2.96it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:50<00:40,  2.99it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:50<00:41,  2.89it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:51<00:40,  2.92it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:51<00:39,  2.98it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:51<00:38,  3.06it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:52<00:37,  3.11it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:52<00:38,  3.02it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:52<00:37,  3.03it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:53<00:40,  2.82it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:53<00:38,  2.90it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:53<00:35,  3.16it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:53<00:32,  3.41it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:54<00:30,  3.56it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:54<00:29,  3.70it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:54<00:28,  3.81it/s]predicting train subjects:  60%|██████    | 160/266 [00:54<00:27,  3.81it/s]predicting train subjects:  61%|██████    | 161/266 [00:55<00:30,  3.44it/s]predicting train subjects:  61%|██████    | 162/266 [00:55<00:30,  3.38it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:55<00:29,  3.55it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:56<00:27,  3.68it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:56<00:26,  3.75it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:56<00:26,  3.81it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:56<00:25,  3.87it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:57<00:25,  3.92it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:57<00:26,  3.62it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:57<00:26,  3.58it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:58<00:25,  3.69it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:58<00:24,  3.79it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:58<00:25,  3.70it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:58<00:25,  3.66it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:59<00:25,  3.51it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:59<00:25,  3.50it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:59<00:27,  3.25it/s]predicting train subjects:  67%|██████▋   | 178/266 [01:00<00:26,  3.38it/s]predicting train subjects:  67%|██████▋   | 179/266 [01:00<00:25,  3.47it/s]predicting train subjects:  68%|██████▊   | 180/266 [01:00<00:25,  3.35it/s]predicting train subjects:  68%|██████▊   | 181/266 [01:00<00:24,  3.44it/s]predicting train subjects:  68%|██████▊   | 182/266 [01:01<00:24,  3.48it/s]predicting train subjects:  69%|██████▉   | 183/266 [01:01<00:24,  3.37it/s]predicting train subjects:  69%|██████▉   | 184/266 [01:01<00:24,  3.28it/s]predicting train subjects:  70%|██████▉   | 185/266 [01:02<00:23,  3.41it/s]predicting train subjects:  70%|██████▉   | 186/266 [01:02<00:23,  3.44it/s]predicting train subjects:  70%|███████   | 187/266 [01:02<00:22,  3.51it/s]predicting train subjects:  71%|███████   | 188/266 [01:02<00:22,  3.49it/s]predicting train subjects:  71%|███████   | 189/266 [01:03<00:21,  3.56it/s]predicting train subjects:  71%|███████▏  | 190/266 [01:03<00:21,  3.59it/s]predicting train subjects:  72%|███████▏  | 191/266 [01:03<00:21,  3.45it/s]predicting train subjects:  72%|███████▏  | 192/266 [01:04<00:22,  3.29it/s]predicting train subjects:  73%|███████▎  | 193/266 [01:04<00:22,  3.26it/s]predicting train subjects:  73%|███████▎  | 194/266 [01:04<00:23,  3.04it/s]predicting train subjects:  73%|███████▎  | 195/266 [01:05<00:22,  3.20it/s]predicting train subjects:  74%|███████▎  | 196/266 [01:05<00:21,  3.28it/s]predicting train subjects:  74%|███████▍  | 197/266 [01:05<00:20,  3.36it/s]predicting train subjects:  74%|███████▍  | 198/266 [01:06<00:20,  3.32it/s]predicting train subjects:  75%|███████▍  | 199/266 [01:06<00:20,  3.31it/s]predicting train subjects:  75%|███████▌  | 200/266 [01:06<00:21,  3.05it/s]predicting train subjects:  76%|███████▌  | 201/266 [01:06<00:20,  3.16it/s]predicting train subjects:  76%|███████▌  | 202/266 [01:07<00:19,  3.29it/s]predicting train subjects:  76%|███████▋  | 203/266 [01:07<00:18,  3.39it/s]predicting train subjects:  77%|███████▋  | 204/266 [01:07<00:18,  3.41it/s]predicting train subjects:  77%|███████▋  | 205/266 [01:08<00:17,  3.46it/s]predicting train subjects:  77%|███████▋  | 206/266 [01:08<00:17,  3.35it/s]predicting train subjects:  78%|███████▊  | 207/266 [01:08<00:18,  3.25it/s]predicting train subjects:  78%|███████▊  | 208/266 [01:09<00:17,  3.31it/s]predicting train subjects:  79%|███████▊  | 209/266 [01:09<00:17,  3.25it/s]predicting train subjects:  79%|███████▉  | 210/266 [01:09<00:16,  3.34it/s]predicting train subjects:  79%|███████▉  | 211/266 [01:09<00:15,  3.44it/s]predicting train subjects:  80%|███████▉  | 212/266 [01:10<00:15,  3.44it/s]predicting train subjects:  80%|████████  | 213/266 [01:10<00:15,  3.34it/s]predicting train subjects:  80%|████████  | 214/266 [01:10<00:16,  3.16it/s]predicting train subjects:  81%|████████  | 215/266 [01:11<00:15,  3.36it/s]predicting train subjects:  81%|████████  | 216/266 [01:11<00:14,  3.48it/s]predicting train subjects:  82%|████████▏ | 217/266 [01:11<00:13,  3.53it/s]predicting train subjects:  82%|████████▏ | 218/266 [01:11<00:13,  3.69it/s]predicting train subjects:  82%|████████▏ | 219/266 [01:12<00:13,  3.60it/s]predicting train subjects:  83%|████████▎ | 220/266 [01:12<00:12,  3.76it/s]predicting train subjects:  83%|████████▎ | 221/266 [01:12<00:12,  3.67it/s]predicting train subjects:  83%|████████▎ | 222/266 [01:13<00:12,  3.51it/s]predicting train subjects:  84%|████████▍ | 223/266 [01:13<00:11,  3.67it/s]predicting train subjects:  84%|████████▍ | 224/266 [01:13<00:11,  3.55it/s]predicting train subjects:  85%|████████▍ | 225/266 [01:13<00:11,  3.53it/s]predicting train subjects:  85%|████████▍ | 226/266 [01:14<00:10,  3.64it/s]predicting train subjects:  85%|████████▌ | 227/266 [01:14<00:10,  3.75it/s]predicting train subjects:  86%|████████▌ | 228/266 [01:14<00:09,  3.86it/s]predicting train subjects:  86%|████████▌ | 229/266 [01:14<00:09,  3.94it/s]predicting train subjects:  86%|████████▋ | 230/266 [01:15<00:09,  3.77it/s]predicting train subjects:  87%|████████▋ | 231/266 [01:15<00:10,  3.34it/s]predicting train subjects:  87%|████████▋ | 232/266 [01:15<00:09,  3.47it/s]predicting train subjects:  88%|████████▊ | 233/266 [01:16<00:09,  3.60it/s]predicting train subjects:  88%|████████▊ | 234/266 [01:16<00:08,  3.67it/s]predicting train subjects:  88%|████████▊ | 235/266 [01:16<00:08,  3.73it/s]predicting train subjects:  89%|████████▊ | 236/266 [01:16<00:08,  3.56it/s]predicting train subjects:  89%|████████▉ | 237/266 [01:17<00:07,  3.65it/s]predicting train subjects:  89%|████████▉ | 238/266 [01:17<00:07,  3.54it/s]predicting train subjects:  90%|████████▉ | 239/266 [01:17<00:07,  3.57it/s]predicting train subjects:  90%|█████████ | 240/266 [01:18<00:07,  3.51it/s]predicting train subjects:  91%|█████████ | 241/266 [01:18<00:07,  3.47it/s]predicting train subjects:  91%|█████████ | 242/266 [01:18<00:06,  3.47it/s]predicting train subjects:  91%|█████████▏| 243/266 [01:18<00:06,  3.55it/s]predicting train subjects:  92%|█████████▏| 244/266 [01:19<00:06,  3.65it/s]predicting train subjects:  92%|█████████▏| 245/266 [01:19<00:05,  3.72it/s]predicting train subjects:  92%|█████████▏| 246/266 [01:19<00:05,  3.38it/s]predicting train subjects:  93%|█████████▎| 247/266 [01:20<00:05,  3.39it/s]predicting train subjects:  93%|█████████▎| 248/266 [01:20<00:05,  3.42it/s]predicting train subjects:  94%|█████████▎| 249/266 [01:20<00:05,  3.16it/s]predicting train subjects:  94%|█████████▍| 250/266 [01:21<00:05,  3.07it/s]predicting train subjects:  94%|█████████▍| 251/266 [01:21<00:04,  3.02it/s]predicting train subjects:  95%|█████████▍| 252/266 [01:21<00:04,  2.93it/s]predicting train subjects:  95%|█████████▌| 253/266 [01:22<00:04,  2.69it/s]predicting train subjects:  95%|█████████▌| 254/266 [01:22<00:04,  2.80it/s]predicting train subjects:  96%|█████████▌| 255/266 [01:22<00:03,  2.79it/s]predicting train subjects:  96%|█████████▌| 256/266 [01:23<00:03,  2.85it/s]predicting train subjects:  97%|█████████▋| 257/266 [01:23<00:03,  2.88it/s]predicting train subjects:  97%|█████████▋| 258/266 [01:23<00:02,  2.84it/s]predicting train subjects:  97%|█████████▋| 259/266 [01:24<00:02,  2.66it/s]predicting train subjects:  98%|█████████▊| 260/266 [01:24<00:02,  2.70it/s]predicting train subjects:  98%|█████████▊| 261/266 [01:25<00:01,  2.79it/s]predicting train subjects:  98%|█████████▊| 262/266 [01:25<00:01,  2.87it/s]predicting train subjects:  99%|█████████▉| 263/266 [01:25<00:01,  2.82it/s]predicting train subjects:  99%|█████████▉| 264/266 [01:26<00:00,  2.78it/s]predicting train subjects: 100%|█████████▉| 265/266 [01:26<00:00,  2.71it/s]predicting train subjects: 100%|██████████| 266/266 [01:26<00:00,  2.79it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 63.54it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:04, 62.46it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:04, 62.15it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 62.62it/s]saving BB  train1-THALAMUS:  11%|█         | 28/266 [00:00<00:03, 62.99it/s]saving BB  train1-THALAMUS:  14%|█▎        | 36/266 [00:00<00:03, 65.14it/s]saving BB  train1-THALAMUS:  17%|█▋        | 44/266 [00:00<00:03, 67.07it/s]saving BB  train1-THALAMUS:  20%|█▉        | 52/266 [00:00<00:03, 69.72it/s]saving BB  train1-THALAMUS:  23%|██▎       | 60/266 [00:00<00:02, 71.80it/s]saving BB  train1-THALAMUS:  26%|██▌       | 69/266 [00:00<00:02, 74.22it/s]saving BB  train1-THALAMUS:  29%|██▉       | 78/266 [00:01<00:02, 76.74it/s]saving BB  train1-THALAMUS:  32%|███▏      | 86/266 [00:01<00:02, 74.48it/s]saving BB  train1-THALAMUS:  35%|███▌      | 94/266 [00:01<00:02, 72.24it/s]saving BB  train1-THALAMUS:  38%|███▊      | 102/266 [00:01<00:02, 70.71it/s]saving BB  train1-THALAMUS:  41%|████▏     | 110/266 [00:01<00:02, 71.51it/s]saving BB  train1-THALAMUS:  44%|████▍     | 118/266 [00:01<00:02, 72.68it/s]saving BB  train1-THALAMUS:  47%|████▋     | 126/266 [00:01<00:01, 70.61it/s]saving BB  train1-THALAMUS:  50%|█████     | 134/266 [00:01<00:01, 70.00it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 142/266 [00:02<00:01, 70.00it/s]saving BB  train1-THALAMUS:  56%|█████▋    | 150/266 [00:02<00:01, 70.76it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 158/266 [00:02<00:01, 72.84it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 167/266 [00:02<00:01, 76.67it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 176/266 [00:02<00:01, 78.53it/s]saving BB  train1-THALAMUS:  70%|██████▉   | 185/266 [00:02<00:01, 80.03it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 194/266 [00:02<00:00, 79.25it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 202/266 [00:02<00:00, 78.68it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 210/266 [00:02<00:00, 76.97it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 218/266 [00:02<00:00, 77.38it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 226/266 [00:03<00:00, 77.50it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 235/266 [00:03<00:00, 78.70it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 244/266 [00:03<00:00, 80.39it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 253/266 [00:03<00:00, 79.19it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 261/266 [00:03<00:00, 76.69it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 73.95it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:23,  1.67s/it]Loading train:   1%|          | 2/266 [00:03<07:10,  1.63s/it]Loading train:   1%|          | 3/266 [00:04<06:45,  1.54s/it]Loading train:   2%|▏         | 4/266 [00:05<06:20,  1.45s/it]Loading train:   2%|▏         | 5/266 [00:07<06:26,  1.48s/it]Loading train:   2%|▏         | 6/266 [00:08<06:09,  1.42s/it]Loading train:   3%|▎         | 7/266 [00:09<05:44,  1.33s/it]Loading train:   3%|▎         | 8/266 [00:10<05:23,  1.26s/it]Loading train:   3%|▎         | 9/266 [00:12<05:28,  1.28s/it]Loading train:   4%|▍         | 10/266 [00:13<05:10,  1.21s/it]Loading train:   4%|▍         | 11/266 [00:14<05:04,  1.19s/it]Loading train:   5%|▍         | 12/266 [00:15<04:51,  1.15s/it]Loading train:   5%|▍         | 13/266 [00:16<04:43,  1.12s/it]Loading train:   5%|▌         | 14/266 [00:17<04:50,  1.15s/it]Loading train:   6%|▌         | 15/266 [00:18<04:46,  1.14s/it]Loading train:   6%|▌         | 16/266 [00:19<04:45,  1.14s/it]Loading train:   6%|▋         | 17/266 [00:20<04:37,  1.12s/it]Loading train:   7%|▋         | 18/266 [00:22<04:42,  1.14s/it]Loading train:   7%|▋         | 19/266 [00:23<04:38,  1.13s/it]Loading train:   8%|▊         | 20/266 [00:24<04:35,  1.12s/it]Loading train:   8%|▊         | 21/266 [00:25<04:38,  1.14s/it]Loading train:   8%|▊         | 22/266 [00:26<04:37,  1.14s/it]Loading train:   9%|▊         | 23/266 [00:27<04:41,  1.16s/it]Loading train:   9%|▉         | 24/266 [00:28<04:29,  1.12s/it]Loading train:   9%|▉         | 25/266 [00:29<04:13,  1.05s/it]Loading train:  10%|▉         | 26/266 [00:30<04:08,  1.03s/it]Loading train:  10%|█         | 27/266 [00:31<04:04,  1.02s/it]Loading train:  11%|█         | 28/266 [00:32<04:01,  1.02s/it]Loading train:  11%|█         | 29/266 [00:33<03:57,  1.00s/it]Loading train:  11%|█▏        | 30/266 [00:34<03:54,  1.01it/s]Loading train:  12%|█▏        | 31/266 [00:35<03:54,  1.00it/s]Loading train:  12%|█▏        | 32/266 [00:36<03:56,  1.01s/it]Loading train:  12%|█▏        | 33/266 [00:37<03:50,  1.01it/s]Loading train:  13%|█▎        | 34/266 [00:38<03:53,  1.01s/it]Loading train:  13%|█▎        | 35/266 [00:39<03:53,  1.01s/it]Loading train:  14%|█▎        | 36/266 [00:40<03:50,  1.00s/it]Loading train:  14%|█▍        | 37/266 [00:41<03:49,  1.00s/it]Loading train:  14%|█▍        | 38/266 [00:42<03:49,  1.01s/it]Loading train:  15%|█▍        | 39/266 [00:43<03:47,  1.00s/it]Loading train:  15%|█▌        | 40/266 [00:44<03:48,  1.01s/it]Loading train:  15%|█▌        | 41/266 [00:45<03:44,  1.00it/s]Loading train:  16%|█▌        | 42/266 [00:46<03:53,  1.04s/it]Loading train:  16%|█▌        | 43/266 [00:47<03:44,  1.01s/it]Loading train:  17%|█▋        | 44/266 [00:48<03:40,  1.01it/s]Loading train:  17%|█▋        | 45/266 [00:49<03:35,  1.03it/s]Loading train:  17%|█▋        | 46/266 [00:50<03:33,  1.03it/s]Loading train:  18%|█▊        | 47/266 [00:51<03:25,  1.06it/s]Loading train:  18%|█▊        | 48/266 [00:52<03:23,  1.07it/s]Loading train:  18%|█▊        | 49/266 [00:53<03:18,  1.09it/s]Loading train:  19%|█▉        | 50/266 [00:54<03:13,  1.12it/s]Loading train:  19%|█▉        | 51/266 [00:55<03:14,  1.10it/s]Loading train:  20%|█▉        | 52/266 [00:56<03:12,  1.11it/s]Loading train:  20%|█▉        | 53/266 [00:57<03:14,  1.09it/s]Loading train:  20%|██        | 54/266 [00:57<03:11,  1.11it/s]Loading train:  21%|██        | 55/266 [00:58<03:14,  1.09it/s]Loading train:  21%|██        | 56/266 [00:59<03:05,  1.13it/s]Loading train:  21%|██▏       | 57/266 [01:00<03:11,  1.09it/s]Loading train:  22%|██▏       | 58/266 [01:01<03:10,  1.09it/s]Loading train:  22%|██▏       | 59/266 [01:02<03:08,  1.10it/s]Loading train:  23%|██▎       | 60/266 [01:03<03:07,  1.10it/s]Loading train:  23%|██▎       | 61/266 [01:04<03:02,  1.12it/s]Loading train:  23%|██▎       | 62/266 [01:05<02:56,  1.15it/s]Loading train:  24%|██▎       | 63/266 [01:05<02:55,  1.15it/s]Loading train:  24%|██▍       | 64/266 [01:06<02:53,  1.17it/s]Loading train:  24%|██▍       | 65/266 [01:07<02:49,  1.18it/s]Loading train:  25%|██▍       | 66/266 [01:08<02:49,  1.18it/s]Loading train:  25%|██▌       | 67/266 [01:09<02:43,  1.22it/s]Loading train:  26%|██▌       | 68/266 [01:10<02:46,  1.19it/s]Loading train:  26%|██▌       | 69/266 [01:10<02:44,  1.20it/s]Loading train:  26%|██▋       | 70/266 [01:11<02:40,  1.22it/s]Loading train:  27%|██▋       | 71/266 [01:12<02:40,  1.21it/s]Loading train:  27%|██▋       | 72/266 [01:13<02:42,  1.19it/s]Loading train:  27%|██▋       | 73/266 [01:14<02:43,  1.18it/s]Loading train:  28%|██▊       | 74/266 [01:15<02:42,  1.18it/s]Loading train:  28%|██▊       | 75/266 [01:15<02:41,  1.19it/s]Loading train:  29%|██▊       | 76/266 [01:16<02:40,  1.18it/s]Loading train:  29%|██▉       | 77/266 [01:17<02:34,  1.22it/s]Loading train:  29%|██▉       | 78/266 [01:18<02:50,  1.10it/s]Loading train:  30%|██▉       | 79/266 [01:19<02:55,  1.07it/s]Loading train:  30%|███       | 80/266 [01:20<02:54,  1.07it/s]Loading train:  30%|███       | 81/266 [01:21<02:58,  1.03it/s]Loading train:  31%|███       | 82/266 [01:22<03:02,  1.01it/s]Loading train:  31%|███       | 83/266 [01:23<03:01,  1.01it/s]Loading train:  32%|███▏      | 84/266 [01:24<03:08,  1.04s/it]Loading train:  32%|███▏      | 85/266 [01:25<03:07,  1.03s/it]Loading train:  32%|███▏      | 86/266 [01:26<03:08,  1.05s/it]Loading train:  33%|███▎      | 87/266 [01:27<03:01,  1.01s/it]Loading train:  33%|███▎      | 88/266 [01:28<02:58,  1.01s/it]Loading train:  33%|███▎      | 89/266 [01:29<02:58,  1.01s/it]Loading train:  34%|███▍      | 90/266 [01:30<02:52,  1.02it/s]Loading train:  34%|███▍      | 91/266 [01:31<02:46,  1.05it/s]Loading train:  35%|███▍      | 92/266 [01:32<02:43,  1.06it/s]Loading train:  35%|███▍      | 93/266 [01:33<02:39,  1.08it/s]Loading train:  35%|███▌      | 94/266 [01:34<02:40,  1.07it/s]Loading train:  36%|███▌      | 95/266 [01:35<02:37,  1.08it/s]Loading train:  36%|███▌      | 96/266 [01:36<02:56,  1.04s/it]Loading train:  36%|███▋      | 97/266 [01:38<03:16,  1.16s/it]Loading train:  37%|███▋      | 98/266 [01:39<03:20,  1.19s/it]Loading train:  37%|███▋      | 99/266 [01:40<03:11,  1.15s/it]Loading train:  38%|███▊      | 100/266 [01:41<03:13,  1.16s/it]Loading train:  38%|███▊      | 101/266 [01:42<03:03,  1.11s/it]Loading train:  38%|███▊      | 102/266 [01:43<02:56,  1.08s/it]Loading train:  39%|███▊      | 103/266 [01:44<02:49,  1.04s/it]Loading train:  39%|███▉      | 104/266 [01:45<02:44,  1.01s/it]Loading train:  39%|███▉      | 105/266 [01:46<02:42,  1.01s/it]Loading train:  40%|███▉      | 106/266 [01:47<02:41,  1.01s/it]Loading train:  40%|████      | 107/266 [01:48<02:35,  1.02it/s]Loading train:  41%|████      | 108/266 [01:49<02:32,  1.04it/s]Loading train:  41%|████      | 109/266 [01:50<02:29,  1.05it/s]Loading train:  41%|████▏     | 110/266 [01:51<02:25,  1.07it/s]Loading train:  42%|████▏     | 111/266 [01:51<02:21,  1.09it/s]Loading train:  42%|████▏     | 112/266 [01:52<02:18,  1.12it/s]Loading train:  42%|████▏     | 113/266 [01:53<02:12,  1.16it/s]Loading train:  43%|████▎     | 114/266 [01:54<02:09,  1.18it/s]Loading train:  43%|████▎     | 115/266 [01:55<02:07,  1.19it/s]Loading train:  44%|████▎     | 116/266 [01:56<02:06,  1.19it/s]Loading train:  44%|████▍     | 117/266 [01:56<02:04,  1.20it/s]Loading train:  44%|████▍     | 118/266 [01:57<02:04,  1.19it/s]Loading train:  45%|████▍     | 119/266 [01:58<02:16,  1.08it/s]Loading train:  45%|████▌     | 120/266 [01:59<02:22,  1.03it/s]Loading train:  45%|████▌     | 121/266 [02:00<02:20,  1.03it/s]Loading train:  46%|████▌     | 122/266 [02:01<02:22,  1.01it/s]Loading train:  46%|████▌     | 123/266 [02:03<02:24,  1.01s/it]Loading train:  47%|████▋     | 124/266 [02:04<02:26,  1.03s/it]Loading train:  47%|████▋     | 125/266 [02:05<02:23,  1.02s/it]Loading train:  47%|████▋     | 126/266 [02:06<02:24,  1.03s/it]Loading train:  48%|████▊     | 127/266 [02:07<02:20,  1.01s/it]Loading train:  48%|████▊     | 128/266 [02:08<02:16,  1.01it/s]Loading train:  48%|████▊     | 129/266 [02:08<02:12,  1.04it/s]Loading train:  49%|████▉     | 130/266 [02:09<02:10,  1.04it/s]Loading train:  49%|████▉     | 131/266 [02:10<02:09,  1.04it/s]Loading train:  50%|████▉     | 132/266 [02:11<02:06,  1.06it/s]Loading train:  50%|█████     | 133/266 [02:12<02:03,  1.07it/s]Loading train:  50%|█████     | 134/266 [02:13<02:01,  1.09it/s]Loading train:  51%|█████     | 135/266 [02:14<02:00,  1.09it/s]Loading train:  51%|█████     | 136/266 [02:15<01:58,  1.09it/s]Loading train:  52%|█████▏    | 137/266 [02:16<02:02,  1.05it/s]Loading train:  52%|█████▏    | 138/266 [02:17<01:59,  1.08it/s]Loading train:  52%|█████▏    | 139/266 [02:18<01:57,  1.08it/s]Loading train:  53%|█████▎    | 140/266 [02:19<01:54,  1.10it/s]Loading train:  53%|█████▎    | 141/266 [02:20<01:55,  1.08it/s]Loading train:  53%|█████▎    | 142/266 [02:21<01:55,  1.08it/s]Loading train:  54%|█████▍    | 143/266 [02:21<01:51,  1.10it/s]Loading train:  54%|█████▍    | 144/266 [02:22<01:51,  1.09it/s]Loading train:  55%|█████▍    | 145/266 [02:23<01:52,  1.08it/s]Loading train:  55%|█████▍    | 146/266 [02:24<01:52,  1.07it/s]Loading train:  55%|█████▌    | 147/266 [02:25<01:54,  1.04it/s]Loading train:  56%|█████▌    | 148/266 [02:26<01:57,  1.01it/s]Loading train:  56%|█████▌    | 149/266 [02:27<01:54,  1.02it/s]Loading train:  56%|█████▋    | 150/266 [02:28<01:53,  1.02it/s]Loading train:  57%|█████▋    | 151/266 [02:29<01:53,  1.01it/s]Loading train:  57%|█████▋    | 152/266 [02:30<01:52,  1.02it/s]Loading train:  58%|█████▊    | 153/266 [02:31<01:50,  1.02it/s]Loading train:  58%|█████▊    | 154/266 [02:32<01:55,  1.04s/it]Loading train:  58%|█████▊    | 155/266 [02:33<01:53,  1.02s/it]Loading train:  59%|█████▊    | 156/266 [02:34<01:44,  1.06it/s]Loading train:  59%|█████▉    | 157/266 [02:35<01:35,  1.14it/s]Loading train:  59%|█████▉    | 158/266 [02:36<01:34,  1.15it/s]Loading train:  60%|█████▉    | 159/266 [02:36<01:29,  1.19it/s]Loading train:  60%|██████    | 160/266 [02:37<01:29,  1.18it/s]Loading train:  61%|██████    | 161/266 [02:38<01:31,  1.15it/s]Loading train:  61%|██████    | 162/266 [02:39<01:31,  1.14it/s]Loading train:  61%|██████▏   | 163/266 [02:40<01:28,  1.16it/s]Loading train:  62%|██████▏   | 164/266 [02:41<01:33,  1.09it/s]Loading train:  62%|██████▏   | 165/266 [02:42<01:28,  1.15it/s]Loading train:  62%|██████▏   | 166/266 [02:43<01:23,  1.19it/s]Loading train:  63%|██████▎   | 167/266 [02:43<01:22,  1.20it/s]Loading train:  63%|██████▎   | 168/266 [02:44<01:21,  1.20it/s]Loading train:  64%|██████▎   | 169/266 [02:45<01:19,  1.23it/s]Loading train:  64%|██████▍   | 170/266 [02:46<01:16,  1.25it/s]Loading train:  64%|██████▍   | 171/266 [02:47<01:15,  1.25it/s]Loading train:  65%|██████▍   | 172/266 [02:47<01:14,  1.25it/s]Loading train:  65%|██████▌   | 173/266 [02:48<01:22,  1.13it/s]Loading train:  65%|██████▌   | 174/266 [02:49<01:19,  1.15it/s]Loading train:  66%|██████▌   | 175/266 [02:50<01:19,  1.14it/s]Loading train:  66%|██████▌   | 176/266 [02:51<01:16,  1.18it/s]Loading train:  67%|██████▋   | 177/266 [02:52<01:14,  1.20it/s]Loading train:  67%|██████▋   | 178/266 [02:53<01:12,  1.21it/s]Loading train:  67%|██████▋   | 179/266 [02:53<01:11,  1.23it/s]Loading train:  68%|██████▊   | 180/266 [02:54<01:13,  1.17it/s]Loading train:  68%|██████▊   | 181/266 [02:55<01:13,  1.16it/s]Loading train:  68%|██████▊   | 182/266 [02:56<01:12,  1.16it/s]Loading train:  69%|██████▉   | 183/266 [02:57<01:15,  1.10it/s]Loading train:  69%|██████▉   | 184/266 [02:58<01:09,  1.18it/s]Loading train:  70%|██████▉   | 185/266 [02:59<01:07,  1.20it/s]Loading train:  70%|██████▉   | 186/266 [02:59<01:03,  1.25it/s]Loading train:  70%|███████   | 187/266 [03:00<01:02,  1.26it/s]Loading train:  71%|███████   | 188/266 [03:01<01:01,  1.27it/s]Loading train:  71%|███████   | 189/266 [03:02<01:02,  1.24it/s]Loading train:  71%|███████▏  | 190/266 [03:02<01:00,  1.27it/s]Loading train:  72%|███████▏  | 191/266 [03:04<01:10,  1.07it/s]Loading train:  72%|███████▏  | 192/266 [03:05<01:16,  1.03s/it]Loading train:  73%|███████▎  | 193/266 [03:06<01:21,  1.12s/it]Loading train:  73%|███████▎  | 194/266 [03:08<01:33,  1.30s/it]Loading train:  73%|███████▎  | 195/266 [03:09<01:24,  1.18s/it]Loading train:  74%|███████▎  | 196/266 [03:10<01:16,  1.10s/it]Loading train:  74%|███████▍  | 197/266 [03:11<01:12,  1.05s/it]Loading train:  74%|███████▍  | 198/266 [03:12<01:09,  1.02s/it]Loading train:  75%|███████▍  | 199/266 [03:13<01:07,  1.01s/it]Loading train:  75%|███████▌  | 200/266 [03:14<01:03,  1.04it/s]Loading train:  76%|███████▌  | 201/266 [03:15<01:04,  1.01it/s]Loading train:  76%|███████▌  | 202/266 [03:16<01:03,  1.02it/s]Loading train:  76%|███████▋  | 203/266 [03:16<01:00,  1.04it/s]Loading train:  77%|███████▋  | 204/266 [03:17<00:59,  1.05it/s]Loading train:  77%|███████▋  | 205/266 [03:18<00:57,  1.07it/s]Loading train:  77%|███████▋  | 206/266 [03:19<00:55,  1.08it/s]Loading train:  78%|███████▊  | 207/266 [03:20<00:54,  1.09it/s]Loading train:  78%|███████▊  | 208/266 [03:21<00:54,  1.07it/s]Loading train:  79%|███████▊  | 209/266 [03:22<00:51,  1.12it/s]Loading train:  79%|███████▉  | 210/266 [03:23<00:51,  1.10it/s]Loading train:  79%|███████▉  | 211/266 [03:24<00:50,  1.08it/s]Loading train:  80%|███████▉  | 212/266 [03:25<00:51,  1.05it/s]Loading train:  80%|████████  | 213/266 [03:26<00:49,  1.08it/s]Loading train:  80%|████████  | 214/266 [03:27<00:47,  1.09it/s]Loading train:  81%|████████  | 215/266 [03:27<00:46,  1.09it/s]Loading train:  81%|████████  | 216/266 [03:28<00:43,  1.14it/s]Loading train:  82%|████████▏ | 217/266 [03:29<00:42,  1.15it/s]Loading train:  82%|████████▏ | 218/266 [03:30<00:42,  1.14it/s]Loading train:  82%|████████▏ | 219/266 [03:31<00:39,  1.18it/s]Loading train:  83%|████████▎ | 220/266 [03:32<00:38,  1.21it/s]Loading train:  83%|████████▎ | 221/266 [03:32<00:38,  1.16it/s]Loading train:  83%|████████▎ | 222/266 [03:33<00:38,  1.14it/s]Loading train:  84%|████████▍ | 223/266 [03:34<00:36,  1.19it/s]Loading train:  84%|████████▍ | 224/266 [03:35<00:34,  1.22it/s]Loading train:  85%|████████▍ | 225/266 [03:36<00:32,  1.28it/s]Loading train:  85%|████████▍ | 226/266 [03:36<00:31,  1.27it/s]Loading train:  85%|████████▌ | 227/266 [03:37<00:30,  1.26it/s]Loading train:  86%|████████▌ | 228/266 [03:38<00:30,  1.25it/s]Loading train:  86%|████████▌ | 229/266 [03:39<00:31,  1.17it/s]Loading train:  86%|████████▋ | 230/266 [03:40<00:29,  1.23it/s]Loading train:  87%|████████▋ | 231/266 [03:41<00:29,  1.18it/s]Loading train:  87%|████████▋ | 232/266 [03:42<00:28,  1.18it/s]Loading train:  88%|████████▊ | 233/266 [03:43<00:29,  1.12it/s]Loading train:  88%|████████▊ | 234/266 [03:43<00:27,  1.16it/s]Loading train:  88%|████████▊ | 235/266 [03:44<00:25,  1.21it/s]Loading train:  89%|████████▊ | 236/266 [03:45<00:24,  1.23it/s]Loading train:  89%|████████▉ | 237/266 [03:46<00:23,  1.21it/s]Loading train:  89%|████████▉ | 238/266 [03:46<00:22,  1.24it/s]Loading train:  90%|████████▉ | 239/266 [03:47<00:21,  1.27it/s]Loading train:  90%|█████████ | 240/266 [03:48<00:20,  1.28it/s]Loading train:  91%|█████████ | 241/266 [03:49<00:19,  1.28it/s]Loading train:  91%|█████████ | 242/266 [03:50<00:19,  1.23it/s]Loading train:  91%|█████████▏| 243/266 [03:50<00:18,  1.26it/s]Loading train:  92%|█████████▏| 244/266 [03:51<00:17,  1.27it/s]Loading train:  92%|█████████▏| 245/266 [03:52<00:17,  1.23it/s]Loading train:  92%|█████████▏| 246/266 [03:53<00:16,  1.22it/s]Loading train:  93%|█████████▎| 247/266 [03:54<00:15,  1.22it/s]Loading train:  93%|█████████▎| 248/266 [03:55<00:14,  1.22it/s]Loading train:  94%|█████████▎| 249/266 [03:56<00:15,  1.11it/s]Loading train:  94%|█████████▍| 250/266 [03:57<00:14,  1.08it/s]Loading train:  94%|█████████▍| 251/266 [03:58<00:15,  1.00s/it]Loading train:  95%|█████████▍| 252/266 [03:59<00:14,  1.02s/it]Loading train:  95%|█████████▌| 253/266 [04:00<00:13,  1.00s/it]Loading train:  95%|█████████▌| 254/266 [04:01<00:11,  1.00it/s]Loading train:  96%|█████████▌| 255/266 [04:02<00:10,  1.01it/s]Loading train:  96%|█████████▌| 256/266 [04:03<00:10,  1.02s/it]Loading train:  97%|█████████▋| 257/266 [04:04<00:09,  1.02s/it]Loading train:  97%|█████████▋| 258/266 [04:05<00:08,  1.03s/it]Loading train:  97%|█████████▋| 259/266 [04:06<00:07,  1.04s/it]Loading train:  98%|█████████▊| 260/266 [04:07<00:06,  1.02s/it]Loading train:  98%|█████████▊| 261/266 [04:08<00:04,  1.01it/s]Loading train:  98%|█████████▊| 262/266 [04:09<00:03,  1.01it/s]Loading train:  99%|█████████▉| 263/266 [04:10<00:02,  1.01it/s]Loading train:  99%|█████████▉| 264/266 [04:11<00:01,  1.02it/s]Loading train: 100%|█████████▉| 265/266 [04:12<00:01,  1.02s/it]Loading train: 100%|██████████| 266/266 [04:13<00:00,  1.00it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:02, 123.38it/s]concatenating: train:  12%|█▏        | 32/266 [00:00<00:01, 134.95it/s]concatenating: train:  20%|█▉        | 53/266 [00:00<00:01, 150.82it/s]concatenating: train:  29%|██▉       | 78/266 [00:00<00:01, 170.43it/s]concatenating: train:  38%|███▊      | 101/266 [00:00<00:00, 182.99it/s]concatenating: train:  45%|████▌     | 120/266 [00:00<00:00, 184.57it/s]concatenating: train:  54%|█████▍    | 144/266 [00:00<00:00, 197.41it/s]concatenating: train:  62%|██████▏   | 164/266 [00:00<00:00, 190.33it/s]concatenating: train:  69%|██████▉   | 183/266 [00:00<00:00, 183.03it/s]concatenating: train:  76%|███████▌  | 202/266 [00:01<00:00, 132.20it/s]concatenating: train:  82%|████████▏ | 218/266 [00:01<00:00, 134.42it/s]concatenating: train:  89%|████████▉ | 237/266 [00:01<00:00, 145.55it/s]concatenating: train:  96%|█████████▌| 255/266 [00:01<00:00, 151.75it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 171.38it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:05,  1.41s/it]Loading test:  40%|████      | 2/5 [00:02<00:04,  1.36s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.33s/it]Loading test:  80%|████████  | 4/5 [00:05<00:01,  1.29s/it]Loading test: 100%|██████████| 5/5 [00:06<00:00,  1.35s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 362.57it/s]2019-08-17 01:14:48.229170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 01:14:48.229288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 01:14:48.229304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 01:14:48.229312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 01:14:48.229717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.55it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.55it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  5.98it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  7.70it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:03,  8.66it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  7.48it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02,  9.53it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02, 10.00it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:01<00:02,  7.96it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:01,  9.82it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 10.17it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 10.38it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  8.08it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.88it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00, 10.01it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 10.01it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  7.72it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 11.30it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 10,813
Non-trainable params: 490,530
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34755808e-02 3.28985122e-02 7.69286461e-02 9.55877110e-03
 2.76652602e-02 7.23784093e-03 8.42777338e-02 1.14341767e-01
 8.97808563e-02 1.36408363e-02 2.91087195e-01 1.88843330e-01
 2.63669589e-04]
Train on 16784 samples, validate on 306 samples
Epoch 1/300
 - 15s - loss: 1.8392 - acc: 0.8047 - mDice: 0.2577 - val_loss: 1.4579 - val_acc: 0.9445 - val_mDice: 0.4805

Epoch 00001: val_mDice improved from -inf to 0.48054, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 0.5724 - acc: 0.9290 - mDice: 0.5615 - val_loss: 1.0687 - val_acc: 0.9498 - val_mDice: 0.5750

Epoch 00002: val_mDice improved from 0.48054 to 0.57501, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 11s - loss: 0.4665 - acc: 0.9354 - mDice: 0.6209 - val_loss: 1.0401 - val_acc: 0.9451 - val_mDice: 0.5969

Epoch 00003: val_mDice improved from 0.57501 to 0.59692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 12s - loss: 0.4366 - acc: 0.9371 - mDice: 0.6379 - val_loss: 0.9624 - val_acc: 0.9497 - val_mDice: 0.5905

Epoch 00004: val_mDice did not improve from 0.59692
Epoch 5/300
 - 11s - loss: 0.4358 - acc: 0.9366 - mDice: 0.6375 - val_loss: 0.9930 - val_acc: 0.9487 - val_mDice: 0.5883

Epoch 00005: val_mDice did not improve from 0.59692
Epoch 6/300
 - 11s - loss: 0.4096 - acc: 0.9390 - mDice: 0.6549 - val_loss: 0.9464 - val_acc: 0.9500 - val_mDice: 0.6093

Epoch 00006: val_mDice improved from 0.59692 to 0.60928, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 11s - loss: 0.3954 - acc: 0.9398 - mDice: 0.6617 - val_loss: 0.8503 - val_acc: 0.9505 - val_mDice: 0.6120

Epoch 00007: val_mDice improved from 0.60928 to 0.61201, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 0.4092 - acc: 0.9393 - mDice: 0.6582 - val_loss: 0.9197 - val_acc: 0.9498 - val_mDice: 0.6193

Epoch 00008: val_mDice improved from 0.61201 to 0.61930, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 11s - loss: 0.3833 - acc: 0.9406 - mDice: 0.6698 - val_loss: 0.9754 - val_acc: 0.9490 - val_mDice: 0.5980

Epoch 00009: val_mDice did not improve from 0.61930
Epoch 10/300
 - 11s - loss: 0.4158 - acc: 0.9393 - mDice: 0.6580 - val_loss: 0.9469 - val_acc: 0.9483 - val_mDice: 0.6079

Epoch 00010: val_mDice did not improve from 0.61930
Epoch 11/300
 - 11s - loss: 0.3961 - acc: 0.9402 - mDice: 0.6645 - val_loss: 1.5484 - val_acc: 0.9451 - val_mDice: 0.5145

Epoch 00011: val_mDice did not improve from 0.61930
Epoch 12/300
 - 11s - loss: 0.3993 - acc: 0.9397 - mDice: 0.6618 - val_loss: 0.7853 - val_acc: 0.9490 - val_mDice: 0.6180

Epoch 00012: val_mDice did not improve from 0.61930
Epoch 13/300
 - 11s - loss: 0.3844 - acc: 0.9410 - mDice: 0.6719 - val_loss: 0.8022 - val_acc: 0.9511 - val_mDice: 0.6120

Epoch 00013: val_mDice did not improve from 0.61930
Epoch 14/300
 - 11s - loss: 0.3753 - acc: 0.9416 - mDice: 0.6757 - val_loss: 0.7653 - val_acc: 0.9478 - val_mDice: 0.6166

Epoch 00014: val_mDice did not improve from 0.61930
Epoch 15/300
 - 11s - loss: 0.3868 - acc: 0.9411 - mDice: 0.6735 - val_loss: 0.9282 - val_acc: 0.9437 - val_mDice: 0.6050

Epoch 00015: val_mDice did not improve from 0.61930
Epoch 16/300
 - 11s - loss: 0.3996 - acc: 0.9403 - mDice: 0.6658 - val_loss: 2.0415 - val_acc: 0.9443 - val_mDice: 0.4547

Epoch 00016: val_mDice did not improve from 0.61930
Epoch 17/300
 - 11s - loss: 0.4128 - acc: 0.9387 - mDice: 0.6555 - val_loss: 0.8454 - val_acc: 0.9490 - val_mDice: 0.6020

Epoch 00017: val_mDice did not improve from 0.61930
Epoch 18/300
 - 11s - loss: 0.3897 - acc: 0.9407 - mDice: 0.6699 - val_loss: 0.8425 - val_acc: 0.9479 - val_mDice: 0.6194

Epoch 00018: val_mDice improved from 0.61930 to 0.61936, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 11s - loss: 0.3900 - acc: 0.9416 - mDice: 0.6769 - val_loss: 0.8462 - val_acc: 0.9482 - val_mDice: 0.6236

Epoch 00019: val_mDice improved from 0.61936 to 0.62358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 0.3821 - acc: 0.9413 - mDice: 0.6749 - val_loss: 0.8498 - val_acc: 0.9480 - val_mDice: 0.6176

Epoch 00020: val_mDice did not improve from 0.62358
Epoch 21/300
 - 11s - loss: 0.3743 - acc: 0.9419 - mDice: 0.6792 - val_loss: 0.8573 - val_acc: 0.9493 - val_mDice: 0.6233

Epoch 00021: val_mDice did not improve from 0.62358
Epoch 22/300
 - 11s - loss: 0.3835 - acc: 0.9415 - mDice: 0.6753 - val_loss: 1.5015 - val_acc: 0.9469 - val_mDice: 0.5484

Epoch 00022: val_mDice did not improve from 0.62358
Epoch 23/300
 - 11s - loss: 0.3723 - acc: 0.9419 - mDice: 0.6806 - val_loss: 0.9264 - val_acc: 0.9493 - val_mDice: 0.6084

Epoch 00023: val_mDice did not improve from 0.62358
Epoch 24/300
 - 11s - loss: 0.3925 - acc: 0.9407 - mDice: 0.6690 - val_loss: 0.8949 - val_acc: 0.9497 - val_mDice: 0.6104

Epoch 00024: val_mDice did not improve from 0.62358
Epoch 25/300
 - 11s - loss: 0.3819 - acc: 0.9416 - mDice: 0.6763 - val_loss: 3.4347 - val_acc: 0.9328 - val_mDice: 0.3498

Epoch 00025: val_mDice did not improve from 0.62358
Epoch 26/300
 - 11s - loss: 0.3942 - acc: 0.9410 - mDice: 0.6718 - val_loss: 0.8699 - val_acc: 0.9503 - val_mDice: 0.6287

Epoch 00026: val_mDice improved from 0.62358 to 0.62869, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 0.3652 - acc: 0.9428 - mDice: 0.6855 - val_loss: 0.8142 - val_acc: 0.9494 - val_mDice: 0.6289

Epoch 00027: val_mDice improved from 0.62869 to 0.62890, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 12s - loss: 0.3685 - acc: 0.9428 - mDice: 0.6857 - val_loss: 0.7045 - val_acc: 0.9506 - val_mDice: 0.6295

Epoch 00028: val_mDice improved from 0.62890 to 0.62955, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 11s - loss: 0.3612 - acc: 0.9427 - mDice: 0.6868 - val_loss: 0.7046 - val_acc: 0.9505 - val_mDice: 0.6267

Epoch 00029: val_mDice did not improve from 0.62955
Epoch 30/300
 - 11s - loss: 0.3770 - acc: 0.9422 - mDice: 0.6805 - val_loss: 0.7324 - val_acc: 0.9501 - val_mDice: 0.6202

Epoch 00030: val_mDice did not improve from 0.62955
Epoch 31/300
 - 12s - loss: 0.3711 - acc: 0.9420 - mDice: 0.6809 - val_loss: 0.8390 - val_acc: 0.9496 - val_mDice: 0.6173

Epoch 00031: val_mDice did not improve from 0.62955
Epoch 32/300
 - 12s - loss: 0.3821 - acc: 0.9423 - mDice: 0.6823 - val_loss: 0.6652 - val_acc: 0.9499 - val_mDice: 0.6341

Epoch 00032: val_mDice improved from 0.62955 to 0.63409, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 12s - loss: 0.3689 - acc: 0.9422 - mDice: 0.6836 - val_loss: 0.7299 - val_acc: 0.9456 - val_mDice: 0.6205

Epoch 00033: val_mDice did not improve from 0.63409
Epoch 34/300
 - 12s - loss: 0.3828 - acc: 0.9421 - mDice: 0.6808 - val_loss: 0.8950 - val_acc: 0.9490 - val_mDice: 0.6246

Epoch 00034: val_mDice did not improve from 0.63409
Epoch 35/300
 - 12s - loss: 0.3637 - acc: 0.9428 - mDice: 0.6883 - val_loss: 0.8782 - val_acc: 0.9490 - val_mDice: 0.6266

Epoch 00035: val_mDice did not improve from 0.63409
Epoch 36/300
 - 12s - loss: 0.3787 - acc: 0.9424 - mDice: 0.6815 - val_loss: 0.9326 - val_acc: 0.9485 - val_mDice: 0.5816

Epoch 00036: val_mDice did not improve from 0.63409
Epoch 37/300
 - 12s - loss: 0.3844 - acc: 0.9411 - mDice: 0.6717 - val_loss: 1.0046 - val_acc: 0.9492 - val_mDice: 0.5881

Epoch 00037: val_mDice did not improve from 0.63409
Epoch 38/300
 - 12s - loss: 0.3800 - acc: 0.9415 - mDice: 0.6774 - val_loss: 0.8506 - val_acc: 0.9497 - val_mDice: 0.6238

Epoch 00038: val_mDice did not improve from 0.63409
Epoch 39/300
 - 12s - loss: 0.3659 - acc: 0.9428 - mDice: 0.6879 - val_loss: 0.8324 - val_acc: 0.9506 - val_mDice: 0.6293

Epoch 00039: val_mDice did not improve from 0.63409
Epoch 40/300
 - 12s - loss: 0.3801 - acc: 0.9430 - mDice: 0.6875 - val_loss: 0.7900 - val_acc: 0.9489 - val_mDice: 0.6252

Epoch 00040: val_mDice did not improve from 0.63409
Epoch 41/300
 - 12s - loss: 0.3562 - acc: 0.9431 - mDice: 0.6894 - val_loss: 0.8211 - val_acc: 0.9484 - val_mDice: 0.6276

Epoch 00041: val_mDice did not improve from 0.63409
Epoch 42/300
 - 12s - loss: 0.3689 - acc: 0.9432 - mDice: 0.6895 - val_loss: 0.7484 - val_acc: 0.9509 - val_mDice: 0.6348

Epoch 00042: val_mDice improved from 0.63409 to 0.63477, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 12s - loss: 0.3513 - acc: 0.9435 - mDice: 0.6945 - val_loss: 0.7554 - val_acc: 0.9506 - val_mDice: 0.6323

Epoch 00043: val_mDice did not improve from 0.63477
Epoch 44/300
 - 12s - loss: 0.3549 - acc: 0.9436 - mDice: 0.6932 - val_loss: 0.9427 - val_acc: 0.9500 - val_mDice: 0.5932

Epoch 00044: val_mDice did not improve from 0.63477
Epoch 45/300
 - 11s - loss: 0.3521 - acc: 0.9436 - mDice: 0.6928 - val_loss: 0.8820 - val_acc: 0.9486 - val_mDice: 0.6262

Epoch 00045: val_mDice did not improve from 0.63477
Epoch 46/300
 - 12s - loss: 0.3495 - acc: 0.9437 - mDice: 0.6956 - val_loss: 0.8276 - val_acc: 0.9489 - val_mDice: 0.6259

Epoch 00046: val_mDice did not improve from 0.63477
Epoch 47/300
 - 11s - loss: 0.3557 - acc: 0.9434 - mDice: 0.6918 - val_loss: 0.9405 - val_acc: 0.9511 - val_mDice: 0.6040

Epoch 00047: val_mDice did not improve from 0.63477
Epoch 48/300
 - 12s - loss: 0.3533 - acc: 0.9434 - mDice: 0.6930 - val_loss: 0.7454 - val_acc: 0.9498 - val_mDice: 0.6263

Epoch 00048: val_mDice did not improve from 0.63477
Epoch 49/300
 - 11s - loss: 0.3717 - acc: 0.9420 - mDice: 0.6818 - val_loss: 1.0889 - val_acc: 0.9495 - val_mDice: 0.5774

Epoch 00049: val_mDice did not improve from 0.63477
Epoch 50/300
 - 11s - loss: 0.3715 - acc: 0.9423 - mDice: 0.6842 - val_loss: 0.8105 - val_acc: 0.9461 - val_mDice: 0.6223

Epoch 00050: val_mDice did not improve from 0.63477
Epoch 51/300
 - 11s - loss: 0.3678 - acc: 0.9423 - mDice: 0.6846 - val_loss: 0.9840 - val_acc: 0.9507 - val_mDice: 0.5994

Epoch 00051: val_mDice did not improve from 0.63477
Epoch 52/300
 - 11s - loss: 0.3595 - acc: 0.9432 - mDice: 0.6908 - val_loss: 0.7394 - val_acc: 0.9488 - val_mDice: 0.6176

Epoch 00052: val_mDice did not improve from 0.63477
Epoch 53/300
 - 11s - loss: 0.3549 - acc: 0.9436 - mDice: 0.6929 - val_loss: 0.6897 - val_acc: 0.9496 - val_mDice: 0.6253

Epoch 00053: val_mDice did not improve from 0.63477
Epoch 54/300
 - 11s - loss: 0.3477 - acc: 0.9438 - mDice: 0.6962 - val_loss: 0.7689 - val_acc: 0.9488 - val_mDice: 0.6248

Epoch 00054: val_mDice did not improve from 0.63477
Epoch 55/300
 - 11s - loss: 0.3482 - acc: 0.9437 - mDice: 0.6958 - val_loss: 0.7714 - val_acc: 0.9497 - val_mDice: 0.6083

Epoch 00055: val_mDice did not improve from 0.63477
Epoch 56/300
 - 11s - loss: 0.3498 - acc: 0.9436 - mDice: 0.6951 - val_loss: 0.8329 - val_acc: 0.9495 - val_mDice: 0.6300

Epoch 00056: val_mDice did not improve from 0.63477
Epoch 57/300
 - 11s - loss: 0.3603 - acc: 0.9428 - mDice: 0.6883 - val_loss: 0.7175 - val_acc: 0.9463 - val_mDice: 0.6236

Epoch 00057: val_mDice did not improve from 0.63477
Epoch 58/300
 - 11s - loss: 0.3550 - acc: 0.9433 - mDice: 0.6940 - val_loss: 0.7797 - val_acc: 0.9509 - val_mDice: 0.6181

Epoch 00058: val_mDice did not improve from 0.63477
Epoch 59/300
 - 11s - loss: 0.3468 - acc: 0.9437 - mDice: 0.6973 - val_loss: 0.6822 - val_acc: 0.9502 - val_mDice: 0.6263

Epoch 00059: val_mDice did not improve from 0.63477
Epoch 60/300
 - 11s - loss: 0.3733 - acc: 0.9428 - mDice: 0.6892 - val_loss: 0.8020 - val_acc: 0.9495 - val_mDice: 0.6248

Epoch 00060: val_mDice did not improve from 0.63477
Epoch 61/300
 - 11s - loss: 0.3450 - acc: 0.9438 - mDice: 0.6981 - val_loss: 0.9741 - val_acc: 0.9504 - val_mDice: 0.6097

Epoch 00061: val_mDice did not improve from 0.63477
Epoch 62/300
 - 11s - loss: 0.3726 - acc: 0.9421 - mDice: 0.6833 - val_loss: 0.7100 - val_acc: 0.9484 - val_mDice: 0.6178

Epoch 00062: val_mDice did not improve from 0.63477
Epoch 63/300
 - 11s - loss: 0.3652 - acc: 0.9432 - mDice: 0.6912 - val_loss: 0.7553 - val_acc: 0.9490 - val_mDice: 0.6225

Epoch 00063: val_mDice did not improve from 0.63477
Epoch 64/300
 - 11s - loss: 0.3539 - acc: 0.9436 - mDice: 0.6953 - val_loss: 0.7364 - val_acc: 0.9506 - val_mDice: 0.6288

Epoch 00064: val_mDice did not improve from 0.63477
Epoch 65/300
 - 11s - loss: 0.3528 - acc: 0.9439 - mDice: 0.6975 - val_loss: 1.0558 - val_acc: 0.9514 - val_mDice: 0.6014

Epoch 00065: val_mDice did not improve from 0.63477
Epoch 66/300
 - 11s - loss: 0.3483 - acc: 0.9440 - mDice: 0.6987 - val_loss: 1.5357 - val_acc: 0.9503 - val_mDice: 0.5882

Epoch 00066: val_mDice did not improve from 0.63477
Epoch 67/300
 - 11s - loss: 0.3737 - acc: 0.9431 - mDice: 0.6907 - val_loss: 1.3901 - val_acc: 0.9487 - val_mDice: 0.5705

Epoch 00067: val_mDice did not improve from 0.63477
Epoch 68/300
 - 11s - loss: 0.3530 - acc: 0.9437 - mDice: 0.6969 - val_loss: 0.8249 - val_acc: 0.9501 - val_mDice: 0.6136

Epoch 00068: val_mDice did not improve from 0.63477
Epoch 69/300
 - 11s - loss: 0.3462 - acc: 0.9439 - mDice: 0.6983 - val_loss: 0.9229 - val_acc: 0.9497 - val_mDice: 0.6035

Epoch 00069: val_mDice did not improve from 0.63477
Epoch 70/300
 - 11s - loss: 0.3630 - acc: 0.9423 - mDice: 0.6874 - val_loss: 0.6445 - val_acc: 0.9499 - val_mDice: 0.6264

Epoch 00070: val_mDice did not improve from 0.63477
Epoch 71/300
 - 11s - loss: 0.3675 - acc: 0.9428 - mDice: 0.6896 - val_loss: 0.7974 - val_acc: 0.9505 - val_mDice: 0.6291

Epoch 00071: val_mDice did not improve from 0.63477
Epoch 72/300
 - 11s - loss: 0.3417 - acc: 0.9438 - mDice: 0.6986 - val_loss: 0.6877 - val_acc: 0.9495 - val_mDice: 0.6317

Epoch 00072: val_mDice did not improve from 0.63477
Epoch 73/300
 - 11s - loss: 0.3422 - acc: 0.9443 - mDice: 0.7010 - val_loss: 0.6890 - val_acc: 0.9502 - val_mDice: 0.6248

Epoch 00073: val_mDice did not improve from 0.63477
Epoch 74/300
 - 11s - loss: 0.3552 - acc: 0.9435 - mDice: 0.6945 - val_loss: 0.7667 - val_acc: 0.9509 - val_mDice: 0.6149

Epoch 00074: val_mDice did not improve from 0.63477
Epoch 75/300
 - 11s - loss: 0.3405 - acc: 0.9443 - mDice: 0.7015 - val_loss: 0.7029 - val_acc: 0.9500 - val_mDice: 0.6309

Epoch 00075: val_mDice did not improve from 0.63477
Epoch 76/300
 - 11s - loss: 0.3430 - acc: 0.9443 - mDice: 0.7021 - val_loss: 0.8358 - val_acc: 0.9488 - val_mDice: 0.6215

Epoch 00076: val_mDice did not improve from 0.63477
Epoch 77/300
 - 11s - loss: 0.3445 - acc: 0.9443 - mDice: 0.7019 - val_loss: 0.6633 - val_acc: 0.9505 - val_mDice: 0.6273

Epoch 00077: val_mDice did not improve from 0.63477
Epoch 78/300
 - 11s - loss: 0.3561 - acc: 0.9433 - mDice: 0.6946 - val_loss: 0.8522 - val_acc: 0.9501 - val_mDice: 0.6238

Epoch 00078: val_mDice did not improve from 0.63477
Epoch 79/300
 - 11s - loss: 0.3432 - acc: 0.9441 - mDice: 0.6995 - val_loss: 0.7407 - val_acc: 0.9457 - val_mDice: 0.6216

Epoch 00079: val_mDice did not improve from 0.63477
Epoch 80/300
 - 11s - loss: 0.3449 - acc: 0.9442 - mDice: 0.6997 - val_loss: 0.6328 - val_acc: 0.9505 - val_mDice: 0.5947

Epoch 00080: val_mDice did not improve from 0.63477
Epoch 81/300
 - 11s - loss: 0.3697 - acc: 0.9420 - mDice: 0.6835 - val_loss: 0.8575 - val_acc: 0.9475 - val_mDice: 0.6230

Epoch 00081: val_mDice did not improve from 0.63477
Epoch 82/300
 - 11s - loss: 0.3433 - acc: 0.9439 - mDice: 0.6988 - val_loss: 0.6905 - val_acc: 0.9505 - val_mDice: 0.6274

Epoch 00082: val_mDice did not improve from 0.63477
Restoring model weights from the end of the best epoch
Epoch 00082: early stopping
{'val_loss': [1.4578551067635903, 1.068651465419071, 1.0401467969214995, 0.9623700169955983, 0.9930447004978953, 0.9464327813753115, 0.8503295539640913, 0.9196896989361133, 0.9753841198737325, 0.9468801109229817, 1.5483569724887025, 0.7853213053902769, 0.8022138090694652, 0.7652985782794703, 0.9282301081941019, 2.0415048003196716, 0.8454088395327525, 0.842521546323315, 0.8462047286672529, 0.849783617686602, 0.8573155691421109, 1.5014914989081862, 0.9264062029473922, 0.894851854229285, 3.434667979969698, 0.8699142054405088, 0.8142461054075777, 0.7044775567802728, 0.7046143416485755, 0.7324263656061459, 0.8389940390399858, 0.6652229712679495, 0.7298998768423118, 0.894975582953372, 0.8782457697625253, 0.9325657792730269, 1.0045735958744497, 0.8505709720592872, 0.8324220365558574, 0.7899833568950104, 0.8211030457534042, 0.7483518228811377, 0.7553963686516082, 0.9427299631966485, 0.8820055912133136, 0.8275674486082364, 0.9405417019635244, 0.7454304753565321, 1.0888984179964252, 0.8105003701315986, 0.9840401570781384, 0.7394150376709459, 0.6897181741941988, 0.76893287624409, 0.7713912805311041, 0.8329111512969521, 0.7175165079777537, 0.7796814539074118, 0.6821647654561436, 0.8019794854852889, 0.9740675662467683, 0.7100064859670752, 0.7553032124354169, 0.7364208051581788, 1.05579395874653, 1.5356654976707658, 1.390117297180338, 0.8249186298816032, 0.9229047956809499, 0.6444916641400531, 0.7973639518996469, 0.687682671289818, 0.6889780995502971, 0.7667386274711758, 0.7029471136386098, 0.8357839617464278, 0.6633015631460676, 0.852209616330714, 0.7406744230416865, 0.6327556667764203, 0.8575313225680706, 0.6905230794077605], 'val_acc': [0.9445203749182957, 0.9497745418860242, 0.9451265763613134, 0.9496566984388564, 0.9487114018864102, 0.949952596542882, 0.9504566718550289, 0.9498203560417774, 0.9490073070027469, 0.94830552308388, 0.9451213424501855, 0.948953626202602, 0.9511270293223313, 0.9478341835776186, 0.94370339469972, 0.9442637582230412, 0.9490203900274887, 0.9478970247935625, 0.9481798394832736, 0.9480318750431335, 0.9492979723643633, 0.9469163807389004, 0.9493411752133588, 0.9497313394266016, 0.9327787308911093, 0.950264211573632, 0.9493647377475415, 0.9505915330126394, 0.950531308557473, 0.9501280359972536, 0.9496069612845875, 0.9498963172918831, 0.9455730502122368, 0.9490046805026484, 0.9489745591201034, 0.9485398889367097, 0.9491971477963566, 0.9497261031780367, 0.9506124815130546, 0.9489313578293994, 0.9483500426890803, 0.9509109909238379, 0.9506111713795881, 0.950049491100062, 0.9485830913961323, 0.9488685166134554, 0.9510602541998321, 0.9498164248622321, 0.9495022039787442, 0.9460836695689782, 0.9506871076970319, 0.9487768606422773, 0.9495990954193414, 0.9488187751738854, 0.9496658826185986, 0.9495283875590056, 0.9462735204914816, 0.9509319339702332, 0.9502144697444891, 0.949502209432764, 0.9503781269578373, 0.9483932583939796, 0.9490478899743822, 0.95059677004035, 0.9513627099835016, 0.9502733797808878, 0.948727115306979, 0.950133297957626, 0.9497182377023634, 0.9498583390042673, 0.9505221516478295, 0.9494747223417743, 0.9501516113873401, 0.9509332585178949, 0.9500429361474281, 0.9488423186189988, 0.9504920361088771, 0.9500835163920534, 0.9457209990694632, 0.9504671498061785, 0.9474898471551783, 0.9505116623990676], 'val_mDice': [0.48054412533255186, 0.5750114148348765, 0.596916152079121, 0.5904909245718538, 0.5882736450316859, 0.609282422737748, 0.6120059705929819, 0.619298828047475, 0.5980072273245824, 0.6079174770054474, 0.5144777899133224, 0.6180103536622197, 0.6120229878651551, 0.6166018883582034, 0.605029533014578, 0.4546792169460674, 0.6020324445433087, 0.6193620677477394, 0.6235839507353851, 0.6175735688774414, 0.6233289588510601, 0.5483714149765719, 0.6083714088388518, 0.6104436930195958, 0.34981991154494774, 0.6286942589711519, 0.6288990234724836, 0.6295453291021141, 0.6267153872481359, 0.6202095309698504, 0.6173432212249905, 0.6340850525919128, 0.6205238644885861, 0.6245522504928065, 0.6266110374841815, 0.5815654704773349, 0.5880508325945318, 0.6237856171586934, 0.6292509681064319, 0.6252098415119975, 0.627597804601286, 0.6347700003412814, 0.6323026322266635, 0.5932144294766819, 0.6261982822340298, 0.625854977303081, 0.6040259501805493, 0.626252094368919, 0.5774090654807154, 0.6223486215850107, 0.5993705357796226, 0.6175688425310297, 0.6252715934822762, 0.6247929014216841, 0.6082687683744368, 0.6300176012457586, 0.6236374908997342, 0.6180988587682543, 0.6262999713128689, 0.6247746294814777, 0.6097072831361122, 0.6178384544997434, 0.6224682397312589, 0.6287734191012538, 0.6014126094143375, 0.5882163588322845, 0.5705000824398465, 0.6136154340080966, 0.6034789611798486, 0.6263704077870238, 0.6291295639147946, 0.6317286142726349, 0.6248414828107248, 0.6148793110076118, 0.6308863709370295, 0.6214592032183229, 0.6273439392155292, 0.6238002355873974, 0.621600436326725, 0.5947347350564658, 0.623046904364053, 0.6273747986925193], 'loss': [1.839185063800889, 0.5724122761770927, 0.46646222316289765, 0.43656034718620085, 0.43580833194622887, 0.4096204875990592, 0.3953757140114264, 0.40924045068519926, 0.3833093423577464, 0.41581487734122885, 0.39612058686133333, 0.3993165670610872, 0.38440339035000315, 0.3753393407381922, 0.38682173737267067, 0.39958364399659624, 0.41276721841699177, 0.38966900580433916, 0.39001791613856535, 0.38206353932761145, 0.37433559767497054, 0.3835220642821123, 0.37230898951623304, 0.3924713331044743, 0.3819489115023465, 0.3942103490332857, 0.36515578119753656, 0.3685385055193683, 0.3611744116668592, 0.37700698906475866, 0.37111878091801453, 0.38213957395507564, 0.3689113929619155, 0.38281246546382447, 0.36368524759397036, 0.37867445596720173, 0.38437055237413703, 0.3800193588644465, 0.36588966588288174, 0.38014758832120576, 0.35616078930201134, 0.36890763051814984, 0.35134964769757626, 0.3548631347588009, 0.3521027657229123, 0.34947405594646985, 0.3556989751599935, 0.35332746702904244, 0.3716856942354951, 0.37147806009883194, 0.3678251486893093, 0.3594614895534754, 0.35485177146567404, 0.3476859832748955, 0.34822015402513645, 0.34981378299264937, 0.360321234755424, 0.35497830723287155, 0.346805906372484, 0.37334958127246115, 0.34501168712817337, 0.3725924340881997, 0.3652075027622588, 0.35386063261854295, 0.3528274669142367, 0.34833235968724674, 0.3737416124882246, 0.3530176489501197, 0.3462473984711845, 0.3630175808172946, 0.36754518556981225, 0.3417075600819547, 0.34224670167632393, 0.3552306645485023, 0.3405336118822614, 0.34295388586016584, 0.34448049106662676, 0.3561289674603337, 0.34323353816828805, 0.3449390075703083, 0.3696861892540871, 0.34332191417112706], 'acc': [0.80468883688337, 0.9289551326087023, 0.9353580602983614, 0.9370909566334933, 0.9365620595554833, 0.9389655467751369, 0.939823808636804, 0.9393482377576874, 0.9405647708445759, 0.9392641925308907, 0.9402108913140143, 0.9396609644644367, 0.9409763204826982, 0.9415561087692318, 0.9411299490397379, 0.9403383357963526, 0.9387412145662808, 0.9406597739080228, 0.9416452394825032, 0.9413169269353237, 0.941926219634662, 0.9415211383718554, 0.9419166461144662, 0.9406586517752978, 0.9416162134978292, 0.9409967530998192, 0.9428282110349011, 0.9427848852623975, 0.9427471696918527, 0.942156711442341, 0.9420292683023856, 0.942316045993504, 0.9422070056810847, 0.9420956051539988, 0.9428473762578347, 0.9424349933905302, 0.9411120952752458, 0.9414898200942518, 0.9428036003395759, 0.9429973557918155, 0.9430690415636828, 0.9431876019282609, 0.9435382840375428, 0.943576021853291, 0.9435552803815491, 0.9436667062434501, 0.9433525718102237, 0.9434299838682149, 0.9420379080709329, 0.9422908628748302, 0.9422508316155385, 0.9431998492338409, 0.943571965562604, 0.9438369252760712, 0.9437477940727235, 0.9435753792353876, 0.9428100695387879, 0.9433271759913238, 0.9437489882182575, 0.9427562891314778, 0.9438415577189484, 0.942081233772411, 0.943211999078156, 0.9435945931108255, 0.943866048807241, 0.9439748969813321, 0.9430730964267561, 0.9437348808457559, 0.9438722311697538, 0.9422929653448645, 0.9428390008179884, 0.9438403870757701, 0.9442702215395733, 0.9435234123022359, 0.9443183227009837, 0.9443077706688353, 0.9443412366242382, 0.943333928319701, 0.9440509728000662, 0.9442291881878451, 0.942013083695229, 0.9439389512610503], 'mDice': [0.25773909893970015, 0.56146240426144, 0.6209316315880494, 0.6379329480578834, 0.6375162519258869, 0.6549162987275164, 0.6617184094267873, 0.6582049359740588, 0.6697647665246427, 0.6579830354315764, 0.6644945847096048, 0.661790887545528, 0.6718743554288938, 0.675679403070442, 0.6734855438807218, 0.6658203914065493, 0.6555182212684925, 0.6699171192851716, 0.6769305470233536, 0.6748844860165999, 0.679170871123288, 0.6753411889076233, 0.6805723487933097, 0.6689927814700583, 0.6763427045952717, 0.6717969042997456, 0.6855073018697356, 0.6856517425185163, 0.686814231870854, 0.6804541422567332, 0.680935180405984, 0.6822678809710294, 0.6835900465216491, 0.6808027127194791, 0.6883499073672568, 0.6815246406967465, 0.6716785698319413, 0.6773958488006155, 0.6878920428817106, 0.6874945393423447, 0.6894156964082054, 0.68953580485996, 0.6944698856644567, 0.6932262370737537, 0.6927637099026497, 0.6956450804229914, 0.6917877034688132, 0.6929660636587184, 0.6818410804916042, 0.6842035492962036, 0.6845595322201318, 0.6907984145035394, 0.6929321705698399, 0.6962163323186714, 0.6957797868278506, 0.6950920857169494, 0.688299380235722, 0.694019811339783, 0.6972965881782446, 0.6892351543866417, 0.698091062749989, 0.6833307373455527, 0.6912399313924878, 0.6953033796938631, 0.6974854876630867, 0.6986864504641414, 0.690748274844811, 0.6968616755578039, 0.698279610558904, 0.6873899844499062, 0.6895620814312402, 0.6985876032420291, 0.7010003524211842, 0.6945071389154666, 0.7015386446529167, 0.7020773252026596, 0.7019135188542172, 0.6945895527634198, 0.6995486207128822, 0.6997438935621564, 0.6834564656090236, 0.6987673164948154]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:02<00:10,  2.70s/it]predicting test subjects:  40%|████      | 2/5 [00:04<00:07,  2.53s/it]predicting test subjects:  60%|██████    | 3/5 [00:06<00:04,  2.34s/it]predicting test subjects:  80%|████████  | 4/5 [00:08<00:02,  2.20s/it]predicting test subjects: 100%|██████████| 5/5 [00:11<00:00,  2.27s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:02<12:20,  2.80s/it]predicting train subjects:   1%|          | 2/266 [00:05<12:00,  2.73s/it]predicting train subjects:   1%|          | 3/266 [00:07<11:09,  2.54s/it]predicting train subjects:   2%|▏         | 4/266 [00:09<10:23,  2.38s/it]predicting train subjects:   2%|▏         | 5/266 [00:12<10:34,  2.43s/it]predicting train subjects:   2%|▏         | 6/266 [00:14<10:54,  2.52s/it]predicting train subjects:   3%|▎         | 7/266 [00:17<11:11,  2.59s/it]predicting train subjects:   3%|▎         | 8/266 [00:20<11:21,  2.64s/it]predicting train subjects:   3%|▎         | 9/266 [00:23<11:26,  2.67s/it]predicting train subjects:   4%|▍         | 10/266 [00:25<11:34,  2.71s/it]predicting train subjects:   4%|▍         | 11/266 [00:28<11:36,  2.73s/it]predicting train subjects:   5%|▍         | 12/266 [00:31<11:33,  2.73s/it]predicting train subjects:   5%|▍         | 13/266 [00:33<11:25,  2.71s/it]predicting train subjects:   5%|▌         | 14/266 [00:36<11:28,  2.73s/it]predicting train subjects:   6%|▌         | 15/266 [00:39<11:30,  2.75s/it]predicting train subjects:   6%|▌         | 16/266 [00:42<11:23,  2.73s/it]predicting train subjects:   6%|▋         | 17/266 [00:44<11:15,  2.71s/it]predicting train subjects:   7%|▋         | 18/266 [00:47<11:09,  2.70s/it]predicting train subjects:   7%|▋         | 19/266 [00:50<11:07,  2.70s/it]predicting train subjects:   8%|▊         | 20/266 [00:53<11:08,  2.72s/it]predicting train subjects:   8%|▊         | 21/266 [00:55<11:05,  2.72s/it]predicting train subjects:   8%|▊         | 22/266 [00:58<11:03,  2.72s/it]predicting train subjects:   9%|▊         | 23/266 [01:01<11:05,  2.74s/it]predicting train subjects:   9%|▉         | 24/266 [01:03<10:46,  2.67s/it]predicting train subjects:   9%|▉         | 25/266 [01:06<10:33,  2.63s/it]predicting train subjects:  10%|▉         | 26/266 [01:08<10:17,  2.57s/it]predicting train subjects:  10%|█         | 27/266 [01:11<10:15,  2.57s/it]predicting train subjects:  11%|█         | 28/266 [01:13<10:07,  2.55s/it]predicting train subjects:  11%|█         | 29/266 [01:16<09:56,  2.52s/it]predicting train subjects:  11%|█▏        | 30/266 [01:18<09:52,  2.51s/it]predicting train subjects:  12%|█▏        | 31/266 [01:21<09:50,  2.51s/it]predicting train subjects:  12%|█▏        | 32/266 [01:23<09:40,  2.48s/it]predicting train subjects:  12%|█▏        | 33/266 [01:26<09:41,  2.50s/it]predicting train subjects:  13%|█▎        | 34/266 [01:28<09:40,  2.50s/it]predicting train subjects:  13%|█▎        | 35/266 [01:31<09:36,  2.49s/it]predicting train subjects:  14%|█▎        | 36/266 [01:33<09:36,  2.51s/it]predicting train subjects:  14%|█▍        | 37/266 [01:36<09:32,  2.50s/it]predicting train subjects:  14%|█▍        | 38/266 [01:38<09:28,  2.50s/it]predicting train subjects:  15%|█▍        | 39/266 [01:41<09:26,  2.50s/it]predicting train subjects:  15%|█▌        | 40/266 [01:43<09:22,  2.49s/it]predicting train subjects:  15%|█▌        | 41/266 [01:46<09:21,  2.49s/it]predicting train subjects:  16%|█▌        | 42/266 [01:48<08:53,  2.38s/it]predicting train subjects:  16%|█▌        | 43/266 [01:50<08:33,  2.30s/it]predicting train subjects:  17%|█▋        | 44/266 [01:52<08:15,  2.23s/it]predicting train subjects:  17%|█▋        | 45/266 [01:54<08:03,  2.19s/it]predicting train subjects:  17%|█▋        | 46/266 [01:56<07:59,  2.18s/it]predicting train subjects:  18%|█▊        | 47/266 [01:58<07:50,  2.15s/it]predicting train subjects:  18%|█▊        | 48/266 [02:00<07:43,  2.13s/it]predicting train subjects:  18%|█▊        | 49/266 [02:03<07:40,  2.12s/it]predicting train subjects:  19%|█▉        | 50/266 [02:05<07:42,  2.14s/it]predicting train subjects:  19%|█▉        | 51/266 [02:07<07:32,  2.11s/it]predicting train subjects:  20%|█▉        | 52/266 [02:09<07:28,  2.10s/it]predicting train subjects:  20%|█▉        | 53/266 [02:11<07:24,  2.09s/it]predicting train subjects:  20%|██        | 54/266 [02:13<07:21,  2.08s/it]predicting train subjects:  21%|██        | 55/266 [02:15<07:19,  2.08s/it]predicting train subjects:  21%|██        | 56/266 [02:17<07:15,  2.07s/it]predicting train subjects:  21%|██▏       | 57/266 [02:19<07:11,  2.07s/it]predicting train subjects:  22%|██▏       | 58/266 [02:21<07:08,  2.06s/it]predicting train subjects:  22%|██▏       | 59/266 [02:23<07:09,  2.08s/it]predicting train subjects:  23%|██▎       | 60/266 [02:25<06:59,  2.04s/it]predicting train subjects:  23%|██▎       | 61/266 [02:27<06:50,  2.00s/it]predicting train subjects:  23%|██▎       | 62/266 [02:29<06:42,  1.97s/it]predicting train subjects:  24%|██▎       | 63/266 [02:31<06:41,  1.98s/it]predicting train subjects:  24%|██▍       | 64/266 [02:33<06:35,  1.96s/it]predicting train subjects:  24%|██▍       | 65/266 [02:35<06:29,  1.94s/it]predicting train subjects:  25%|██▍       | 66/266 [02:37<06:32,  1.96s/it]predicting train subjects:  25%|██▌       | 67/266 [02:39<06:31,  1.96s/it]predicting train subjects:  26%|██▌       | 68/266 [02:41<06:28,  1.96s/it]predicting train subjects:  26%|██▌       | 69/266 [02:43<06:26,  1.96s/it]predicting train subjects:  26%|██▋       | 70/266 [02:45<06:23,  1.96s/it]predicting train subjects:  27%|██▋       | 71/266 [02:47<06:19,  1.95s/it]predicting train subjects:  27%|██▋       | 72/266 [02:49<06:16,  1.94s/it]predicting train subjects:  27%|██▋       | 73/266 [02:50<06:13,  1.94s/it]predicting train subjects:  28%|██▊       | 74/266 [02:52<06:12,  1.94s/it]predicting train subjects:  28%|██▊       | 75/266 [02:54<06:09,  1.93s/it]predicting train subjects:  29%|██▊       | 76/266 [02:56<06:09,  1.94s/it]predicting train subjects:  29%|██▉       | 77/266 [02:58<06:03,  1.93s/it]predicting train subjects:  29%|██▉       | 78/266 [03:01<06:35,  2.11s/it]predicting train subjects:  30%|██▉       | 79/266 [03:03<06:54,  2.22s/it]predicting train subjects:  30%|███       | 80/266 [03:06<07:11,  2.32s/it]predicting train subjects:  30%|███       | 81/266 [03:08<07:18,  2.37s/it]predicting train subjects:  31%|███       | 82/266 [03:11<07:23,  2.41s/it]predicting train subjects:  31%|███       | 83/266 [03:13<07:25,  2.43s/it]predicting train subjects:  32%|███▏      | 84/266 [03:16<07:28,  2.47s/it]predicting train subjects:  32%|███▏      | 85/266 [03:18<07:25,  2.46s/it]predicting train subjects:  32%|███▏      | 86/266 [03:21<07:22,  2.46s/it]predicting train subjects:  33%|███▎      | 87/266 [03:23<07:22,  2.47s/it]predicting train subjects:  33%|███▎      | 88/266 [03:26<07:18,  2.47s/it]predicting train subjects:  33%|███▎      | 89/266 [03:28<07:21,  2.49s/it]predicting train subjects:  34%|███▍      | 90/266 [03:31<07:21,  2.51s/it]predicting train subjects:  34%|███▍      | 91/266 [03:33<07:16,  2.50s/it]predicting train subjects:  35%|███▍      | 92/266 [03:36<07:13,  2.49s/it]predicting train subjects:  35%|███▍      | 93/266 [03:38<07:09,  2.48s/it]predicting train subjects:  35%|███▌      | 94/266 [03:41<07:06,  2.48s/it]predicting train subjects:  36%|███▌      | 95/266 [03:43<07:07,  2.50s/it]predicting train subjects:  36%|███▌      | 96/266 [03:45<06:53,  2.43s/it]predicting train subjects:  36%|███▋      | 97/266 [03:48<06:57,  2.47s/it]predicting train subjects:  37%|███▋      | 98/266 [03:50<06:55,  2.47s/it]predicting train subjects:  37%|███▋      | 99/266 [03:52<06:21,  2.28s/it]predicting train subjects:  38%|███▊      | 100/266 [03:54<06:06,  2.21s/it]predicting train subjects:  38%|███▊      | 101/266 [03:57<06:04,  2.21s/it]predicting train subjects:  38%|███▊      | 102/266 [03:59<06:04,  2.22s/it]predicting train subjects:  39%|███▊      | 103/266 [04:01<05:59,  2.21s/it]predicting train subjects:  39%|███▉      | 104/266 [04:03<05:55,  2.19s/it]predicting train subjects:  39%|███▉      | 105/266 [04:05<05:53,  2.20s/it]predicting train subjects:  40%|███▉      | 106/266 [04:08<05:51,  2.20s/it]predicting train subjects:  40%|████      | 107/266 [04:10<05:48,  2.19s/it]predicting train subjects:  41%|████      | 108/266 [04:12<05:45,  2.19s/it]predicting train subjects:  41%|████      | 109/266 [04:14<05:44,  2.20s/it]predicting train subjects:  41%|████▏     | 110/266 [04:16<05:41,  2.19s/it]predicting train subjects:  42%|████▏     | 111/266 [04:18<05:37,  2.18s/it]predicting train subjects:  42%|████▏     | 112/266 [04:21<05:36,  2.19s/it]predicting train subjects:  42%|████▏     | 113/266 [04:23<05:35,  2.19s/it]predicting train subjects:  43%|████▎     | 114/266 [04:25<05:34,  2.20s/it]predicting train subjects:  43%|████▎     | 115/266 [04:27<05:32,  2.20s/it]predicting train subjects:  44%|████▎     | 116/266 [04:29<05:29,  2.20s/it]predicting train subjects:  44%|████▍     | 117/266 [04:32<05:26,  2.19s/it]predicting train subjects:  44%|████▍     | 118/266 [04:34<05:25,  2.20s/it]predicting train subjects:  45%|████▍     | 119/266 [04:36<05:35,  2.28s/it]predicting train subjects:  45%|████▌     | 120/266 [04:39<05:40,  2.33s/it]predicting train subjects:  45%|████▌     | 121/266 [04:41<05:42,  2.36s/it]predicting train subjects:  46%|████▌     | 122/266 [04:44<05:43,  2.39s/it]predicting train subjects:  46%|████▌     | 123/266 [04:46<05:45,  2.42s/it]predicting train subjects:  47%|████▋     | 124/266 [04:49<05:49,  2.46s/it]predicting train subjects:  47%|████▋     | 125/266 [04:51<05:47,  2.47s/it]predicting train subjects:  47%|████▋     | 126/266 [04:54<05:49,  2.50s/it]predicting train subjects:  48%|████▊     | 127/266 [04:56<05:47,  2.50s/it]predicting train subjects:  48%|████▊     | 128/266 [04:59<05:43,  2.49s/it]predicting train subjects:  48%|████▊     | 129/266 [05:01<05:39,  2.48s/it]predicting train subjects:  49%|████▉     | 130/266 [05:04<05:37,  2.48s/it]predicting train subjects:  49%|████▉     | 131/266 [05:06<05:34,  2.48s/it]predicting train subjects:  50%|████▉     | 132/266 [05:09<05:36,  2.51s/it]predicting train subjects:  50%|█████     | 133/266 [05:11<05:35,  2.52s/it]predicting train subjects:  50%|█████     | 134/266 [05:14<05:32,  2.52s/it]predicting train subjects:  51%|█████     | 135/266 [05:16<05:30,  2.53s/it]predicting train subjects:  51%|█████     | 136/266 [05:19<05:29,  2.54s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:21<05:24,  2.51s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:24<05:17,  2.48s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:26<05:11,  2.45s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:29<05:07,  2.44s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:31<05:06,  2.45s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:33<05:04,  2.46s/it]predicting train subjects:  54%|█████▍    | 143/266 [05:36<05:01,  2.45s/it]predicting train subjects:  54%|█████▍    | 144/266 [05:38<04:58,  2.45s/it]predicting train subjects:  55%|█████▍    | 145/266 [05:41<04:58,  2.47s/it]predicting train subjects:  55%|█████▍    | 146/266 [05:43<04:53,  2.44s/it]predicting train subjects:  55%|█████▌    | 147/266 [05:46<04:49,  2.43s/it]predicting train subjects:  56%|█████▌    | 148/266 [05:48<04:46,  2.43s/it]predicting train subjects:  56%|█████▌    | 149/266 [05:50<04:42,  2.41s/it]predicting train subjects:  56%|█████▋    | 150/266 [05:53<04:40,  2.42s/it]predicting train subjects:  57%|█████▋    | 151/266 [05:55<04:37,  2.41s/it]predicting train subjects:  57%|█████▋    | 152/266 [05:58<04:34,  2.41s/it]predicting train subjects:  58%|█████▊    | 153/266 [06:00<04:32,  2.41s/it]predicting train subjects:  58%|█████▊    | 154/266 [06:03<04:30,  2.42s/it]predicting train subjects:  58%|█████▊    | 155/266 [06:04<04:07,  2.23s/it]predicting train subjects:  59%|█████▊    | 156/266 [06:06<03:53,  2.12s/it]predicting train subjects:  59%|█████▉    | 157/266 [06:08<03:41,  2.03s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:10<03:31,  1.96s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:12<03:24,  1.91s/it]predicting train subjects:  60%|██████    | 160/266 [06:13<03:20,  1.89s/it]predicting train subjects:  61%|██████    | 161/266 [06:15<03:14,  1.85s/it]predicting train subjects:  61%|██████    | 162/266 [06:17<03:12,  1.85s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:19<03:10,  1.85s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:21<03:06,  1.83s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:23<03:04,  1.83s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:24<03:02,  1.82s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:26<02:59,  1.82s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:28<02:57,  1.81s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:30<02:55,  1.81s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:32<02:52,  1.80s/it]predicting train subjects:  64%|██████▍   | 171/266 [06:33<02:52,  1.81s/it]predicting train subjects:  65%|██████▍   | 172/266 [06:35<02:51,  1.82s/it]predicting train subjects:  65%|██████▌   | 173/266 [06:37<02:57,  1.91s/it]predicting train subjects:  65%|██████▌   | 174/266 [06:39<02:59,  1.96s/it]predicting train subjects:  66%|██████▌   | 175/266 [06:41<02:59,  1.98s/it]predicting train subjects:  66%|██████▌   | 176/266 [06:44<03:01,  2.02s/it]predicting train subjects:  67%|██████▋   | 177/266 [06:46<03:00,  2.03s/it]predicting train subjects:  67%|██████▋   | 178/266 [06:48<02:58,  2.02s/it]predicting train subjects:  67%|██████▋   | 179/266 [06:50<02:57,  2.04s/it]predicting train subjects:  68%|██████▊   | 180/266 [06:52<02:54,  2.02s/it]predicting train subjects:  68%|██████▊   | 181/266 [06:54<02:52,  2.03s/it]predicting train subjects:  68%|██████▊   | 182/266 [06:56<02:50,  2.02s/it]predicting train subjects:  69%|██████▉   | 183/266 [06:58<02:47,  2.02s/it]predicting train subjects:  69%|██████▉   | 184/266 [07:00<02:45,  2.02s/it]predicting train subjects:  70%|██████▉   | 185/266 [07:02<02:46,  2.05s/it]predicting train subjects:  70%|██████▉   | 186/266 [07:04<02:44,  2.05s/it]predicting train subjects:  70%|███████   | 187/266 [07:06<02:43,  2.06s/it]predicting train subjects:  71%|███████   | 188/266 [07:08<02:42,  2.08s/it]predicting train subjects:  71%|███████   | 189/266 [07:10<02:39,  2.07s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:12<02:36,  2.06s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:14<02:37,  2.09s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:16<02:30,  2.03s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:18<02:27,  2.01s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:21<02:36,  2.17s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:23<02:34,  2.17s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:25<02:30,  2.16s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:27<02:29,  2.17s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:29<02:28,  2.18s/it]predicting train subjects:  75%|███████▍  | 199/266 [07:32<02:25,  2.17s/it]predicting train subjects:  75%|███████▌  | 200/266 [07:34<02:22,  2.16s/it]predicting train subjects:  76%|███████▌  | 201/266 [07:36<02:19,  2.15s/it]predicting train subjects:  76%|███████▌  | 202/266 [07:38<02:17,  2.14s/it]predicting train subjects:  76%|███████▋  | 203/266 [07:40<02:15,  2.16s/it]predicting train subjects:  77%|███████▋  | 204/266 [07:42<02:13,  2.15s/it]predicting train subjects:  77%|███████▋  | 205/266 [07:44<02:10,  2.13s/it]predicting train subjects:  77%|███████▋  | 206/266 [07:47<02:08,  2.15s/it]predicting train subjects:  78%|███████▊  | 207/266 [07:49<02:06,  2.14s/it]predicting train subjects:  78%|███████▊  | 208/266 [07:51<02:04,  2.14s/it]predicting train subjects:  79%|███████▊  | 209/266 [07:53<02:01,  2.13s/it]predicting train subjects:  79%|███████▉  | 210/266 [07:55<01:58,  2.12s/it]predicting train subjects:  79%|███████▉  | 211/266 [07:57<01:57,  2.13s/it]predicting train subjects:  80%|███████▉  | 212/266 [07:59<01:56,  2.16s/it]predicting train subjects:  80%|████████  | 213/266 [08:01<01:51,  2.10s/it]predicting train subjects:  80%|████████  | 214/266 [08:03<01:46,  2.05s/it]predicting train subjects:  81%|████████  | 215/266 [08:05<01:42,  2.01s/it]predicting train subjects:  81%|████████  | 216/266 [08:07<01:38,  1.97s/it]predicting train subjects:  82%|████████▏ | 217/266 [08:09<01:36,  1.96s/it]predicting train subjects:  82%|████████▏ | 218/266 [08:11<01:35,  1.98s/it]predicting train subjects:  82%|████████▏ | 219/266 [08:13<01:31,  1.95s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:15<01:28,  1.92s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:17<01:26,  1.93s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:19<01:24,  1.93s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:21<01:23,  1.93s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:23<01:21,  1.94s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:25<01:19,  1.93s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:26<01:16,  1.92s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:28<01:14,  1.92s/it]predicting train subjects:  86%|████████▌ | 228/266 [08:30<01:12,  1.92s/it]predicting train subjects:  86%|████████▌ | 229/266 [08:32<01:10,  1.91s/it]predicting train subjects:  86%|████████▋ | 230/266 [08:34<01:09,  1.93s/it]predicting train subjects:  87%|████████▋ | 231/266 [08:36<01:07,  1.94s/it]predicting train subjects:  87%|████████▋ | 232/266 [08:38<01:06,  1.95s/it]predicting train subjects:  88%|████████▊ | 233/266 [08:40<01:03,  1.94s/it]predicting train subjects:  88%|████████▊ | 234/266 [08:42<01:02,  1.94s/it]predicting train subjects:  88%|████████▊ | 235/266 [08:44<01:01,  1.97s/it]predicting train subjects:  89%|████████▊ | 236/266 [08:46<00:58,  1.96s/it]predicting train subjects:  89%|████████▉ | 237/266 [08:48<00:56,  1.96s/it]predicting train subjects:  89%|████████▉ | 238/266 [08:50<00:54,  1.95s/it]predicting train subjects:  90%|████████▉ | 239/266 [08:52<00:52,  1.94s/it]predicting train subjects:  90%|█████████ | 240/266 [08:54<00:50,  1.94s/it]predicting train subjects:  91%|█████████ | 241/266 [08:56<00:48,  1.94s/it]predicting train subjects:  91%|█████████ | 242/266 [08:58<00:46,  1.93s/it]predicting train subjects:  91%|█████████▏| 243/266 [08:59<00:44,  1.94s/it]predicting train subjects:  92%|█████████▏| 244/266 [09:01<00:42,  1.93s/it]predicting train subjects:  92%|█████████▏| 245/266 [09:03<00:40,  1.94s/it]predicting train subjects:  92%|█████████▏| 246/266 [09:05<00:38,  1.94s/it]predicting train subjects:  93%|█████████▎| 247/266 [09:07<00:36,  1.94s/it]predicting train subjects:  93%|█████████▎| 248/266 [09:09<00:35,  1.95s/it]predicting train subjects:  94%|█████████▎| 249/266 [09:12<00:35,  2.11s/it]predicting train subjects:  94%|█████████▍| 250/266 [09:14<00:35,  2.23s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:17<00:34,  2.32s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:19<00:33,  2.39s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:22<00:31,  2.42s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:24<00:29,  2.46s/it]predicting train subjects:  96%|█████████▌| 255/266 [09:27<00:27,  2.48s/it]predicting train subjects:  96%|█████████▌| 256/266 [09:29<00:24,  2.50s/it]predicting train subjects:  97%|█████████▋| 257/266 [09:32<00:22,  2.52s/it]predicting train subjects:  97%|█████████▋| 258/266 [09:34<00:20,  2.51s/it]predicting train subjects:  97%|█████████▋| 259/266 [09:37<00:17,  2.52s/it]predicting train subjects:  98%|█████████▊| 260/266 [09:39<00:15,  2.52s/it]predicting train subjects:  98%|█████████▊| 261/266 [09:42<00:12,  2.53s/it]predicting train subjects:  98%|█████████▊| 262/266 [09:45<00:10,  2.52s/it]predicting train subjects:  99%|█████████▉| 263/266 [09:47<00:07,  2.53s/it]predicting train subjects:  99%|█████████▉| 264/266 [09:50<00:05,  2.51s/it]predicting train subjects: 100%|█████████▉| 265/266 [09:52<00:02,  2.51s/it]predicting train subjects: 100%|██████████| 266/266 [09:55<00:00,  2.52s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:11,  1.63s/it]Loading train:   1%|          | 2/266 [00:03<06:53,  1.57s/it]Loading train:   1%|          | 3/266 [00:04<06:23,  1.46s/it]Loading train:   2%|▏         | 4/266 [00:05<05:55,  1.36s/it]Loading train:   2%|▏         | 5/266 [00:06<05:57,  1.37s/it]Loading train:   2%|▏         | 6/266 [00:07<05:19,  1.23s/it]Loading train:   3%|▎         | 7/266 [00:08<04:48,  1.11s/it]Loading train:   3%|▎         | 8/266 [00:09<04:28,  1.04s/it]Loading train:   3%|▎         | 9/266 [00:10<04:11,  1.02it/s]Loading train:   4%|▍         | 10/266 [00:11<04:01,  1.06it/s]Loading train:   4%|▍         | 11/266 [00:11<03:55,  1.08it/s]Loading train:   5%|▍         | 12/266 [00:12<03:48,  1.11it/s]Loading train:   5%|▍         | 13/266 [00:13<03:46,  1.12it/s]Loading train:   5%|▌         | 14/266 [00:14<03:45,  1.12it/s]Loading train:   6%|▌         | 15/266 [00:15<03:41,  1.13it/s]Loading train:   6%|▌         | 16/266 [00:16<03:42,  1.13it/s]Loading train:   6%|▋         | 17/266 [00:17<03:40,  1.13it/s]Loading train:   7%|▋         | 18/266 [00:18<03:39,  1.13it/s]Loading train:   7%|▋         | 19/266 [00:18<03:39,  1.13it/s]Loading train:   8%|▊         | 20/266 [00:19<03:37,  1.13it/s]Loading train:   8%|▊         | 21/266 [00:20<03:37,  1.12it/s]Loading train:   8%|▊         | 22/266 [00:21<03:43,  1.09it/s]Loading train:   9%|▊         | 23/266 [00:22<03:35,  1.13it/s]Loading train:   9%|▉         | 24/266 [00:23<03:50,  1.05it/s]Loading train:   9%|▉         | 25/266 [00:24<03:45,  1.07it/s]Loading train:  10%|▉         | 26/266 [00:25<03:37,  1.10it/s]Loading train:  10%|█         | 27/266 [00:26<03:34,  1.11it/s]Loading train:  11%|█         | 28/266 [00:27<03:30,  1.13it/s]Loading train:  11%|█         | 29/266 [00:28<03:30,  1.13it/s]Loading train:  11%|█▏        | 30/266 [00:28<03:23,  1.16it/s]Loading train:  12%|█▏        | 31/266 [00:29<03:20,  1.17it/s]Loading train:  12%|█▏        | 32/266 [00:30<03:22,  1.16it/s]Loading train:  12%|█▏        | 33/266 [00:31<03:19,  1.17it/s]Loading train:  13%|█▎        | 34/266 [00:32<03:18,  1.17it/s]Loading train:  13%|█▎        | 35/266 [00:33<03:14,  1.18it/s]Loading train:  14%|█▎        | 36/266 [00:33<03:15,  1.18it/s]Loading train:  14%|█▍        | 37/266 [00:34<03:18,  1.16it/s]Loading train:  14%|█▍        | 38/266 [00:35<03:23,  1.12it/s]Loading train:  15%|█▍        | 39/266 [00:36<03:25,  1.11it/s]Loading train:  15%|█▌        | 40/266 [00:37<03:25,  1.10it/s]Loading train:  15%|█▌        | 41/266 [00:38<03:25,  1.10it/s]Loading train:  16%|█▌        | 42/266 [00:39<03:22,  1.11it/s]Loading train:  16%|█▌        | 43/266 [00:40<03:13,  1.15it/s]Loading train:  17%|█▋        | 44/266 [00:40<02:58,  1.24it/s]Loading train:  17%|█▋        | 45/266 [00:41<02:55,  1.26it/s]Loading train:  17%|█▋        | 46/266 [00:42<02:50,  1.29it/s]Loading train:  18%|█▊        | 47/266 [00:43<02:46,  1.31it/s]Loading train:  18%|█▊        | 48/266 [00:43<02:40,  1.36it/s]Loading train:  18%|█▊        | 49/266 [00:44<02:35,  1.40it/s]Loading train:  19%|█▉        | 50/266 [00:45<02:33,  1.41it/s]Loading train:  19%|█▉        | 51/266 [00:45<02:29,  1.43it/s]Loading train:  20%|█▉        | 52/266 [00:46<02:28,  1.44it/s]Loading train:  20%|█▉        | 53/266 [00:47<02:28,  1.43it/s]Loading train:  20%|██        | 54/266 [00:47<02:30,  1.41it/s]Loading train:  21%|██        | 55/266 [00:48<02:34,  1.36it/s]Loading train:  21%|██        | 56/266 [00:49<02:31,  1.39it/s]Loading train:  21%|██▏       | 57/266 [00:50<02:25,  1.43it/s]Loading train:  22%|██▏       | 58/266 [00:50<02:28,  1.40it/s]Loading train:  22%|██▏       | 59/266 [00:51<02:25,  1.42it/s]Loading train:  23%|██▎       | 60/266 [00:52<02:22,  1.44it/s]Loading train:  23%|██▎       | 61/266 [00:52<02:20,  1.46it/s]Loading train:  23%|██▎       | 62/266 [00:53<02:16,  1.49it/s]Loading train:  24%|██▎       | 63/266 [00:54<02:13,  1.52it/s]Loading train:  24%|██▍       | 64/266 [00:54<02:15,  1.49it/s]Loading train:  24%|██▍       | 65/266 [00:55<02:13,  1.51it/s]Loading train:  25%|██▍       | 66/266 [00:56<02:14,  1.48it/s]Loading train:  25%|██▌       | 67/266 [00:56<02:13,  1.49it/s]Loading train:  26%|██▌       | 68/266 [00:57<02:11,  1.50it/s]Loading train:  26%|██▌       | 69/266 [00:58<02:10,  1.51it/s]Loading train:  26%|██▋       | 70/266 [00:58<02:12,  1.48it/s]Loading train:  27%|██▋       | 71/266 [00:59<02:15,  1.44it/s]Loading train:  27%|██▋       | 72/266 [01:00<02:16,  1.42it/s]Loading train:  27%|██▋       | 73/266 [01:00<02:14,  1.43it/s]Loading train:  28%|██▊       | 74/266 [01:01<02:18,  1.39it/s]Loading train:  28%|██▊       | 75/266 [01:02<02:22,  1.34it/s]Loading train:  29%|██▊       | 76/266 [01:03<02:19,  1.36it/s]Loading train:  29%|██▉       | 77/266 [01:03<02:16,  1.38it/s]Loading train:  29%|██▉       | 78/266 [01:04<02:32,  1.23it/s]Loading train:  30%|██▉       | 79/266 [01:05<02:31,  1.23it/s]Loading train:  30%|███       | 80/266 [01:06<02:33,  1.21it/s]Loading train:  30%|███       | 81/266 [01:07<02:33,  1.21it/s]Loading train:  31%|███       | 82/266 [01:08<02:31,  1.21it/s]Loading train:  31%|███       | 83/266 [01:09<02:33,  1.20it/s]Loading train:  32%|███▏      | 84/266 [01:10<02:33,  1.18it/s]Loading train:  32%|███▏      | 85/266 [01:10<02:32,  1.18it/s]Loading train:  32%|███▏      | 86/266 [01:11<02:29,  1.20it/s]Loading train:  33%|███▎      | 87/266 [01:12<02:33,  1.16it/s]Loading train:  33%|███▎      | 88/266 [01:13<02:34,  1.15it/s]Loading train:  33%|███▎      | 89/266 [01:14<02:32,  1.16it/s]Loading train:  34%|███▍      | 90/266 [01:15<02:30,  1.17it/s]Loading train:  34%|███▍      | 91/266 [01:16<02:28,  1.18it/s]Loading train:  35%|███▍      | 92/266 [01:16<02:29,  1.16it/s]Loading train:  35%|███▍      | 93/266 [01:17<02:27,  1.17it/s]Loading train:  35%|███▌      | 94/266 [01:18<02:29,  1.15it/s]Loading train:  36%|███▌      | 95/266 [01:19<02:29,  1.15it/s]Loading train:  36%|███▌      | 96/266 [01:20<02:46,  1.02it/s]Loading train:  36%|███▋      | 97/266 [01:22<03:12,  1.14s/it]Loading train:  37%|███▋      | 98/266 [01:23<03:18,  1.18s/it]Loading train:  37%|███▋      | 99/266 [01:24<03:06,  1.12s/it]Loading train:  38%|███▊      | 100/266 [01:25<03:01,  1.09s/it]Loading train:  38%|███▊      | 101/266 [01:26<02:47,  1.01s/it]Loading train:  38%|███▊      | 102/266 [01:27<02:29,  1.10it/s]Loading train:  39%|███▊      | 103/266 [01:27<02:14,  1.21it/s]Loading train:  39%|███▉      | 104/266 [01:28<02:08,  1.26it/s]Loading train:  39%|███▉      | 105/266 [01:29<02:04,  1.30it/s]Loading train:  40%|███▉      | 106/266 [01:29<02:01,  1.31it/s]Loading train:  40%|████      | 107/266 [01:30<01:56,  1.36it/s]Loading train:  41%|████      | 108/266 [01:31<01:59,  1.32it/s]Loading train:  41%|████      | 109/266 [01:32<02:00,  1.30it/s]Loading train:  41%|████▏     | 110/266 [01:32<01:57,  1.33it/s]Loading train:  42%|████▏     | 111/266 [01:33<01:54,  1.36it/s]Loading train:  42%|████▏     | 112/266 [01:34<01:50,  1.40it/s]Loading train:  42%|████▏     | 113/266 [01:34<01:48,  1.42it/s]Loading train:  43%|████▎     | 114/266 [01:35<01:46,  1.43it/s]Loading train:  43%|████▎     | 115/266 [01:36<01:50,  1.36it/s]Loading train:  44%|████▎     | 116/266 [01:37<01:49,  1.37it/s]Loading train:  44%|████▍     | 117/266 [01:37<01:47,  1.39it/s]Loading train:  44%|████▍     | 118/266 [01:38<01:44,  1.42it/s]Loading train:  45%|████▍     | 119/266 [01:39<01:56,  1.27it/s]Loading train:  45%|████▌     | 120/266 [01:40<02:01,  1.20it/s]Loading train:  45%|████▌     | 121/266 [01:41<02:02,  1.19it/s]Loading train:  46%|████▌     | 122/266 [01:42<01:59,  1.20it/s]Loading train:  46%|████▌     | 123/266 [01:42<01:57,  1.22it/s]Loading train:  47%|████▋     | 124/266 [01:43<02:01,  1.17it/s]Loading train:  47%|████▋     | 125/266 [01:44<02:04,  1.13it/s]Loading train:  47%|████▋     | 126/266 [01:45<02:03,  1.14it/s]Loading train:  48%|████▊     | 127/266 [01:46<02:01,  1.15it/s]Loading train:  48%|████▊     | 128/266 [01:47<02:01,  1.14it/s]Loading train:  48%|████▊     | 129/266 [01:48<01:56,  1.17it/s]Loading train:  49%|████▉     | 130/266 [01:48<01:52,  1.21it/s]Loading train:  49%|████▉     | 131/266 [01:49<01:51,  1.21it/s]Loading train:  50%|████▉     | 132/266 [01:50<01:52,  1.19it/s]Loading train:  50%|█████     | 133/266 [01:51<01:49,  1.21it/s]Loading train:  50%|█████     | 134/266 [01:52<01:51,  1.18it/s]Loading train:  51%|█████     | 135/266 [01:53<01:50,  1.19it/s]Loading train:  51%|█████     | 136/266 [01:53<01:48,  1.19it/s]Loading train:  52%|█████▏    | 137/266 [01:54<01:49,  1.18it/s]Loading train:  52%|█████▏    | 138/266 [01:55<01:50,  1.16it/s]Loading train:  52%|█████▏    | 139/266 [01:56<01:49,  1.16it/s]Loading train:  53%|█████▎    | 140/266 [01:57<01:47,  1.17it/s]Loading train:  53%|█████▎    | 141/266 [01:58<01:45,  1.18it/s]Loading train:  53%|█████▎    | 142/266 [01:59<01:43,  1.20it/s]Loading train:  54%|█████▍    | 143/266 [01:59<01:41,  1.21it/s]Loading train:  54%|█████▍    | 144/266 [02:00<01:39,  1.23it/s]Loading train:  55%|█████▍    | 145/266 [02:01<01:41,  1.19it/s]Loading train:  55%|█████▍    | 146/266 [02:02<01:38,  1.22it/s]Loading train:  55%|█████▌    | 147/266 [02:03<01:37,  1.22it/s]Loading train:  56%|█████▌    | 148/266 [02:03<01:36,  1.23it/s]Loading train:  56%|█████▌    | 149/266 [02:04<01:35,  1.22it/s]Loading train:  56%|█████▋    | 150/266 [02:05<01:32,  1.25it/s]Loading train:  57%|█████▋    | 151/266 [02:06<01:32,  1.24it/s]Loading train:  57%|█████▋    | 152/266 [02:07<01:31,  1.24it/s]Loading train:  58%|█████▊    | 153/266 [02:08<01:32,  1.22it/s]Loading train:  58%|█████▊    | 154/266 [02:08<01:32,  1.21it/s]Loading train:  58%|█████▊    | 155/266 [02:09<01:32,  1.20it/s]Loading train:  59%|█████▊    | 156/266 [02:10<01:25,  1.28it/s]Loading train:  59%|█████▉    | 157/266 [02:11<01:21,  1.33it/s]Loading train:  59%|█████▉    | 158/266 [02:11<01:19,  1.36it/s]Loading train:  60%|█████▉    | 159/266 [02:12<01:15,  1.42it/s]Loading train:  60%|██████    | 160/266 [02:13<01:12,  1.47it/s]Loading train:  61%|██████    | 161/266 [02:13<01:11,  1.48it/s]Loading train:  61%|██████    | 162/266 [02:14<01:12,  1.44it/s]Loading train:  61%|██████▏   | 163/266 [02:15<01:11,  1.45it/s]Loading train:  62%|██████▏   | 164/266 [02:15<01:08,  1.49it/s]Loading train:  62%|██████▏   | 165/266 [02:16<01:05,  1.54it/s]Loading train:  62%|██████▏   | 166/266 [02:16<01:04,  1.56it/s]Loading train:  63%|██████▎   | 167/266 [02:17<01:03,  1.55it/s]Loading train:  63%|██████▎   | 168/266 [02:18<01:03,  1.55it/s]Loading train:  64%|██████▎   | 169/266 [02:18<01:02,  1.56it/s]Loading train:  64%|██████▍   | 170/266 [02:19<01:00,  1.60it/s]Loading train:  64%|██████▍   | 171/266 [02:20<01:00,  1.57it/s]Loading train:  65%|██████▍   | 172/266 [02:20<00:59,  1.57it/s]Loading train:  65%|██████▌   | 173/266 [02:21<01:02,  1.49it/s]Loading train:  65%|██████▌   | 174/266 [02:22<01:03,  1.44it/s]Loading train:  66%|██████▌   | 175/266 [02:22<01:02,  1.46it/s]Loading train:  66%|██████▌   | 176/266 [02:23<01:01,  1.45it/s]Loading train:  67%|██████▋   | 177/266 [02:24<01:00,  1.47it/s]Loading train:  67%|██████▋   | 178/266 [02:24<00:59,  1.49it/s]Loading train:  67%|██████▋   | 179/266 [02:25<00:58,  1.49it/s]Loading train:  68%|██████▊   | 180/266 [02:26<00:57,  1.50it/s]Loading train:  68%|██████▊   | 181/266 [02:26<00:57,  1.47it/s]Loading train:  68%|██████▊   | 182/266 [02:27<00:57,  1.46it/s]Loading train:  69%|██████▉   | 183/266 [02:28<00:57,  1.45it/s]Loading train:  69%|██████▉   | 184/266 [02:29<00:56,  1.46it/s]Loading train:  70%|██████▉   | 185/266 [02:29<00:55,  1.47it/s]Loading train:  70%|██████▉   | 186/266 [02:30<00:53,  1.49it/s]Loading train:  70%|███████   | 187/266 [02:31<00:53,  1.47it/s]Loading train:  71%|███████   | 188/266 [02:31<00:52,  1.48it/s]Loading train:  71%|███████   | 189/266 [02:32<00:52,  1.46it/s]Loading train:  71%|███████▏  | 190/266 [02:33<00:51,  1.48it/s]Loading train:  72%|███████▏  | 191/266 [02:34<01:04,  1.16it/s]Loading train:  72%|███████▏  | 192/266 [02:35<01:08,  1.07it/s]Loading train:  73%|███████▎  | 193/266 [02:36<01:10,  1.04it/s]Loading train:  73%|███████▎  | 194/266 [02:37<01:17,  1.08s/it]Loading train:  73%|███████▎  | 195/266 [02:38<01:09,  1.02it/s]Loading train:  74%|███████▎  | 196/266 [02:39<01:02,  1.12it/s]Loading train:  74%|███████▍  | 197/266 [02:39<00:57,  1.19it/s]Loading train:  74%|███████▍  | 198/266 [02:40<00:56,  1.20it/s]Loading train:  75%|███████▍  | 199/266 [02:41<00:53,  1.25it/s]Loading train:  75%|███████▌  | 200/266 [02:42<00:52,  1.26it/s]Loading train:  76%|███████▌  | 201/266 [02:43<00:52,  1.25it/s]Loading train:  76%|███████▌  | 202/266 [02:43<00:49,  1.29it/s]Loading train:  76%|███████▋  | 203/266 [02:44<00:48,  1.30it/s]Loading train:  77%|███████▋  | 204/266 [02:45<00:46,  1.33it/s]Loading train:  77%|███████▋  | 205/266 [02:46<00:45,  1.34it/s]Loading train:  77%|███████▋  | 206/266 [02:46<00:45,  1.32it/s]Loading train:  78%|███████▊  | 207/266 [02:47<00:43,  1.34it/s]Loading train:  78%|███████▊  | 208/266 [02:48<00:42,  1.37it/s]Loading train:  79%|███████▊  | 209/266 [02:48<00:41,  1.36it/s]Loading train:  79%|███████▉  | 210/266 [02:49<00:40,  1.38it/s]Loading train:  79%|███████▉  | 211/266 [02:50<00:39,  1.38it/s]Loading train:  80%|███████▉  | 212/266 [02:51<00:39,  1.36it/s]Loading train:  80%|████████  | 213/266 [02:51<00:38,  1.38it/s]Loading train:  80%|████████  | 214/266 [02:52<00:38,  1.36it/s]Loading train:  81%|████████  | 215/266 [02:53<00:36,  1.40it/s]Loading train:  81%|████████  | 216/266 [02:54<00:35,  1.39it/s]Loading train:  82%|████████▏ | 217/266 [02:54<00:36,  1.34it/s]Loading train:  82%|████████▏ | 218/266 [02:55<00:35,  1.34it/s]Loading train:  82%|████████▏ | 219/266 [02:56<00:34,  1.37it/s]Loading train:  83%|████████▎ | 220/266 [02:56<00:32,  1.42it/s]Loading train:  83%|████████▎ | 221/266 [02:57<00:31,  1.41it/s]Loading train:  83%|████████▎ | 222/266 [02:58<00:31,  1.40it/s]Loading train:  84%|████████▍ | 223/266 [02:59<00:30,  1.42it/s]Loading train:  84%|████████▍ | 224/266 [02:59<00:29,  1.44it/s]Loading train:  85%|████████▍ | 225/266 [03:00<00:28,  1.46it/s]Loading train:  85%|████████▍ | 226/266 [03:01<00:27,  1.44it/s]Loading train:  85%|████████▌ | 227/266 [03:01<00:26,  1.45it/s]Loading train:  86%|████████▌ | 228/266 [03:02<00:26,  1.41it/s]Loading train:  86%|████████▌ | 229/266 [03:03<00:25,  1.43it/s]Loading train:  86%|████████▋ | 230/266 [03:03<00:24,  1.45it/s]Loading train:  87%|████████▋ | 231/266 [03:04<00:24,  1.42it/s]Loading train:  87%|████████▋ | 232/266 [03:05<00:23,  1.42it/s]Loading train:  88%|████████▊ | 233/266 [03:05<00:22,  1.45it/s]Loading train:  88%|████████▊ | 234/266 [03:06<00:21,  1.47it/s]Loading train:  88%|████████▊ | 235/266 [03:07<00:21,  1.47it/s]Loading train:  89%|████████▊ | 236/266 [03:07<00:20,  1.48it/s]Loading train:  89%|████████▉ | 237/266 [03:08<00:19,  1.49it/s]Loading train:  89%|████████▉ | 238/266 [03:09<00:18,  1.51it/s]Loading train:  90%|████████▉ | 239/266 [03:09<00:18,  1.50it/s]Loading train:  90%|█████████ | 240/266 [03:10<00:18,  1.43it/s]Loading train:  91%|█████████ | 241/266 [03:11<00:17,  1.44it/s]Loading train:  91%|█████████ | 242/266 [03:12<00:16,  1.45it/s]Loading train:  91%|█████████▏| 243/266 [03:12<00:16,  1.43it/s]Loading train:  92%|█████████▏| 244/266 [03:13<00:15,  1.46it/s]Loading train:  92%|█████████▏| 245/266 [03:14<00:14,  1.48it/s]Loading train:  92%|█████████▏| 246/266 [03:14<00:13,  1.50it/s]Loading train:  93%|█████████▎| 247/266 [03:15<00:13,  1.44it/s]Loading train:  93%|█████████▎| 248/266 [03:16<00:12,  1.43it/s]Loading train:  94%|█████████▎| 249/266 [03:17<00:13,  1.30it/s]Loading train:  94%|█████████▍| 250/266 [03:17<00:12,  1.30it/s]Loading train:  94%|█████████▍| 251/266 [03:18<00:11,  1.31it/s]Loading train:  95%|█████████▍| 252/266 [03:19<00:10,  1.30it/s]Loading train:  95%|█████████▌| 253/266 [03:20<00:10,  1.29it/s]Loading train:  95%|█████████▌| 254/266 [03:21<00:09,  1.28it/s]Loading train:  96%|█████████▌| 255/266 [03:21<00:08,  1.28it/s]Loading train:  96%|█████████▌| 256/266 [03:22<00:07,  1.30it/s]Loading train:  97%|█████████▋| 257/266 [03:23<00:07,  1.27it/s]Loading train:  97%|█████████▋| 258/266 [03:24<00:06,  1.29it/s]Loading train:  97%|█████████▋| 259/266 [03:24<00:05,  1.29it/s]Loading train:  98%|█████████▊| 260/266 [03:25<00:04,  1.30it/s]Loading train:  98%|█████████▊| 261/266 [03:26<00:03,  1.30it/s]Loading train:  98%|█████████▊| 262/266 [03:27<00:03,  1.31it/s]Loading train:  99%|█████████▉| 263/266 [03:28<00:02,  1.29it/s]Loading train:  99%|█████████▉| 264/266 [03:28<00:01,  1.29it/s]Loading train: 100%|█████████▉| 265/266 [03:29<00:00,  1.28it/s]Loading train: 100%|██████████| 266/266 [03:30<00:00,  1.26it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   4%|▍         | 11/266 [00:00<00:02, 100.03it/s]concatenating: train:   8%|▊         | 22/266 [00:00<00:02, 100.27it/s]concatenating: train:  14%|█▎        | 36/266 [00:00<00:02, 109.60it/s]concatenating: train:  18%|█▊        | 49/266 [00:00<00:01, 114.00it/s]concatenating: train:  23%|██▎       | 62/266 [00:00<00:01, 115.63it/s]concatenating: train:  28%|██▊       | 75/266 [00:00<00:01, 118.22it/s]concatenating: train:  33%|███▎      | 89/266 [00:00<00:01, 122.40it/s]concatenating: train:  41%|████      | 108/266 [00:00<00:01, 134.48it/s]concatenating: train:  47%|████▋     | 125/266 [00:00<00:00, 142.93it/s]concatenating: train:  53%|█████▎    | 141/266 [00:01<00:00, 146.85it/s]concatenating: train:  59%|█████▊    | 156/266 [00:01<00:00, 145.67it/s]concatenating: train:  64%|██████▍   | 171/266 [00:01<00:00, 138.68it/s]concatenating: train:  70%|██████▉   | 186/266 [00:01<00:00, 139.46it/s]concatenating: train:  76%|███████▌  | 201/266 [00:01<00:00, 131.98it/s]concatenating: train:  81%|████████  | 215/266 [00:01<00:00, 133.25it/s]concatenating: train:  90%|█████████ | 240/266 [00:01<00:00, 154.80it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 148.21it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:04,  1.22s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.21s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.14s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.09s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.14s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 658.96it/s]2019-08-17 01:44:20.230082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 01:44:20.230173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 01:44:20.230186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 01:44:20.230195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 01:44:20.230579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.66it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.67it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.34it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  8.12it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:03,  8.92it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  7.55it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02,  9.76it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02, 10.13it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:01<00:02,  8.13it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:01, 10.06it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 10.48it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 10.52it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  7.26it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.07it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00,  9.46it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00,  9.61it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.14it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 11.42it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 88, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 88, 48, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 88, 48, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 88, 48, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 88, 48, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 88, 48, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 88, 48, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 44, 24, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 44, 24, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 44, 24, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 44, 24, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 24, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 44, 24, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 24, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 44, 24, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 44, 24, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 22, 12, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 22, 12, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 22, 12, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 22, 12, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 22, 12, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 22, 12, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 22, 12, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 22, 12, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 22, 12, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 22, 12, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 44, 24, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 44, 24, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 44, 24, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 44, 24, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 44, 24, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 44, 24, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 44, 24, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 44, 24, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 44, 24, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 44, 24, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 88, 48, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 88, 48, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 88, 48, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 88, 48, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 88, 48, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 88, 48, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 88, 48, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 88, 48, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 88, 48, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 88, 48, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 88, 48, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 18,013
Non-trainable params: 871,240
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.28050642e-02 3.25509927e-02 7.61160196e-02 9.45779817e-03
 2.73730215e-02 7.16184673e-03 8.86576389e-02 1.13671216e-01
 8.88324670e-02 1.34967430e-02 2.88012330e-01 1.91613348e-01
 2.51514543e-04]
Train on 10237 samples, validate on 189 samples
Epoch 1/300
 - 18s - loss: 2.9214 - acc: 0.6775 - mDice: 0.0937 - val_loss: 3.6459 - val_acc: 0.8620 - val_mDice: 0.0699

Epoch 00001: val_mDice improved from -inf to 0.06995, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 1.5152 - acc: 0.8833 - mDice: 0.2263 - val_loss: 1.3035 - val_acc: 0.9111 - val_mDice: 0.2937

Epoch 00002: val_mDice improved from 0.06995 to 0.29370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 13s - loss: 1.1991 - acc: 0.8874 - mDice: 0.2930 - val_loss: 1.4022 - val_acc: 0.8737 - val_mDice: 0.2772

Epoch 00003: val_mDice did not improve from 0.29370
Epoch 4/300
 - 13s - loss: 1.0699 - acc: 0.8901 - mDice: 0.3321 - val_loss: 0.9875 - val_acc: 0.9054 - val_mDice: 0.3624

Epoch 00004: val_mDice improved from 0.29370 to 0.36243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 13s - loss: 0.9967 - acc: 0.8931 - mDice: 0.3586 - val_loss: 1.0474 - val_acc: 0.8918 - val_mDice: 0.3464

Epoch 00005: val_mDice did not improve from 0.36243
Epoch 6/300
 - 13s - loss: 0.9302 - acc: 0.8961 - mDice: 0.3815 - val_loss: 0.9247 - val_acc: 0.9186 - val_mDice: 0.4129

Epoch 00006: val_mDice improved from 0.36243 to 0.41293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 13s - loss: 0.9030 - acc: 0.8985 - mDice: 0.3955 - val_loss: 1.1662 - val_acc: 0.9238 - val_mDice: 0.4149

Epoch 00007: val_mDice improved from 0.41293 to 0.41494, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 0.8616 - acc: 0.9008 - mDice: 0.4091 - val_loss: 0.8057 - val_acc: 0.9182 - val_mDice: 0.4395

Epoch 00008: val_mDice improved from 0.41494 to 0.43954, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 13s - loss: 0.8327 - acc: 0.9028 - mDice: 0.4214 - val_loss: 0.8217 - val_acc: 0.9138 - val_mDice: 0.4315

Epoch 00009: val_mDice did not improve from 0.43954
Epoch 10/300
 - 13s - loss: 0.8104 - acc: 0.9046 - mDice: 0.4311 - val_loss: 0.8596 - val_acc: 0.9184 - val_mDice: 0.4215

Epoch 00010: val_mDice did not improve from 0.43954
Epoch 11/300
 - 13s - loss: 0.8306 - acc: 0.9045 - mDice: 0.4248 - val_loss: 0.7743 - val_acc: 0.9198 - val_mDice: 0.4578

Epoch 00011: val_mDice improved from 0.43954 to 0.45779, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 13s - loss: 0.8062 - acc: 0.9057 - mDice: 0.4348 - val_loss: 0.8094 - val_acc: 0.9261 - val_mDice: 0.4565

Epoch 00012: val_mDice did not improve from 0.45779
Epoch 13/300
 - 13s - loss: 0.7637 - acc: 0.9084 - mDice: 0.4517 - val_loss: 0.7708 - val_acc: 0.9119 - val_mDice: 0.4580

Epoch 00013: val_mDice improved from 0.45779 to 0.45799, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 0.7538 - acc: 0.9091 - mDice: 0.4568 - val_loss: 0.7825 - val_acc: 0.9174 - val_mDice: 0.4582

Epoch 00014: val_mDice improved from 0.45799 to 0.45820, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 13s - loss: 0.7364 - acc: 0.9103 - mDice: 0.4648 - val_loss: 0.7650 - val_acc: 0.9124 - val_mDice: 0.4668

Epoch 00015: val_mDice improved from 0.45820 to 0.46683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 13s - loss: 0.7451 - acc: 0.9104 - mDice: 0.4617 - val_loss: 0.9415 - val_acc: 0.9290 - val_mDice: 0.4484

Epoch 00016: val_mDice did not improve from 0.46683
Epoch 17/300
 - 13s - loss: 0.7262 - acc: 0.9112 - mDice: 0.4704 - val_loss: 0.6925 - val_acc: 0.9250 - val_mDice: 0.4955

Epoch 00017: val_mDice improved from 0.46683 to 0.49550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 0.7017 - acc: 0.9127 - mDice: 0.4808 - val_loss: 0.7138 - val_acc: 0.9142 - val_mDice: 0.4853

Epoch 00018: val_mDice did not improve from 0.49550
Epoch 19/300
 - 13s - loss: 0.7047 - acc: 0.9130 - mDice: 0.4830 - val_loss: 0.9418 - val_acc: 0.9070 - val_mDice: 0.4314

Epoch 00019: val_mDice did not improve from 0.49550
Epoch 20/300
 - 13s - loss: 0.6878 - acc: 0.9138 - mDice: 0.4878 - val_loss: 0.6959 - val_acc: 0.9232 - val_mDice: 0.5007

Epoch 00020: val_mDice improved from 0.49550 to 0.50066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 14s - loss: 0.6793 - acc: 0.9143 - mDice: 0.4922 - val_loss: 0.7363 - val_acc: 0.9243 - val_mDice: 0.4853

Epoch 00021: val_mDice did not improve from 0.50066
Epoch 22/300
 - 13s - loss: 0.6687 - acc: 0.9151 - mDice: 0.4975 - val_loss: 0.6631 - val_acc: 0.9308 - val_mDice: 0.5089

Epoch 00022: val_mDice improved from 0.50066 to 0.50893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 13s - loss: 0.6609 - acc: 0.9158 - mDice: 0.5010 - val_loss: 0.7142 - val_acc: 0.9327 - val_mDice: 0.4985

Epoch 00023: val_mDice did not improve from 0.50893
Epoch 24/300
 - 14s - loss: 0.6713 - acc: 0.9158 - mDice: 0.4993 - val_loss: 3.4988 - val_acc: 0.9125 - val_mDice: 0.2139

Epoch 00024: val_mDice did not improve from 0.50893
Epoch 25/300
 - 13s - loss: 0.7029 - acc: 0.9138 - mDice: 0.4820 - val_loss: 0.7227 - val_acc: 0.9195 - val_mDice: 0.4850

Epoch 00025: val_mDice did not improve from 0.50893
Epoch 26/300
 - 14s - loss: 0.6496 - acc: 0.9169 - mDice: 0.5076 - val_loss: 0.7499 - val_acc: 0.9182 - val_mDice: 0.4864

Epoch 00026: val_mDice did not improve from 0.50893
Epoch 27/300
 - 13s - loss: 0.6439 - acc: 0.9175 - mDice: 0.5104 - val_loss: 0.7160 - val_acc: 0.9317 - val_mDice: 0.5208

Epoch 00027: val_mDice improved from 0.50893 to 0.52084, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 13s - loss: 0.6372 - acc: 0.9178 - mDice: 0.5142 - val_loss: 0.6688 - val_acc: 0.9330 - val_mDice: 0.5280

Epoch 00028: val_mDice improved from 0.52084 to 0.52804, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 14s - loss: 0.6800 - acc: 0.9155 - mDice: 0.4942 - val_loss: 0.6970 - val_acc: 0.9312 - val_mDice: 0.5002

Epoch 00029: val_mDice did not improve from 0.52804
Epoch 30/300
 - 13s - loss: 0.6455 - acc: 0.9176 - mDice: 0.5095 - val_loss: 0.6658 - val_acc: 0.9222 - val_mDice: 0.5078

Epoch 00030: val_mDice did not improve from 0.52804
Epoch 31/300
 - 13s - loss: 0.6310 - acc: 0.9184 - mDice: 0.5171 - val_loss: 0.6248 - val_acc: 0.9236 - val_mDice: 0.5247

Epoch 00031: val_mDice did not improve from 0.52804
Epoch 32/300
 - 14s - loss: 0.6385 - acc: 0.9181 - mDice: 0.5145 - val_loss: 0.6039 - val_acc: 0.9328 - val_mDice: 0.5399

Epoch 00032: val_mDice improved from 0.52804 to 0.53992, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 13s - loss: 0.6245 - acc: 0.9188 - mDice: 0.5209 - val_loss: 0.8009 - val_acc: 0.9235 - val_mDice: 0.4946

Epoch 00033: val_mDice did not improve from 0.53992
Epoch 34/300
 - 13s - loss: 0.6426 - acc: 0.9182 - mDice: 0.5155 - val_loss: 3.4650 - val_acc: 0.9023 - val_mDice: 0.2257

Epoch 00034: val_mDice did not improve from 0.53992
Epoch 35/300
 - 13s - loss: 0.6349 - acc: 0.9181 - mDice: 0.5152 - val_loss: 0.6800 - val_acc: 0.9287 - val_mDice: 0.5131

Epoch 00035: val_mDice did not improve from 0.53992
Epoch 36/300
 - 13s - loss: 0.6125 - acc: 0.9195 - mDice: 0.5272 - val_loss: 0.6296 - val_acc: 0.9235 - val_mDice: 0.5250

Epoch 00036: val_mDice did not improve from 0.53992
Epoch 37/300
 - 13s - loss: 0.6089 - acc: 0.9199 - mDice: 0.5292 - val_loss: 0.6289 - val_acc: 0.9267 - val_mDice: 0.5244

Epoch 00037: val_mDice did not improve from 0.53992
Epoch 38/300
 - 14s - loss: 0.6046 - acc: 0.9205 - mDice: 0.5320 - val_loss: 0.6918 - val_acc: 0.9341 - val_mDice: 0.5185

Epoch 00038: val_mDice did not improve from 0.53992
Epoch 39/300
 - 14s - loss: 0.6040 - acc: 0.9203 - mDice: 0.5322 - val_loss: 0.6093 - val_acc: 0.9349 - val_mDice: 0.5372

Epoch 00039: val_mDice did not improve from 0.53992
Epoch 40/300
 - 13s - loss: 0.6264 - acc: 0.9193 - mDice: 0.5216 - val_loss: 5.1651 - val_acc: 0.9217 - val_mDice: 0.2071

Epoch 00040: val_mDice did not improve from 0.53992
Epoch 41/300
 - 13s - loss: 0.6252 - acc: 0.9190 - mDice: 0.5203 - val_loss: 0.5847 - val_acc: 0.9302 - val_mDice: 0.5491

Epoch 00041: val_mDice improved from 0.53992 to 0.54907, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 13s - loss: 0.6022 - acc: 0.9204 - mDice: 0.5332 - val_loss: 0.6244 - val_acc: 0.9327 - val_mDice: 0.5359

Epoch 00042: val_mDice did not improve from 0.54907
Epoch 43/300
 - 13s - loss: 0.5985 - acc: 0.9205 - mDice: 0.5351 - val_loss: 0.7011 - val_acc: 0.9336 - val_mDice: 0.5163

Epoch 00043: val_mDice did not improve from 0.54907
Epoch 44/300
 - 13s - loss: 0.5912 - acc: 0.9210 - mDice: 0.5388 - val_loss: 0.5968 - val_acc: 0.9327 - val_mDice: 0.5457

Epoch 00044: val_mDice did not improve from 0.54907
Epoch 45/300
 - 13s - loss: 0.5896 - acc: 0.9213 - mDice: 0.5398 - val_loss: 0.6624 - val_acc: 0.9245 - val_mDice: 0.5166

Epoch 00045: val_mDice did not improve from 0.54907
Epoch 46/300
 - 14s - loss: 0.5879 - acc: 0.9215 - mDice: 0.5408 - val_loss: 0.6343 - val_acc: 0.9286 - val_mDice: 0.5303

Epoch 00046: val_mDice did not improve from 0.54907
Epoch 47/300
 - 14s - loss: 0.5813 - acc: 0.9219 - mDice: 0.5442 - val_loss: 0.6055 - val_acc: 0.9289 - val_mDice: 0.5403

Epoch 00047: val_mDice did not improve from 0.54907
Epoch 48/300
 - 13s - loss: 0.5794 - acc: 0.9219 - mDice: 0.5451 - val_loss: 0.6920 - val_acc: 0.9297 - val_mDice: 0.5051

Epoch 00048: val_mDice did not improve from 0.54907
Epoch 49/300
 - 13s - loss: 0.5802 - acc: 0.9219 - mDice: 0.5447 - val_loss: 0.6423 - val_acc: 0.9306 - val_mDice: 0.5287

Epoch 00049: val_mDice did not improve from 0.54907
Epoch 50/300
 - 13s - loss: 0.5859 - acc: 0.9218 - mDice: 0.5423 - val_loss: 0.9789 - val_acc: 0.9288 - val_mDice: 0.4760

Epoch 00050: val_mDice did not improve from 0.54907
Epoch 51/300
 - 13s - loss: 0.5895 - acc: 0.9214 - mDice: 0.5406 - val_loss: 0.6237 - val_acc: 0.9311 - val_mDice: 0.5333

Epoch 00051: val_mDice did not improve from 0.54907
Epoch 52/300
 - 13s - loss: 0.5740 - acc: 0.9224 - mDice: 0.5483 - val_loss: 0.6694 - val_acc: 0.9273 - val_mDice: 0.5184

Epoch 00052: val_mDice did not improve from 0.54907
Epoch 53/300
 - 13s - loss: 0.5735 - acc: 0.9225 - mDice: 0.5489 - val_loss: 0.5985 - val_acc: 0.9338 - val_mDice: 0.5584

Epoch 00053: val_mDice improved from 0.54907 to 0.55844, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 13s - loss: 0.6036 - acc: 0.9208 - mDice: 0.5338 - val_loss: 0.6019 - val_acc: 0.9284 - val_mDice: 0.5422

Epoch 00054: val_mDice did not improve from 0.55844
Epoch 55/300
 - 14s - loss: 0.5736 - acc: 0.9226 - mDice: 0.5485 - val_loss: 0.5703 - val_acc: 0.9334 - val_mDice: 0.5593

Epoch 00055: val_mDice improved from 0.55844 to 0.55934, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 13s - loss: 0.5650 - acc: 0.9229 - mDice: 0.5529 - val_loss: 0.6905 - val_acc: 0.9238 - val_mDice: 0.5102

Epoch 00056: val_mDice did not improve from 0.55934
Epoch 57/300
 - 14s - loss: 0.5669 - acc: 0.9232 - mDice: 0.5525 - val_loss: 0.6282 - val_acc: 0.9344 - val_mDice: 0.5464

Epoch 00057: val_mDice did not improve from 0.55934
Epoch 58/300
 - 13s - loss: 0.5627 - acc: 0.9233 - mDice: 0.5547 - val_loss: 0.5652 - val_acc: 0.9322 - val_mDice: 0.5567

Epoch 00058: val_mDice did not improve from 0.55934
Epoch 59/300
 - 13s - loss: 0.5687 - acc: 0.9234 - mDice: 0.5549 - val_loss: 0.5689 - val_acc: 0.9334 - val_mDice: 0.5576

Epoch 00059: val_mDice did not improve from 0.55934
Epoch 60/300
 - 13s - loss: 0.5614 - acc: 0.9233 - mDice: 0.5558 - val_loss: 0.7898 - val_acc: 0.9306 - val_mDice: 0.5042

Epoch 00060: val_mDice did not improve from 0.55934
Epoch 61/300
 - 13s - loss: 0.5586 - acc: 0.9235 - mDice: 0.5568 - val_loss: 0.6641 - val_acc: 0.9321 - val_mDice: 0.5397

Epoch 00061: val_mDice did not improve from 0.55934
Epoch 62/300
 - 14s - loss: 0.5593 - acc: 0.9233 - mDice: 0.5564 - val_loss: 0.6617 - val_acc: 0.9344 - val_mDice: 0.5582

Epoch 00062: val_mDice did not improve from 0.55934
Epoch 63/300
 - 14s - loss: 0.5615 - acc: 0.9233 - mDice: 0.5556 - val_loss: 0.5475 - val_acc: 0.9306 - val_mDice: 0.5661

Epoch 00063: val_mDice improved from 0.55934 to 0.56614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 13s - loss: 0.5656 - acc: 0.9235 - mDice: 0.5563 - val_loss: 0.6572 - val_acc: 0.9333 - val_mDice: 0.5303

Epoch 00064: val_mDice did not improve from 0.56614
Epoch 65/300
 - 13s - loss: 0.5549 - acc: 0.9237 - mDice: 0.5591 - val_loss: 0.7558 - val_acc: 0.9380 - val_mDice: 0.5428

Epoch 00065: val_mDice did not improve from 0.56614
Epoch 66/300
 - 13s - loss: 0.5546 - acc: 0.9238 - mDice: 0.5592 - val_loss: 0.8024 - val_acc: 0.9276 - val_mDice: 0.5165

Epoch 00066: val_mDice did not improve from 0.56614
Epoch 67/300
 - 13s - loss: 0.5526 - acc: 0.9238 - mDice: 0.5608 - val_loss: 0.5738 - val_acc: 0.9296 - val_mDice: 0.5537

Epoch 00067: val_mDice did not improve from 0.56614
Epoch 68/300
 - 13s - loss: 0.5540 - acc: 0.9238 - mDice: 0.5601 - val_loss: 0.5470 - val_acc: 0.9365 - val_mDice: 0.5778

Epoch 00068: val_mDice improved from 0.56614 to 0.57775, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute3_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 13s - loss: 0.5526 - acc: 0.9238 - mDice: 0.5607 - val_loss: 0.5800 - val_acc: 0.9309 - val_mDice: 0.5551

Epoch 00069: val_mDice did not improve from 0.57775
Epoch 70/300
 - 13s - loss: 0.5491 - acc: 0.9240 - mDice: 0.5622 - val_loss: 0.6326 - val_acc: 0.9264 - val_mDice: 0.5273

Epoch 00070: val_mDice did not improve from 0.57775
Epoch 71/300
 - 14s - loss: 0.5522 - acc: 0.9245 - mDice: 0.5644 - val_loss: 0.6005 - val_acc: 0.9351 - val_mDice: 0.5447

Epoch 00071: val_mDice did not improve from 0.57775
Epoch 72/300
 - 14s - loss: 0.5493 - acc: 0.9240 - mDice: 0.5623 - val_loss: 0.7508 - val_acc: 0.9329 - val_mDice: 0.5169

Epoch 00072: val_mDice did not improve from 0.57775
Epoch 73/300
 - 13s - loss: 0.5457 - acc: 0.9244 - mDice: 0.5642 - val_loss: 0.6183 - val_acc: 0.9325 - val_mDice: 0.5558

Epoch 00073: val_mDice did not improve from 0.57775
Epoch 74/300
 - 13s - loss: 0.5620 - acc: 0.9232 - mDice: 0.5556 - val_loss: 0.7020 - val_acc: 0.9298 - val_mDice: 0.5200

Epoch 00074: val_mDice did not improve from 0.57775
Epoch 75/300
 - 13s - loss: 0.5523 - acc: 0.9241 - mDice: 0.5609 - val_loss: 0.6090 - val_acc: 0.9237 - val_mDice: 0.5384

Epoch 00075: val_mDice did not improve from 0.57775
Epoch 76/300
 - 13s - loss: 0.5415 - acc: 0.9246 - mDice: 0.5666 - val_loss: 0.6414 - val_acc: 0.9279 - val_mDice: 0.5249

Epoch 00076: val_mDice did not improve from 0.57775
Epoch 77/300
 - 13s - loss: 0.5408 - acc: 0.9245 - mDice: 0.5669 - val_loss: 0.5542 - val_acc: 0.9290 - val_mDice: 0.5648

Epoch 00077: val_mDice did not improve from 0.57775
Epoch 78/300
 - 13s - loss: 0.5403 - acc: 0.9247 - mDice: 0.5673 - val_loss: 0.6605 - val_acc: 0.9273 - val_mDice: 0.5249

Epoch 00078: val_mDice did not improve from 0.57775
Epoch 79/300
 - 14s - loss: 0.5849 - acc: 0.9224 - mDice: 0.5444 - val_loss: 0.6965 - val_acc: 0.9268 - val_mDice: 0.4969

Epoch 00079: val_mDice did not improve from 0.57775
Epoch 80/300
 - 14s - loss: 0.5595 - acc: 0.9232 - mDice: 0.5562 - val_loss: 0.6817 - val_acc: 0.9327 - val_mDice: 0.5243

Epoch 00080: val_mDice did not improve from 0.57775
Epoch 81/300
 - 13s - loss: 0.5420 - acc: 0.9245 - mDice: 0.5662 - val_loss: 0.9073 - val_acc: 0.9313 - val_mDice: 0.5022

Epoch 00081: val_mDice did not improve from 0.57775
Epoch 82/300
 - 13s - loss: 0.5486 - acc: 0.9243 - mDice: 0.5660 - val_loss: 0.7745 - val_acc: 0.9180 - val_mDice: 0.4937

Epoch 00082: val_mDice did not improve from 0.57775
Epoch 83/300
 - 14s - loss: 0.5377 - acc: 0.9247 - mDice: 0.5689 - val_loss: 0.6657 - val_acc: 0.9377 - val_mDice: 0.5710

Epoch 00083: val_mDice did not improve from 0.57775
Epoch 84/300
 - 13s - loss: 0.5360 - acc: 0.9248 - mDice: 0.5700 - val_loss: 0.6140 - val_acc: 0.9285 - val_mDice: 0.5394

Epoch 00084: val_mDice did not improve from 0.57775
Epoch 85/300
 - 13s - loss: 0.5563 - acc: 0.9238 - mDice: 0.5599 - val_loss: 10.6039 - val_acc: 0.9112 - val_mDice: 0.0598

Epoch 00085: val_mDice did not improve from 0.57775
Epoch 86/300
 - 13s - loss: 0.5719 - acc: 0.9232 - mDice: 0.5525 - val_loss: 0.6811 - val_acc: 0.9258 - val_mDice: 0.5192

Epoch 00086: val_mDice did not improve from 0.57775
Epoch 87/300
 - 14s - loss: 0.5429 - acc: 0.9247 - mDice: 0.5656 - val_loss: 0.6077 - val_acc: 0.9328 - val_mDice: 0.5489

Epoch 00087: val_mDice did not improve from 0.57775
Epoch 88/300
 - 14s - loss: 0.5389 - acc: 0.9249 - mDice: 0.5684 - val_loss: 0.5783 - val_acc: 0.9341 - val_mDice: 0.5550

Epoch 00088: val_mDice did not improve from 0.57775
Epoch 89/300
 - 13s - loss: 0.5336 - acc: 0.9251 - mDice: 0.5713 - val_loss: 0.5525 - val_acc: 0.9312 - val_mDice: 0.5674

Epoch 00089: val_mDice did not improve from 0.57775
Epoch 90/300
 - 13s - loss: 0.5302 - acc: 0.9251 - mDice: 0.5731 - val_loss: 0.6149 - val_acc: 0.9350 - val_mDice: 0.5514

Epoch 00090: val_mDice did not improve from 0.57775
Epoch 91/300
 - 13s - loss: 0.5276 - acc: 0.9254 - mDice: 0.5744 - val_loss: 0.6093 - val_acc: 0.9299 - val_mDice: 0.5362

Epoch 00091: val_mDice did not improve from 0.57775
Epoch 92/300
 - 13s - loss: 0.5303 - acc: 0.9254 - mDice: 0.5731 - val_loss: 0.6308 - val_acc: 0.9319 - val_mDice: 0.5650

Epoch 00092: val_mDice did not improve from 0.57775
Epoch 93/300
 - 13s - loss: 0.5346 - acc: 0.9255 - mDice: 0.5739 - val_loss: 0.6094 - val_acc: 0.9309 - val_mDice: 0.5456

Epoch 00093: val_mDice did not improve from 0.57775
Epoch 94/300
 - 13s - loss: 0.5272 - acc: 0.9255 - mDice: 0.5747 - val_loss: 0.6362 - val_acc: 0.9282 - val_mDice: 0.5297

Epoch 00094: val_mDice did not improve from 0.57775
Epoch 95/300
 - 14s - loss: 0.5526 - acc: 0.9239 - mDice: 0.5634 - val_loss: 3.0288 - val_acc: 0.9173 - val_mDice: 0.2672

Epoch 00095: val_mDice did not improve from 0.57775
Epoch 96/300
 - 14s - loss: 0.5656 - acc: 0.9231 - mDice: 0.5526 - val_loss: 0.6251 - val_acc: 0.9269 - val_mDice: 0.5354

Epoch 00096: val_mDice did not improve from 0.57775
Epoch 97/300
 - 13s - loss: 0.5354 - acc: 0.9249 - mDice: 0.5701 - val_loss: 0.6777 - val_acc: 0.9349 - val_mDice: 0.5497

Epoch 00097: val_mDice did not improve from 0.57775
Epoch 98/300
 - 13s - loss: 0.5321 - acc: 0.9254 - mDice: 0.5724 - val_loss: 0.5620 - val_acc: 0.9286 - val_mDice: 0.5596

Epoch 00098: val_mDice did not improve from 0.57775
Epoch 99/300
 - 13s - loss: 0.5261 - acc: 0.9257 - mDice: 0.5757 - val_loss: 0.5846 - val_acc: 0.9343 - val_mDice: 0.5581

Epoch 00099: val_mDice did not improve from 0.57775
Epoch 100/300
 - 13s - loss: 0.5474 - acc: 0.9245 - mDice: 0.5670 - val_loss: 0.5759 - val_acc: 0.9368 - val_mDice: 0.5579

Epoch 00100: val_mDice did not improve from 0.57775
Epoch 101/300
 - 13s - loss: 0.5268 - acc: 0.9255 - mDice: 0.5749 - val_loss: 0.6580 - val_acc: 0.9332 - val_mDice: 0.5631

Epoch 00101: val_mDice did not improve from 0.57775
Epoch 102/300
 - 13s - loss: 0.5223 - acc: 0.9258 - mDice: 0.5779 - val_loss: 0.6110 - val_acc: 0.9328 - val_mDice: 0.5498

Epoch 00102: val_mDice did not improve from 0.57775
Epoch 103/300
 - 14s - loss: 0.5192 - acc: 0.9259 - mDice: 0.5794 - val_loss: 0.5824 - val_acc: 0.9297 - val_mDice: 0.5555

Epoch 00103: val_mDice did not improve from 0.57775
Epoch 104/300
 - 14s - loss: 0.5240 - acc: 0.9258 - mDice: 0.5771 - val_loss: 0.6506 - val_acc: 0.9315 - val_mDice: 0.5394

Epoch 00104: val_mDice did not improve from 0.57775
Epoch 105/300
 - 13s - loss: 0.5292 - acc: 0.9258 - mDice: 0.5770 - val_loss: 0.6792 - val_acc: 0.9364 - val_mDice: 0.5511

Epoch 00105: val_mDice did not improve from 0.57775
Epoch 106/300
 - 13s - loss: 0.5288 - acc: 0.9258 - mDice: 0.5778 - val_loss: 0.6259 - val_acc: 0.9321 - val_mDice: 0.5509

Epoch 00106: val_mDice did not improve from 0.57775
Epoch 107/300
 - 13s - loss: 0.5225 - acc: 0.9260 - mDice: 0.5777 - val_loss: 0.6104 - val_acc: 0.9314 - val_mDice: 0.5537

Epoch 00107: val_mDice did not improve from 0.57775
Epoch 108/300
 - 13s - loss: 0.5190 - acc: 0.9261 - mDice: 0.5794 - val_loss: 0.6250 - val_acc: 0.9273 - val_mDice: 0.5299

Epoch 00108: val_mDice did not improve from 0.57775
Restoring model weights from the end of the best epoch
Epoch 00108: early stopping
{'val_loss': [3.6458959276714022, 1.3035067540627938, 1.4021527571652932, 0.9875156185614369, 1.047409388754103, 0.9246657146978631, 1.1662401935410878, 0.8056857768820707, 0.8217471500553152, 0.8596052555810838, 0.77434393905458, 0.8093795653373476, 0.770842262046047, 0.7824958897141553, 0.7650016211958789, 0.9414806974627984, 0.6924756598220301, 0.7138129576804146, 0.9418277302116314, 0.6958758421045131, 0.7362956373149125, 0.6631488885198321, 0.7142437277016817, 3.4988083347441656, 0.7227396305906709, 0.7498786812106137, 0.7159871310784073, 0.6687700250161388, 0.6970079698890606, 0.6658362765160818, 0.6247509289040136, 0.6039136868936045, 0.8008781378231351, 3.465034464679698, 0.679989356843252, 0.6295596499922415, 0.6288675971762844, 0.6917721594452227, 0.6092910277780401, 5.165146991689369, 0.5846541546009205, 0.6244219653505497, 0.7011076208775636, 0.5967660625144918, 0.6624113362302225, 0.6343479434018413, 0.6055080732971272, 0.691963117274027, 0.6422821021584607, 0.9788717567605316, 0.62365205861904, 0.6694448460977545, 0.5984776936195515, 0.6019445588349035, 0.5702732773054213, 0.690450075442198, 0.6281972251556538, 0.5651686555809445, 0.5689434333453103, 0.7898245822815668, 0.66405280778017, 0.6617304681154786, 0.5474690405464677, 0.6572086852063578, 0.7558316124810113, 0.8023668496066301, 0.5738244104006934, 0.5469534029720953, 0.5799840789623362, 0.6326342193537919, 0.6004921636253437, 0.7507936005239133, 0.6182559186188632, 0.7019550491262365, 0.6089706515508985, 0.6414442462895913, 0.5542326875464626, 0.6605315377157202, 0.696460727661375, 0.6816923993605154, 0.9072823193338182, 0.774475873778106, 0.6657441904935887, 0.6140438239410441, 10.603854088556199, 0.6810914974994761, 0.6076727815406032, 0.5782532233094412, 0.5525241117313425, 0.6149273596743428, 0.6093370977532927, 0.6308059239829028, 0.6093701986724107, 0.6362416709541644, 3.028771852059339, 0.6251138976957432, 0.6777018835304907, 0.5620204941936271, 0.5846020812710757, 0.5758588520622758, 0.6579580184013124, 0.6110181243962081, 0.5824086251397612, 0.6505658047855216, 0.6792469475635146, 0.6258927503275493, 0.6104430062745614, 0.6249867431701176], 'val_acc': [0.8620129716459406, 0.9110687646285567, 0.8736584391543474, 0.9054270323622163, 0.8918425264181914, 0.9185693819056112, 0.9238115249487459, 0.9181848199909957, 0.9138169950909085, 0.9184240761888091, 0.91976436263039, 0.9260599578499163, 0.9118528962135315, 0.9174482661580282, 0.9124491438663825, 0.928994822123694, 0.924992716186261, 0.9142165606614774, 0.9069652525836198, 0.9232140442050955, 0.9243300963961889, 0.9307534849202191, 0.9326649387046774, 0.9125468554320159, 0.9195088258496037, 0.9181998589682201, 0.9316716358144447, 0.9329718222063055, 0.9311818713864322, 0.9222169524778134, 0.9235797855589125, 0.9328440708458108, 0.923485850846326, 0.9023180742743154, 0.9286503476440591, 0.9234933849995729, 0.9267388670532791, 0.9340979147840429, 0.9349471920381778, 0.921720926093046, 0.9302060787009183, 0.9327451176744289, 0.9336319550004586, 0.9326900161763347, 0.9245042343619009, 0.9285952363695417, 0.9288895773509193, 0.9297476332023661, 0.9305856120649469, 0.9287543145437089, 0.9311467702426608, 0.927269976605814, 0.9337634737529452, 0.9284186129847531, 0.9334202683161176, 0.9237977483915905, 0.9343885404092295, 0.9321626497324181, 0.9334115061179671, 0.9305680800998022, 0.9321238133642409, 0.9344298596735354, 0.9306194334433823, 0.9332950058437529, 0.9379809896151224, 0.9276156848069852, 0.929592301921239, 0.9364503275149714, 0.9308662137026509, 0.9263944108650167, 0.9351388509311374, 0.932929240206562, 0.9325371652683884, 0.9298265440754159, 0.9237125806076817, 0.9278712143342962, 0.9290211106103564, 0.9273413687786728, 0.9268015045968313, 0.932656172722105, 0.9313434561093649, 0.9180031885545721, 0.9376741080057054, 0.9285213278714942, 0.9111677272609933, 0.9257994139636004, 0.9328440787300231, 0.9340653712787326, 0.9311705811944588, 0.9350073315479137, 0.9298979321484844, 0.9318682850984038, 0.9309213092087438, 0.9282219438325792, 0.9173430740517914, 0.9268553603893865, 0.9348620097473185, 0.9286352859603034, 0.9343384132183418, 0.9367897709210714, 0.9331634754226321, 0.9327601516057574, 0.9296574444367142, 0.931505024433136, 0.936425264234896, 0.93209124904461, 0.9313509435880751, 0.9272587021822651], 'val_mDice': [0.06994534812118641, 0.2937028321324202, 0.27721551722950405, 0.36242950372595006, 0.34644035561374886, 0.41293019245541285, 0.4149430502974798, 0.43954185487101316, 0.4314646569509355, 0.4215100816002599, 0.45779141499882653, 0.45652981662245656, 0.4579884384675001, 0.4581978954966106, 0.4668309587650198, 0.44838313987015416, 0.49550337293160657, 0.4852691478830166, 0.4314054089879233, 0.5006632136289405, 0.48527375226298336, 0.5089332732574019, 0.49849838523006945, 0.21388258426277726, 0.4850424140218704, 0.4864359885927231, 0.5208430097847389, 0.5280380362556094, 0.500203914112515, 0.5078105131785074, 0.5246704233386529, 0.5399229315222887, 0.4946274271717778, 0.22568975144593173, 0.5130672398067656, 0.5249681229944583, 0.5244488268302231, 0.5185348911890908, 0.537154355061748, 0.2070606556676683, 0.549068550268809, 0.535927103940772, 0.5163050656596189, 0.5456768583368372, 0.5165761084783644, 0.530326645525675, 0.5402621945376118, 0.5051093243417286, 0.5286903400269766, 0.47602784381341684, 0.533299225663382, 0.518423214160576, 0.5584374000786474, 0.5421755824770246, 0.5593371927422821, 0.5101933245936399, 0.5463802237359304, 0.5567033398088324, 0.5576344750545643, 0.5041621015185401, 0.5396804062146989, 0.5581781056822923, 0.5661387840906779, 0.5302733873564099, 0.5428098027037564, 0.516452740739893, 0.5536976743627477, 0.5777515784773246, 0.5551084453466708, 0.5273027224515481, 0.544694442913015, 0.5168990524357588, 0.5557959199582458, 0.5199806122552781, 0.5384031193596976, 0.5249225762155321, 0.5648400758309339, 0.5249400186160255, 0.49692766344736494, 0.5243227582759958, 0.5022117757292651, 0.4937074774156803, 0.5709635189601353, 0.5393933546606196, 0.059772345577440565, 0.5192357046263558, 0.5488534692734007, 0.555038917001593, 0.5673664764121726, 0.5514457925917611, 0.5362079320130525, 0.5649566587316927, 0.5456107028577694, 0.5296957376142027, 0.26722829603644277, 0.5353922843933105, 0.5496908442053214, 0.5595868800682996, 0.5581491485474601, 0.5579079802704867, 0.5630995577605313, 0.5497686935480309, 0.5554928234014561, 0.5393846057079457, 0.5510559469934494, 0.5509137760394465, 0.5537239111289776, 0.5298995180104775], 'loss': [2.92143989742367, 1.5151580399899578, 1.1991277421558346, 1.0698686139802744, 0.9966510753444341, 0.9301855718936959, 0.9029636105965203, 0.861638917795578, 0.8327436989914384, 0.810390711244084, 0.8305766274422026, 0.8062259780250863, 0.7637036899529573, 0.7538260086050627, 0.7363836302461015, 0.745128218796007, 0.7262300727329438, 0.701669199550316, 0.7046749925105684, 0.6878424819773199, 0.6793049725806607, 0.6686539934682254, 0.6608578976073791, 0.6713399762484062, 0.7029181911899461, 0.6496291548418349, 0.6439060340161038, 0.6372210721060367, 0.6799723374735299, 0.6455146239362122, 0.6310125364739761, 0.6384871563688321, 0.6245247609880598, 0.6425811476023667, 0.6349190732656518, 0.6124806212729118, 0.6089212093733363, 0.6045807725786755, 0.6040170714179088, 0.6264380012462646, 0.6251642112272191, 0.6022161560050675, 0.5984766245400683, 0.5911940422574675, 0.589638791053314, 0.5879454591130563, 0.5813383459811962, 0.5794359819025247, 0.5802409804529938, 0.585902490186845, 0.5894786207613284, 0.5739510407715183, 0.5734966618481169, 0.603567019190839, 0.5736400763003565, 0.5649979712791457, 0.5668682226320024, 0.5627072541767514, 0.5687300147017639, 0.5614265904385118, 0.5585903433124523, 0.559339243988163, 0.5615215782740974, 0.5656303159890376, 0.5549140252835981, 0.5546018440206151, 0.5526050617653632, 0.5539703154407861, 0.5525533038738173, 0.5491202404935274, 0.5521592816664184, 0.5492937291251899, 0.5456842074895425, 0.5619589682725581, 0.5523009675140899, 0.5415114489100787, 0.5407671906105694, 0.5402858166987162, 0.5848615913404156, 0.5594648448030756, 0.542010161523138, 0.5486455630415339, 0.5376535035663254, 0.535956039767113, 0.5563239790189891, 0.5719095330994332, 0.5429348280109139, 0.5388690103971715, 0.5336229593373957, 0.5302365519425897, 0.5275677506306726, 0.5303393950089237, 0.5345888269317911, 0.5271910371591282, 0.5526361638206843, 0.5655665985655479, 0.5353600496051949, 0.5321207855774157, 0.5260540749035252, 0.5473594261855368, 0.5268267703333485, 0.5222758848219746, 0.5191768775743654, 0.523981758215307, 0.5291812561028076, 0.5288436747516077, 0.5224825133944997, 0.5189981901215962], 'acc': [0.6774829291877127, 0.8832692190796599, 0.8873940921160489, 0.8900687717431875, 0.8930736634313705, 0.8960912369058458, 0.898470892219836, 0.9008375996605684, 0.9028400235533819, 0.9046042975683493, 0.9045093628628198, 0.9057302389385249, 0.9084273041825958, 0.9091063817287779, 0.9102747121732473, 0.9103802368430592, 0.9112425207788968, 0.9126751379970462, 0.9129872943922518, 0.9138402351530034, 0.9143030347448664, 0.9151351581365554, 0.9157826461234898, 0.9158238356754138, 0.9137983983983935, 0.9168523699729322, 0.9174702074875458, 0.9178162670652533, 0.915510174765116, 0.9176137032214932, 0.9184441653338565, 0.9180913524340288, 0.9187622206441893, 0.9182383662452336, 0.9181337891477875, 0.9195215205167133, 0.9199170487472249, 0.9204553553889689, 0.9203336420755106, 0.9192567006025718, 0.9190228273186866, 0.920409241632825, 0.9205165020164433, 0.9209968768969241, 0.9212553330414275, 0.9215407132159177, 0.9218943780307205, 0.9218963442678341, 0.9218516193633784, 0.9217808765802368, 0.9214374059297219, 0.9223701313416031, 0.9224930926203972, 0.9207631177276655, 0.9225563176549062, 0.9229036523888933, 0.9231784827333998, 0.9233360630792601, 0.9234208901657238, 0.9233270914535732, 0.92354734472969, 0.9232610883546944, 0.9232830341246324, 0.9234954961880792, 0.9237231042709529, 0.9237538830335221, 0.9238447026488161, 0.9238109832992192, 0.9237981211909724, 0.92395709267093, 0.9244587226131529, 0.923956748784095, 0.924369222923125, 0.9232232082259251, 0.9241294971233911, 0.9245879053254373, 0.9245135078349127, 0.9247436614384511, 0.922359722485309, 0.9232345196235875, 0.9244876546856015, 0.9243351829998705, 0.9246662584112249, 0.9248444680544831, 0.9238037881504643, 0.9232241569045606, 0.9247153781640456, 0.92494994560303, 0.9250667082549562, 0.9251311648237853, 0.9254304133926947, 0.9253626532348278, 0.9254678545303733, 0.9254543755254068, 0.9239403977284176, 0.9231470075973697, 0.9249073027912337, 0.9254224821088183, 0.9257266356328554, 0.9245060867917156, 0.9254874670087471, 0.9257511293572515, 0.9259351190207608, 0.9258304718556879, 0.9257502731278247, 0.925807669151804, 0.9259729530471976, 0.9261274129789858], 'mDice': [0.09368542759563991, 0.22625560355021104, 0.29298619177351476, 0.33209956223009574, 0.3585718732315942, 0.38151781094299, 0.395529465760595, 0.40907662733697886, 0.4214088407894521, 0.4310673352178447, 0.4248313342813427, 0.43482853829878154, 0.4516503825118512, 0.4567952032048243, 0.46479745474285106, 0.46165877542214384, 0.47043551459051125, 0.4808435508623083, 0.4829707904067882, 0.4877523911515169, 0.4922190296809469, 0.49754020883210925, 0.501011644616611, 0.4992762104752498, 0.481982202283966, 0.5075569655136681, 0.5104072367386903, 0.514172577172084, 0.4941744635355965, 0.509519675781313, 0.5171236707338123, 0.5145205921221725, 0.5209239519025545, 0.5154980433555486, 0.5152151438190076, 0.5272403610885708, 0.5292101341252496, 0.5319856484210898, 0.5321888697092638, 0.5216120499802763, 0.5202947377960605, 0.5331826637158511, 0.535099734112517, 0.5387550055707027, 0.5397577532115387, 0.5407798413630865, 0.5441775203706045, 0.5450759983346881, 0.5447207747684671, 0.5423147438314728, 0.5405776470890848, 0.5483483981897533, 0.5489011946525194, 0.5337524045150681, 0.5484991801824958, 0.5529123793836566, 0.5525343128045788, 0.5547207802362341, 0.5548869548300881, 0.55584759833676, 0.5567782858194312, 0.5563846002906471, 0.5556301641734396, 0.5563297373388875, 0.5591101586917677, 0.5592290898185462, 0.5608148936521056, 0.5601102566944967, 0.5606580230131668, 0.5622084673999564, 0.5643809025346086, 0.5623179456970357, 0.5641643391568947, 0.5556473129776361, 0.5608971534127473, 0.5666249835934852, 0.566914335002398, 0.5673186222393903, 0.5443875667253975, 0.5562246613662271, 0.5661953311827286, 0.5659838553994407, 0.5688586996043463, 0.5699846286017692, 0.5598731547043847, 0.552503145939323, 0.5655830420122093, 0.5683656481894429, 0.57132611696732, 0.5731049673368867, 0.5744264950460669, 0.5730790066406971, 0.5738980192172524, 0.5746729822320445, 0.5633540260287553, 0.5526208736950781, 0.5701141386010212, 0.5723812136145161, 0.5756862687857295, 0.5669836961246042, 0.5749284932072348, 0.5778980893328097, 0.5794272443169179, 0.5771443176935622, 0.5770206035749748, 0.5777786157729047, 0.5776554198892505, 0.5794355092924811]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:02<00:11,  2.76s/it]predicting test subjects:  40%|████      | 2/5 [00:04<00:07,  2.57s/it]predicting test subjects:  60%|██████    | 3/5 [00:06<00:04,  2.37s/it]predicting test subjects:  80%|████████  | 4/5 [00:08<00:02,  2.24s/it]predicting test subjects: 100%|██████████| 5/5 [00:11<00:00,  2.27s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:02<11:48,  2.67s/it]predicting train subjects:   1%|          | 2/266 [00:05<11:30,  2.62s/it]predicting train subjects:   1%|          | 3/266 [00:07<10:40,  2.44s/it]predicting train subjects:   2%|▏         | 4/266 [00:09<09:56,  2.28s/it]predicting train subjects:   2%|▏         | 5/266 [00:11<10:13,  2.35s/it]predicting train subjects:   2%|▏         | 6/266 [00:14<10:33,  2.44s/it]predicting train subjects:   3%|▎         | 7/266 [00:16<10:43,  2.48s/it]predicting train subjects:   3%|▎         | 8/266 [00:19<10:48,  2.51s/it]predicting train subjects:   3%|▎         | 9/266 [00:22<10:56,  2.55s/it]predicting train subjects:   4%|▍         | 10/266 [00:24<10:59,  2.58s/it]predicting train subjects:   4%|▍         | 11/266 [00:27<11:03,  2.60s/it]predicting train subjects:   5%|▍         | 12/266 [00:29<10:58,  2.59s/it]predicting train subjects:   5%|▍         | 13/266 [00:32<10:56,  2.59s/it]predicting train subjects:   5%|▌         | 14/266 [00:35<10:52,  2.59s/it]predicting train subjects:   6%|▌         | 15/266 [00:37<10:50,  2.59s/it]predicting train subjects:   6%|▌         | 16/266 [00:40<10:52,  2.61s/it]predicting train subjects:   6%|▋         | 17/266 [00:42<10:50,  2.61s/it]predicting train subjects:   7%|▋         | 18/266 [00:45<10:47,  2.61s/it]predicting train subjects:   7%|▋         | 19/266 [00:48<10:41,  2.60s/it]predicting train subjects:   8%|▊         | 20/266 [00:50<10:36,  2.59s/it]predicting train subjects:   8%|▊         | 21/266 [00:53<10:33,  2.58s/it]predicting train subjects:   8%|▊         | 22/266 [00:55<10:25,  2.56s/it]predicting train subjects:   9%|▊         | 23/266 [00:58<10:26,  2.58s/it]predicting train subjects:   9%|▉         | 24/266 [01:00<10:14,  2.54s/it]predicting train subjects:   9%|▉         | 25/266 [01:03<09:59,  2.49s/it]predicting train subjects:  10%|▉         | 26/266 [01:05<09:55,  2.48s/it]predicting train subjects:  10%|█         | 27/266 [01:08<09:47,  2.46s/it]predicting train subjects:  11%|█         | 28/266 [01:10<09:44,  2.46s/it]predicting train subjects:  11%|█         | 29/266 [01:12<09:35,  2.43s/it]predicting train subjects:  11%|█▏        | 30/266 [01:15<09:28,  2.41s/it]predicting train subjects:  12%|█▏        | 31/266 [01:17<09:30,  2.43s/it]predicting train subjects:  12%|█▏        | 32/266 [01:20<09:25,  2.42s/it]predicting train subjects:  12%|█▏        | 33/266 [01:22<09:26,  2.43s/it]predicting train subjects:  13%|█▎        | 34/266 [01:24<09:19,  2.41s/it]predicting train subjects:  13%|█▎        | 35/266 [01:27<09:15,  2.41s/it]predicting train subjects:  14%|█▎        | 36/266 [01:29<09:14,  2.41s/it]predicting train subjects:  14%|█▍        | 37/266 [01:32<09:14,  2.42s/it]predicting train subjects:  14%|█▍        | 38/266 [01:34<09:10,  2.41s/it]predicting train subjects:  15%|█▍        | 39/266 [01:37<09:07,  2.41s/it]predicting train subjects:  15%|█▌        | 40/266 [01:39<09:11,  2.44s/it]predicting train subjects:  15%|█▌        | 41/266 [01:41<09:06,  2.43s/it]predicting train subjects:  16%|█▌        | 42/266 [01:43<08:36,  2.30s/it]predicting train subjects:  16%|█▌        | 43/266 [01:45<08:13,  2.21s/it]predicting train subjects:  17%|█▋        | 44/266 [01:47<07:57,  2.15s/it]predicting train subjects:  17%|█▋        | 45/266 [01:49<07:44,  2.10s/it]predicting train subjects:  17%|█▋        | 46/266 [01:51<07:37,  2.08s/it]predicting train subjects:  18%|█▊        | 47/266 [01:53<07:26,  2.04s/it]predicting train subjects:  18%|█▊        | 48/266 [01:55<07:18,  2.01s/it]predicting train subjects:  18%|█▊        | 49/266 [01:57<07:15,  2.01s/it]predicting train subjects:  19%|█▉        | 50/266 [01:59<07:10,  1.99s/it]predicting train subjects:  19%|█▉        | 51/266 [02:01<07:08,  1.99s/it]predicting train subjects:  20%|█▉        | 52/266 [02:03<07:05,  1.99s/it]predicting train subjects:  20%|█▉        | 53/266 [02:05<07:00,  1.98s/it]predicting train subjects:  20%|██        | 54/266 [02:07<06:57,  1.97s/it]predicting train subjects:  21%|██        | 55/266 [02:09<06:53,  1.96s/it]predicting train subjects:  21%|██        | 56/266 [02:11<06:51,  1.96s/it]predicting train subjects:  21%|██▏       | 57/266 [02:13<06:53,  1.98s/it]predicting train subjects:  22%|██▏       | 58/266 [02:15<06:46,  1.95s/it]predicting train subjects:  22%|██▏       | 59/266 [02:17<06:46,  1.96s/it]predicting train subjects:  23%|██▎       | 60/266 [02:19<06:45,  1.97s/it]predicting train subjects:  23%|██▎       | 61/266 [02:21<06:38,  1.94s/it]predicting train subjects:  23%|██▎       | 62/266 [02:23<06:29,  1.91s/it]predicting train subjects:  24%|██▎       | 63/266 [02:25<06:23,  1.89s/it]predicting train subjects:  24%|██▍       | 64/266 [02:26<06:18,  1.87s/it]predicting train subjects:  24%|██▍       | 65/266 [02:28<06:16,  1.87s/it]predicting train subjects:  25%|██▍       | 66/266 [02:30<06:15,  1.88s/it]predicting train subjects:  25%|██▌       | 67/266 [02:32<06:11,  1.87s/it]predicting train subjects:  26%|██▌       | 68/266 [02:34<06:11,  1.88s/it]predicting train subjects:  26%|██▌       | 69/266 [02:36<06:09,  1.87s/it]predicting train subjects:  26%|██▋       | 70/266 [02:38<06:06,  1.87s/it]predicting train subjects:  27%|██▋       | 71/266 [02:40<06:06,  1.88s/it]predicting train subjects:  27%|██▋       | 72/266 [02:41<06:09,  1.90s/it]predicting train subjects:  27%|██▋       | 73/266 [02:43<06:03,  1.89s/it]predicting train subjects:  28%|██▊       | 74/266 [02:45<05:59,  1.87s/it]predicting train subjects:  28%|██▊       | 75/266 [02:47<05:57,  1.87s/it]predicting train subjects:  29%|██▊       | 76/266 [02:49<05:54,  1.87s/it]predicting train subjects:  29%|██▉       | 77/266 [02:51<05:51,  1.86s/it]predicting train subjects:  29%|██▉       | 78/266 [02:53<06:26,  2.06s/it]predicting train subjects:  30%|██▉       | 79/266 [02:56<06:45,  2.17s/it]predicting train subjects:  30%|███       | 80/266 [02:58<06:58,  2.25s/it]predicting train subjects:  30%|███       | 81/266 [03:01<07:08,  2.31s/it]predicting train subjects:  31%|███       | 82/266 [03:03<07:10,  2.34s/it]predicting train subjects:  31%|███       | 83/266 [03:05<07:14,  2.37s/it]predicting train subjects:  32%|███▏      | 84/266 [03:08<07:15,  2.39s/it]predicting train subjects:  32%|███▏      | 85/266 [03:10<07:15,  2.41s/it]predicting train subjects:  32%|███▏      | 86/266 [03:13<07:14,  2.41s/it]predicting train subjects:  33%|███▎      | 87/266 [03:15<07:14,  2.43s/it]predicting train subjects:  33%|███▎      | 88/266 [03:18<07:12,  2.43s/it]predicting train subjects:  33%|███▎      | 89/266 [03:20<07:08,  2.42s/it]predicting train subjects:  34%|███▍      | 90/266 [03:22<07:06,  2.43s/it]predicting train subjects:  34%|███▍      | 91/266 [03:25<07:03,  2.42s/it]predicting train subjects:  35%|███▍      | 92/266 [03:27<07:01,  2.42s/it]predicting train subjects:  35%|███▍      | 93/266 [03:30<06:59,  2.43s/it]predicting train subjects:  35%|███▌      | 94/266 [03:32<06:54,  2.41s/it]predicting train subjects:  36%|███▌      | 95/266 [03:35<06:55,  2.43s/it]predicting train subjects:  36%|███▌      | 96/266 [03:37<06:38,  2.34s/it]predicting train subjects:  36%|███▋      | 97/266 [03:39<06:43,  2.39s/it]predicting train subjects:  37%|███▋      | 98/266 [03:42<06:40,  2.38s/it]predicting train subjects:  37%|███▋      | 99/266 [03:43<06:02,  2.17s/it]predicting train subjects:  38%|███▊      | 100/266 [03:45<05:50,  2.11s/it]predicting train subjects:  38%|███▊      | 101/266 [03:47<05:47,  2.10s/it]predicting train subjects:  38%|███▊      | 102/266 [03:49<05:45,  2.11s/it]predicting train subjects:  39%|███▊      | 103/266 [03:52<05:43,  2.11s/it]predicting train subjects:  39%|███▉      | 104/266 [03:54<05:42,  2.12s/it]predicting train subjects:  39%|███▉      | 105/266 [03:56<05:38,  2.10s/it]predicting train subjects:  40%|███▉      | 106/266 [03:58<05:36,  2.11s/it]predicting train subjects:  40%|████      | 107/266 [04:00<05:35,  2.11s/it]predicting train subjects:  41%|████      | 108/266 [04:02<05:37,  2.14s/it]predicting train subjects:  41%|████      | 109/266 [04:04<05:37,  2.15s/it]predicting train subjects:  41%|████▏     | 110/266 [04:07<05:37,  2.16s/it]predicting train subjects:  42%|████▏     | 111/266 [04:09<05:40,  2.20s/it]predicting train subjects:  42%|████▏     | 112/266 [04:11<05:41,  2.21s/it]predicting train subjects:  42%|████▏     | 113/266 [04:13<05:33,  2.18s/it]predicting train subjects:  43%|████▎     | 114/266 [04:15<05:29,  2.17s/it]predicting train subjects:  43%|████▎     | 115/266 [04:18<05:28,  2.17s/it]predicting train subjects:  44%|████▎     | 116/266 [04:20<05:26,  2.18s/it]predicting train subjects:  44%|████▍     | 117/266 [04:22<05:26,  2.19s/it]predicting train subjects:  44%|████▍     | 118/266 [04:24<05:22,  2.18s/it]predicting train subjects:  45%|████▍     | 119/266 [04:27<05:39,  2.31s/it]predicting train subjects:  45%|████▌     | 120/266 [04:29<05:41,  2.34s/it]predicting train subjects:  45%|████▌     | 121/266 [04:32<05:44,  2.37s/it]predicting train subjects:  46%|████▌     | 122/266 [04:34<05:49,  2.42s/it]predicting train subjects:  46%|████▌     | 123/266 [04:37<05:52,  2.46s/it]predicting train subjects:  47%|████▋     | 124/266 [04:39<05:49,  2.46s/it]predicting train subjects:  47%|████▋     | 125/266 [04:42<05:51,  2.49s/it]predicting train subjects:  47%|████▋     | 126/266 [04:44<05:49,  2.50s/it]predicting train subjects:  48%|████▊     | 127/266 [04:47<05:46,  2.49s/it]predicting train subjects:  48%|████▊     | 128/266 [04:49<05:43,  2.49s/it]predicting train subjects:  48%|████▊     | 129/266 [04:52<05:37,  2.47s/it]predicting train subjects:  49%|████▉     | 130/266 [04:54<05:35,  2.47s/it]predicting train subjects:  49%|████▉     | 131/266 [04:56<05:32,  2.47s/it]predicting train subjects:  50%|████▉     | 132/266 [04:59<05:32,  2.48s/it]predicting train subjects:  50%|█████     | 133/266 [05:02<05:33,  2.51s/it]predicting train subjects:  50%|█████     | 134/266 [05:04<05:31,  2.51s/it]predicting train subjects:  51%|█████     | 135/266 [05:07<05:27,  2.50s/it]predicting train subjects:  51%|█████     | 136/266 [05:09<05:22,  2.48s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:12<05:23,  2.51s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:14<05:17,  2.48s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:17<05:19,  2.51s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:19<05:11,  2.47s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:21<05:03,  2.43s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:24<04:59,  2.42s/it]predicting train subjects:  54%|█████▍    | 143/266 [05:26<04:56,  2.41s/it]predicting train subjects:  54%|█████▍    | 144/266 [05:29<05:00,  2.46s/it]predicting train subjects:  55%|█████▍    | 145/266 [05:31<05:04,  2.51s/it]predicting train subjects:  55%|█████▍    | 146/266 [05:34<04:58,  2.49s/it]predicting train subjects:  55%|█████▌    | 147/266 [05:36<04:54,  2.48s/it]predicting train subjects:  56%|█████▌    | 148/266 [05:39<04:49,  2.45s/it]predicting train subjects:  56%|█████▌    | 149/266 [05:41<04:47,  2.46s/it]predicting train subjects:  56%|█████▋    | 150/266 [05:43<04:43,  2.45s/it]predicting train subjects:  57%|█████▋    | 151/266 [05:46<04:38,  2.42s/it]predicting train subjects:  57%|█████▋    | 152/266 [05:48<04:36,  2.43s/it]predicting train subjects:  58%|█████▊    | 153/266 [05:51<04:33,  2.42s/it]predicting train subjects:  58%|█████▊    | 154/266 [05:53<04:30,  2.41s/it]predicting train subjects:  58%|█████▊    | 155/266 [05:55<04:07,  2.23s/it]predicting train subjects:  59%|█████▊    | 156/266 [05:57<03:48,  2.08s/it]predicting train subjects:  59%|█████▉    | 157/266 [05:58<03:35,  1.98s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:00<03:25,  1.91s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:02<03:20,  1.87s/it]predicting train subjects:  60%|██████    | 160/266 [06:04<03:14,  1.84s/it]predicting train subjects:  61%|██████    | 161/266 [06:05<03:10,  1.81s/it]predicting train subjects:  61%|██████    | 162/266 [06:07<03:07,  1.80s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:09<03:05,  1.80s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:11<03:02,  1.79s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:12<03:00,  1.78s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:14<02:57,  1.77s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:16<02:56,  1.78s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:18<02:56,  1.80s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:20<02:52,  1.78s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:21<02:49,  1.77s/it]predicting train subjects:  64%|██████▍   | 171/266 [06:23<02:47,  1.76s/it]predicting train subjects:  65%|██████▍   | 172/266 [06:25<02:44,  1.75s/it]predicting train subjects:  65%|██████▌   | 173/266 [06:27<02:48,  1.82s/it]predicting train subjects:  65%|██████▌   | 174/266 [06:29<02:50,  1.85s/it]predicting train subjects:  66%|██████▌   | 175/266 [06:31<02:51,  1.88s/it]predicting train subjects:  66%|██████▌   | 176/266 [06:33<02:51,  1.91s/it]predicting train subjects:  67%|██████▋   | 177/266 [06:35<02:50,  1.91s/it]predicting train subjects:  67%|██████▋   | 178/266 [06:37<02:49,  1.93s/it]predicting train subjects:  67%|██████▋   | 179/266 [06:39<02:49,  1.94s/it]predicting train subjects:  68%|██████▊   | 180/266 [06:41<02:49,  1.97s/it]predicting train subjects:  68%|██████▊   | 181/266 [06:43<02:49,  1.99s/it]predicting train subjects:  68%|██████▊   | 182/266 [06:45<02:46,  1.98s/it]predicting train subjects:  69%|██████▉   | 183/266 [06:47<02:45,  2.00s/it]predicting train subjects:  69%|██████▉   | 184/266 [06:49<02:42,  1.98s/it]predicting train subjects:  70%|██████▉   | 185/266 [06:50<02:39,  1.97s/it]predicting train subjects:  70%|██████▉   | 186/266 [06:52<02:38,  1.98s/it]predicting train subjects:  70%|███████   | 187/266 [06:54<02:36,  1.98s/it]predicting train subjects:  71%|███████   | 188/266 [06:56<02:34,  1.98s/it]predicting train subjects:  71%|███████   | 189/266 [06:59<02:34,  2.01s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:00<02:31,  1.99s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:03<02:33,  2.05s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:05<02:28,  2.00s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:06<02:25,  1.99s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:09<02:33,  2.13s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:11<02:29,  2.11s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:13<02:27,  2.11s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:15<02:27,  2.14s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:18<02:26,  2.16s/it]predicting train subjects:  75%|███████▍  | 199/266 [07:20<02:23,  2.14s/it]predicting train subjects:  75%|███████▌  | 200/266 [07:22<02:21,  2.14s/it]predicting train subjects:  76%|███████▌  | 201/266 [07:24<02:19,  2.15s/it]predicting train subjects:  76%|███████▌  | 202/266 [07:26<02:17,  2.15s/it]predicting train subjects:  76%|███████▋  | 203/266 [07:28<02:15,  2.15s/it]predicting train subjects:  77%|███████▋  | 204/266 [07:30<02:12,  2.14s/it]predicting train subjects:  77%|███████▋  | 205/266 [07:32<02:10,  2.15s/it]predicting train subjects:  77%|███████▋  | 206/266 [07:35<02:08,  2.15s/it]predicting train subjects:  78%|███████▊  | 207/266 [07:37<02:05,  2.13s/it]predicting train subjects:  78%|███████▊  | 208/266 [07:39<02:03,  2.13s/it]predicting train subjects:  79%|███████▊  | 209/266 [07:41<02:00,  2.12s/it]predicting train subjects:  79%|███████▉  | 210/266 [07:43<01:57,  2.10s/it]predicting train subjects:  79%|███████▉  | 211/266 [07:45<01:55,  2.10s/it]predicting train subjects:  80%|███████▉  | 212/266 [07:47<01:52,  2.09s/it]predicting train subjects:  80%|████████  | 213/266 [07:49<01:46,  2.00s/it]predicting train subjects:  80%|████████  | 214/266 [07:51<01:42,  1.98s/it]predicting train subjects:  81%|████████  | 215/266 [07:53<01:40,  1.98s/it]predicting train subjects:  81%|████████  | 216/266 [07:55<01:38,  1.98s/it]predicting train subjects:  82%|████████▏ | 217/266 [07:57<01:36,  1.97s/it]predicting train subjects:  82%|████████▏ | 218/266 [07:59<01:33,  1.95s/it]predicting train subjects:  82%|████████▏ | 219/266 [08:01<01:30,  1.93s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:03<01:28,  1.93s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:04<01:26,  1.93s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:06<01:25,  1.95s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:08<01:22,  1.92s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:10<01:19,  1.90s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:12<01:18,  1.93s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:14<01:16,  1.92s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:16<01:13,  1.90s/it]predicting train subjects:  86%|████████▌ | 228/266 [08:18<01:11,  1.89s/it]predicting train subjects:  86%|████████▌ | 229/266 [08:20<01:10,  1.90s/it]predicting train subjects:  86%|████████▋ | 230/266 [08:22<01:08,  1.91s/it]predicting train subjects:  87%|████████▋ | 231/266 [08:24<01:06,  1.91s/it]predicting train subjects:  87%|████████▋ | 232/266 [08:25<01:04,  1.90s/it]predicting train subjects:  88%|████████▊ | 233/266 [08:27<01:02,  1.90s/it]predicting train subjects:  88%|████████▊ | 234/266 [08:29<01:00,  1.89s/it]predicting train subjects:  88%|████████▊ | 235/266 [08:31<00:59,  1.91s/it]predicting train subjects:  89%|████████▊ | 236/266 [08:33<00:58,  1.94s/it]predicting train subjects:  89%|████████▉ | 237/266 [08:35<00:56,  1.93s/it]predicting train subjects:  89%|████████▉ | 238/266 [08:37<00:53,  1.92s/it]predicting train subjects:  90%|████████▉ | 239/266 [08:39<00:51,  1.91s/it]predicting train subjects:  90%|█████████ | 240/266 [08:41<00:49,  1.89s/it]predicting train subjects:  91%|█████████ | 241/266 [08:43<00:47,  1.91s/it]predicting train subjects:  91%|█████████ | 242/266 [08:45<00:45,  1.91s/it]predicting train subjects:  91%|█████████▏| 243/266 [08:46<00:43,  1.91s/it]predicting train subjects:  92%|█████████▏| 244/266 [08:48<00:42,  1.91s/it]predicting train subjects:  92%|█████████▏| 245/266 [08:50<00:40,  1.92s/it]predicting train subjects:  92%|█████████▏| 246/266 [08:52<00:38,  1.91s/it]predicting train subjects:  93%|█████████▎| 247/266 [08:54<00:36,  1.90s/it]predicting train subjects:  93%|█████████▎| 248/266 [08:56<00:33,  1.89s/it]predicting train subjects:  94%|█████████▎| 249/266 [08:58<00:34,  2.05s/it]predicting train subjects:  94%|█████████▍| 250/266 [09:01<00:34,  2.13s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:03<00:33,  2.22s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:05<00:31,  2.26s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:08<00:30,  2.34s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:10<00:28,  2.39s/it]predicting train subjects:  96%|█████████▌| 255/266 [09:13<00:26,  2.41s/it]predicting train subjects:  96%|█████████▌| 256/266 [09:15<00:24,  2.42s/it]predicting train subjects:  97%|█████████▋| 257/266 [09:18<00:21,  2.42s/it]predicting train subjects:  97%|█████████▋| 258/266 [09:20<00:19,  2.41s/it]predicting train subjects:  97%|█████████▋| 259/266 [09:23<00:16,  2.40s/it]predicting train subjects:  98%|█████████▊| 260/266 [09:25<00:14,  2.41s/it]predicting train subjects:  98%|█████████▊| 261/266 [09:27<00:12,  2.41s/it]predicting train subjects:  98%|█████████▊| 262/266 [09:30<00:09,  2.39s/it]predicting train subjects:  99%|█████████▉| 263/266 [09:32<00:07,  2.38s/it]predicting train subjects:  99%|█████████▉| 264/266 [09:34<00:04,  2.38s/it]predicting train subjects: 100%|█████████▉| 265/266 [09:37<00:02,  2.39s/it]predicting train subjects: 100%|██████████| 266/266 [09:39<00:00,  2.38s/it]

