2019-04-19 18:58:41.025215: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-19 18:58:43.000724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-04-19 18:58:43.000803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-04-19 18:58:43.399057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 18:58:43.399135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 18:58:43.399149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 18:58:43.399608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:   1%|          | 1/148 [00:00<00:20,  7.13it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:20,  7.27it/s]Loading Dataset: train:   2%|▏         | 3/148 [00:00<00:19,  7.52it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:17,  8.02it/s]Loading Dataset: train:   3%|▎         | 5/148 [00:00<00:16,  8.49it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:16,  8.40it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:17,  7.86it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:01<00:16,  8.21it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:01<00:16,  8.55it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:14,  9.26it/s]Loading Dataset: train:   9%|▉         | 14/148 [00:01<00:14,  9.22it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:14,  9.31it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:13,  9.59it/s]Loading Dataset: train:  12%|█▏        | 18/148 [00:01<00:13,  9.31it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:02<00:14,  8.86it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:02<00:14,  8.88it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:02<00:14,  8.74it/s]Loading Dataset: train:  16%|█▌        | 24/148 [00:02<00:14,  8.73it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:14,  8.63it/s]Loading Dataset: train:  18%|█▊        | 26/148 [00:02<00:14,  8.41it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:03<00:14,  8.35it/s]Loading Dataset: train:  19%|█▉        | 28/148 [00:03<00:14,  8.30it/s]Loading Dataset: train:  20%|█▉        | 29/148 [00:03<00:14,  8.12it/s]Loading Dataset: train:  20%|██        | 30/148 [00:03<00:14,  8.28it/s]Loading Dataset: train:  21%|██        | 31/148 [00:03<00:14,  8.29it/s]Loading Dataset: train:  22%|██▏       | 32/148 [00:03<00:14,  8.19it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:03<00:13,  8.30it/s]Loading Dataset: train:  23%|██▎       | 34/148 [00:03<00:13,  8.16it/s]Loading Dataset: train:  24%|██▎       | 35/148 [00:04<00:13,  8.26it/s]Loading Dataset: train:  24%|██▍       | 36/148 [00:04<00:13,  8.35it/s]Loading Dataset: train:  25%|██▌       | 37/148 [00:04<00:13,  8.29it/s]Loading Dataset: train:  26%|██▌       | 38/148 [00:04<00:13,  8.44it/s]Loading Dataset: train:  26%|██▋       | 39/148 [00:04<00:12,  8.57it/s]Loading Dataset: train:  27%|██▋       | 40/148 [00:04<00:13,  8.29it/s]Loading Dataset: train:  28%|██▊       | 41/148 [00:04<00:12,  8.57it/s]Loading Dataset: train:  28%|██▊       | 42/148 [00:04<00:12,  8.73it/s]Loading Dataset: train:  29%|██▉       | 43/148 [00:04<00:12,  8.66it/s]Loading Dataset: train:  30%|██▉       | 44/148 [00:05<00:11,  8.81it/s]Loading Dataset: train:  30%|███       | 45/148 [00:05<00:11,  8.72it/s]2019-04-19 18:58:49.447803: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading Dataset: train:  31%|███       | 46/148 [00:05<00:11,  8.79it/s]Loading Dataset: train:  32%|███▏      | 47/148 [00:05<00:11,  8.76it/s]Loading Dataset: train:  32%|███▏      | 48/148 [00:05<00:11,  8.98it/s]Loading Dataset: train:  33%|███▎      | 49/148 [00:05<00:10,  9.21it/s]Loading Dataset: train:  34%|███▍      | 50/148 [00:05<00:10,  9.16it/s]Loading Dataset: train:  34%|███▍      | 51/148 [00:05<00:10,  9.34it/s]Loading Dataset: train:  35%|███▌      | 52/148 [00:05<00:10,  9.44it/s]Loading Dataset: train:  36%|███▌      | 53/148 [00:06<00:10,  9.00it/s]Loading Dataset: train:  36%|███▋      | 54/148 [00:06<00:10,  8.87it/s]Loading Dataset: train:  37%|███▋      | 55/148 [00:06<00:10,  8.66it/s]Loading Dataset: train:  38%|███▊      | 56/148 [00:06<00:10,  8.78it/s]Loading Dataset: train:  39%|███▊      | 57/148 [00:06<00:10,  8.77it/s]Loading Dataset: train:  39%|███▉      | 58/148 [00:06<00:10,  8.62it/s]Loading Dataset: train:  40%|███▉      | 59/148 [00:06<00:11,  8.07it/s]Loading Dataset: train:  41%|████      | 60/148 [00:06<00:11,  7.96it/s]Loading Dataset: train:  41%|████      | 61/148 [00:07<00:11,  7.87it/s]Loading Dataset: train:  42%|████▏     | 62/148 [00:07<00:11,  7.28it/s]Loading Dataset: train:  43%|████▎     | 63/148 [00:07<00:11,  7.14it/s]Loading Dataset: train:  43%|████▎     | 64/148 [00:07<00:11,  7.03it/s]Loading Dataset: train:  45%|████▍     | 66/148 [00:07<00:10,  7.83it/s]Loading Dataset: train:  46%|████▌     | 68/148 [00:07<00:09,  8.61it/s]Loading Dataset: train:  47%|████▋     | 70/148 [00:08<00:08,  9.16it/s]Loading Dataset: train:  48%|████▊     | 71/148 [00:08<00:09,  7.95it/s]2019-04-19 18:58:52.541897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:85:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-04-19 18:58:52.541973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
Loading Dataset: train:  49%|████▊     | 72/148 [00:08<00:09,  8.24it/s]Loading Dataset: train:  49%|████▉     | 73/148 [00:08<00:08,  8.56it/s]Loading Dataset: train:  50%|█████     | 74/148 [00:08<00:08,  8.81it/s]Loading Dataset: train:  51%|█████     | 75/148 [00:08<00:08,  8.86it/s]2019-04-19 18:58:52.952358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 18:58:52.952422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 18:58:52.952437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 18:58:52.952910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
Loading Dataset: train:  51%|█████▏    | 76/148 [00:08<00:08,  8.76it/s]Loading Dataset: train:  53%|█████▎    | 78/148 [00:08<00:07,  9.12it/s]Loading Dataset: train:  53%|█████▎    | 79/148 [00:09<00:07,  9.10it/s]Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:  54%|█████▍    | 80/148 [00:09<00:09,  7.46it/s]Loading Dataset: train:  55%|█████▍    | 81/148 [00:09<00:08,  8.06it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:13, 10.96it/s]Loading Dataset: train:  55%|█████▌    | 82/148 [00:09<00:07,  8.34it/s]Loading Dataset: train:  56%|█████▌    | 83/148 [00:09<00:07,  8.48it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:12, 11.19it/s]Loading Dataset: train:  57%|█████▋    | 84/148 [00:09<00:07,  8.77it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:12, 11.25it/s]Loading Dataset: train:  57%|█████▋    | 85/148 [00:09<00:06,  9.02it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:13, 10.63it/s]Loading Dataset: train:  58%|█████▊    | 86/148 [00:09<00:06,  8.97it/s]Loading Dataset: train:  59%|█████▉    | 87/148 [00:10<00:06,  9.22it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:00<00:12, 11.39it/s]Loading Dataset: train:  59%|█████▉    | 88/148 [00:10<00:06,  9.28it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:00<00:11, 11.84it/s]Loading Dataset: train:  60%|██████    | 89/148 [00:10<00:06,  9.28it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:10, 12.61it/s]Loading Dataset: train:  61%|██████    | 90/148 [00:10<00:08,  7.17it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:11, 11.98it/s]Loading Dataset: train:  62%|██████▏   | 92/148 [00:10<00:07,  7.85it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:10, 12.11it/s]Loading Dataset: train:  63%|██████▎   | 93/148 [00:10<00:06,  8.24it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:01<00:11, 11.60it/s]Loading Dataset: train:  64%|██████▍   | 95/148 [00:10<00:05,  8.87it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:01<00:10, 11.63it/s]Loading Dataset: train:  66%|██████▌   | 97/148 [00:11<00:05,  9.93it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:01<00:10, 11.76it/s]Loading Dataset: train:  67%|██████▋   | 99/148 [00:11<00:04, 10.49it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:10, 11.60it/s]Loading Dataset: train:  68%|██████▊   | 101/148 [00:11<00:04, 10.59it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:02<00:10, 11.52it/s]Loading Dataset: train:  70%|██████▉   | 103/148 [00:11<00:04, 10.02it/s]Loading Dataset: train:  20%|█▉        | 29/148 [00:02<00:10, 11.48it/s]Loading Dataset: train:  71%|███████   | 105/148 [00:11<00:04,  9.69it/s]Loading Dataset: train:  21%|██        | 31/148 [00:02<00:10, 11.34it/s]Loading Dataset: train:  72%|███████▏  | 106/148 [00:11<00:04,  9.59it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:02<00:10, 11.35it/s]Loading Dataset: train:  72%|███████▏  | 107/148 [00:12<00:04,  9.30it/s]Loading Dataset: train:  73%|███████▎  | 108/148 [00:12<00:04,  9.41it/s]Loading Dataset: train:  24%|██▎       | 35/148 [00:02<00:09, 11.47it/s]Loading Dataset: train:  74%|███████▎  | 109/148 [00:12<00:04,  9.34it/s]2019-04-19 18:58:56.625761: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading Dataset: train:  74%|███████▍  | 110/148 [00:12<00:04,  9.10it/s]Loading Dataset: train:  25%|██▌       | 37/148 [00:03<00:09, 11.53it/s]Loading Dataset: train:  75%|███████▌  | 111/148 [00:12<00:04,  9.25it/s]Loading Dataset: train:  26%|██▋       | 39/148 [00:03<00:09, 11.56it/s]Loading Dataset: train:  76%|███████▌  | 112/148 [00:12<00:03,  9.30it/s]Loading Dataset: train:  28%|██▊       | 41/148 [00:03<00:09, 11.67it/s]Loading Dataset: train:  77%|███████▋  | 114/148 [00:12<00:03, 10.10it/s]Loading Dataset: train:  29%|██▉       | 43/148 [00:03<00:08, 11.89it/s]Loading Dataset: train:  78%|███████▊  | 116/148 [00:12<00:02, 10.95it/s]Loading Dataset: train:  30%|███       | 45/148 [00:03<00:08, 12.03it/s]Loading Dataset: train:  80%|███████▉  | 118/148 [00:13<00:02, 11.39it/s]Loading Dataset: train:  32%|███▏      | 47/148 [00:03<00:08, 12.11it/s]Loading Dataset: train:  81%|████████  | 120/148 [00:13<00:02, 10.16it/s]Loading Dataset: train:  33%|███▎      | 49/148 [00:04<00:08, 12.17it/s]Loading Dataset: train:  34%|███▍      | 51/148 [00:04<00:07, 12.31it/s]Loading Dataset: train:  82%|████████▏ | 122/148 [00:13<00:02,  9.43it/s]Loading Dataset: train:  83%|████████▎ | 123/148 [00:13<00:02,  9.19it/s]Loading Dataset: train:  36%|███▌      | 53/148 [00:04<00:07, 12.14it/s]Loading Dataset: train:  84%|████████▍ | 124/148 [00:13<00:02,  8.99it/s]Loading Dataset: train:  37%|███▋      | 55/148 [00:04<00:07, 11.95it/s]Loading Dataset: train:  84%|████████▍ | 125/148 [00:13<00:02,  8.57it/s]Loading Dataset: train:  85%|████████▌ | 126/148 [00:14<00:02,  8.41it/s]Loading Dataset: train:  39%|███▊      | 57/148 [00:04<00:07, 11.82it/s]Loading Dataset: train:  86%|████████▌ | 127/148 [00:14<00:02,  8.37it/s]Loading Dataset: train:  40%|███▉      | 59/148 [00:05<00:07, 11.32it/s]Loading Dataset: train:  86%|████████▋ | 128/148 [00:14<00:02,  8.28it/s]Loading Dataset: train:  87%|████████▋ | 129/148 [00:14<00:02,  8.09it/s]Loading Dataset: train:  41%|████      | 61/148 [00:05<00:08, 10.80it/s]Loading Dataset: train:  88%|████████▊ | 130/148 [00:14<00:02,  8.14it/s]Loading Dataset: train:  89%|████████▊ | 131/148 [00:14<00:02,  8.26it/s]Loading Dataset: train:  43%|████▎     | 63/148 [00:05<00:08, 10.44it/s]Loading Dataset: train:  89%|████████▉ | 132/148 [00:14<00:01,  8.56it/s]Loading Dataset: train:  44%|████▍     | 65/148 [00:05<00:07, 10.82it/s]Loading Dataset: train:  90%|████████▉ | 133/148 [00:14<00:01,  8.73it/s]Loading Dataset: train:  45%|████▌     | 67/148 [00:05<00:06, 11.77it/s]Loading Dataset: train:  91%|█████████ | 134/148 [00:15<00:01,  8.68it/s]Loading Dataset: train:  91%|█████████ | 135/148 [00:15<00:01,  8.89it/s]Loading Dataset: train:  47%|████▋     | 69/148 [00:05<00:06, 12.18it/s]Loading Dataset: train:  92%|█████████▏| 136/148 [00:15<00:01,  8.77it/s]Loading Dataset: train:  48%|████▊     | 71/148 [00:06<00:06, 11.55it/s]Loading Dataset: train:  93%|█████████▎| 137/148 [00:15<00:01,  8.04it/s]Loading Dataset: train:  93%|█████████▎| 138/148 [00:15<00:01,  8.12it/s]2019-04-19 18:58:59.771195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:88:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-04-19 18:58:59.771258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
Loading Dataset: train:  49%|████▉     | 73/148 [00:06<00:07, 10.38it/s]Loading Dataset: train:  95%|█████████▍| 140/148 [00:15<00:00,  8.85it/s]Loading Dataset: train:  51%|█████     | 75/148 [00:06<00:06, 10.82it/s]Loading Dataset: train:  96%|█████████▌| 142/148 [00:15<00:00,  9.38it/s]Loading Dataset: train:  52%|█████▏    | 77/148 [00:06<00:06, 11.00it/s]Loading Dataset: train:  97%|█████████▋| 143/148 [00:16<00:00,  9.04it/s]Loading Dataset: train:  97%|█████████▋| 144/148 [00:16<00:00,  9.09it/s]Loading Dataset: train:  53%|█████▎    | 79/148 [00:06<00:06, 10.99it/s]Loading Dataset: train:  98%|█████████▊| 145/148 [00:16<00:00,  9.10it/s]Loading Dataset: train:  55%|█████▍    | 81/148 [00:07<00:06, 11.00it/s]Loading Dataset: train:  99%|█████████▊| 146/148 [00:16<00:00,  9.04it/s]Loading Dataset: train:  99%|█████████▉| 147/148 [00:16<00:00,  8.87it/s]Loading Dataset: train:  56%|█████▌    | 83/148 [00:07<00:05, 11.07it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:16<00:00,  8.93it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]2019-04-19 18:59:00.841648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 18:59:00.841728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 18:59:00.841742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 18:59:00.845688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)
Loading Dataset: train:  57%|█████▋    | 85/148 [00:07<00:05, 10.97it/s]concatenating train images:   5%|▍         | 7/148 [00:00<00:02, 63.72it/s]concatenating train images:   9%|▉         | 14/148 [00:00<00:02, 64.96it/s]Loading Dataset: train:  59%|█████▉    | 87/148 [00:07<00:05, 10.92it/s]concatenating train images:  14%|█▍        | 21/148 [00:00<00:01, 65.51it/s]concatenating train images:  19%|█▉        | 28/148 [00:00<00:01, 65.07it/s]Loading Dataset: train:  60%|██████    | 89/148 [00:07<00:05, 10.81it/s]concatenating train images:  24%|██▎       | 35/148 [00:00<00:01, 64.65it/s]concatenating train images:  28%|██▊       | 42/148 [00:00<00:01, 64.37it/s]Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:  61%|██████▏   | 91/148 [00:08<00:06,  9.34it/s]concatenating train images:  33%|███▎      | 49/148 [00:00<00:01, 64.15it/s]concatenating train images:  38%|███▊      | 56/148 [00:00<00:01, 64.38it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:12, 11.41it/s]Loading Dataset: train:  63%|██████▎   | 93/148 [00:08<00:05,  9.87it/s]concatenating train images:  43%|████▎     | 63/148 [00:00<00:01, 62.92it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:12, 11.54it/s]concatenating train images:  48%|████▊     | 71/148 [00:01<00:01, 65.01it/s]Loading Dataset: train:  64%|██████▍   | 95/148 [00:08<00:05, 10.42it/s]Loading Dataset: train:  66%|██████▌   | 97/148 [00:08<00:04, 12.16it/s]concatenating train images:  53%|█████▎    | 78/148 [00:01<00:01, 65.03it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:12, 11.69it/s]Loading Dataset: train:  67%|██████▋   | 99/148 [00:08<00:03, 13.72it/s]concatenating train images:  57%|█████▋    | 85/148 [00:01<00:00, 65.82it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:13, 10.80it/s]concatenating train images:  62%|██████▏   | 92/148 [00:01<00:00, 65.77it/s]Loading Dataset: train:  68%|██████▊   | 101/148 [00:08<00:03, 13.94it/s]2019-04-19 18:59:02.237465: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
concatenating train images:  67%|██████▋   | 99/148 [00:01<00:00, 66.44it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:00<00:12, 11.34it/s]Loading Dataset: train:  70%|██████▉   | 103/148 [00:08<00:03, 13.13it/s]concatenating train images:  72%|███████▏  | 107/148 [00:01<00:00, 67.56it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:00<00:11, 11.83it/s]concatenating train images:  78%|███████▊  | 115/148 [00:01<00:00, 69.30it/s]Loading Dataset: train:  71%|███████   | 105/148 [00:09<00:03, 12.62it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:10, 12.63it/s]concatenating train images:  83%|████████▎ | 123/148 [00:01<00:00, 69.48it/s]concatenating train images:  88%|████████▊ | 130/148 [00:01<00:00, 69.30it/s]Loading Dataset: train:  72%|███████▏  | 107/148 [00:09<00:03, 11.88it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:10, 12.32it/s]concatenating train images:  93%|█████████▎| 138/148 [00:02<00:00, 70.76it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:10, 13.07it/s]Loading Dataset: train:  74%|███████▎  | 109/148 [00:09<00:03, 11.88it/s]concatenating train images:  99%|█████████▊| 146/148 [00:02<00:00, 71.89it/s]concatenating train images: 100%|██████████| 148/148 [00:02<00:00, 67.40it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:01<00:10, 12.48it/s]Loading Dataset: train:  75%|███████▌  | 111/148 [00:09<00:03, 11.85it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:01<00:10, 12.62it/s]Loading Dataset: train:  76%|███████▋  | 113/148 [00:09<00:02, 12.62it/s]Loading Dataset: train:  78%|███████▊  | 115/148 [00:09<00:02, 14.11it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:01<00:10, 12.21it/s]Loading Dataset: train:  79%|███████▉  | 117/148 [00:09<00:02, 14.23it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:11, 10.58it/s]Loading Dataset: train:  80%|████████  | 119/148 [00:10<00:02, 12.44it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:02<00:11, 10.65it/s]Loading Dataset: train:  82%|████████▏ | 121/148 [00:10<00:02, 11.66it/s]2019-04-19 18:59:04.000726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:89:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-04-19 18:59:04.000833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
Loading Dataset: train:  20%|█▉        | 29/148 [00:02<00:11, 10.80it/s]Loading Dataset: train:  83%|████████▎ | 123/148 [00:10<00:02, 11.48it/s]Loading Dataset: train:  21%|██        | 31/148 [00:02<00:10, 10.95it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: train:  84%|████████▍ | 125/148 [00:10<00:02, 11.21it/s]Loading Dataset: test:  20%|██        | 1/5 [00:00<00:00,  8.21it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:02<00:10, 11.05it/s]2019-04-19 18:59:04.404203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 18:59:04.404273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 18:59:04.404286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 18:59:04.404772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
Loading Dataset: train:  86%|████████▌ | 127/148 [00:10<00:01, 10.93it/s]Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00,  8.12it/s]Loading Dataset: train:  24%|██▎       | 35/148 [00:03<00:10, 11.22it/s]Loading Dataset: test:  80%|████████  | 4/5 [00:00<00:00,  9.00it/s]Loading Dataset: train:  87%|████████▋ | 129/148 [00:11<00:01, 10.61it/s]Loading Dataset: train:  25%|██▌       | 37/148 [00:03<00:09, 11.20it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00,  8.45it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 68.14it/s]Loading Dataset: train:  89%|████████▊ | 131/148 [00:11<00:01, 10.62it/s]
Loading Dataset: trainS:   0%|          | 0/148 [00:00<?, ?it/s]Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:  26%|██▋       | 39/148 [00:03<00:09, 11.21it/s]Loading Dataset: train:  90%|████████▉ | 133/148 [00:11<00:01, 11.03it/s]Loading Dataset: trainS:   1%|          | 1/148 [00:00<00:18,  8.08it/s]Loading Dataset: train:  28%|██▊       | 41/148 [00:03<00:09, 11.40it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:13, 10.80it/s]Loading Dataset: trainS:   1%|▏         | 2/148 [00:00<00:18,  8.02it/s]Loading Dataset: train:  91%|█████████ | 135/148 [00:11<00:01, 11.31it/s]Loading Dataset: train:  29%|██▉       | 43/148 [00:03<00:08, 11.69it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:13, 11.05it/s]Loading Dataset: trainS:   2%|▏         | 3/148 [00:00<00:18,  8.02it/s]Loading Dataset: train:  93%|█████████▎| 137/148 [00:11<00:00, 11.53it/s]Loading Dataset: trainS:   3%|▎         | 4/148 [00:00<00:17,  8.27it/s]Loading Dataset: train:  30%|███       | 45/148 [00:03<00:08, 11.84it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:12, 11.31it/s]Loading Dataset: train:  94%|█████████▍| 139/148 [00:11<00:00, 12.38it/s]Loading Dataset: trainS:   3%|▎         | 5/148 [00:00<00:16,  8.47it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:13, 10.45it/s]Loading Dataset: train:  32%|███▏      | 47/148 [00:04<00:08, 12.01it/s]Loading Dataset: train:  95%|█████████▌| 141/148 [00:12<00:00, 12.50it/s]Loading Dataset: trainS:   4%|▍         | 6/148 [00:00<00:17,  8.19it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:00<00:12, 11.14it/s]Loading Dataset: train:  33%|███▎      | 49/148 [00:04<00:08, 12.11it/s]Loading Dataset: trainS:   5%|▍         | 7/148 [00:00<00:18,  7.59it/s]Loading Dataset: train:  97%|█████████▋| 143/148 [00:12<00:00, 12.10it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:00<00:11, 11.69it/s]Loading Dataset: train:  34%|███▍      | 51/148 [00:04<00:07, 12.20it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:10, 12.51it/s]Loading Dataset: train:  98%|█████████▊| 145/148 [00:12<00:00, 11.77it/s]Loading Dataset: trainS:   6%|▌         | 9/148 [00:01<00:17,  8.09it/s]Loading Dataset: train:  36%|███▌      | 53/148 [00:04<00:07, 12.00it/s]Loading Dataset: trainS:   7%|▋         | 10/148 [00:01<00:16,  8.57it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:10, 12.21it/s]Loading Dataset: train:  99%|█████████▉| 147/148 [00:12<00:00, 11.64it/s]Loading Dataset: trainS:   7%|▋         | 11/148 [00:01<00:15,  8.57it/s]Loading Dataset: train:  37%|███▋      | 55/148 [00:04<00:07, 11.81it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:12<00:00, 11.61it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:10, 12.98it/s]Loading Dataset: trainS:   8%|▊         | 12/148 [00:01<00:15,  8.92it/s]concatenating train images:  15%|█▍        | 22/148 [00:00<00:00, 216.51it/s]Loading Dataset: train:  39%|███▊      | 57/148 [00:04<00:07, 11.74it/s]concatenating train images:  33%|███▎      | 49/148 [00:00<00:00, 230.15it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:01<00:10, 12.42it/s]Loading Dataset: trainS:   9%|▉         | 14/148 [00:01<00:14,  9.21it/s]concatenating train images:  51%|█████     | 75/148 [00:00<00:00, 237.67it/s]Loading Dataset: train:  40%|███▉      | 59/148 [00:05<00:07, 11.29it/s]Loading Dataset: trainS:  10%|█         | 15/148 [00:01<00:14,  8.87it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:01<00:10, 11.84it/s]concatenating train images:  70%|███████   | 104/148 [00:00<00:00, 249.60it/s]concatenating train images:  89%|████████▉ | 132/148 [00:00<00:00, 257.22it/s]Loading Dataset: train:  41%|████      | 61/148 [00:05<00:08, 10.66it/s]concatenating train images: 100%|██████████| 148/148 [00:00<00:00, 263.05it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:01<00:10, 11.43it/s]Loading Dataset: trainS:  11%|█▏        | 17/148 [00:01<00:14,  8.99it/s]Loading Dataset: trainS:  12%|█▏        | 18/148 [00:02<00:14,  8.82it/s]Loading Dataset: train:  43%|████▎     | 63/148 [00:05<00:08, 10.37it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:11, 10.92it/s]Loading Dataset: trainS:  13%|█▎        | 19/148 [00:02<00:15,  8.58it/s]Loading Dataset: train:  44%|████▍     | 65/148 [00:05<00:07, 10.72it/s]Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00, 12.02it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:02<00:11, 10.59it/s]Loading Dataset: test:  80%|████████  | 4/5 [00:00<00:00, 13.57it/s]Loading Dataset: trainS:  14%|█▍        | 21/148 [00:02<00:14,  8.86it/s]Loading Dataset: train:  45%|████▌     | 67/148 [00:05<00:06, 11.59it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00, 13.77it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 299.26it/s]Loading Dataset: train:  20%|█▉        | 29/148 [00:02<00:15,  7.84it/s]Loading Dataset: train:  47%|████▋     | 69/148 [00:06<00:08,  9.16it/s]Loading Dataset: trainS:  16%|█▌        | 23/148 [00:02<00:16,  7.57it/s]Loading Dataset: train:  20%|██        | 30/148 [00:02<00:14,  8.29it/s]Loading Dataset: trainS:  16%|█▌        | 24/148 [00:02<00:16,  7.63it/s]Loading Dataset: train:  48%|████▊     | 71/148 [00:06<00:07, 10.07it/s]Loading Dataset: train:  21%|██        | 31/148 [00:02<00:13,  8.69it/s]Loading Dataset: trainS:  17%|█▋        | 25/148 [00:03<00:15,  7.79it/s]Loading Dataset: train:  22%|██▏       | 32/148 [00:03<00:12,  9.02it/s]Loading Dataset: train:  49%|████▉     | 73/148 [00:06<00:07, 10.59it/s]Loading Dataset: trainS:  18%|█▊        | 26/148 [00:03<00:15,  7.89it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:03<00:12,  9.23it/s]Loading Dataset: train:  51%|█████     | 75/148 [00:06<00:06, 11.01it/s]Loading Dataset: train:  23%|██▎       | 34/148 [00:03<00:12,  9.44it/s]Loading Dataset: trainS:  18%|█▊        | 27/148 [00:03<00:15,  7.79it/s]Loading Dataset: trainS:  19%|█▉        | 28/148 [00:03<00:14,  8.02it/s]Loading Dataset: train:  52%|█████▏    | 77/148 [00:06<00:06, 11.52it/s]Loading Dataset: train:  24%|██▍       | 36/148 [00:03<00:11,  9.72it/s]Loading Dataset: trainS:  20%|█▉        | 29/148 [00:03<00:15,  7.76it/s]Loading Dataset: train:  53%|█████▎    | 79/148 [00:06<00:05, 12.11it/s]Loading Dataset: trainS:  20%|██        | 30/148 [00:03<00:14,  7.87it/s]Loading Dataset: train:  26%|██▌       | 38/148 [00:03<00:11,  9.83it/s]Loading Dataset: train:  55%|█████▍    | 81/148 [00:07<00:05, 12.48it/s]Loading Dataset: trainS:  21%|██        | 31/148 [00:03<00:14,  8.05it/s]Loading Dataset: train:  56%|█████▌    | 83/148 [00:07<00:05, 12.68it/s]Loading Dataset: train:  27%|██▋       | 40/148 [00:03<00:10,  9.99it/s]Loading Dataset: trainS:  22%|██▏       | 32/148 [00:03<00:14,  7.99it/s]Loading Dataset: train:  57%|█████▋    | 85/148 [00:07<00:04, 12.81it/s]Loading Dataset: trainS:  22%|██▏       | 33/148 [00:04<00:14,  8.12it/s]Loading Dataset: train:  28%|██▊       | 42/148 [00:04<00:10, 10.23it/s]Loading Dataset: trainS:  23%|██▎       | 34/148 [00:04<00:14,  8.05it/s]Loading Dataset: train:  59%|█████▉    | 87/148 [00:07<00:04, 12.87it/s]Loading Dataset: train:  30%|██▉       | 44/148 [00:04<00:09, 10.54it/s]Loading Dataset: trainS:  24%|██▎       | 35/148 [00:04<00:14,  8.04it/s]Loading Dataset: train:  60%|██████    | 89/148 [00:07<00:04, 12.98it/s]Loading Dataset: trainS:  24%|██▍       | 36/148 [00:04<00:13,  8.19it/s]Loading Dataset: train:  31%|███       | 46/148 [00:04<00:09, 10.83it/s]Loading Dataset: train:  61%|██████▏   | 91/148 [00:07<00:04, 12.86it/s]Loading Dataset: trainS:  25%|██▌       | 37/148 [00:04<00:13,  8.27it/s]Loading Dataset: train:  32%|███▏      | 48/148 [00:04<00:08, 11.24it/s]Loading Dataset: train:  63%|██████▎   | 93/148 [00:07<00:04, 12.97it/s]Loading Dataset: trainS:  26%|██▌       | 38/148 [00:04<00:13,  8.31it/s]Loading Dataset: train:  34%|███▍      | 50/148 [00:04<00:08, 11.69it/s]Loading Dataset: train:  64%|██████▍   | 95/148 [00:08<00:03, 13.50it/s]Loading Dataset: trainS:  26%|██▋       | 39/148 [00:04<00:13,  8.05it/s]Loading Dataset: train:  35%|███▌      | 52/148 [00:04<00:08, 11.99it/s]Loading Dataset: trainS:  27%|██▋       | 40/148 [00:04<00:13,  8.10it/s]Loading Dataset: train:  66%|██████▌   | 98/148 [00:08<00:03, 14.98it/s]Loading Dataset: trainS:  28%|██▊       | 41/148 [00:04<00:12,  8.34it/s]Loading Dataset: train:  68%|██████▊   | 100/148 [00:08<00:03, 15.95it/s]Loading Dataset: train:  36%|███▋      | 54/148 [00:05<00:07, 11.92it/s]Loading Dataset: trainS:  28%|██▊       | 42/148 [00:05<00:12,  8.22it/s]Loading Dataset: train:  69%|██████▉   | 102/148 [00:08<00:03, 14.54it/s]Loading Dataset: train:  38%|███▊      | 56/148 [00:05<00:07, 11.77it/s]Loading Dataset: trainS:  29%|██▉       | 43/148 [00:05<00:12,  8.53it/s]Loading Dataset: trainS:  30%|██▉       | 44/148 [00:05<00:11,  8.84it/s]Loading Dataset: train:  70%|███████   | 104/148 [00:08<00:03, 13.63it/s]Loading Dataset: train:  39%|███▉      | 58/148 [00:05<00:07, 11.81it/s]Loading Dataset: trainS:  30%|███       | 45/148 [00:05<00:11,  8.84it/s]Loading Dataset: train:  72%|███████▏  | 106/148 [00:08<00:03, 12.96it/s]Loading Dataset: trainS:  31%|███       | 46/148 [00:05<00:11,  8.81it/s]Loading Dataset: train:  41%|████      | 60/148 [00:05<00:07, 11.19it/s]Loading Dataset: trainS:  32%|███▏      | 47/148 [00:05<00:11,  8.82it/s]Loading Dataset: train:  73%|███████▎  | 108/148 [00:09<00:03, 12.81it/s]Loading Dataset: trainS:  32%|███▏      | 48/148 [00:05<00:11,  8.99it/s]Loading Dataset: train:  42%|████▏     | 62/148 [00:05<00:08, 10.66it/s]Loading Dataset: train:  74%|███████▍  | 110/148 [00:09<00:03, 12.43it/s]Loading Dataset: trainS:  33%|███▎      | 49/148 [00:05<00:11,  8.94it/s]Loading Dataset: train:  43%|████▎     | 64/148 [00:05<00:08, 10.32it/s]Loading Dataset: trainS:  34%|███▍      | 50/148 [00:05<00:10,  8.98it/s]Loading Dataset: train:  76%|███████▌  | 112/148 [00:09<00:02, 12.42it/s]Loading Dataset: trainS:  34%|███▍      | 51/148 [00:06<00:10,  9.08it/s]Loading Dataset: train:  45%|████▍     | 66/148 [00:06<00:07, 11.29it/s]Loading Dataset: train:  78%|███████▊  | 115/148 [00:09<00:02, 14.00it/s]Loading Dataset: trainS:  35%|███▌      | 52/148 [00:06<00:11,  8.35it/s]Loading Dataset: train:  46%|████▌     | 68/148 [00:06<00:06, 12.20it/s]Loading Dataset: train:  79%|███████▉  | 117/148 [00:09<00:02, 15.11it/s]Loading Dataset: trainS:  36%|███▌      | 53/148 [00:06<00:11,  7.99it/s]Loading Dataset: train:  47%|████▋     | 70/148 [00:06<00:06, 12.99it/s]Loading Dataset: train:  80%|████████  | 119/148 [00:09<00:01, 14.91it/s]Loading Dataset: trainS:  36%|███▋      | 54/148 [00:06<00:11,  8.06it/s]Loading Dataset: train:  49%|████▊     | 72/148 [00:06<00:05, 12.88it/s]Loading Dataset: train:  82%|████████▏ | 121/148 [00:09<00:01, 13.72it/s]Loading Dataset: trainS:  37%|███▋      | 55/148 [00:06<00:11,  8.24it/s]Loading Dataset: train:  50%|█████     | 74/148 [00:06<00:05, 12.77it/s]Loading Dataset: trainS:  38%|███▊      | 56/148 [00:06<00:10,  8.41it/s]Loading Dataset: train:  83%|████████▎ | 123/148 [00:10<00:01, 12.90it/s]Loading Dataset: trainS:  39%|███▊      | 57/148 [00:06<00:11,  8.22it/s]Loading Dataset: train:  51%|█████▏    | 76/148 [00:06<00:05, 12.69it/s]Loading Dataset: train:  84%|████████▍ | 125/148 [00:10<00:01, 12.17it/s]Loading Dataset: trainS:  39%|███▉      | 58/148 [00:06<00:10,  8.29it/s]Loading Dataset: train:  53%|█████▎    | 78/148 [00:06<00:05, 13.04it/s]Loading Dataset: trainS:  40%|███▉      | 59/148 [00:07<00:11,  7.75it/s]Loading Dataset: train:  86%|████████▌ | 127/148 [00:10<00:01, 11.61it/s]Loading Dataset: train:  54%|█████▍    | 80/148 [00:07<00:05, 13.28it/s]Loading Dataset: trainS:  41%|████      | 60/148 [00:07<00:11,  7.38it/s]Loading Dataset: train:  55%|█████▌    | 82/148 [00:07<00:04, 13.47it/s]Loading Dataset: train:  87%|████████▋ | 129/148 [00:10<00:01, 11.21it/s]Loading Dataset: trainS:  41%|████      | 61/148 [00:07<00:12,  7.17it/s]Loading Dataset: train:  57%|█████▋    | 84/148 [00:07<00:04, 13.47it/s]Loading Dataset: train:  89%|████████▊ | 131/148 [00:10<00:01, 11.25it/s]Loading Dataset: trainS:  42%|████▏     | 62/148 [00:07<00:12,  7.12it/s]Loading Dataset: train:  58%|█████▊    | 86/148 [00:07<00:04, 13.50it/s]Loading Dataset: train:  90%|████████▉ | 133/148 [00:11<00:01, 11.31it/s]Loading Dataset: trainS:  43%|████▎     | 63/148 [00:07<00:11,  7.23it/s]Loading Dataset: train:  59%|█████▉    | 88/148 [00:07<00:04, 13.32it/s]Loading Dataset: train:  91%|█████████ | 135/148 [00:11<00:01, 11.53it/s]Loading Dataset: trainS:  43%|████▎     | 64/148 [00:07<00:11,  7.10it/s]Loading Dataset: train:  61%|██████    | 90/148 [00:07<00:04, 13.28it/s]Loading Dataset: train:  93%|█████████▎| 137/148 [00:11<00:00, 12.09it/s]Loading Dataset: trainS:  45%|████▍     | 66/148 [00:08<00:10,  7.92it/s]Loading Dataset: train:  62%|██████▏   | 92/148 [00:08<00:04, 13.32it/s]Loading Dataset: train:  94%|█████████▍| 139/148 [00:11<00:00, 12.92it/s]Loading Dataset: train:  64%|██████▎   | 94/148 [00:08<00:04, 13.30it/s]Loading Dataset: trainS:  46%|████▌     | 68/148 [00:08<00:09,  8.50it/s]Loading Dataset: train:  95%|█████████▌| 141/148 [00:11<00:00, 13.58it/s]Loading Dataset: train:  66%|██████▌   | 97/148 [00:08<00:03, 14.83it/s]Loading Dataset: train:  97%|█████████▋| 143/148 [00:11<00:00, 13.34it/s]Loading Dataset: trainS:  47%|████▋     | 70/148 [00:08<00:08,  9.16it/s]Loading Dataset: train:  68%|██████▊   | 100/148 [00:08<00:02, 16.21it/s]Loading Dataset: trainS:  48%|████▊     | 71/148 [00:08<00:08,  8.94it/s]Loading Dataset: train:  98%|█████████▊| 145/148 [00:11<00:00, 12.85it/s]Loading Dataset: trainS:  49%|████▊     | 72/148 [00:08<00:08,  9.04it/s]Loading Dataset: train:  69%|██████▉   | 102/148 [00:08<00:03, 14.68it/s]Loading Dataset: train:  99%|█████████▉| 147/148 [00:12<00:00, 12.51it/s]Loading Dataset: trainS:  49%|████▉     | 73/148 [00:08<00:08,  9.04it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:12<00:00, 12.16it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:  70%|███████   | 104/148 [00:08<00:03, 13.71it/s]Loading Dataset: trainS:  50%|█████     | 74/148 [00:08<00:08,  8.91it/s]concatenating train images:  18%|█▊        | 27/148 [00:00<00:00, 269.48it/s]Loading Dataset: trainS:  51%|█████     | 75/148 [00:08<00:08,  9.09it/s]Loading Dataset: train:  72%|███████▏  | 106/148 [00:08<00:03, 13.10it/s]concatenating train images:  37%|███▋      | 55/148 [00:00<00:00, 270.93it/s]Loading Dataset: trainS:  51%|█████▏    | 76/148 [00:09<00:07,  9.04it/s]concatenating train images:  55%|█████▌    | 82/148 [00:00<00:00, 269.81it/s]Loading Dataset: train:  73%|███████▎  | 108/148 [00:09<00:03, 12.68it/s]Loading Dataset: trainS:  52%|█████▏    | 77/148 [00:09<00:07,  9.23it/s]concatenating train images:  74%|███████▎  | 109/148 [00:00<00:00, 269.54it/s]Loading Dataset: trainS:  53%|█████▎    | 78/148 [00:09<00:07,  9.11it/s]concatenating train images:  93%|█████████▎| 138/148 [00:00<00:00, 273.37it/s]Loading Dataset: train:  74%|███████▍  | 110/148 [00:09<00:03, 12.35it/s]concatenating train images: 100%|██████████| 148/148 [00:00<00:00, 272.33it/s]Loading Dataset: trainS:  53%|█████▎    | 79/148 [00:09<00:07,  9.26it/s]Loading Dataset: train:  76%|███████▌  | 112/148 [00:09<00:02, 12.24it/s]Loading Dataset: trainS:  54%|█████▍    | 80/148 [00:09<00:07,  8.82it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: train:  77%|███████▋  | 114/148 [00:09<00:02, 13.85it/s]Loading Dataset: trainS:  55%|█████▍    | 81/148 [00:09<00:07,  9.07it/s]Loading Dataset: train:  78%|███████▊  | 116/148 [00:09<00:02, 15.13it/s]Loading Dataset: trainS:  55%|█████▌    | 82/148 [00:09<00:07,  9.30it/s]Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00, 11.90it/s]Loading Dataset: train:  80%|███████▉  | 118/148 [00:09<00:01, 16.14it/s]Loading Dataset: trainS:  56%|█████▌    | 83/148 [00:09<00:07,  9.15it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00, 12.83it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: trainS:  57%|█████▋    | 84/148 [00:09<00:07,  8.98it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 292.41it/s]Loading Dataset: trainS:  57%|█████▋    | 85/148 [00:10<00:07,  8.30it/s]Loading Dataset: train:  81%|████████  | 120/148 [00:10<00:02, 11.60it/s]Loading Dataset: trainS:  58%|█████▊    | 86/148 [00:10<00:07,  8.26it/s]Loading Dataset: train:  82%|████████▏ | 122/148 [00:10<00:02, 11.10it/s]Loading Dataset: trainS:  59%|█████▉    | 87/148 [00:10<00:07,  8.54it/s]Loading Dataset: trainS:  59%|█████▉    | 88/148 [00:10<00:06,  8.64it/s]Loading Dataset: train:  84%|████████▍ | 124/148 [00:10<00:02, 11.18it/s]Loading Dataset: trainS:  60%|██████    | 89/148 [00:10<00:06,  8.64it/s]Loading Dataset: trainS:  61%|██████    | 90/148 [00:10<00:06,  8.98it/s]Loading Dataset: train:  85%|████████▌ | 126/148 [00:10<00:02, 10.84it/s]Loading Dataset: trainS:  62%|██████▏   | 92/148 [00:10<00:06,  9.26it/s]Loading Dataset: train:  86%|████████▋ | 128/148 [00:10<00:01, 10.63it/s]Loading Dataset: trainS:  63%|██████▎   | 93/148 [00:10<00:05,  9.25it/s]Loading Dataset: train:  88%|████████▊ | 130/148 [00:11<00:01, 10.57it/s]Loading Dataset: trainS:  64%|██████▎   | 94/148 [00:11<00:05,  9.46it/s]Loading Dataset: train:  89%|████████▉ | 132/148 [00:11<00:01, 11.15it/s]Loading Dataset: trainS:  65%|██████▍   | 96/148 [00:11<00:05, 10.14it/s]Loading Dataset: train:  91%|█████████ | 134/148 [00:11<00:01, 11.57it/s]Loading Dataset: trainS:  66%|██████▌   | 98/148 [00:11<00:04, 10.86it/s]Loading Dataset: train:  92%|█████████▏| 136/148 [00:11<00:01, 11.88it/s]Loading Dataset: trainS:  68%|██████▊   | 100/148 [00:11<00:04, 11.39it/s]Loading Dataset: train:  93%|█████████▎| 138/148 [00:11<00:00, 12.72it/s]Loading Dataset: trainS:  69%|██████▉   | 102/148 [00:11<00:04, 10.73it/s]Loading Dataset: train:  95%|█████████▍| 140/148 [00:11<00:00, 13.23it/s]Loading Dataset: train:  96%|█████████▌| 142/148 [00:11<00:00, 13.64it/s]Loading Dataset: trainS:  70%|███████   | 104/148 [00:11<00:04, 10.16it/s]Loading Dataset: train:  97%|█████████▋| 144/148 [00:12<00:00, 12.93it/s]Loading Dataset: trainS:  72%|███████▏  | 106/148 [00:12<00:04,  9.81it/s]Loading Dataset: train:  99%|█████████▊| 146/148 [00:12<00:00, 12.23it/s]Loading Dataset: trainS:  73%|███████▎  | 108/148 [00:12<00:04,  9.53it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:12<00:00, 11.95it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: trainS:  74%|███████▎  | 109/148 [00:12<00:04,  9.54it/s]concatenating train images:  18%|█▊        | 27/148 [00:00<00:00, 268.20it/s]Loading Dataset: trainS:  74%|███████▍  | 110/148 [00:12<00:04,  9.31it/s]concatenating train images:  36%|███▋      | 54/148 [00:00<00:00, 267.50it/s]Loading Dataset: trainS:  75%|███████▌  | 111/148 [00:12<00:03,  9.38it/s]concatenating train images:  55%|█████▍    | 81/148 [00:00<00:00, 267.16it/s]Loading Dataset: trainS:  76%|███████▌  | 112/148 [00:12<00:03,  9.41it/s]concatenating train images:  73%|███████▎  | 108/148 [00:00<00:00, 266.42it/s]concatenating train images:  90%|████████▉ | 133/148 [00:00<00:00, 259.85it/s]Loading Dataset: trainS:  77%|███████▋  | 114/148 [00:12<00:03, 10.16it/s]concatenating train images: 100%|██████████| 148/148 [00:00<00:00, 262.52it/s]Loading Dataset: trainS:  78%|███████▊  | 116/148 [00:13<00:02, 10.82it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: trainS:  80%|███████▉  | 118/148 [00:13<00:02, 11.17it/s]Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00, 11.98it/s]Loading Dataset: trainS:  81%|████████  | 120/148 [00:13<00:02, 10.08it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00, 12.93it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 293.95it/s]Loading Dataset: trainS:  82%|████████▏ | 122/148 [00:13<00:02,  9.42it/s]Loading Dataset: trainS:  83%|████████▎ | 123/148 [00:13<00:02,  9.12it/s]Loading Dataset: trainS:  84%|████████▍ | 124/148 [00:14<00:02,  8.87it/s]Loading Dataset: trainS:  84%|████████▍ | 125/148 [00:14<00:02,  8.42it/s]Loading Dataset: trainS:  85%|████████▌ | 126/148 [00:14<00:02,  8.28it/s]Loading Dataset: trainS:  86%|████████▌ | 127/148 [00:14<00:02,  8.17it/s]Loading Dataset: trainS:  86%|████████▋ | 128/148 [00:14<00:02,  7.97it/s]Loading Dataset: trainS:  87%|████████▋ | 129/148 [00:14<00:02,  7.96it/s]Loading Dataset: trainS:  88%|████████▊ | 130/148 [00:14<00:02,  7.77it/s]Loading Dataset: trainS:  89%|████████▊ | 131/148 [00:14<00:02,  7.96it/s]Loading Dataset: trainS:  89%|████████▉ | 132/148 [00:15<00:01,  8.27it/s]Loading Dataset: trainS:  90%|████████▉ | 133/148 [00:15<00:01,  8.44it/s]Loading Dataset: trainS:  91%|█████████ | 134/148 [00:15<00:01,  8.60it/s]Loading Dataset: trainS:  91%|█████████ | 135/148 [00:15<00:01,  8.62it/s]Loading Dataset: trainS:  92%|█████████▏| 136/148 [00:15<00:01,  8.57it/s]Loading Dataset: trainS:  93%|█████████▎| 138/148 [00:15<00:01,  9.24it/s]Loading Dataset: trainS:  95%|█████████▍| 140/148 [00:15<00:00,  9.67it/s]Loading Dataset: trainS:  96%|█████████▌| 142/148 [00:16<00:00,  9.89it/s]Loading Dataset: trainS:  97%|█████████▋| 144/148 [00:16<00:00,  9.49it/s]Loading Dataset: trainS:  98%|█████████▊| 145/148 [00:16<00:00,  9.03it/s]Loading Dataset: trainS:  99%|█████████▊| 146/148 [00:16<00:00,  9.00it/s]Loading Dataset: trainS:  99%|█████████▉| 147/148 [00:16<00:00,  9.07it/s]Loading Dataset: trainS: 100%|██████████| 148/148 [00:16<00:00,  8.73it/s]
Loading Dataset: testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: testS:  20%|██        | 1/5 [00:00<00:00,  9.03it/s]Loading Dataset: testS:  40%|████      | 2/5 [00:00<00:00,  8.67it/s]Loading Dataset: testS:  80%|████████  | 4/5 [00:00<00:00,  9.43it/s]Loading Dataset: testS: 100%|██████████| 5/5 [00:00<00:00,  8.68it/s]*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [4.0] GPU:   5 Cascade
---------------------------------------------------------------
 Nucleus: 4.0  | GPU: 5  | SD 2  | Dropout 0.2  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.2
---------------------------------------------------------------
InputDimensions [60, 88, 44]
(5722, 60, 88, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 60, 88, 1)    0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 60, 88, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 60, 88, 20)   200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 88, 20)   3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 44, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 30, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 30, 44, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 44, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 22, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 15, 22, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 22, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 22, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 15, 22, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 15, 22, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 30, 44, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 44, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 44, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 30, 44, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30, 44, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 60, 88, 20)   3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 60, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 60, 88, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 60, 88, 20)   3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 60, 88, 20)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 60, 88, 2)    42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/4-VA/sd2
------------------------------------------------------------------
Train on 5722 samples, validate on 188 samples
Epoch 1/100
 - 7s - loss: 0.0086 - acc: 0.9964 - mDice: 0.5380 - val_loss: 0.0086 - val_acc: 0.9962 - val_mDice: 0.5554
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [5.0] GPU:   6 Cascade
---------------------------------------------------------------
 Nucleus: 5.0  | GPU: 6  | SD 2  | Dropout 0.2  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.2
---------------------------------------------------------------
InputDimensions [60, 88, 44]
(5722, 60, 88, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 60, 88, 1)    0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 60, 88, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 60, 88, 20)   200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 88, 20)   3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 44, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 30, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 30, 44, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 44, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 22, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 15, 22, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 22, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 22, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 15, 22, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 15, 22, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 30, 44, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 44, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 44, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 30, 44, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30, 44, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 60, 88, 20)   3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 60, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 60, 88, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 60, 88, 20)   3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 60, 88, 20)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 60, 88, 2)    42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/5-VLa/sd2
------------------------------------------------------------------
Train on 5722 samples, validate on 188 samples
Epoch 1/100
 - 7s - loss: 0.0045 - acc: 0.9983 - mDice: 0.3946 - val_loss: 0.0036 - val_acc: 0.9986 - val_mDice: 0.4342

Epoch 00001: val_mDice improved from -inf to 0.55537, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 2/100
 - 4s - loss: 0.0069 - acc: 0.9970 - mDice: 0.6199 - val_loss: 0.0078 - val_acc: 0.9966 - val_mDice: 0.5983
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [14.0] GPU:   7 Cascade
---------------------------------------------------------------
 Nucleus: 14.0  | GPU: 7  | SD 2  | Dropout 0.2  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.2
---------------------------------------------------------------
InputDimensions [60, 88, 44]
(5722, 60, 88, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 60, 88, 1)    0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 60, 88, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 60, 88, 20)   200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 88, 20)   3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 44, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 30, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 30, 44, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 44, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 22, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 15, 22, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 22, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 22, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 15, 22, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 15, 22, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 30, 44, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 44, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 44, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 30, 44, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30, 44, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 60, 88, 20)   3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 60, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 60, 88, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 60, 88, 20)   3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 60, 88, 20)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 60, 88, 2)    42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/14-MTT/sd2
------------------------------------------------------------------
Train on 5722 samples, validate on 188 samples
Epoch 1/100
 - 7s - loss: 0.0020 - acc: 0.9993 - mDice: 0.2686 - val_loss: 0.0011 - val_acc: 0.9996 - val_mDice: 0.3563

Epoch 00002: val_mDice improved from 0.55537 to 0.59834, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 3/100
 - 4s - loss: 0.0060 - acc: 0.9974 - mDice: 0.6720 - val_loss: 0.0079 - val_acc: 0.9967 - val_mDice: 0.6143

Epoch 00001: val_mDice improved from -inf to 0.43417, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 2/100
 - 4s - loss: 0.0032 - acc: 0.9987 - mDice: 0.5231 - val_loss: 0.0052 - val_acc: 0.9978 - val_mDice: 0.4922

Epoch 00001: val_mDice improved from -inf to 0.35630, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 2/100
 - 4s - loss: 0.0015 - acc: 0.9994 - mDice: 0.3276 - val_loss: 0.0013 - val_acc: 0.9996 - val_mDice: 0.3664

Epoch 00003: val_mDice improved from 0.59834 to 0.61431, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 4/100
 - 4s - loss: 0.0054 - acc: 0.9977 - mDice: 0.7047 - val_loss: 0.0085 - val_acc: 0.9965 - val_mDice: 0.6241

Epoch 00002: val_mDice improved from 0.43417 to 0.49221, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 3/100
 - 4s - loss: 0.0027 - acc: 0.9989 - mDice: 0.6052 - val_loss: 0.0045 - val_acc: 0.9983 - val_mDice: 0.4367

Epoch 00002: val_mDice improved from 0.35630 to 0.36637, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 3/100
 - 4s - loss: 0.0013 - acc: 0.9994 - mDice: 0.3672 - val_loss: 0.0014 - val_acc: 0.9994 - val_mDice: 0.3712

Epoch 00004: val_mDice improved from 0.61431 to 0.62406, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 5/100
 - 4s - loss: 0.0053 - acc: 0.9977 - mDice: 0.7128 - val_loss: 0.0086 - val_acc: 0.9968 - val_mDice: 0.6251
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [1.0] GPU:   0 Cascade
---------------------------------------------------------------
 Nucleus: 1.0  | GPU: 0  | SD 2  | Dropout 0.2  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.2
---------------------------------------------------------------
InputDimensions [116, 112, 72]
(9265, 116, 112, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 112, 1)  0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 112, 1)  4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 112, 20) 200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 112, 20) 3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 56, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 56, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 58, 56, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 56, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 56, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 28, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 28, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 28, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 28, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 28, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 29, 28, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 56, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 56, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 56, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 56, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 56, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 56, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 112, 20) 3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 116, 112, 40) 0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 112, 20) 7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 112, 20) 3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 112, 20) 0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 112, 2)  42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/1-THALAMUS/sd2
------------------------------------------------------------------
Train on 9265 samples, validate on 325 samples
Epoch 1/100
 - 19s - loss: 0.0134 - acc: 0.9947 - mDice: 0.8490 - val_loss: 0.0097 - val_acc: 0.9961 - val_mDice: 0.8707

Epoch 00003: val_mDice did not improve from 0.49221
Epoch 4/100
 - 4s - loss: 0.0024 - acc: 0.9990 - mDice: 0.6487 - val_loss: 0.0043 - val_acc: 0.9983 - val_mDice: 0.5516

Epoch 00003: val_mDice improved from 0.36637 to 0.37122, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 4/100
 - 4s - loss: 0.0012 - acc: 0.9995 - mDice: 0.4016 - val_loss: 0.0017 - val_acc: 0.9992 - val_mDice: 0.3819

Epoch 00005: val_mDice improved from 0.62406 to 0.62506, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 6/100
 - 4s - loss: 0.0048 - acc: 0.9979 - mDice: 0.7383 - val_loss: 0.0091 - val_acc: 0.9962 - val_mDice: 0.6276

Epoch 00004: val_mDice improved from 0.49221 to 0.55157, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 5/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.6842 - val_loss: 0.0041 - val_acc: 0.9984 - val_mDice: 0.5343

Epoch 00004: val_mDice improved from 0.37122 to 0.38190, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 5/100
 - 4s - loss: 0.0012 - acc: 0.9995 - mDice: 0.4362 - val_loss: 0.0012 - val_acc: 0.9996 - val_mDice: 0.4202

Epoch 00006: val_mDice improved from 0.62506 to 0.62755, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 7/100
 - 4s - loss: 0.0045 - acc: 0.9981 - mDice: 0.7537 - val_loss: 0.0096 - val_acc: 0.9962 - val_mDice: 0.6403

Epoch 00005: val_mDice did not improve from 0.55157
Epoch 6/100
 - 4s - loss: 0.0020 - acc: 0.9992 - mDice: 0.7080 - val_loss: 0.0047 - val_acc: 0.9982 - val_mDice: 0.5604

Epoch 00005: val_mDice improved from 0.38190 to 0.42024, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 6/100
 - 4s - loss: 0.0011 - acc: 0.9996 - mDice: 0.4764 - val_loss: 0.0013 - val_acc: 0.9995 - val_mDice: 0.4370

Epoch 00007: val_mDice improved from 0.62755 to 0.64031, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 8/100
 - 4s - loss: 0.0044 - acc: 0.9981 - mDice: 0.7637 - val_loss: 0.0087 - val_acc: 0.9966 - val_mDice: 0.6453

Epoch 00006: val_mDice improved from 0.55157 to 0.56045, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 7/100
 - 4s - loss: 0.0020 - acc: 0.9992 - mDice: 0.7116 - val_loss: 0.0050 - val_acc: 0.9982 - val_mDice: 0.5754

Epoch 00006: val_mDice improved from 0.42024 to 0.43702, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 7/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.4947 - val_loss: 0.0014 - val_acc: 0.9995 - val_mDice: 0.4403

Epoch 00008: val_mDice improved from 0.64031 to 0.64527, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 9/100
 - 4s - loss: 0.0042 - acc: 0.9982 - mDice: 0.7717 - val_loss: 0.0089 - val_acc: 0.9966 - val_mDice: 0.6580

Epoch 00007: val_mDice improved from 0.56045 to 0.57542, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 8/100
 - 4s - loss: 0.0018 - acc: 0.9993 - mDice: 0.7449 - val_loss: 0.0040 - val_acc: 0.9986 - val_mDice: 0.6045

Epoch 00001: val_mDice improved from -inf to 0.87074, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/100
 - 15s - loss: 0.0105 - acc: 0.9955 - mDice: 0.8734 - val_loss: 0.0108 - val_acc: 0.9955 - val_mDice: 0.8721

Epoch 00007: val_mDice improved from 0.43702 to 0.44027, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 8/100
 - 4s - loss: 9.7668e-04 - acc: 0.9996 - mDice: 0.5179 - val_loss: 0.0017 - val_acc: 0.9992 - val_mDice: 0.4397

Epoch 00009: val_mDice improved from 0.64527 to 0.65800, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 10/100
 - 4s - loss: 0.0040 - acc: 0.9983 - mDice: 0.7813 - val_loss: 0.0093 - val_acc: 0.9966 - val_mDice: 0.6506

Epoch 00008: val_mDice improved from 0.57542 to 0.60447, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 9/100
 - 4s - loss: 0.0017 - acc: 0.9993 - mDice: 0.7565 - val_loss: 0.0044 - val_acc: 0.9985 - val_mDice: 0.5846

Epoch 00008: val_mDice did not improve from 0.44027
Epoch 9/100
 - 4s - loss: 9.2537e-04 - acc: 0.9996 - mDice: 0.5471 - val_loss: 0.0019 - val_acc: 0.9992 - val_mDice: 0.4407

Epoch 00010: val_mDice did not improve from 0.65800
Epoch 11/100
 - 4s - loss: 0.0039 - acc: 0.9983 - mDice: 0.7895 - val_loss: 0.0100 - val_acc: 0.9962 - val_mDice: 0.6504

Epoch 00009: val_mDice did not improve from 0.60447
Epoch 10/100
 - 4s - loss: 0.0017 - acc: 0.9993 - mDice: 0.7629 - val_loss: 0.0044 - val_acc: 0.9985 - val_mDice: 0.6083

Epoch 00009: val_mDice improved from 0.44027 to 0.44069, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 10/100
 - 4s - loss: 8.8000e-04 - acc: 0.9996 - mDice: 0.5667 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.4777

Epoch 00011: val_mDice did not improve from 0.65800
Epoch 12/100
 - 4s - loss: 0.0038 - acc: 0.9984 - mDice: 0.7958 - val_loss: 0.0096 - val_acc: 0.9965 - val_mDice: 0.6575

Epoch 00010: val_mDice improved from 0.60447 to 0.60830, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 11/100
 - 4s - loss: 0.0015 - acc: 0.9994 - mDice: 0.7786 - val_loss: 0.0046 - val_acc: 0.9984 - val_mDice: 0.6045

Epoch 00010: val_mDice improved from 0.44069 to 0.47765, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 11/100
 - 4s - loss: 8.6585e-04 - acc: 0.9996 - mDice: 0.5766 - val_loss: 0.0015 - val_acc: 0.9994 - val_mDice: 0.4919

Epoch 00012: val_mDice did not improve from 0.65800
Epoch 13/100
 - 4s - loss: 0.0037 - acc: 0.9984 - mDice: 0.8008 - val_loss: 0.0123 - val_acc: 0.9959 - val_mDice: 0.6452

Epoch 00011: val_mDice did not improve from 0.60830
Epoch 12/100
 - 4s - loss: 0.0015 - acc: 0.9994 - mDice: 0.7904 - val_loss: 0.0042 - val_acc: 0.9985 - val_mDice: 0.6350

Epoch 00002: val_mDice improved from 0.87074 to 0.87211, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/100
 - 15s - loss: 0.0097 - acc: 0.9959 - mDice: 0.8842 - val_loss: 0.0098 - val_acc: 0.9958 - val_mDice: 0.8783

Epoch 00013: val_mDice did not improve from 0.65800
Epoch 14/100
 - 4s - loss: 0.0037 - acc: 0.9984 - mDice: 0.8016 - val_loss: 0.0101 - val_acc: 0.9966 - val_mDice: 0.6660

Epoch 00011: val_mDice improved from 0.47765 to 0.49193, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 12/100
 - 4s - loss: 8.5811e-04 - acc: 0.9996 - mDice: 0.5788 - val_loss: 0.0019 - val_acc: 0.9992 - val_mDice: 0.4764

Epoch 00012: val_mDice improved from 0.60830 to 0.63498, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 13/100
 - 4s - loss: 0.0015 - acc: 0.9994 - mDice: 0.7888 - val_loss: 0.0044 - val_acc: 0.9986 - val_mDice: 0.6122

Epoch 00012: val_mDice did not improve from 0.49193
Epoch 13/100
 - 4s - loss: 8.1938e-04 - acc: 0.9996 - mDice: 0.6021 - val_loss: 0.0018 - val_acc: 0.9993 - val_mDice: 0.4893

Epoch 00014: val_mDice improved from 0.65800 to 0.66595, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 15/100
 - 4s - loss: 0.0035 - acc: 0.9985 - mDice: 0.8112 - val_loss: 0.0107 - val_acc: 0.9964 - val_mDice: 0.6560

Epoch 00013: val_mDice did not improve from 0.63498
Epoch 14/100
 - 4s - loss: 0.0014 - acc: 0.9994 - mDice: 0.7930 - val_loss: 0.0043 - val_acc: 0.9986 - val_mDice: 0.6387

Epoch 00015: val_mDice did not improve from 0.66595
Epoch 16/100
 - 4s - loss: 0.0034 - acc: 0.9985 - mDice: 0.8132 - val_loss: 0.0108 - val_acc: 0.9968 - val_mDice: 0.6640

Epoch 00013: val_mDice did not improve from 0.49193
Epoch 14/100
 - 4s - loss: 8.2689e-04 - acc: 0.9996 - mDice: 0.5973 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.4749

Epoch 00014: val_mDice improved from 0.63498 to 0.63868, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 15/100
 - 4s - loss: 0.0013 - acc: 0.9994 - mDice: 0.8097 - val_loss: 0.0045 - val_acc: 0.9986 - val_mDice: 0.6307

Epoch 00016: val_mDice did not improve from 0.66595
Epoch 17/100
 - 4s - loss: 0.0035 - acc: 0.9985 - mDice: 0.8129 - val_loss: 0.0103 - val_acc: 0.9965 - val_mDice: 0.6588

Epoch 00014: val_mDice did not improve from 0.49193
Epoch 15/100
 - 4s - loss: 7.9676e-04 - acc: 0.9997 - mDice: 0.6090 - val_loss: 0.0015 - val_acc: 0.9994 - val_mDice: 0.4807

Epoch 00003: val_mDice improved from 0.87211 to 0.87833, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/100
 - 15s - loss: 0.0090 - acc: 0.9961 - mDice: 0.8926 - val_loss: 0.0093 - val_acc: 0.9960 - val_mDice: 0.8832

Epoch 00015: val_mDice did not improve from 0.63868
Epoch 16/100
 - 4s - loss: 0.0014 - acc: 0.9994 - mDice: 0.8069 - val_loss: 0.0046 - val_acc: 0.9985 - val_mDice: 0.6050

Epoch 00017: val_mDice did not improve from 0.66595
Epoch 18/100
 - 4s - loss: 0.0033 - acc: 0.9986 - mDice: 0.8244 - val_loss: 0.0112 - val_acc: 0.9967 - val_mDice: 0.6584

Epoch 00015: val_mDice did not improve from 0.49193
Epoch 16/100
 - 4s - loss: 7.7982e-04 - acc: 0.9997 - mDice: 0.6156 - val_loss: 0.0018 - val_acc: 0.9993 - val_mDice: 0.4815

Epoch 00016: val_mDice did not improve from 0.63868
Epoch 17/100
 - 4s - loss: 0.0013 - acc: 0.9994 - mDice: 0.8112 - val_loss: 0.0043 - val_acc: 0.9987 - val_mDice: 0.6195

Epoch 00018: val_mDice did not improve from 0.66595
Epoch 19/100
 - 4s - loss: 0.0033 - acc: 0.9986 - mDice: 0.8231 - val_loss: 0.0117 - val_acc: 0.9964 - val_mDice: 0.6643

Epoch 00016: val_mDice did not improve from 0.49193
Epoch 17/100
 - 4s - loss: 7.3692e-04 - acc: 0.9997 - mDice: 0.6377 - val_loss: 0.0014 - val_acc: 0.9995 - val_mDice: 0.4909

Epoch 00017: val_mDice did not improve from 0.63868
Epoch 18/100
 - 4s - loss: 0.0013 - acc: 0.9994 - mDice: 0.8121 - val_loss: 0.0042 - val_acc: 0.9987 - val_mDice: 0.6346

Epoch 00019: val_mDice did not improve from 0.66595
Epoch 20/100
 - 4s - loss: 0.0032 - acc: 0.9986 - mDice: 0.8281 - val_loss: 0.0111 - val_acc: 0.9968 - val_mDice: 0.6618

Epoch 00017: val_mDice did not improve from 0.49193
Epoch 18/100
 - 4s - loss: 7.3499e-04 - acc: 0.9997 - mDice: 0.6388 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.5061

Epoch 00018: val_mDice did not improve from 0.63868
Epoch 19/100
 - 4s - loss: 0.0014 - acc: 0.9994 - mDice: 0.8073 - val_loss: 0.0048 - val_acc: 0.9986 - val_mDice: 0.5910

Epoch 00020: val_mDice did not improve from 0.66595
Epoch 21/100
 - 4s - loss: 0.0031 - acc: 0.9987 - mDice: 0.8310 - val_loss: 0.0113 - val_acc: 0.9966 - val_mDice: 0.6678

Epoch 00018: val_mDice improved from 0.49193 to 0.50615, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 19/100
 - 4s - loss: 7.1591e-04 - acc: 0.9997 - mDice: 0.6478 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.5068

Epoch 00004: val_mDice improved from 0.87833 to 0.88323, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/100
 - 15s - loss: 0.0086 - acc: 0.9963 - mDice: 0.8985 - val_loss: 0.0100 - val_acc: 0.9957 - val_mDice: 0.8851

Epoch 00019: val_mDice did not improve from 0.63868

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 20/100
 - 4s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8234 - val_loss: 0.0045 - val_acc: 0.9986 - val_mDice: 0.6371

Epoch 00021: val_mDice improved from 0.66595 to 0.66778, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 22/100
 - 4s - loss: 0.0030 - acc: 0.9987 - mDice: 0.8361 - val_loss: 0.0110 - val_acc: 0.9968 - val_mDice: 0.6796

Epoch 00019: val_mDice improved from 0.50615 to 0.50677, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 20/100
 - 4s - loss: 7.0864e-04 - acc: 0.9997 - mDice: 0.6509 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.5298

Epoch 00020: val_mDice did not improve from 0.63868
Epoch 21/100
 - 4s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8325 - val_loss: 0.0046 - val_acc: 0.9986 - val_mDice: 0.6293

Epoch 00022: val_mDice improved from 0.66778 to 0.67962, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 23/100
 - 4s - loss: 0.0031 - acc: 0.9987 - mDice: 0.8349 - val_loss: 0.0118 - val_acc: 0.9967 - val_mDice: 0.6767

Epoch 00020: val_mDice improved from 0.50677 to 0.52983, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 21/100
 - 4s - loss: 7.0399e-04 - acc: 0.9997 - mDice: 0.6530 - val_loss: 0.0018 - val_acc: 0.9993 - val_mDice: 0.5183

Epoch 00021: val_mDice did not improve from 0.63868
Epoch 22/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8370 - val_loss: 0.0045 - val_acc: 0.9987 - val_mDice: 0.6378

Epoch 00023: val_mDice did not improve from 0.67962
Epoch 24/100
 - 4s - loss: 0.0030 - acc: 0.9987 - mDice: 0.8382 - val_loss: 0.0120 - val_acc: 0.9966 - val_mDice: 0.6673

Epoch 00021: val_mDice did not improve from 0.52983
Epoch 22/100
 - 4s - loss: 6.8894e-04 - acc: 0.9997 - mDice: 0.6591 - val_loss: 0.0015 - val_acc: 0.9994 - val_mDice: 0.5457

Epoch 00022: val_mDice did not improve from 0.63868
Epoch 23/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8405 - val_loss: 0.0045 - val_acc: 0.9987 - val_mDice: 0.6295

Epoch 00024: val_mDice did not improve from 0.67962
Epoch 25/100
 - 4s - loss: 0.0030 - acc: 0.9987 - mDice: 0.8413 - val_loss: 0.0108 - val_acc: 0.9968 - val_mDice: 0.6678

Epoch 00022: val_mDice improved from 0.52983 to 0.54574, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 23/100
 - 4s - loss: 6.8603e-04 - acc: 0.9997 - mDice: 0.6620 - val_loss: 0.0015 - val_acc: 0.9994 - val_mDice: 0.5283

Epoch 00005: val_mDice improved from 0.88323 to 0.88511, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/100
 - 15s - loss: 0.0080 - acc: 0.9966 - mDice: 0.9056 - val_loss: 0.0098 - val_acc: 0.9958 - val_mDice: 0.8868

Epoch 00023: val_mDice did not improve from 0.63868
Epoch 24/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8396 - val_loss: 0.0048 - val_acc: 0.9986 - val_mDice: 0.6503

Epoch 00025: val_mDice did not improve from 0.67962
Epoch 26/100
 - 4s - loss: 0.0030 - acc: 0.9987 - mDice: 0.8413 - val_loss: 0.0125 - val_acc: 0.9967 - val_mDice: 0.6710

Epoch 00023: val_mDice did not improve from 0.54574
Epoch 24/100
 - 4s - loss: 6.6503e-04 - acc: 0.9997 - mDice: 0.6724 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.5406

Epoch 00024: val_mDice improved from 0.63868 to 0.65032, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 25/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8420 - val_loss: 0.0048 - val_acc: 0.9986 - val_mDice: 0.6365

Epoch 00026: val_mDice did not improve from 0.67962
Epoch 27/100
 - 4s - loss: 0.0029 - acc: 0.9988 - mDice: 0.8456 - val_loss: 0.0120 - val_acc: 0.9968 - val_mDice: 0.6607

Epoch 00024: val_mDice did not improve from 0.54574
Epoch 25/100
 - 4s - loss: 6.5992e-04 - acc: 0.9997 - mDice: 0.6770 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5391

Epoch 00025: val_mDice did not improve from 0.65032
Epoch 26/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8424 - val_loss: 0.0049 - val_acc: 0.9986 - val_mDice: 0.6273

Epoch 00025: val_mDice did not improve from 0.54574
Epoch 26/100
 - 4s - loss: 6.4559e-04 - acc: 0.9997 - mDice: 0.6806 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.5421

Epoch 00027: val_mDice did not improve from 0.67962
Epoch 28/100
 - 4s - loss: 0.0029 - acc: 0.9988 - mDice: 0.8454 - val_loss: 0.0115 - val_acc: 0.9968 - val_mDice: 0.6782

Epoch 00026: val_mDice did not improve from 0.65032
Epoch 27/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8441 - val_loss: 0.0046 - val_acc: 0.9987 - val_mDice: 0.6463

Epoch 00026: val_mDice did not improve from 0.54574
Epoch 27/100
 - 4s - loss: 6.3939e-04 - acc: 0.9997 - mDice: 0.6825 - val_loss: 0.0015 - val_acc: 0.9995 - val_mDice: 0.5415

Epoch 00028: val_mDice did not improve from 0.67962
Epoch 29/100
 - 4s - loss: 0.0028 - acc: 0.9988 - mDice: 0.8484 - val_loss: 0.0130 - val_acc: 0.9966 - val_mDice: 0.6727

Epoch 00006: val_mDice improved from 0.88511 to 0.88676, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/100
 - 15s - loss: 0.0077 - acc: 0.9967 - mDice: 0.9092 - val_loss: 0.0102 - val_acc: 0.9957 - val_mDice: 0.8919

Epoch 00027: val_mDice did not improve from 0.65032

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 28/100
 - 4s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8492 - val_loss: 0.0049 - val_acc: 0.9986 - val_mDice: 0.6567

Epoch 00027: val_mDice did not improve from 0.54574
Epoch 28/100
 - 4s - loss: 6.4413e-04 - acc: 0.9997 - mDice: 0.6841 - val_loss: 0.0018 - val_acc: 0.9993 - val_mDice: 0.5264

Epoch 00029: val_mDice did not improve from 0.67962
Epoch 30/100
 - 4s - loss: 0.0028 - acc: 0.9988 - mDice: 0.8495 - val_loss: 0.0125 - val_acc: 0.9966 - val_mDice: 0.6748

Epoch 00028: val_mDice improved from 0.65032 to 0.65666, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 29/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8512 - val_loss: 0.0049 - val_acc: 0.9986 - val_mDice: 0.6495

Epoch 00028: val_mDice did not improve from 0.54574
Epoch 29/100
 - 4s - loss: 6.3662e-04 - acc: 0.9997 - mDice: 0.6853 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5613

Epoch 00030: val_mDice did not improve from 0.67962
Epoch 31/100
 - 4s - loss: 0.0028 - acc: 0.9988 - mDice: 0.8503 - val_loss: 0.0120 - val_acc: 0.9968 - val_mDice: 0.6781

Epoch 00029: val_mDice did not improve from 0.65666
Epoch 30/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8531 - val_loss: 0.0044 - val_acc: 0.9987 - val_mDice: 0.6600

Epoch 00029: val_mDice improved from 0.54574 to 0.56131, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 30/100
 - 4s - loss: 6.1833e-04 - acc: 0.9997 - mDice: 0.6947 - val_loss: 0.0015 - val_acc: 0.9995 - val_mDice: 0.5507

Epoch 00031: val_mDice did not improve from 0.67962
Epoch 32/100
 - 4s - loss: 0.0027 - acc: 0.9988 - mDice: 0.8536 - val_loss: 0.0123 - val_acc: 0.9968 - val_mDice: 0.6774

Epoch 00030: val_mDice improved from 0.65666 to 0.66003, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/5-VLa/sd2/best_model_weights.h5
Epoch 31/100
 - 4s - loss: 0.0010 - acc: 0.9995 - mDice: 0.8517 - val_loss: 0.0047 - val_acc: 0.9987 - val_mDice: 0.6504

Epoch 00030: val_mDice did not improve from 0.56131
Epoch 31/100
 - 4s - loss: 6.1508e-04 - acc: 0.9997 - mDice: 0.6958 - val_loss: 0.0014 - val_acc: 0.9995 - val_mDice: 0.5545

Epoch 00032: val_mDice did not improve from 0.67962
Epoch 33/100
 - 4s - loss: 0.0027 - acc: 0.9988 - mDice: 0.8546 - val_loss: 0.0118 - val_acc: 0.9968 - val_mDice: 0.6760

Epoch 00007: val_mDice improved from 0.88676 to 0.89189, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/100
 - 15s - loss: 0.0074 - acc: 0.9968 - mDice: 0.9132 - val_loss: 0.0104 - val_acc: 0.9957 - val_mDice: 0.8875

Epoch 00031: val_mDice did not improve from 0.66003
Epoch 32/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8518 - val_loss: 0.0047 - val_acc: 0.9987 - val_mDice: 0.6444

Epoch 00031: val_mDice did not improve from 0.56131
Epoch 32/100
 - 4s - loss: 6.0266e-04 - acc: 0.9997 - mDice: 0.7030 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5481

Epoch 00033: val_mDice did not improve from 0.67962
Epoch 34/100
 - 4s - loss: 0.0027 - acc: 0.9988 - mDice: 0.8535 - val_loss: 0.0116 - val_acc: 0.9969 - val_mDice: 0.6806

Epoch 00032: val_mDice did not improve from 0.66003

Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 33/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8562 - val_loss: 0.0049 - val_acc: 0.9986 - val_mDice: 0.6484

Epoch 00032: val_mDice did not improve from 0.56131
Epoch 33/100
 - 4s - loss: 6.0164e-04 - acc: 0.9997 - mDice: 0.6986 - val_loss: 0.0016 - val_acc: 0.9994 - val_mDice: 0.5504

Epoch 00034: val_mDice improved from 0.67962 to 0.68055, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 35/100
 - 4s - loss: 0.0027 - acc: 0.9988 - mDice: 0.8558 - val_loss: 0.0123 - val_acc: 0.9967 - val_mDice: 0.6680

Epoch 00033: val_mDice did not improve from 0.66003
Epoch 34/100
 - 4s - loss: 9.9950e-04 - acc: 0.9996 - mDice: 0.8574 - val_loss: 0.0047 - val_acc: 0.9987 - val_mDice: 0.6488

Epoch 00033: val_mDice did not improve from 0.56131
Epoch 34/100
 - 4s - loss: 6.2400e-04 - acc: 0.9997 - mDice: 0.6937 - val_loss: 0.0015 - val_acc: 0.9995 - val_mDice: 0.5428

Epoch 00035: val_mDice did not improve from 0.68055
Epoch 36/100
 - 4s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8595 - val_loss: 0.0127 - val_acc: 0.9967 - val_mDice: 0.6683

Epoch 00034: val_mDice did not improve from 0.66003
Epoch 35/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8580 - val_loss: 0.0046 - val_acc: 0.9987 - val_mDice: 0.6550

Epoch 00034: val_mDice did not improve from 0.56131
Epoch 35/100
 - 4s - loss: 6.1268e-04 - acc: 0.9997 - mDice: 0.6974 - val_loss: 0.0019 - val_acc: 0.9993 - val_mDice: 0.5139

Epoch 00036: val_mDice did not improve from 0.68055
Epoch 37/100
 - 4s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8584 - val_loss: 0.0123 - val_acc: 0.9968 - val_mDice: 0.6777

Epoch 00008: val_mDice did not improve from 0.89189
Epoch 9/100
 - 15s - loss: 0.0071 - acc: 0.9970 - mDice: 0.9163 - val_loss: 0.0118 - val_acc: 0.9952 - val_mDice: 0.8884

Epoch 00035: val_mDice did not improve from 0.66003
Epoch 36/100
 - 4s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8562 - val_loss: 0.0048 - val_acc: 0.9986 - val_mDice: 0.6553

Epoch 00035: val_mDice did not improve from 0.56131
Epoch 36/100
 - 4s - loss: 5.9361e-04 - acc: 0.9997 - mDice: 0.7058 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5421

Epoch 00037: val_mDice did not improve from 0.68055
Epoch 38/100
 - 4s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8619 - val_loss: 0.0126 - val_acc: 0.9967 - val_mDice: 0.6757

Epoch 00036: val_mDice did not improve from 0.66003
Epoch 37/100
 - 4s - loss: 9.9370e-04 - acc: 0.9996 - mDice: 0.8595 - val_loss: 0.0050 - val_acc: 0.9986 - val_mDice: 0.6500

Epoch 00036: val_mDice did not improve from 0.56131

Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 37/100
 - 4s - loss: 5.5541e-04 - acc: 0.9998 - mDice: 0.7231 - val_loss: 0.0017 - val_acc: 0.9995 - val_mDice: 0.5647

Epoch 00038: val_mDice did not improve from 0.68055
Epoch 39/100
 - 4s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8626 - val_loss: 0.0135 - val_acc: 0.9965 - val_mDice: 0.6758

Epoch 00037: val_mDice did not improve from 0.66003

Epoch 00037: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 38/100
 - 4s - loss: 9.9109e-04 - acc: 0.9996 - mDice: 0.8596 - val_loss: 0.0049 - val_acc: 0.9987 - val_mDice: 0.6515

Epoch 00037: val_mDice improved from 0.56131 to 0.56467, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 38/100
 - 4s - loss: 5.4948e-04 - acc: 0.9998 - mDice: 0.7247 - val_loss: 0.0020 - val_acc: 0.9993 - val_mDice: 0.5442

Epoch 00039: val_mDice did not improve from 0.68055
Epoch 40/100
 - 4s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8631 - val_loss: 0.0134 - val_acc: 0.9964 - val_mDice: 0.6804

Epoch 00038: val_mDice did not improve from 0.66003
Epoch 39/100
 - 4s - loss: 9.8660e-04 - acc: 0.9996 - mDice: 0.8599 - val_loss: 0.0048 - val_acc: 0.9987 - val_mDice: 0.6540

Epoch 00038: val_mDice did not improve from 0.56467
Epoch 39/100
 - 4s - loss: 5.4119e-04 - acc: 0.9998 - mDice: 0.7289 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5561

Epoch 00040: val_mDice did not improve from 0.68055

Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 41/100
 - 4s - loss: 0.0024 - acc: 0.9989 - mDice: 0.8680 - val_loss: 0.0123 - val_acc: 0.9969 - val_mDice: 0.6878

Epoch 00009: val_mDice did not improve from 0.89189
Epoch 10/100
 - 15s - loss: 0.0069 - acc: 0.9970 - mDice: 0.9188 - val_loss: 0.0103 - val_acc: 0.9958 - val_mDice: 0.8916

Epoch 00039: val_mDice did not improve from 0.66003
Epoch 40/100
 - 4s - loss: 9.8402e-04 - acc: 0.9996 - mDice: 0.8598 - val_loss: 0.0047 - val_acc: 0.9987 - val_mDice: 0.6579

Epoch 00039: val_mDice did not improve from 0.56467
Epoch 40/100
 - 4s - loss: 5.3525e-04 - acc: 0.9998 - mDice: 0.7328 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5668

Epoch 00041: val_mDice improved from 0.68055 to 0.68785, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 42/100
 - 4s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8722 - val_loss: 0.0127 - val_acc: 0.9968 - val_mDice: 0.6840

Epoch 00040: val_mDice did not improve from 0.66003
Epoch 41/100
 - 4s - loss: 9.7725e-04 - acc: 0.9996 - mDice: 0.8607 - val_loss: 0.0048 - val_acc: 0.9987 - val_mDice: 0.6459

Epoch 00040: val_mDice improved from 0.56467 to 0.56679, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 41/100
 - 4s - loss: 5.3252e-04 - acc: 0.9998 - mDice: 0.7325 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5534

Epoch 00042: val_mDice did not improve from 0.68785
Epoch 43/100
 - 4s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8727 - val_loss: 0.0130 - val_acc: 0.9967 - val_mDice: 0.6809

Epoch 00041: val_mDice did not improve from 0.66003

Epoch 00041: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 42/100
 - 4s - loss: 9.8363e-04 - acc: 0.9996 - mDice: 0.8607 - val_loss: 0.0048 - val_acc: 0.9987 - val_mDice: 0.6598

Epoch 00041: val_mDice did not improve from 0.56679
Epoch 42/100
 - 4s - loss: 5.2899e-04 - acc: 0.9998 - mDice: 0.7350 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5624

Epoch 00043: val_mDice did not improve from 0.68785
Epoch 44/100
 - 4s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8735 - val_loss: 0.0131 - val_acc: 0.9968 - val_mDice: 0.6837

Epoch 00042: val_mDice did not improve from 0.66003
Epoch 43/100
 - 4s - loss: 9.7463e-04 - acc: 0.9996 - mDice: 0.8613 - val_loss: 0.0048 - val_acc: 0.9987 - val_mDice: 0.6557

Epoch 00042: val_mDice did not improve from 0.56679
Epoch 43/100
 - 4s - loss: 5.2212e-04 - acc: 0.9998 - mDice: 0.7396 - val_loss: 0.0016 - val_acc: 0.9995 - val_mDice: 0.5667

Epoch 00044: val_mDice did not improve from 0.68785
Epoch 45/100
 - 4s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8763 - val_loss: 0.0132 - val_acc: 0.9968 - val_mDice: 0.6900

Epoch 00010: val_mDice did not improve from 0.89189
Epoch 11/100
 - 15s - loss: 0.0067 - acc: 0.9971 - mDice: 0.9218 - val_loss: 0.0108 - val_acc: 0.9956 - val_mDice: 0.8902

Epoch 00043: val_mDice did not improve from 0.66003
Epoch 44/100
 - 4s - loss: 9.8121e-04 - acc: 0.9996 - mDice: 0.8607 - val_loss: 0.0049 - val_acc: 0.9987 - val_mDice: 0.6554

Epoch 00043: val_mDice did not improve from 0.56679
Epoch 44/100
 - 4s - loss: 5.2387e-04 - acc: 0.9998 - mDice: 0.7363 - val_loss: 0.0016 - val_acc: 0.9995 - val_mDice: 0.5761

Epoch 00045: val_mDice improved from 0.68785 to 0.68997, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 46/100
 - 4s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8750 - val_loss: 0.0129 - val_acc: 0.9967 - val_mDice: 0.6876

Epoch 00044: val_mDice did not improve from 0.66003
Epoch 45/100
 - 4s - loss: 9.7729e-04 - acc: 0.9996 - mDice: 0.8610 - val_loss: 0.0049 - val_acc: 0.9987 - val_mDice: 0.6551

Epoch 00044: val_mDice improved from 0.56679 to 0.57615, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 45/100
 - 4s - loss: 5.1889e-04 - acc: 0.9998 - mDice: 0.7389 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5576

Epoch 00046: val_mDice did not improve from 0.68997
Epoch 47/100
 - 4s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8755 - val_loss: 0.0129 - val_acc: 0.9968 - val_mDice: 0.6880

Epoch 00045: val_mDice did not improve from 0.66003
Epoch 46/100
 - 4s - loss: 9.7106e-04 - acc: 0.9996 - mDice: 0.8620 - val_loss: 0.0048 - val_acc: 0.9987 - val_mDice: 0.6523

Epoch 00045: val_mDice did not improve from 0.57615
Epoch 46/100
 - 4s - loss: 5.1492e-04 - acc: 0.9998 - mDice: 0.7404 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5596

Epoch 00047: val_mDice did not improve from 0.68997
Epoch 48/100
 - 4s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8766 - val_loss: 0.0129 - val_acc: 0.9968 - val_mDice: 0.6894

Epoch 00046: val_mDice did not improve from 0.66003
Epoch 47/100
 - 4s - loss: 9.6717e-04 - acc: 0.9996 - mDice: 0.8627 - val_loss: 0.0049 - val_acc: 0.9987 - val_mDice: 0.6484

Epoch 00046: val_mDice did not improve from 0.57615
Epoch 47/100
 - 4s - loss: 5.1127e-04 - acc: 0.9998 - mDice: 0.7447 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5664

Epoch 00048: val_mDice did not improve from 0.68997

Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 49/100
 - 4s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8792 - val_loss: 0.0131 - val_acc: 0.9968 - val_mDice: 0.6894

Epoch 00011: val_mDice did not improve from 0.89189
Epoch 12/100
 - 15s - loss: 0.0065 - acc: 0.9972 - mDice: 0.9238 - val_loss: 0.0105 - val_acc: 0.9959 - val_mDice: 0.8947

Epoch 00047: val_mDice did not improve from 0.66003

Epoch 00047: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 48/100
 - 4s - loss: 9.7614e-04 - acc: 0.9996 - mDice: 0.8624 - val_loss: 0.0049 - val_acc: 0.9987 - val_mDice: 0.6513

Epoch 00047: val_mDice did not improve from 0.57615
Epoch 48/100
 - 4s - loss: 5.1266e-04 - acc: 0.9998 - mDice: 0.7433 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5590

Epoch 00049: val_mDice did not improve from 0.68997
Epoch 50/100
 - 4s - loss: 0.0022 - acc: 0.9990 - mDice: 0.8816 - val_loss: 0.0131 - val_acc: 0.9968 - val_mDice: 0.6890

Epoch 00048: val_mDice did not improve from 0.66003
Epoch 49/100
 - 4s - loss: 9.7324e-04 - acc: 0.9996 - mDice: 0.8619 - val_loss: 0.0049 - val_acc: 0.9987 - val_mDice: 0.6523

Epoch 00048: val_mDice did not improve from 0.57615
Epoch 49/100
 - 4s - loss: 5.0654e-04 - acc: 0.9998 - mDice: 0.7440 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5647

Epoch 00050: val_mDice did not improve from 0.68997
Epoch 51/100
 - 4s - loss: 0.0022 - acc: 0.9990 - mDice: 0.8810 - val_loss: 0.0134 - val_acc: 0.9969 - val_mDice: 0.6908

Epoch 00049: val_mDice did not improve from 0.66003
Epoch 50/100
 - 4s - loss: 9.6828e-04 - acc: 0.9996 - mDice: 0.8631 - val_loss: 0.0050 - val_acc: 0.9987 - val_mDice: 0.6492

Epoch 00049: val_mDice did not improve from 0.57615
Epoch 50/100
 - 4s - loss: 5.1041e-04 - acc: 0.9998 - mDice: 0.7444 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5747

predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:01,  2.20it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.82it/s]predicting test subjects:  80%|████████  | 4/5 [00:00<00:00,  3.69it/s]predicting test subjects: 100%|██████████| 5/5 [00:00<00:00,  4.43it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:   1%|▏         | 2/148 [00:00<00:13, 10.55it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:14, 10.00it/s]predicting train subjects:   3%|▎         | 4/148 [00:00<00:15,  9.46it/s]predicting train subjects:   3%|▎         | 5/148 [00:00<00:14,  9.56it/s]predicting train subjects:   4%|▍         | 6/148 [00:00<00:14,  9.55it/s]predicting train subjects:   5%|▍         | 7/148 [00:00<00:15,  9.16it/s]predicting train subjects:   5%|▌         | 8/148 [00:00<00:15,  9.22it/s]
Epoch 00051: val_mDice improved from 0.68997 to 0.69083, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 52/100
 - 4s - loss: 0.0022 - acc: 0.9990 - mDice: 0.8816 - val_loss: 0.0135 - val_acc: 0.9968 - val_mDice: 0.6856
predicting train subjects:   7%|▋         | 10/148 [00:01<00:14,  9.78it/s]predicting train subjects:   8%|▊         | 12/148 [00:01<00:13,  9.99it/s]predicting train subjects:   9%|▉         | 14/148 [00:01<00:13, 10.28it/s]predicting train subjects:  11%|█         | 16/148 [00:01<00:12, 10.93it/s]predicting train subjects:  12%|█▏        | 18/148 [00:01<00:12, 10.49it/s]predicting train subjects:  14%|█▎        | 20/148 [00:01<00:11, 10.70it/s]predicting train subjects:  15%|█▍        | 22/148 [00:02<00:11, 10.71it/s]predicting train subjects:  16%|█▌        | 24/148 [00:02<00:11, 10.51it/s]predicting train subjects:  18%|█▊        | 26/148 [00:02<00:11, 10.39it/s]predicting train subjects:  19%|█▉        | 28/148 [00:02<00:11, 10.31it/s]predicting train subjects:  20%|██        | 30/148 [00:02<00:11, 10.26it/s]predicting train subjects:  22%|██▏       | 32/148 [00:03<00:11, 10.25it/s]predicting train subjects:  23%|██▎       | 34/148 [00:03<00:11, 10.14it/s]predicting train subjects:  24%|██▍       | 36/148 [00:03<00:11, 10.07it/s]
Epoch 00050: val_mDice did not improve from 0.57615

Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 51/100
 - 4s - loss: 4.9145e-04 - acc: 0.9998 - mDice: 0.7534 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5631
predicting train subjects:  26%|██▌       | 38/148 [00:03<00:11,  9.81it/s]predicting train subjects:  27%|██▋       | 40/148 [00:03<00:10,  9.87it/s]predicting train subjects:  28%|██▊       | 42/148 [00:04<00:10, 10.02it/s]predicting train subjects:  30%|██▉       | 44/148 [00:04<00:10, 10.18it/s]predicting train subjects:  31%|███       | 46/148 [00:04<00:10, 10.12it/s]predicting train subjects:  32%|███▏      | 48/148 [00:04<00:10,  9.64it/s]predicting train subjects:  34%|███▍      | 50/148 [00:04<00:09,  9.86it/s]predicting train subjects:  35%|███▌      | 52/148 [00:05<00:09, 10.08it/s]predicting train subjects:  36%|███▋      | 54/148 [00:05<00:09,  9.93it/s]predicting train subjects:  37%|███▋      | 55/148 [00:05<00:09,  9.86it/s]predicting train subjects:  38%|███▊      | 56/148 [00:05<00:09,  9.68it/s]predicting train subjects:  39%|███▊      | 57/148 [00:05<00:09,  9.59it/s]predicting train subjects:  40%|███▉      | 59/148 [00:05<00:09,  9.34it/s]predicting train subjects:  41%|████      | 60/148 [00:06<00:09,  9.05it/s]predicting train subjects:  41%|████      | 61/148 [00:06<00:09,  8.81it/s]predicting train subjects:  42%|████▏     | 62/148 [00:06<00:09,  8.76it/s]predicting train subjects:  43%|████▎     | 63/148 [00:06<00:09,  8.67it/s]predicting train subjects:  43%|████▎     | 64/148 [00:06<00:09,  8.67it/s]
Epoch 00052: val_mDice did not improve from 0.69083
Epoch 53/100
 - 4s - loss: 0.0022 - acc: 0.9990 - mDice: 0.8819 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6869
predicting train subjects:  45%|████▍     | 66/148 [00:06<00:08,  9.42it/s]predicting train subjects:  46%|████▌     | 68/148 [00:06<00:07, 10.03it/s]
Epoch 00012: val_mDice improved from 0.89189 to 0.89473, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/100
 - 15s - loss: 0.0064 - acc: 0.9973 - mDice: 0.9249 - val_loss: 0.0106 - val_acc: 0.9958 - val_mDice: 0.8927
predicting train subjects:  47%|████▋     | 70/148 [00:06<00:07, 10.60it/s]predicting train subjects:  49%|████▊     | 72/148 [00:07<00:07, 10.42it/s]predicting train subjects:  50%|█████     | 74/148 [00:07<00:07, 10.13it/s]predicting train subjects:  51%|█████▏    | 76/148 [00:07<00:07, 10.02it/s]predicting train subjects:  53%|█████▎    | 78/148 [00:07<00:06, 10.52it/s]predicting train subjects:  54%|█████▍    | 80/148 [00:07<00:06, 10.96it/s]predicting train subjects:  55%|█████▌    | 82/148 [00:08<00:05, 11.04it/s]predicting train subjects:  57%|█████▋    | 84/148 [00:08<00:05, 11.31it/s]predicting train subjects:  58%|█████▊    | 86/148 [00:08<00:05, 11.19it/s]predicting train subjects:  59%|█████▉    | 88/148 [00:08<00:05, 11.43it/s]predicting train subjects:  61%|██████    | 90/148 [00:08<00:05, 11.14it/s]predicting train subjects:  62%|██████▏   | 92/148 [00:08<00:05, 11.20it/s]
Epoch 00051: val_mDice did not improve from 0.57615
Epoch 52/100
 - 4s - loss: 4.8768e-04 - acc: 0.9998 - mDice: 0.7578 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5710
predicting train subjects:  64%|██████▎   | 94/148 [00:09<00:04, 11.21it/s]predicting train subjects:  65%|██████▍   | 96/148 [00:09<00:04, 12.38it/s]predicting train subjects:  66%|██████▌   | 98/148 [00:09<00:03, 13.22it/s]predicting train subjects:  68%|██████▊   | 100/148 [00:09<00:03, 13.87it/s]predicting train subjects:  69%|██████▉   | 102/148 [00:09<00:03, 12.46it/s]predicting train subjects:  70%|███████   | 104/148 [00:09<00:03, 11.23it/s]predicting train subjects:  72%|███████▏  | 106/148 [00:10<00:03, 10.59it/s]predicting train subjects:  73%|███████▎  | 108/148 [00:10<00:04, 10.00it/s]predicting train subjects:  74%|███████▍  | 110/148 [00:10<00:03, 10.10it/s]predicting train subjects:  76%|███████▌  | 112/148 [00:10<00:03, 10.13it/s]predicting train subjects:  77%|███████▋  | 114/148 [00:10<00:02, 11.37it/s]predicting train subjects:  78%|███████▊  | 116/148 [00:11<00:02, 12.52it/s]predicting train subjects:  80%|███████▉  | 118/148 [00:11<00:02, 13.58it/s]predicting train subjects:  81%|████████  | 120/148 [00:11<00:02, 12.19it/s]predicting train subjects:  82%|████████▏ | 122/148 [00:11<00:02, 11.43it/s]predicting train subjects:  84%|████████▍ | 124/148 [00:11<00:02, 10.63it/s]predicting train subjects:  85%|████████▌ | 126/148 [00:11<00:02, 10.15it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:12<00:02,  9.73it/s]
Epoch 00053: val_mDice did not improve from 0.69083

Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 54/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8829 - val_loss: 0.0134 - val_acc: 0.9969 - val_mDice: 0.6855
predicting train subjects:  88%|████████▊ | 130/148 [00:12<00:01,  9.50it/s]predicting train subjects:  89%|████████▉ | 132/148 [00:12<00:01,  9.76it/s]predicting train subjects:  91%|█████████ | 134/148 [00:12<00:01,  9.93it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:13<00:01, 10.04it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:13<00:00, 10.71it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:13<00:00, 11.25it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:13<00:00, 11.63it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:13<00:00, 10.97it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:13<00:00, 10.56it/s]predicting train subjects: 100%|██████████| 148/148 [00:14<00:00, 10.28it/s]

Epoch 00050: val_mDice did not improve from 0.66003
Restoring model weights from the end of the best epoch
Epoch 00050: early stopping
{'val_loss': [0.0036478322891003276, 0.005247911666222709, 0.0045475171125949699, 0.0043256456190918354, 0.0041138293677346503, 0.0047130190688086321, 0.005019188194753642, 0.0040088265123678019, 0.0043976188716577718, 0.0043647282538895908, 0.0046196077910668036, 0.0041992926791785878, 0.0043626532374702867, 0.0043339576532549044, 0.0044501429384059092, 0.004598965511677113, 0.0042799289655336675, 0.0041897535680773413, 0.004799233244216823, 0.0044505761916491578, 0.0046102680919810814, 0.0044710497926682864, 0.0044594199515562106, 0.0048074995940353008, 0.0047908622097778825, 0.00493118858480073, 0.0046382391171410995, 0.0048608913561923707, 0.004911501386618995, 0.0044442440898335997, 0.0046867020329103828, 0.0047010412796380675, 0.0049161434748229826, 0.0046978841060178076, 0.0046105310717161662, 0.0048224553267689459, 0.0050369673705798514, 0.0048843608733187332, 0.0048355447485091841, 0.004741322724743092, 0.004823092599102157, 0.0048266962428200749, 0.0048267477726999753, 0.0048836627677875631, 0.0049315896479690327, 0.004846516788560659, 0.004942546281567279, 0.004893009322992665, 0.0049121751786863552, 0.004988559809057636], 'val_acc': [0.99858507450590739, 0.99779326357740039, 0.99825362448996691, 0.99831708695026156, 0.99843647505374666, 0.99822344424876763, 0.99822092309911203, 0.99858607510302932, 0.99854629597765332, 0.99847277174604698, 0.99840222901486331, 0.99852514140149384, 0.99860472247955645, 0.99861833643405995, 0.99856545443230482, 0.99854678169209909, 0.99871300509635441, 0.99870145194073945, 0.99856392880703537, 0.99862742043556052, 0.99863495091174515, 0.99869338882730363, 0.99870951505417516, 0.99861832882495638, 0.99858105943558062, 0.99855637169898825, 0.99865612324247965, 0.99863344684560251, 0.99864906326253366, 0.99869287648099536, 0.99867626327149417, 0.9986697004196492, 0.99864453711408252, 0.99872159070156985, 0.99871907462465004, 0.99864905438524609, 0.99862687765283786, 0.99866569929934568, 0.99866772205271614, 0.99867578009341629, 0.99869641471416393, 0.99867976726369656, 0.99868331564233659, 0.99866313883598812, 0.99865458747173874, 0.9986913546602777, 0.99867175360943405, 0.9986545976172102, 0.99866568915387421, 0.9986641521149493], 'val_mDice': [0.43417273937387668, 0.49221131903059939, 0.43668515314447121, 0.55157274768707598, 0.53427197958560702, 0.5604483421812666, 0.57541581798107067, 0.60447161375208103, 0.58457246486176839, 0.60830207581215712, 0.6045285158968986, 0.63498210019253665, 0.61224510568253543, 0.63868481301246804, 0.63072155637943994, 0.60496582376196029, 0.61949964787097689, 0.63459403844589879, 0.59096438834007747, 0.63708433699100575, 0.62929642707743538, 0.63784443698030835, 0.62951940044443655, 0.65031501080127474, 0.63645266725661909, 0.62732031497549501, 0.64629087422756437, 0.65665910725897936, 0.64951780755469135, 0.66003443459247024, 0.65043413258613425, 0.64443086816909467, 0.64840576623348478, 0.64883532803109356, 0.65498510446954283, 0.65525179594121086, 0.6499722422437465, 0.65150363014099444, 0.65404683858790291, 0.65786604297922013, 0.64593955684215465, 0.65981913500643796, 0.65566882300884166, 0.65537039396610663, 0.65513112570377108, 0.65230503741731038, 0.648420199434808, 0.65126696672845397, 0.65229300615635322, 0.6491552198186834], 'loss': [0.0045493334178265598, 0.0031994894783805871, 0.0026850487898686256, 0.0024015930753392653, 0.0021739433989842213, 0.0020237568186772469, 0.0020031794426817549, 0.0017686624638052649, 0.0016820888616953908, 0.0016560682645884637, 0.0015281091204283539, 0.0014704808043312751, 0.0014628756882337029, 0.0014223084155746339, 0.001333121622835925, 0.0013654116339404077, 0.0013163574777653572, 0.0013073078350661071, 0.0013622454278384606, 0.0012170246336202222, 0.001165213921235608, 0.0011445558933480631, 0.0011147858955604746, 0.0011243103290351393, 0.0011099152856909079, 0.0011054976130482519, 0.0010939552847252705, 0.001050366995900437, 0.0010446640861011345, 0.0010366731515604767, 0.0010380763430324145, 0.0010426103775296718, 0.001008473479804869, 0.00099950056167938568, 0.0010075880876523878, 0.001010188064033381, 0.00099370208836939574, 0.00099109305910003104, 0.00098660223763657432, 0.00098402184267078804, 0.00097724961742972944, 0.00098362992713289712, 0.00097462725720219557, 0.00098121148742535513, 0.00097728725373829608, 0.00097105768903906332, 0.00096716999436172993, 0.00097614338433344599, 0.00097324023714496751, 0.0009682792810510293], 'acc': [0.99828396756846516, 0.99867326639949416, 0.99888118062974351, 0.99899821719353288, 0.99908938886235121, 0.99915889858657858, 0.9991672702578972, 0.99925799625647393, 0.99928889425431711, 0.99930495150811771, 0.99935059468673682, 0.99937568386715214, 0.99938072590759608, 0.99939898042902164, 0.9994331274737599, 0.99942162448565719, 0.99943734511175764, 0.99944060626291642, 0.9994225323887731, 0.99948222715927149, 0.99949942477638598, 0.99950999838203369, 0.99952055400837059, 0.99951752458823728, 0.99952133970217283, 0.99952665026560394, 0.99952934759974021, 0.99954772960135507, 0.99955066389572533, 0.99955186488452552, 0.99954993186902019, 0.99955211374068831, 0.99956426575483037, 0.99956764165690959, 0.9995662987711379, 0.99956345724834506, 0.99956887856266463, 0.99957411183380906, 0.9995754354694264, 0.99957202552540736, 0.99957655933260792, 0.99957240748680454, 0.99957525559298355, 0.99957283874026381, 0.99957563640853819, 0.99957984262977773, 0.99957887891371411, 0.99957641074808279, 0.99957722419210771, 0.99958316667727432], 'mDice': [0.39456547692037885, 0.52313647871040792, 0.60520245911399206, 0.64870377703173021, 0.68422457962259464, 0.70802355985548149, 0.71157148179371266, 0.74490660783170359, 0.75649881765001603, 0.76293321203327147, 0.77858864252736459, 0.79040283541794387, 0.78878708891583593, 0.79304943004299489, 0.80969815775465603, 0.80691353289741852, 0.81115733489670228, 0.81211215870149567, 0.80733090262028018, 0.82343069732460017, 0.83246206670572442, 0.83700338725817036, 0.84047718818518546, 0.83961596238117286, 0.84196661492404046, 0.84237818218369198, 0.84407691655980666, 0.84921478823072127, 0.8512162776811687, 0.85309109269885153, 0.85170278554194379, 0.85178292903896813, 0.85623889669843667, 0.85743818210997147, 0.85803829852787195, 0.856245500293073, 0.8594906899696545, 0.85958875388125777, 0.85988088430786669, 0.85982319732430679, 0.86068085751638512, 0.86073174800792807, 0.86134920410658256, 0.86069136395482737, 0.86099283826888673, 0.86197089161417861, 0.86273111389967494, 0.86244442070417993, 0.86192977751355537, 0.86306089089909288], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05]}

Epoch 00052: val_mDice did not improve from 0.57615
Epoch 53/100
 - 4s - loss: 4.8418e-04 - acc: 0.9998 - mDice: 0.7561 - val_loss: 0.0017 - val_acc: 0.9995 - val_mDice: 0.5764

Epoch 00054: val_mDice did not improve from 0.69083
Epoch 55/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8832 - val_loss: 0.0134 - val_acc: 0.9968 - val_mDice: 0.6885

Epoch 00053: val_mDice improved from 0.57615 to 0.57641, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 54/100
 - 4s - loss: 4.8377e-04 - acc: 0.9998 - mDice: 0.7564 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5711

Epoch 00055: val_mDice did not improve from 0.69083
Epoch 56/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8841 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6877

Epoch 00054: val_mDice did not improve from 0.57641
Epoch 55/100
 - 4s - loss: 4.8149e-04 - acc: 0.9998 - mDice: 0.7585 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5604

Epoch 00056: val_mDice did not improve from 0.69083
Epoch 57/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8834 - val_loss: 0.0136 - val_acc: 0.9968 - val_mDice: 0.6903

Epoch 00013: val_mDice did not improve from 0.89473
Epoch 14/100
 - 15s - loss: 0.0062 - acc: 0.9974 - mDice: 0.9278 - val_loss: 0.0109 - val_acc: 0.9957 - val_mDice: 0.8926

Epoch 00055: val_mDice did not improve from 0.57641
Epoch 56/100
 - 4s - loss: 4.7890e-04 - acc: 0.9998 - mDice: 0.7600 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5700

Epoch 00057: val_mDice did not improve from 0.69083

Epoch 00057: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 58/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8854 - val_loss: 0.0136 - val_acc: 0.9968 - val_mDice: 0.6889

Epoch 00056: val_mDice did not improve from 0.57641
Epoch 57/100
 - 4s - loss: 4.7673e-04 - acc: 0.9998 - mDice: 0.7632 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5862

Epoch 00058: val_mDice did not improve from 0.69083
Epoch 59/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8844 - val_loss: 0.0138 - val_acc: 0.9968 - val_mDice: 0.6900

Epoch 00057: val_mDice improved from 0.57641 to 0.58623, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/14-MTT/sd2/best_model_weights.h5
Epoch 58/100
 - 4s - loss: 4.7933e-04 - acc: 0.9998 - mDice: 0.7573 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5726

Epoch 00059: val_mDice did not improve from 0.69083
Epoch 60/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8843 - val_loss: 0.0136 - val_acc: 0.9968 - val_mDice: 0.6911

Epoch 00058: val_mDice did not improve from 0.58623
Epoch 59/100
 - 4s - loss: 4.7003e-04 - acc: 0.9998 - mDice: 0.7625 - val_loss: 0.0017 - val_acc: 0.9995 - val_mDice: 0.5733

Epoch 00014: val_mDice did not improve from 0.89473
Epoch 15/100
 - 15s - loss: 0.0061 - acc: 0.9974 - mDice: 0.9291 - val_loss: 0.0111 - val_acc: 0.9957 - val_mDice: 0.8953

Epoch 00060: val_mDice improved from 0.69083 to 0.69110, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 61/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8849 - val_loss: 0.0135 - val_acc: 0.9969 - val_mDice: 0.6905

Epoch 00059: val_mDice did not improve from 0.58623

Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 60/100
 - 4s - loss: 4.6625e-04 - acc: 0.9998 - mDice: 0.7656 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5710

Epoch 00061: val_mDice did not improve from 0.69110
Epoch 62/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8857 - val_loss: 0.0136 - val_acc: 0.9969 - val_mDice: 0.6898

Epoch 00060: val_mDice did not improve from 0.58623
Epoch 61/100
 - 4s - loss: 4.6805e-04 - acc: 0.9998 - mDice: 0.7627 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5713

Epoch 00062: val_mDice did not improve from 0.69110

Epoch 00062: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 63/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8855 - val_loss: 0.0135 - val_acc: 0.9969 - val_mDice: 0.6930

Epoch 00061: val_mDice did not improve from 0.58623
Epoch 62/100
 - 4s - loss: 4.6049e-04 - acc: 0.9998 - mDice: 0.7660 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5774

Epoch 00063: val_mDice improved from 0.69110 to 0.69299, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/4-VA/sd2/best_model_weights.h5
Epoch 64/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8856 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6914

Epoch 00062: val_mDice did not improve from 0.58623
Epoch 63/100
 - 4s - loss: 4.6073e-04 - acc: 0.9998 - mDice: 0.7676 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5716

Epoch 00015: val_mDice improved from 0.89473 to 0.89530, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/100
 - 15s - loss: 0.0061 - acc: 0.9974 - mDice: 0.9291 - val_loss: 0.0113 - val_acc: 0.9958 - val_mDice: 0.8983

Epoch 00064: val_mDice did not improve from 0.69299
Epoch 65/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8859 - val_loss: 0.0137 - val_acc: 0.9969 - val_mDice: 0.6880

Epoch 00063: val_mDice did not improve from 0.58623
Epoch 64/100
 - 4s - loss: 4.6155e-04 - acc: 0.9998 - mDice: 0.7657 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5695

Epoch 00064: val_mDice did not improve from 0.58623

Epoch 00064: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 65/100
 - 4s - loss: 4.5351e-04 - acc: 0.9998 - mDice: 0.7678 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5749

Epoch 00065: val_mDice did not improve from 0.69299
Epoch 66/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8864 - val_loss: 0.0135 - val_acc: 0.9968 - val_mDice: 0.6903

Epoch 00065: val_mDice did not improve from 0.58623
Epoch 66/100
 - 4s - loss: 4.5554e-04 - acc: 0.9998 - mDice: 0.7678 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5812

Epoch 00066: val_mDice did not improve from 0.69299

Epoch 00066: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 67/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8861 - val_loss: 0.0140 - val_acc: 0.9968 - val_mDice: 0.6892

Epoch 00066: val_mDice did not improve from 0.58623
Epoch 67/100
 - 4s - loss: 4.5726e-04 - acc: 0.9998 - mDice: 0.7681 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5737

Epoch 00067: val_mDice did not improve from 0.69299
Epoch 68/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8860 - val_loss: 0.0138 - val_acc: 0.9968 - val_mDice: 0.6893

Epoch 00016: val_mDice improved from 0.89530 to 0.89830, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 17/100
 - 15s - loss: 0.0058 - acc: 0.9975 - mDice: 0.9323 - val_loss: 0.0108 - val_acc: 0.9960 - val_mDice: 0.9023

Epoch 00067: val_mDice did not improve from 0.58623
Epoch 68/100
 - 4s - loss: 4.5195e-04 - acc: 0.9998 - mDice: 0.7683 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5771

Epoch 00068: val_mDice did not improve from 0.69299
Epoch 69/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8863 - val_loss: 0.0138 - val_acc: 0.9968 - val_mDice: 0.6887

Epoch 00068: val_mDice did not improve from 0.58623

Epoch 00068: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 69/100
 - 4s - loss: 4.5474e-04 - acc: 0.9998 - mDice: 0.7706 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5732

Epoch 00069: val_mDice did not improve from 0.69299
Epoch 70/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8861 - val_loss: 0.0135 - val_acc: 0.9969 - val_mDice: 0.6910

Epoch 00069: val_mDice did not improve from 0.58623
Epoch 70/100
 - 4s - loss: 4.5550e-04 - acc: 0.9998 - mDice: 0.7680 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5764

Epoch 00070: val_mDice did not improve from 0.69299

Epoch 00070: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 71/100
 - 4s - loss: 0.0022 - acc: 0.9991 - mDice: 0.8858 - val_loss: 0.0135 - val_acc: 0.9968 - val_mDice: 0.6893

Epoch 00070: val_mDice did not improve from 0.58623
Epoch 71/100
 - 4s - loss: 4.5547e-04 - acc: 0.9998 - mDice: 0.7677 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5785

Epoch 00071: val_mDice did not improve from 0.69299
Epoch 72/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8874 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6900

Epoch 00071: val_mDice did not improve from 0.58623
Epoch 72/100
 - 4s - loss: 4.5523e-04 - acc: 0.9998 - mDice: 0.7695 - val_loss: 0.0017 - val_acc: 0.9994 - val_mDice: 0.5760

Epoch 00017: val_mDice improved from 0.89830 to 0.90231, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/100
 - 15s - loss: 0.0057 - acc: 0.9975 - mDice: 0.9330 - val_loss: 0.0129 - val_acc: 0.9954 - val_mDice: 0.8963

Epoch 00072: val_mDice did not improve from 0.69299
Epoch 73/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8870 - val_loss: 0.0140 - val_acc: 0.9968 - val_mDice: 0.6907

Epoch 00072: val_mDice did not improve from 0.58623
Epoch 73/100
 - 4s - loss: 4.4971e-04 - acc: 0.9998 - mDice: 0.7692 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5787

Epoch 00073: val_mDice did not improve from 0.69299
Epoch 74/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8876 - val_loss: 0.0142 - val_acc: 0.9968 - val_mDice: 0.6892

Epoch 00073: val_mDice did not improve from 0.58623

Epoch 00073: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 74/100
 - 4s - loss: 4.5467e-04 - acc: 0.9998 - mDice: 0.7709 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5780

Epoch 00074: val_mDice did not improve from 0.69299

Epoch 00074: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 75/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8882 - val_loss: 0.0140 - val_acc: 0.9968 - val_mDice: 0.6916

Epoch 00074: val_mDice did not improve from 0.58623
Epoch 75/100
 - 4s - loss: 4.5201e-04 - acc: 0.9998 - mDice: 0.7689 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5805

Epoch 00075: val_mDice did not improve from 0.69299
Epoch 76/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8876 - val_loss: 0.0138 - val_acc: 0.9969 - val_mDice: 0.6911

Epoch 00075: val_mDice did not improve from 0.58623
Epoch 76/100
 - 4s - loss: 4.5266e-04 - acc: 0.9998 - mDice: 0.7675 - val_loss: 0.0018 - val_acc: 0.9994 - val_mDice: 0.5814

Epoch 00018: val_mDice did not improve from 0.90231
Epoch 19/100
 - 15s - loss: 0.0056 - acc: 0.9976 - mDice: 0.9343 - val_loss: 0.0112 - val_acc: 0.9959 - val_mDice: 0.9013

Epoch 00076: val_mDice did not improve from 0.69299
Epoch 77/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8873 - val_loss: 0.0140 - val_acc: 0.9968 - val_mDice: 0.6906

Epoch 00076: val_mDice did not improve from 0.58623
Epoch 77/100
 - 4s - loss: 4.5261e-04 - acc: 0.9998 - mDice: 0.7703 - val_loss: 0.0019 - val_acc: 0.9994 - val_mDice: 0.5765

Epoch 00077: val_mDice did not improve from 0.58623

Epoch 00077: ReduceLROnPlateau reducing learning rate to 9e-05.
Restoring model weights from the end of the best epoch
Epoch 00077: early stopping
{'val_loss': [0.0011330378425129233, 0.001269407359823743, 0.0014335913063482717, 0.001742233819466956, 0.0012069635657950284, 0.0013121944540081189, 0.0013924834536428146, 0.0017311638666990591, 0.001911601675872473, 0.001556276980331762, 0.0015168518491802698, 0.001891158352744706, 0.0018353958445739872, 0.0016146603565504575, 0.0015322527119969117, 0.0017722922159319229, 0.0013846432359175796, 0.0015601126967552811, 0.0015738246605751362, 0.0016206659852190221, 0.0017846812682345192, 0.001522361102731938, 0.0014591598685117478, 0.0016220165317521451, 0.0016509869849269695, 0.0015974381487776942, 0.0015235955906199648, 0.0018467877030451882, 0.0016663929947869892, 0.001501107583456534, 0.0014088412999433088, 0.0017215627731081652, 0.0015794854447681536, 0.0014607990259661318, 0.00191181573144933, 0.001727146195604446, 0.0016626917900755367, 0.0020103879827768245, 0.0016897775733130092, 0.0016706253332581292, 0.0017027517835153862, 0.0017557917360929733, 0.0016421763488943589, 0.0016247008684428131, 0.001838343579599515, 0.0016879642461525633, 0.0017648490661002221, 0.0017865114211243517, 0.0016987655222653708, 0.0016827396924627271, 0.0017636801200383839, 0.0017748575489175446, 0.0016515598721564451, 0.0017173127766619337, 0.0017504324205219746, 0.0017026097692073659, 0.0017700782615414007, 0.0018266144238333118, 0.0017302871305257717, 0.0017624735941198912, 0.0017671995137402035, 0.0017285047227794186, 0.0017473256968437358, 0.0017701492352569675, 0.0018481842460150413, 0.0017513164685682414, 0.0017450447144739805, 0.001829271150277333, 0.0018433991353958845, 0.0017635958997453463, 0.0017522103326553678, 0.0017436065468342697, 0.0018475518334022862, 0.0018169201831234262, 0.0017640738837183157, 0.0017612733026134207, 0.0018579383291541895], 'val_acc': [0.99961265097273155, 0.99955470257617063, 0.99944594312221446, 0.99923841496731369, 0.99955875949656714, 0.99949325779651077, 0.9994640565933065, 0.99923437199694043, 0.99918953281767819, 0.99935476577028315, 0.99938296764454948, 0.99924798341507604, 0.99927668875836306, 0.99937842881425898, 0.99940108872474509, 0.99926155044677412, 0.99948673172199975, 0.99937487916743506, 0.99938697383758868, 0.99937340300133892, 0.99930789115581109, 0.99941770573879807, 0.99944340041343205, 0.99941469380196102, 0.99941316817669157, 0.99943080607880941, 0.99950634291831486, 0.99929783699360297, 0.99943131715693367, 0.99947314059480707, 0.99948674186747122, 0.99937339285586746, 0.99944239347539054, 0.99951996828647371, 0.99932199589749604, 0.9994005814511725, 0.99945597445711176, 0.9993028716838106, 0.99940059540119575, 0.99944190522457688, 0.9994197437103759, 0.99939554422459709, 0.99945296125209082, 0.99945146099049997, 0.99937037584629462, 0.9994283140973842, 0.99939657779450108, 0.99939958465860246, 0.9994157070809222, 0.99942878205725483, 0.99942023703392513, 0.99940914676544512, 0.99945499161456497, 0.99943684517069065, 0.99941518585732647, 0.99944241757088514, 0.99941921741404427, 0.99939606164364103, 0.99946202876720025, 0.99943986218026348, 0.99944240869359768, 0.99944089575016748, 0.99943885777858976, 0.99943028865976535, 0.99940513803603803, 0.99944140302374007, 0.99944844271274325, 0.99941568678997927, 0.99942026112941984, 0.99943986725299916, 0.99943987866665451, 0.99944390261426885, 0.99941670133712446, 0.99941065590432354, 0.99942274803810927, 0.99944089701835148, 0.9994157083491062], 'val_mDice': [0.35630194240428031, 0.36637377548725047, 0.37121843594185849, 0.38190440040953616, 0.42023801549952083, 0.43701649409659366, 0.44026713358595015, 0.43974014292372032, 0.44068924797342179, 0.47765181166060428, 0.49193007958696244, 0.47643918496497134, 0.48928666178216323, 0.4749037762905689, 0.48074309876624571, 0.48148651643002288, 0.49087774816979751, 0.50614881895958108, 0.50676832744415767, 0.52982872787942281, 0.51834753536163491, 0.54573580051990267, 0.52830073364237518, 0.54055246140094515, 0.53912825343456672, 0.54213507695400964, 0.54152140718825315, 0.52637439839383393, 0.56131142504671783, 0.55070696295575894, 0.55446250514781226, 0.54807314974196408, 0.55035172624790918, 0.54284536965349883, 0.51392148910684787, 0.54213810854769773, 0.56466717986350368, 0.54415707131649582, 0.55605426684338999, 0.56678578828243498, 0.55342087086210856, 0.56238237847673134, 0.56672167778015137, 0.57614933937153923, 0.55759656556109161, 0.55960070389382388, 0.56640235160259489, 0.55896753643421415, 0.56465040749691897, 0.57473567318409047, 0.56311033697838475, 0.57103719102575423, 0.57640552393933564, 0.57105286577914627, 0.56037273305527713, 0.57001601571732374, 0.58623431337640641, 0.57260227266778341, 0.57327270634630889, 0.57096539659703027, 0.57127393940661819, 0.57740473874071807, 0.57162686548334485, 0.56950831096223065, 0.57487300482202086, 0.58121977334326891, 0.57369214549977732, 0.57707542815106982, 0.57319349557795418, 0.57637064000393479, 0.57847837437974647, 0.57604057104029549, 0.57870809321707872, 0.57803203958146121, 0.58045620487091387, 0.58138857877000849, 0.57649671270492231], 'loss': [0.0019757468496702873, 0.0014882916294488169, 0.0013330653113490755, 0.0012461419552536952, 0.0011576665029071639, 0.0010698841395958795, 0.0010330886591850216, 0.00097668004688201038, 0.00092537141089206866, 0.00087999929479062584, 0.00086585080296426814, 0.0008581071131029686, 0.00081938380566423999, 0.00082689090884996701, 0.00079676479563668462, 0.00077982130965002447, 0.0007369223447138628, 0.0007349867137090056, 0.00071590735469379994, 0.00070864078218500824, 0.00070399317436870121, 0.00068894133962331449, 0.00068603142457862826, 0.00066502742624422676, 0.00065992477016112671, 0.00064558955375349284, 0.00063938778474841008, 0.00064412670838101343, 0.00063661922693704386, 0.00061833101829054265, 0.00061508291851615547, 0.00060265759372030972, 0.00060163732188483923, 0.00062399688283012761, 0.00061267656167937861, 0.00059361138528322608, 0.00055540973739389278, 0.00054947831654531844, 0.00054119244991072176, 0.00053525049323137959, 0.00053252263166930314, 0.0005289866128272017, 0.00052211804726461845, 0.00052386953130490398, 0.00051889298466232473, 0.00051491937737976065, 0.00051127161669488883, 0.00051266335746259079, 0.00050653664232288642, 0.00051040517132931447, 0.00049145066256891081, 0.00048767672413445773, 0.00048418291327252211, 0.00048377104655530906, 0.00048149499824872501, 0.00047890202317511218, 0.00047673074580984219, 0.00047932876281436024, 0.00047002898241734744, 0.00046625194573213078, 0.00046805107161371451, 0.00046049269122224232, 0.0004607302055175604, 0.00046154861987855874, 0.00045351041137878527, 0.00045553905227480664, 0.00045725880163467508, 0.00045195474950658539, 0.00045473827648302749, 0.00045549968934920022, 0.00045546964279267571, 0.00045522860450261914, 0.00044970737306084722, 0.00045467205138312902, 0.00045201291918298196, 0.00045266449775342363, 0.0004526055866770371], 'acc': [0.99931320715767236, 0.99938389414134288, 0.9994381592641215, 0.99947269341613143, 0.9995134271387035, 0.99955195255189544, 0.99956420221265396, 0.99958529492351234, 0.99960610221541457, 0.99962366458545482, 0.99963139085576314, 0.99963342447624215, 0.99964838572125136, 0.99964694723054603, 0.99965917666197568, 0.99966364248946427, 0.99968517472468155, 0.99968162261285554, 0.99969078716553417, 0.99969681127635557, 0.99969588626893569, 0.999703398725029, 0.999702816866195, 0.9997147316492675, 0.99971832307390884, 0.99972236521049884, 0.99972355913674238, 0.99972090511545353, 0.99972560959068968, 0.99973514206298619, 0.99973358361298537, 0.99973904276094172, 0.99973960980715681, 0.9997316621600727, 0.99973586696463423, 0.99974180955396841, 0.99976021376409463, 0.99975971776011607, 0.99976468292494325, 0.99976779222071732, 0.99976960221440259, 0.9997702062192475, 0.99977252586285414, 0.9997726394054316, 0.99977312290930986, 0.99977499254930691, 0.99977781746779981, 0.99977659662467377, 0.99977905924859389, 0.99977785567643962, 0.99978667422634282, 0.9997861969725147, 0.9997895374576431, 0.9997888589938676, 0.99979150874428879, 0.99979187130969704, 0.99979276533770167, 0.99978927347635893, 0.99979557725609036, 0.99979698489238145, 0.99979667030652475, 0.99979839054949005, 0.9998005298166498, 0.99979885644906052, 0.99980163842970904, 0.99980108684195124, 0.99980150542864221, 0.9998050310610892, 0.99980124171819362, 0.99980259814574057, 0.99980161776287657, 0.99980249754076689, 0.99980704895227535, 0.99980505824880728, 0.99980325396350112, 0.99980259918741565, 0.99980367073767751], 'mDice': [0.2685505606335637, 0.32759808341925978, 0.36717090665856594, 0.40155285329137053, 0.43617452111789207, 0.47635466587455988, 0.49468214188535409, 0.51792048999622375, 0.54707488301999496, 0.56667262258457995, 0.57664168591434328, 0.57882797736798242, 0.60205327379082685, 0.59731482280296533, 0.60903297383982746, 0.61557163589278208, 0.63773969827269972, 0.63879555859627402, 0.64782391936993189, 0.6508560872127942, 0.652988287524217, 0.65911646670121826, 0.66204644082851738, 0.67239490285781411, 0.67702202943187206, 0.68059395312512938, 0.68253342999481981, 0.68406489334169784, 0.68527654420658657, 0.69467791965981951, 0.69578150549121609, 0.70296390477030146, 0.69857017104753205, 0.69366443961369162, 0.69737213577102508, 0.70576392498019691, 0.72311995296951603, 0.72474519944365978, 0.72893756255116837, 0.73278282145777185, 0.73246927867190736, 0.7349981523943464, 0.73964250523158193, 0.73625771026851328, 0.73893599758528195, 0.74042378229013239, 0.74474610804428132, 0.74334206121945368, 0.74396261707213907, 0.74436608172513019, 0.75341813668884705, 0.75778494607064739, 0.7561042238390141, 0.75635232909998251, 0.75851025215940271, 0.7600040010579272, 0.76320728179417641, 0.75732800048866789, 0.76254542488103483, 0.76558500345000768, 0.76269847804622526, 0.76600292609397114, 0.76763142113583938, 0.76570738059830223, 0.76776688943080906, 0.76776280371053618, 0.76811497809794427, 0.76829713597575999, 0.77058544385187366, 0.76797015874880004, 0.76773387316597463, 0.76949640788718643, 0.76921330138712818, 0.7708590037545221, 0.76885819674704881, 0.76746441223370532, 0.77027875482931873], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05]}
Epoch 00077: val_mDice did not improve from 0.69299
Epoch 78/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8877 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6919

predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:01,  2.28it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.90it/s]predicting test subjects:  80%|████████  | 4/5 [00:00<00:00,  3.80it/s]predicting test subjects: 100%|██████████| 5/5 [00:00<00:00,  4.59it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:   1%|▏         | 2/148 [00:00<00:13, 10.85it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:14, 10.33it/s]predicting train subjects:   3%|▎         | 5/148 [00:00<00:13, 10.57it/s]predicting train subjects:   5%|▍         | 7/148 [00:00<00:13, 10.22it/s]predicting train subjects:   5%|▌         | 8/148 [00:00<00:13, 10.07it/s]predicting train subjects:   7%|▋         | 10/148 [00:00<00:13, 10.39it/s]predicting train subjects:   8%|▊         | 12/148 [00:01<00:13, 10.38it/s]predicting train subjects:   9%|▉         | 14/148 [00:01<00:12, 10.59it/s]predicting train subjects:  11%|█         | 16/148 [00:01<00:11, 11.22it/s]predicting train subjects:  12%|█▏        | 18/148 [00:01<00:12, 10.63it/s]predicting train subjects:  14%|█▎        | 20/148 [00:01<00:11, 10.92it/s]predicting train subjects:  15%|█▍        | 22/148 [00:02<00:11, 11.01it/s]predicting train subjects:  16%|█▌        | 24/148 [00:02<00:11, 10.83it/s]predicting train subjects:  18%|█▊        | 26/148 [00:02<00:11, 10.70it/s]predicting train subjects:  19%|█▉        | 28/148 [00:02<00:11, 10.56it/s]predicting train subjects:  20%|██        | 30/148 [00:02<00:11, 10.51it/s]predicting train subjects:  22%|██▏       | 32/148 [00:03<00:11, 10.39it/s]predicting train subjects:  23%|██▎       | 34/148 [00:03<00:11, 10.30it/s]predicting train subjects:  24%|██▍       | 36/148 [00:03<00:10, 10.35it/s]predicting train subjects:  26%|██▌       | 38/148 [00:03<00:11,  9.94it/s]predicting train subjects:  27%|██▋       | 40/148 [00:03<00:10,  9.98it/s]predicting train subjects:  28%|██▊       | 42/148 [00:04<00:10, 10.21it/s]
Epoch 00078: val_mDice did not improve from 0.69299

Epoch 00078: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 79/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8868 - val_loss: 0.0138 - val_acc: 0.9968 - val_mDice: 0.6926
predicting train subjects:  30%|██▉       | 44/148 [00:04<00:09, 10.48it/s]predicting train subjects:  31%|███       | 46/148 [00:04<00:09, 10.61it/s]predicting train subjects:  32%|███▏      | 48/148 [00:04<00:09, 10.55it/s]predicting train subjects:  34%|███▍      | 50/148 [00:04<00:09, 10.58it/s]predicting train subjects:  35%|███▌      | 52/148 [00:04<00:09, 10.64it/s]predicting train subjects:  36%|███▋      | 54/148 [00:05<00:09,  9.76it/s]predicting train subjects:  37%|███▋      | 55/148 [00:05<00:09,  9.80it/s]predicting train subjects:  39%|███▊      | 57/148 [00:05<00:09,  9.88it/s]predicting train subjects:  40%|███▉      | 59/148 [00:05<00:09,  9.69it/s]predicting train subjects:  41%|████      | 60/148 [00:05<00:09,  9.49it/s]predicting train subjects:  41%|████      | 61/148 [00:05<00:09,  9.38it/s]predicting train subjects:  42%|████▏     | 62/148 [00:06<00:09,  9.19it/s]predicting train subjects:  43%|████▎     | 63/148 [00:06<00:09,  9.14it/s]predicting train subjects:  43%|████▎     | 64/148 [00:06<00:09,  9.14it/s]predicting train subjects:  45%|████▍     | 66/148 [00:06<00:08, 10.03it/s]predicting train subjects:  46%|████▌     | 68/148 [00:06<00:07, 10.65it/s]predicting train subjects:  47%|████▋     | 70/148 [00:06<00:06, 11.18it/s]predicting train subjects:  49%|████▊     | 72/148 [00:06<00:06, 11.21it/s]predicting train subjects:  50%|█████     | 74/148 [00:07<00:06, 11.09it/s]predicting train subjects:  51%|█████▏    | 76/148 [00:07<00:06, 11.09it/s]predicting train subjects:  53%|█████▎    | 78/148 [00:07<00:06, 11.54it/s]predicting train subjects:  54%|█████▍    | 80/148 [00:07<00:05, 11.46it/s]predicting train subjects:  55%|█████▌    | 82/148 [00:07<00:05, 11.76it/s]predicting train subjects:  57%|█████▋    | 84/148 [00:07<00:05, 11.83it/s]predicting train subjects:  58%|█████▊    | 86/148 [00:08<00:05, 11.89it/s]predicting train subjects:  59%|█████▉    | 88/148 [00:08<00:05, 11.97it/s]predicting train subjects:  61%|██████    | 90/148 [00:08<00:05, 11.57it/s]predicting train subjects:  62%|██████▏   | 92/148 [00:08<00:04, 11.50it/s]predicting train subjects:  64%|██████▎   | 94/148 [00:08<00:04, 11.43it/s]predicting train subjects:  65%|██████▍   | 96/148 [00:08<00:04, 12.74it/s]predicting train subjects:  66%|██████▌   | 98/148 [00:09<00:03, 13.90it/s]predicting train subjects:  68%|██████▊   | 100/148 [00:09<00:03, 14.71it/s]predicting train subjects:  69%|██████▉   | 102/148 [00:09<00:03, 13.23it/s]predicting train subjects:  70%|███████   | 104/148 [00:09<00:03, 12.27it/s]
Epoch 00079: val_mDice did not improve from 0.69299
Epoch 80/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8869 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6922
predicting train subjects:  72%|███████▏  | 106/148 [00:09<00:03, 11.71it/s]predicting train subjects:  73%|███████▎  | 108/148 [00:09<00:03, 10.99it/s]predicting train subjects:  74%|███████▍  | 110/148 [00:10<00:03, 10.84it/s]predicting train subjects:  76%|███████▌  | 112/148 [00:10<00:03, 10.75it/s]predicting train subjects:  77%|███████▋  | 114/148 [00:10<00:02, 12.19it/s]predicting train subjects:  78%|███████▊  | 116/148 [00:10<00:02, 13.42it/s]predicting train subjects:  80%|███████▉  | 118/148 [00:10<00:02, 14.51it/s]predicting train subjects:  81%|████████  | 120/148 [00:10<00:02, 12.96it/s]predicting train subjects:  82%|████████▏ | 122/148 [00:11<00:02, 12.10it/s]predicting train subjects:  84%|████████▍ | 124/148 [00:11<00:02, 11.52it/s]predicting train subjects:  85%|████████▌ | 126/148 [00:11<00:02, 10.87it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:11<00:01, 10.28it/s]predicting train subjects:  88%|████████▊ | 130/148 [00:11<00:01,  9.96it/s]predicting train subjects:  89%|████████▉ | 132/148 [00:12<00:01, 10.21it/s]predicting train subjects:  91%|█████████ | 134/148 [00:12<00:01, 10.30it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:12<00:01, 10.45it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:12<00:00, 10.98it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:12<00:00, 11.47it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:12<00:00, 11.87it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:13<00:00, 11.12it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:13<00:00, 10.80it/s]predicting train subjects: 100%|██████████| 148/148 [00:13<00:00, 10.68it/s]


Epoch 00019: val_mDice did not improve from 0.90231
Epoch 20/100
 - 15s - loss: 0.0056 - acc: 0.9976 - mDice: 0.9351 - val_loss: 0.0109 - val_acc: 0.9960 - val_mDice: 0.9002

Epoch 00080: val_mDice did not improve from 0.69299
Epoch 81/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8874 - val_loss: 0.0141 - val_acc: 0.9968 - val_mDice: 0.6915

Epoch 00081: val_mDice did not improve from 0.69299
Epoch 82/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8872 - val_loss: 0.0140 - val_acc: 0.9968 - val_mDice: 0.6900

Epoch 00082: val_mDice did not improve from 0.69299

Epoch 00082: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 83/100
 - 4s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8893 - val_loss: 0.0141 - val_acc: 0.9968 - val_mDice: 0.6905

Epoch 00083: val_mDice did not improve from 0.69299
Restoring model weights from the end of the best epoch
Epoch 00083: early stopping
{'val_loss': [0.0086070085935136108, 0.0077601532709408311, 0.007919872714642515, 0.0085371484227002928, 0.0086036014786743109, 0.0091460351534980409, 0.009593509177261211, 0.008706578647995249, 0.0088734330926486787, 0.0092837990241798948, 0.010028490975023584, 0.0096070623381974852, 0.012316210789883391, 0.010067590769935162, 0.010695313995188855, 0.010782887287279393, 0.010315584930333686, 0.011179963146593976, 0.011726356012390015, 0.011066014482144346, 0.011340682692033179, 0.010950583627724901, 0.011812064400378694, 0.012019910988338451, 0.010838983421947094, 0.012511665734680408, 0.012002994306385517, 0.011522872214938732, 0.013025314586752273, 0.012526727063541717, 0.011960034575709638, 0.012254547506095247, 0.011795982936436826, 0.011606223980321529, 0.012329452829633622, 0.012653370645452054, 0.012289174911665155, 0.012564624005809743, 0.013508600321538906, 0.013395866398957181, 0.012259399835416612, 0.012682070044126916, 0.012978782322495542, 0.01308326845235647, 0.013239385461078045, 0.012878612417014355, 0.012944836367635017, 0.012872853732489526, 0.013073306273114173, 0.013146782769484723, 0.013364954296737275, 0.013493200169598802, 0.013746918456510026, 0.013442117621765491, 0.013419209167044213, 0.013694210096876672, 0.013649196542323903, 0.013629509136080742, 0.013824439191437781, 0.01360298344429503, 0.013506103901469961, 0.013615018826849918, 0.01350285467553012, 0.013723180629312992, 0.013709422062210579, 0.0135152866429788, 0.013977635453673119, 0.013828487155285287, 0.013843642468465136, 0.013539669064289711, 0.013537807845847403, 0.013697441449349231, 0.01399327547071462, 0.014150324594625768, 0.013972575439417616, 0.013797932859272399, 0.013967522538881352, 0.013691429999914575, 0.01384909950355266, 0.013656242810031201, 0.014051486955995255, 0.013998818603601861, 0.014132991809635721], 'val_acc': [0.99616225095505406, 0.99661557851953708, 0.99666946999570161, 0.99650680765192556, 0.99676167204024946, 0.99620910020584752, 0.99624841263953678, 0.99656875463242223, 0.99659947004724059, 0.99658637731633293, 0.99623479741684939, 0.99650930216971867, 0.99593004013629671, 0.99664127446235495, 0.99639898919044656, 0.99681607586272214, 0.99652844667434692, 0.9966835874192258, 0.99644787768100174, 0.99677578439103798, 0.99663122664106651, 0.99680044930031964, 0.99673698303547309, 0.99657982334177542, 0.99681657299082327, 0.99665637853297784, 0.99678182475110322, 0.99675714208724653, 0.99656925936962693, 0.99656823721337828, 0.99683421596567678, 0.99676721019947778, 0.99676118759398769, 0.99687952944572933, 0.99673847949251215, 0.99665336659614079, 0.99681053136257414, 0.99670976146738577, 0.99649418414907254, 0.99644635078754829, 0.99686038494110107, 0.99683115837421821, 0.99674254148564445, 0.99680850480465177, 0.99676619311596482, 0.9967405098549863, 0.99682563923774881, 0.99683068534161179, 0.99684578941223467, 0.99683318493214057, 0.99685082790699409, 0.99682464117699476, 0.99679946138503706, 0.99686845312727257, 0.99684630049035905, 0.99677675835629731, 0.99681958619584432, 0.99680750547571384, 0.99676519632339478, 0.99681004564812847, 0.99685383096654367, 0.9968619194436581, 0.99685133644875057, 0.99682011122399183, 0.99685281261484671, 0.99684881022635929, 0.99679842781513295, 0.99682109533472263, 0.99680145623836114, 0.99687250877948519, 0.99684429675974739, 0.99682965684444347, 0.99677274201778654, 0.99676215268195945, 0.9968291698618138, 0.99687399382286879, 0.99684928325896571, 0.99684981843258469, 0.99682664998034209, 0.99684476852416992, 0.99678432941436768, 0.99682210100458024, 0.99679592315186849], 'val_mDice': [0.55536602278973191, 0.59833946380209413, 0.61431087585205724, 0.62406400924033312, 0.62505887163446305, 0.62755113966921539, 0.64030608598222127, 0.64527271022187904, 0.65799525316725382, 0.65064339815302097, 0.6504311206492972, 0.65746256011597659, 0.64517529974592491, 0.66595034269576381, 0.65599820588497404, 0.66398892630922035, 0.65884292125701904, 0.65843929762535902, 0.6642547325885042, 0.66183715424639111, 0.66777589600136944, 0.67962015309232349, 0.67671242673346332, 0.66727853962715633, 0.66776045839837261, 0.67098978605676207, 0.660681260393021, 0.67818314724780149, 0.67273173433669065, 0.67478806034047556, 0.67811522838917182, 0.67738016869159456, 0.67600330393365093, 0.68055390170279972, 0.66799217082084494, 0.66826598060891984, 0.67768223742221267, 0.67574891511430135, 0.67578773168807338, 0.68039609523529698, 0.68784823696664044, 0.68399020585607972, 0.68085792090030428, 0.68372141554000532, 0.68997331248952987, 0.68758188029553025, 0.68798769281265582, 0.68940259674762161, 0.68941226538191447, 0.68901141907306429, 0.69083145197401652, 0.68560555387050548, 0.68689856123416981, 0.68553284254479918, 0.68849370200583271, 0.68774291175477054, 0.69027418405451668, 0.68886585818960311, 0.68999600030006247, 0.69110495866613186, 0.6905372929065785, 0.68981075413683623, 0.69299347857211502, 0.69136918352005328, 0.68799238002046625, 0.69032888843658125, 0.68915942881969694, 0.68932848407867109, 0.68874556333460701, 0.69097586134646805, 0.68929977493083228, 0.68999507959852824, 0.69068501731182663, 0.68921752813014581, 0.69161428796484115, 0.69112004244581182, 0.69057889948499962, 0.6919370179480695, 0.69255075302529845, 0.69217107904718278, 0.69149139333278575, 0.68998206169047249, 0.6904889192986996], 'loss': [0.0086097428610965691, 0.0068732509275586488, 0.0059798897243385149, 0.005425746730381961, 0.0052722447691110931, 0.0048330017887654834, 0.0045232583859975114, 0.0043590683950653596, 0.0042185711798742179, 0.0040461679225398846, 0.003875776891958272, 0.0037922143634396993, 0.0036597480440472585, 0.0036839355108494506, 0.0034924734415397261, 0.0034484731901615433, 0.0034755873110546374, 0.0032633076827518698, 0.0032667957238328234, 0.0031952216016986087, 0.0031466073249471599, 0.0030490758385130342, 0.0030718829557826962, 0.0030125027997871716, 0.0029645048132982028, 0.0029555481569812207, 0.0028704033374502519, 0.0028716793677866282, 0.0028386557391235634, 0.0028078878947495581, 0.0027813458214951785, 0.0027492849162933722, 0.0027083739259826166, 0.0027392078586552931, 0.0026847336719438243, 0.0026155346780735445, 0.0026391244520931625, 0.0025829093258799281, 0.0025669970189614111, 0.0025623967603815162, 0.002439367162680259, 0.0024014374450218924, 0.0023775996841498082, 0.0023681682170580646, 0.0023238237204869358, 0.0023501427782805584, 0.0023276212783735436, 0.0023233979628400209, 0.002266178341020564, 0.0022240339336302866, 0.0022289008822562926, 0.002238408472720691, 0.0022170081269484287, 0.0022024324031794762, 0.0022060075539272646, 0.0021838121000926046, 0.0021928003457743037, 0.0021577046356710474, 0.0021750369168295858, 0.0021759347629800767, 0.0021720527748101057, 0.0021555443857583273, 0.0021546174296738149, 0.0021557330482010134, 0.002155493701961476, 0.0021417171826012553, 0.0021479752875684815, 0.0021507867038372368, 0.0021508027169865169, 0.0021454530426052686, 0.0021658041714653188, 0.0021187002733248886, 0.0021287124742415584, 0.0021217426102691775, 0.0021195795800607185, 0.0021222819719360722, 0.0021245798363966548, 0.0021254167712512644, 0.00213480317894023, 0.0021386430302975982, 0.002127784248168018, 0.0021141748627981744, 0.0020852283433363578], 'acc': [0.99640879821460671, 0.99703036509623522, 0.9974262314799448, 0.99767442738747358, 0.99774456795009436, 0.99793258920827554, 0.99806738341450485, 0.99813306669137225, 0.99819386217903316, 0.99827060221125719, 0.99834212368078812, 0.99837856301474681, 0.99843142620961345, 0.9984236614389963, 0.99850016036511968, 0.99851994631549601, 0.9985129493843713, 0.99860185184748762, 0.99859867803036273, 0.99862997521890784, 0.99865694835193508, 0.99869408571232621, 0.99868379060891244, 0.99871167493674851, 0.99872882228262649, 0.99873241035307425, 0.99877252740403311, 0.99876842401695187, 0.99878385359905097, 0.99879286879636486, 0.99880448999374905, 0.99882006347283581, 0.99883642735409595, 0.99882641473211475, 0.99884950070896172, 0.99887465491073191, 0.99886770770917277, 0.9988945856122694, 0.9988942666930446, 0.99890151906371827, 0.99894881427642224, 0.99896456013189172, 0.99897892285126655, 0.99898287611214354, 0.99900148038637471, 0.99899048748569697, 0.99899850277915692, 0.99899569865249749, 0.99902524351448663, 0.99904029051018362, 0.99903619837319257, 0.99903310541088286, 0.99904594368052957, 0.99905126241069286, 0.99905029804879075, 0.99905807744452524, 0.99905245025355427, 0.99907305556466974, 0.99906078892460826, 0.99906251502178711, 0.99906242906276432, 0.99907204722324816, 0.99907204520239856, 0.9990750397472522, 0.99907081958840083, 0.9990766287391647, 0.99907009195756424, 0.99907882936098324, 0.99907188418027359, 0.99907579846167138, 0.99906976410076775, 0.99908788172526197, 0.99908458838634495, 0.99908356804482712, 0.99908973438595605, 0.99908293766477063, 0.99908589712600948, 0.99908839402103788, 0.99907949224130044, 0.99907379888313208, 0.99908477430450293, 0.99908588237589113, 0.99909992930106628], 'mDice': [0.53802994665504544, 0.61987649246513821, 0.67200710422631871, 0.7047347337455443, 0.71283314743478665, 0.73833336950140127, 0.75370858726864798, 0.76373278600942085, 0.77171226020294315, 0.78125693393061935, 0.78952253264840855, 0.79584594053723767, 0.80084817184930246, 0.80162475964107205, 0.81123461834995203, 0.81317756981001332, 0.81289109180374441, 0.8244341871447598, 0.82307159079825365, 0.82814496032677509, 0.83095679883980245, 0.83613927943104005, 0.83490669193987832, 0.83818939439652895, 0.84133863703265388, 0.84134572450617151, 0.84563469051356421, 0.84540177056190224, 0.84837163863420406, 0.84950002397702562, 0.85027672774889218, 0.85359503094944156, 0.85455137947479853, 0.85346176094293846, 0.85583511295955272, 0.85954060670338661, 0.85836204829144336, 0.86189183514670697, 0.86257247116881541, 0.86305259995758843, 0.86802422150328873, 0.87222937433838799, 0.87269171323446237, 0.87351951441619835, 0.8762849560153938, 0.87498074340803644, 0.87552932164169861, 0.87656109973043461, 0.8792096237876289, 0.88157010653434786, 0.88098373499753302, 0.88157886064623425, 0.88194350440449665, 0.88287273533733435, 0.88321849942332342, 0.88406048734385578, 0.88338934429372706, 0.8854298703906337, 0.88439225448423731, 0.88430980544788607, 0.88485520169769116, 0.88566274930029976, 0.88554036638112554, 0.88564064485215921, 0.88591364170862474, 0.88640478129405231, 0.88613935622748941, 0.88604481944834512, 0.88627016888094801, 0.88607815559074543, 0.88584850941534354, 0.88740912410022077, 0.88700892159839639, 0.88763119675653956, 0.88817157859112239, 0.88756604227558045, 0.88727580536416006, 0.88772787915706131, 0.8867882573492083, 0.88685944885775914, 0.88738803945192712, 0.88719806636558962, 0.88926063466013749], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:01,  2.21it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.84it/s]predicting test subjects:  80%|████████  | 4/5 [00:00<00:00,  3.71it/s]predicting test subjects: 100%|██████████| 5/5 [00:00<00:00,  4.46it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:   1%|▏         | 2/148 [00:00<00:14, 10.25it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:14,  9.76it/s]predicting train subjects:   3%|▎         | 5/148 [00:00<00:14, 10.03it/s]predicting train subjects:   4%|▍         | 6/148 [00:00<00:14,  9.93it/s]predicting train subjects:   5%|▍         | 7/148 [00:00<00:14,  9.56it/s]predicting train subjects:   5%|▌         | 8/148 [00:00<00:14,  9.59it/s]predicting train subjects:   7%|▋         | 10/148 [00:00<00:13, 10.12it/s]predicting train subjects:   7%|▋         | 11/148 [00:01<00:13,  9.93it/s]predicting train subjects:   8%|▊         | 12/148 [00:01<00:14,  9.63it/s]predicting train subjects:   9%|▉         | 14/148 [00:01<00:13, 10.02it/s]predicting train subjects:  11%|█         | 16/148 [00:01<00:12, 10.70it/s]predicting train subjects:  12%|█▏        | 18/148 [00:01<00:12, 10.32it/s]predicting train subjects:  14%|█▎        | 20/148 [00:01<00:12, 10.63it/s]predicting train subjects:  15%|█▍        | 22/148 [00:02<00:11, 10.77it/s]predicting train subjects:  16%|█▌        | 24/148 [00:02<00:11, 10.66it/s]predicting train subjects:  18%|█▊        | 26/148 [00:02<00:11, 10.58it/s]predicting train subjects:  19%|█▉        | 28/148 [00:02<00:11, 10.56it/s]predicting train subjects:  20%|██        | 30/148 [00:02<00:11, 10.61it/s]predicting train subjects:  22%|██▏       | 32/148 [00:03<00:10, 10.61it/s]predicting train subjects:  23%|██▎       | 34/148 [00:03<00:10, 10.60it/s]predicting train subjects:  24%|██▍       | 36/148 [00:03<00:10, 10.29it/s]predicting train subjects:  26%|██▌       | 38/148 [00:03<00:10, 10.16it/s]predicting train subjects:  27%|██▋       | 40/148 [00:03<00:10, 10.28it/s]predicting train subjects:  28%|██▊       | 42/148 [00:04<00:10, 10.46it/s]predicting train subjects:  30%|██▉       | 44/148 [00:04<00:09, 10.67it/s]predicting train subjects:  31%|███       | 46/148 [00:04<00:09, 10.64it/s]predicting train subjects:  32%|███▏      | 48/148 [00:04<00:09, 10.67it/s]predicting train subjects:  34%|███▍      | 50/148 [00:04<00:09, 10.76it/s]predicting train subjects:  35%|███▌      | 52/148 [00:04<00:08, 10.71it/s]predicting train subjects:  36%|███▋      | 54/148 [00:05<00:08, 10.50it/s]predicting train subjects:  38%|███▊      | 56/148 [00:05<00:08, 10.27it/s]predicting train subjects:  39%|███▉      | 58/148 [00:05<00:08, 10.20it/s]
Epoch 00020: val_mDice did not improve from 0.90231
Epoch 21/100
 - 15s - loss: 0.0055 - acc: 0.9976 - mDice: 0.9356 - val_loss: 0.0120 - val_acc: 0.9957 - val_mDice: 0.8995
predicting train subjects:  41%|████      | 60/148 [00:05<00:09,  9.68it/s]predicting train subjects:  41%|████      | 61/148 [00:05<00:09,  9.36it/s]predicting train subjects:  42%|████▏     | 62/148 [00:06<00:09,  9.17it/s]predicting train subjects:  43%|████▎     | 63/148 [00:06<00:09,  8.98it/s]predicting train subjects:  43%|████▎     | 64/148 [00:06<00:09,  8.96it/s]predicting train subjects:  45%|████▍     | 66/148 [00:06<00:08,  9.73it/s]predicting train subjects:  46%|████▌     | 68/148 [00:06<00:07, 10.35it/s]predicting train subjects:  47%|████▋     | 70/148 [00:06<00:07, 10.78it/s]predicting train subjects:  49%|████▊     | 72/148 [00:06<00:07, 10.69it/s]predicting train subjects:  50%|█████     | 74/148 [00:07<00:06, 10.58it/s]predicting train subjects:  51%|█████▏    | 76/148 [00:07<00:06, 10.70it/s]predicting train subjects:  53%|█████▎    | 78/148 [00:07<00:06, 11.20it/s]predicting train subjects:  54%|█████▍    | 80/148 [00:07<00:05, 11.56it/s]predicting train subjects:  55%|█████▌    | 82/148 [00:07<00:05, 11.80it/s]predicting train subjects:  57%|█████▋    | 84/148 [00:07<00:05, 11.77it/s]predicting train subjects:  58%|█████▊    | 86/148 [00:08<00:05, 11.69it/s]predicting train subjects:  59%|█████▉    | 88/148 [00:08<00:05, 10.90it/s]predicting train subjects:  61%|██████    | 90/148 [00:08<00:05, 10.97it/s]predicting train subjects:  62%|██████▏   | 92/148 [00:08<00:05, 11.07it/s]predicting train subjects:  64%|██████▎   | 94/148 [00:08<00:04, 11.10it/s]predicting train subjects:  65%|██████▍   | 96/148 [00:08<00:04, 12.41it/s]predicting train subjects:  66%|██████▌   | 98/148 [00:09<00:03, 13.49it/s]predicting train subjects:  68%|██████▊   | 100/148 [00:09<00:03, 14.41it/s]predicting train subjects:  69%|██████▉   | 102/148 [00:09<00:03, 12.87it/s]predicting train subjects:  70%|███████   | 104/148 [00:09<00:03, 11.94it/s]predicting train subjects:  72%|███████▏  | 106/148 [00:09<00:03, 11.30it/s]predicting train subjects:  73%|███████▎  | 108/148 [00:10<00:04,  9.81it/s]predicting train subjects:  74%|███████▍  | 110/148 [00:10<00:04,  9.29it/s]predicting train subjects:  76%|███████▌  | 112/148 [00:10<00:03,  9.56it/s]predicting train subjects:  77%|███████▋  | 114/148 [00:10<00:03, 10.95it/s]predicting train subjects:  78%|███████▊  | 116/148 [00:10<00:02, 12.19it/s]predicting train subjects:  80%|███████▉  | 118/148 [00:10<00:02, 13.34it/s]predicting train subjects:  81%|████████  | 120/148 [00:11<00:02, 12.06it/s]predicting train subjects:  82%|████████▏ | 122/148 [00:11<00:02, 11.11it/s]predicting train subjects:  84%|████████▍ | 124/148 [00:11<00:02, 10.72it/s]predicting train subjects:  85%|████████▌ | 126/148 [00:11<00:02, 10.22it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:11<00:02,  9.76it/s]predicting train subjects:  88%|████████▊ | 130/148 [00:12<00:01,  9.35it/s]predicting train subjects:  89%|████████▊ | 131/148 [00:12<00:01,  9.51it/s]predicting train subjects:  90%|████████▉ | 133/148 [00:12<00:01,  9.68it/s]predicting train subjects:  91%|█████████ | 134/148 [00:12<00:01,  9.72it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:12<00:01,  9.89it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:12<00:00, 10.58it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:13<00:00, 11.17it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:13<00:00, 11.70it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:13<00:00, 11.10it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:13<00:00, 10.63it/s]predicting train subjects: 100%|██████████| 148/148 [00:13<00:00, 10.31it/s]


Epoch 00021: val_mDice did not improve from 0.90231
Epoch 22/100
 - 15s - loss: 0.0055 - acc: 0.9977 - mDice: 0.9358 - val_loss: 0.0119 - val_acc: 0.9958 - val_mDice: 0.9025

Epoch 00022: val_mDice improved from 0.90231 to 0.90248, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/100
 - 15s - loss: 0.0054 - acc: 0.9977 - mDice: 0.9372 - val_loss: 0.0114 - val_acc: 0.9960 - val_mDice: 0.9028

Epoch 00023: val_mDice improved from 0.90248 to 0.90281, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5

Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 24/100
 - 15s - loss: 0.0051 - acc: 0.9978 - mDice: 0.9405 - val_loss: 0.0130 - val_acc: 0.9956 - val_mDice: 0.8997

Epoch 00024: val_mDice did not improve from 0.90281
Epoch 25/100
 - 15s - loss: 0.0050 - acc: 0.9979 - mDice: 0.9418 - val_loss: 0.0123 - val_acc: 0.9957 - val_mDice: 0.8982

Epoch 00025: val_mDice did not improve from 0.90281
Epoch 26/100
 - 15s - loss: 0.0050 - acc: 0.9979 - mDice: 0.9423 - val_loss: 0.0129 - val_acc: 0.9957 - val_mDice: 0.9020

Epoch 00026: val_mDice did not improve from 0.90281
Epoch 27/100
 - 15s - loss: 0.0050 - acc: 0.9979 - mDice: 0.9425 - val_loss: 0.0133 - val_acc: 0.9956 - val_mDice: 0.9009

Epoch 00027: val_mDice did not improve from 0.90281
Epoch 28/100
 - 15s - loss: 0.0049 - acc: 0.9979 - mDice: 0.9434 - val_loss: 0.0135 - val_acc: 0.9955 - val_mDice: 0.9007

Epoch 00028: val_mDice did not improve from 0.90281

Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 29/100
 - 14s - loss: 0.0048 - acc: 0.9979 - mDice: 0.9443 - val_loss: 0.0129 - val_acc: 0.9957 - val_mDice: 0.9031

Epoch 00029: val_mDice improved from 0.90281 to 0.90306, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/100
 - 15s - loss: 0.0048 - acc: 0.9980 - mDice: 0.9449 - val_loss: 0.0132 - val_acc: 0.9957 - val_mDice: 0.9040

Epoch 00030: val_mDice improved from 0.90306 to 0.90401, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 31/100
 - 15s - loss: 0.0047 - acc: 0.9980 - mDice: 0.9452 - val_loss: 0.0132 - val_acc: 0.9957 - val_mDice: 0.9035

Epoch 00031: val_mDice did not improve from 0.90401
Epoch 32/100
 - 14s - loss: 0.0047 - acc: 0.9980 - mDice: 0.9453 - val_loss: 0.0135 - val_acc: 0.9957 - val_mDice: 0.9034

Epoch 00032: val_mDice did not improve from 0.90401

Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 33/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9463 - val_loss: 0.0133 - val_acc: 0.9957 - val_mDice: 0.9044

Epoch 00033: val_mDice improved from 0.90401 to 0.90443, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 34/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9465 - val_loss: 0.0134 - val_acc: 0.9957 - val_mDice: 0.9040

Epoch 00034: val_mDice did not improve from 0.90443
Epoch 35/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9469 - val_loss: 0.0134 - val_acc: 0.9958 - val_mDice: 0.9054

Epoch 00035: val_mDice improved from 0.90443 to 0.90537, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 36/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9466 - val_loss: 0.0134 - val_acc: 0.9957 - val_mDice: 0.9048

Epoch 00036: val_mDice did not improve from 0.90537
Epoch 37/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9468 - val_loss: 0.0127 - val_acc: 0.9959 - val_mDice: 0.9064

Epoch 00037: val_mDice improved from 0.90537 to 0.90637, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5

Epoch 00037: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 38/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9472 - val_loss: 0.0130 - val_acc: 0.9958 - val_mDice: 0.9049

Epoch 00038: val_mDice did not improve from 0.90637
Epoch 39/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9472 - val_loss: 0.0135 - val_acc: 0.9957 - val_mDice: 0.9034

Epoch 00039: val_mDice did not improve from 0.90637
Epoch 40/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9472 - val_loss: 0.0135 - val_acc: 0.9957 - val_mDice: 0.9045

Epoch 00040: val_mDice did not improve from 0.90637
Epoch 41/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9475 - val_loss: 0.0132 - val_acc: 0.9958 - val_mDice: 0.9054

Epoch 00041: val_mDice did not improve from 0.90637

Epoch 00041: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 42/100
 - 14s - loss: 0.0046 - acc: 0.9980 - mDice: 0.9475 - val_loss: 0.0133 - val_acc: 0.9957 - val_mDice: 0.9051
2019-04-19 19:14:53.343264: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-19 19:14:54.000067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:85:00.0
totalMemory: 15.89GiB freeMemory: 11.57GiB
2019-04-19 19:14:54.000137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-04-19 19:14:54.588518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 19:14:54.588586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 19:14:54.588599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 19:14:54.589086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11202 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:13, 10.83it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:12, 11.15it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:12, 11.38it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:13, 10.75it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:00<00:12, 11.42it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:00<00:11, 11.98it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:10, 12.69it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:10, 12.40it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:09, 13.21it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:01<00:10, 12.64it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:01<00:09, 12.80it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:01<00:09, 12.69it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:10, 12.26it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:02<00:10, 12.00it/s]Loading Dataset: train:  20%|█▉        | 29/148 [00:02<00:10, 11.79it/s]Loading Dataset: train:  21%|██        | 31/148 [00:02<00:10, 11.66it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:02<00:09, 11.59it/s]Loading Dataset: train:  24%|██▎       | 35/148 [00:02<00:09, 11.57it/s]Loading Dataset: train:  25%|██▌       | 37/148 [00:03<00:09, 11.65it/s]Loading Dataset: train:  26%|██▋       | 39/148 [00:03<00:09, 11.71it/s]Loading Dataset: train:  28%|██▊       | 41/148 [00:03<00:09, 11.79it/s]Loading Dataset: train:  29%|██▉       | 43/148 [00:03<00:08, 11.96it/s]Loading Dataset: train:  30%|███       | 45/148 [00:03<00:08, 12.15it/s]Loading Dataset: train:  32%|███▏      | 47/148 [00:03<00:08, 12.31it/s]Loading Dataset: train:  33%|███▎      | 49/148 [00:04<00:07, 12.41it/s]Loading Dataset: train:  34%|███▍      | 51/148 [00:04<00:07, 12.58it/s]Loading Dataset: train:  36%|███▌      | 53/148 [00:04<00:07, 12.45it/s]Loading Dataset: train:  37%|███▋      | 55/148 [00:04<00:07, 12.10it/s]Loading Dataset: train:  39%|███▊      | 57/148 [00:04<00:07, 11.98it/s]Loading Dataset: train:  40%|███▉      | 59/148 [00:04<00:07, 11.52it/s]Loading Dataset: train:  41%|████      | 61/148 [00:05<00:07, 10.99it/s]Loading Dataset: train:  43%|████▎     | 63/148 [00:05<00:07, 10.63it/s]Loading Dataset: train:  44%|████▍     | 65/148 [00:05<00:07, 10.93it/s]Loading Dataset: train:  45%|████▌     | 67/148 [00:05<00:06, 11.84it/s]Loading Dataset: train:  47%|████▋     | 69/148 [00:05<00:06, 12.66it/s]Loading Dataset: train:  48%|████▊     | 71/148 [00:05<00:06, 12.76it/s]Loading Dataset: train:  49%|████▉     | 73/148 [00:06<00:05, 12.69it/s]Loading Dataset: train:  51%|█████     | 75/148 [00:06<00:05, 12.40it/s]Loading Dataset: train:  52%|█████▏    | 77/148 [00:06<00:05, 12.54it/s]Loading Dataset: train:  53%|█████▎    | 79/148 [00:06<00:05, 12.90it/s]Loading Dataset: train:  55%|█████▍    | 81/148 [00:06<00:05, 13.04it/s]Loading Dataset: train:  56%|█████▌    | 83/148 [00:06<00:04, 13.06it/s]Loading Dataset: train:  57%|█████▋    | 85/148 [00:06<00:04, 13.10it/s]Loading Dataset: train:  59%|█████▉    | 87/148 [00:07<00:04, 12.99it/s]Loading Dataset: train:  60%|██████    | 89/148 [00:07<00:04, 13.07it/s]Loading Dataset: train:  61%|██████▏   | 91/148 [00:07<00:04, 12.90it/s]Loading Dataset: train:  63%|██████▎   | 93/148 [00:07<00:04, 12.98it/s]Loading Dataset: train:  64%|██████▍   | 95/148 [00:07<00:03, 13.64it/s]Loading Dataset: train:  66%|██████▌   | 97/148 [00:07<00:03, 15.03it/s]Loading Dataset: train:  68%|██████▊   | 100/148 [00:07<00:02, 16.31it/s]Loading Dataset: train:  69%|██████▉   | 102/148 [00:08<00:03, 14.43it/s]Loading Dataset: train:  70%|███████   | 104/148 [00:08<00:03, 13.60it/s]Loading Dataset: train:  72%|███████▏  | 106/148 [00:08<00:03, 13.03it/s]Loading Dataset: train:  73%|███████▎  | 108/148 [00:08<00:03, 12.74it/s]Loading Dataset: train:  74%|███████▍  | 110/148 [00:08<00:03, 12.63it/s]Loading Dataset: train:  76%|███████▌  | 112/148 [00:08<00:02, 12.38it/s]Loading Dataset: train:  77%|███████▋  | 114/148 [00:09<00:02, 13.87it/s]
Epoch 00042: val_mDice did not improve from 0.90637
Epoch 43/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9476 - val_loss: 0.0133 - val_acc: 0.9958 - val_mDice: 0.9054
Loading Dataset: train:  78%|███████▊  | 116/148 [00:09<00:02, 15.13it/s]Loading Dataset: train:  80%|███████▉  | 118/148 [00:09<00:01, 16.27it/s]Loading Dataset: train:  81%|████████  | 120/148 [00:09<00:01, 14.38it/s]Loading Dataset: train:  82%|████████▏ | 122/148 [00:09<00:01, 13.30it/s]Loading Dataset: train:  84%|████████▍ | 124/148 [00:09<00:01, 12.39it/s]Loading Dataset: train:  85%|████████▌ | 126/148 [00:10<00:01, 11.61it/s]Loading Dataset: train:  86%|████████▋ | 128/148 [00:10<00:01, 11.12it/s]Loading Dataset: train:  88%|████████▊ | 130/148 [00:10<00:01, 10.78it/s]Loading Dataset: train:  89%|████████▉ | 132/148 [00:10<00:01, 11.17it/s]Loading Dataset: train:  91%|█████████ | 134/148 [00:10<00:01, 11.36it/s]Loading Dataset: train:  92%|█████████▏| 136/148 [00:10<00:01, 11.59it/s]Loading Dataset: train:  93%|█████████▎| 138/148 [00:11<00:00, 12.42it/s]Loading Dataset: train:  95%|█████████▍| 140/148 [00:11<00:00, 13.13it/s]Loading Dataset: train:  96%|█████████▌| 142/148 [00:11<00:00, 13.56it/s]Loading Dataset: train:  97%|█████████▋| 144/148 [00:11<00:00, 12.89it/s]Loading Dataset: train:  99%|█████████▊| 146/148 [00:11<00:00, 12.49it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:11<00:00, 12.18it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]concatenating train images:  19%|█▉        | 28/148 [00:00<00:00, 270.36it/s]concatenating train images:  37%|███▋      | 55/148 [00:00<00:00, 268.42it/s]concatenating train images:  55%|█████▌    | 82/148 [00:00<00:00, 268.08it/s]concatenating train images:  74%|███████▎  | 109/148 [00:00<00:00, 266.27it/s]concatenating train images:  93%|█████████▎| 137/148 [00:00<00:00, 269.50it/s]concatenating train images: 100%|██████████| 148/148 [00:00<00:00, 269.22it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00, 11.50it/s]Loading Dataset: test:  80%|████████  | 4/5 [00:00<00:00, 13.12it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00, 13.49it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 301.84it/s]2019-04-19 19:15:13.796574: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-19 19:15:14.200826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:88:00.0
totalMemory: 15.89GiB freeMemory: 11.62GiB
2019-04-19 19:15:14.200892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-04-19 19:15:14.626894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 19:15:14.626964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 19:15:14.626978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 19:15:14.627385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11246 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:88:00.0, compute capability: 6.0)
Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:13, 10.97it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:12, 11.17it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:12, 11.36it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:13, 10.64it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:00<00:12, 11.37it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:00<00:11, 11.87it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:10, 12.66it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:10, 12.30it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:10, 13.03it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:01<00:10, 12.31it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:01<00:10, 12.43it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:01<00:10, 12.35it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:10, 12.03it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:02<00:10, 11.88it/s]Loading Dataset: train:  20%|█▉        | 29/148 [00:02<00:10, 11.70it/s]Loading Dataset: train:  21%|██        | 31/148 [00:02<00:10, 11.63it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:02<00:09, 11.59it/s]Loading Dataset: train:  24%|██▎       | 35/148 [00:02<00:09, 11.60it/s]Loading Dataset: train:  25%|██▌       | 37/148 [00:03<00:09, 11.66it/s]Loading Dataset: train:  26%|██▋       | 39/148 [00:03<00:09, 11.67it/s]Loading Dataset: train:  28%|██▊       | 41/148 [00:03<00:09, 11.75it/s]Loading Dataset: train:  29%|██▉       | 43/148 [00:03<00:08, 11.95it/s]Loading Dataset: train:  30%|███       | 45/148 [00:03<00:08, 12.03it/s]Loading Dataset: train:  32%|███▏      | 47/148 [00:03<00:08, 12.15it/s]Loading Dataset: train:  33%|███▎      | 49/148 [00:04<00:08, 12.23it/s]Loading Dataset: train:  34%|███▍      | 51/148 [00:04<00:07, 12.36it/s]Loading Dataset: train:  36%|███▌      | 53/148 [00:04<00:07, 12.23it/s]Loading Dataset: train:  37%|███▋      | 55/148 [00:04<00:07, 12.05it/s]Loading Dataset: train:  39%|███▊      | 57/148 [00:04<00:07, 11.89it/s]Loading Dataset: train:  40%|███▉      | 59/148 [00:04<00:07, 11.37it/s]Loading Dataset: train:  41%|████      | 61/148 [00:05<00:08, 10.84it/s]Loading Dataset: train:  43%|████▎     | 63/148 [00:05<00:08, 10.43it/s]Loading Dataset: train:  44%|████▍     | 65/148 [00:05<00:07, 10.73it/s]Loading Dataset: train:  45%|████▌     | 67/148 [00:05<00:07, 11.57it/s]Loading Dataset: train:  47%|████▋     | 69/148 [00:05<00:06, 12.23it/s]Loading Dataset: train:  48%|████▊     | 71/148 [00:05<00:06, 12.51it/s]Loading Dataset: train:  49%|████▉     | 73/148 [00:06<00:06, 12.44it/s]Loading Dataset: train:  51%|█████     | 75/148 [00:06<00:05, 12.45it/s]Loading Dataset: train:  52%|█████▏    | 77/148 [00:06<00:05, 12.66it/s]Loading Dataset: train:  53%|█████▎    | 79/148 [00:06<00:05, 12.90it/s]Loading Dataset: train:  55%|█████▍    | 81/148 [00:06<00:05, 13.12it/s]Loading Dataset: train:  56%|█████▌    | 83/148 [00:06<00:04, 13.14it/s]Loading Dataset: train:  57%|█████▋    | 85/148 [00:07<00:04, 13.27it/s]Loading Dataset: train:  59%|█████▉    | 87/148 [00:07<00:04, 13.37it/s]Loading Dataset: train:  60%|██████    | 89/148 [00:07<00:04, 13.31it/s]Loading Dataset: train:  61%|██████▏   | 91/148 [00:07<00:04, 13.27it/s]Loading Dataset: train:  63%|██████▎   | 93/148 [00:07<00:04, 13.13it/s]Loading Dataset: train:  64%|██████▍   | 95/148 [00:07<00:03, 13.81it/s]Loading Dataset: train:  66%|██████▌   | 98/148 [00:07<00:03, 15.44it/s]Loading Dataset: train:  68%|██████▊   | 100/148 [00:08<00:02, 16.47it/s]Loading Dataset: train:  69%|██████▉   | 102/148 [00:08<00:03, 14.76it/s]Loading Dataset: train:  70%|███████   | 104/148 [00:08<00:03, 13.65it/s]Loading Dataset: train:  72%|███████▏  | 106/148 [00:08<00:03, 13.05it/s]Loading Dataset: train:  73%|███████▎  | 108/148 [00:08<00:03, 12.76it/s]Loading Dataset: train:  74%|███████▍  | 110/148 [00:08<00:03, 12.54it/s]Loading Dataset: train:  76%|███████▌  | 112/148 [00:09<00:02, 12.54it/s]Loading Dataset: train:  78%|███████▊  | 115/148 [00:09<00:02, 14.14it/s]Loading Dataset: train:  79%|███████▉  | 117/148 [00:09<00:02, 15.47it/s]Loading Dataset: train:  80%|████████  | 119/148 [00:09<00:01, 15.12it/s]Loading Dataset: train:  82%|████████▏ | 121/148 [00:09<00:01, 13.68it/s]Loading Dataset: train:  83%|████████▎ | 123/148 [00:09<00:01, 12.84it/s]Loading Dataset: train:  84%|████████▍ | 125/148 [00:09<00:01, 12.11it/s]Loading Dataset: train:  86%|████████▌ | 127/148 [00:10<00:01, 11.47it/s]Loading Dataset: train:  87%|████████▋ | 129/148 [00:10<00:01, 11.02it/s]Loading Dataset: train:  89%|████████▊ | 131/148 [00:10<00:01, 11.06it/s]
Epoch 00043: val_mDice did not improve from 0.90637
Epoch 44/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9477 - val_loss: 0.0133 - val_acc: 0.9958 - val_mDice: 0.9053
Loading Dataset: train:  90%|████████▉ | 133/148 [00:10<00:01, 11.27it/s]Loading Dataset: train:  91%|█████████ | 135/148 [00:10<00:01, 11.60it/s]Loading Dataset: train:  93%|█████████▎| 137/148 [00:10<00:00, 12.06it/s]Loading Dataset: train:  94%|█████████▍| 139/148 [00:11<00:00, 12.85it/s]Loading Dataset: train:  95%|█████████▌| 141/148 [00:11<00:00, 13.50it/s]Loading Dataset: train:  97%|█████████▋| 143/148 [00:11<00:00, 13.19it/s]Loading Dataset: train:  98%|█████████▊| 145/148 [00:11<00:00, 12.76it/s]Loading Dataset: train:  99%|█████████▉| 147/148 [00:11<00:00, 12.47it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:11<00:00, 12.50it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]concatenating train images:  19%|█▉        | 28/148 [00:00<00:00, 277.68it/s]concatenating train images:  39%|███▊      | 57/148 [00:00<00:00, 280.75it/s]concatenating train images:  58%|█████▊    | 86/148 [00:00<00:00, 280.72it/s]concatenating train images:  76%|███████▌  | 112/148 [00:00<00:00, 273.90it/s]concatenating train images:  95%|█████████▍| 140/148 [00:00<00:00, 274.99it/s]concatenating train images: 100%|██████████| 148/148 [00:00<00:00, 277.33it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00, 12.20it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00, 13.07it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 309.96it/s]*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [2.0] GPU:   5 Cascade
---------------------------------------------------------------
 Nucleus: 2.0  | GPU: 5  | SD 2  | Dropout 0.1  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.1
---------------------------------------------------------------
InputDimensions [60, 88, 44]
(5722, 60, 88, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 60, 88, 1)    0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 60, 88, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 60, 88, 20)   200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 88, 20)   3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 44, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 30, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 30, 44, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 44, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 22, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 15, 22, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 22, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 22, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 15, 22, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 15, 22, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 30, 44, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 44, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 44, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 30, 44, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30, 44, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 60, 88, 20)   3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 60, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 60, 88, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 60, 88, 20)   3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 60, 88, 20)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 60, 88, 2)    42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/2-AV/sd2
------------------------------------------------------------------
Train on 5722 samples, validate on 188 samples
Epoch 1/100
 - 11s - loss: 0.0042 - acc: 0.9984 - mDice: 0.5202 - val_loss: 0.0036 - val_acc: 0.9986 - val_mDice: 0.5762

Epoch 00001: val_mDice improved from -inf to 0.57616, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/2-AV/sd2/best_model_weights.h5
Epoch 2/100
 - 7s - loss: 0.0028 - acc: 0.9988 - mDice: 0.6358 - val_loss: 0.0041 - val_acc: 0.9985 - val_mDice: 0.6129

Epoch 00044: val_mDice did not improve from 0.90637
Epoch 45/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9477 - val_loss: 0.0136 - val_acc: 0.9957 - val_mDice: 0.9054
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [4.0] GPU:   6 Cascade
---------------------------------------------------------------
 Nucleus: 4.0  | GPU: 6  | SD 2  | Dropout 0.1  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.1
---------------------------------------------------------------
InputDimensions [60, 88, 44]
(5722, 60, 88, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 60, 88, 1)    0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 60, 88, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 60, 88, 20)   200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 88, 20)   3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 44, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 30, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 30, 44, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 44, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 22, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 15, 22, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 22, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 22, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 15, 22, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 15, 22, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 30, 44, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 44, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 44, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 30, 44, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30, 44, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 60, 88, 20)   3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 60, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 60, 88, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 60, 88, 20)   3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 60, 88, 20)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 60, 88, 2)    42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/4-VA/sd2
------------------------------------------------------------------
Train on 5722 samples, validate on 188 samples
Epoch 1/100
 - 10s - loss: 0.0088 - acc: 0.9963 - mDice: 0.5309 - val_loss: 0.0078 - val_acc: 0.9966 - val_mDice: 0.5421

Epoch 00002: val_mDice improved from 0.57616 to 0.61294, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/2-AV/sd2/best_model_weights.h5
Epoch 3/100
 - 7s - loss: 0.0025 - acc: 0.9989 - mDice: 0.6885 - val_loss: 0.0037 - val_acc: 0.9986 - val_mDice: 0.6411
2019-04-19 19:15:52.433787: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-19 19:15:52.806177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:89:00.0
totalMemory: 15.89GiB freeMemory: 11.62GiB
2019-04-19 19:15:52.806248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-04-19 19:15:53.382933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 19:15:53.383000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-04-19 19:15:53.383013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-04-19 19:15:53.383425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11246 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
Using TensorFlow backend.
Loading Dataset: train:   0%|          | 0/148 [00:00<?, ?it/s]Loading Dataset: train:   1%|▏         | 2/148 [00:00<00:13, 11.00it/s]Loading Dataset: train:   3%|▎         | 4/148 [00:00<00:12, 11.13it/s]Loading Dataset: train:   4%|▍         | 6/148 [00:00<00:12, 11.34it/s]Loading Dataset: train:   5%|▍         | 7/148 [00:00<00:13, 10.64it/s]Loading Dataset: train:   6%|▌         | 9/148 [00:00<00:12, 11.30it/s]Loading Dataset: train:   7%|▋         | 11/148 [00:00<00:11, 11.83it/s]Loading Dataset: train:   9%|▉         | 13/148 [00:01<00:10, 12.58it/s]Loading Dataset: train:  10%|█         | 15/148 [00:01<00:10, 12.21it/s]Loading Dataset: train:  11%|█▏        | 17/148 [00:01<00:10, 12.94it/s]Loading Dataset: train:  13%|█▎        | 19/148 [00:01<00:10, 12.45it/s]Loading Dataset: train:  14%|█▍        | 21/148 [00:01<00:10, 12.56it/s]Loading Dataset: train:  16%|█▌        | 23/148 [00:01<00:09, 12.56it/s]Loading Dataset: train:  17%|█▋        | 25/148 [00:02<00:10, 12.19it/s]Loading Dataset: train:  18%|█▊        | 27/148 [00:02<00:10, 11.86it/s]Loading Dataset: train:  20%|█▉        | 29/148 [00:02<00:10, 11.59it/s]Loading Dataset: train:  21%|██        | 31/148 [00:02<00:10, 11.49it/s]Loading Dataset: train:  22%|██▏       | 33/148 [00:02<00:10, 11.44it/s]Loading Dataset: train:  24%|██▎       | 35/148 [00:02<00:09, 11.46it/s]Loading Dataset: train:  25%|██▌       | 37/148 [00:03<00:09, 11.54it/s]Loading Dataset: train:  26%|██▋       | 39/148 [00:03<00:09, 11.56it/s]Loading Dataset: train:  28%|██▊       | 41/148 [00:03<00:09, 11.66it/s]Loading Dataset: train:  29%|██▉       | 43/148 [00:03<00:08, 11.84it/s]Loading Dataset: train:  30%|███       | 45/148 [00:03<00:08, 11.95it/s]Loading Dataset: train:  32%|███▏      | 47/148 [00:03<00:08, 12.07it/s]Loading Dataset: train:  33%|███▎      | 49/148 [00:04<00:08, 12.14it/s]Loading Dataset: train:  34%|███▍      | 51/148 [00:04<00:07, 12.25it/s]Loading Dataset: train:  36%|███▌      | 53/148 [00:04<00:07, 12.09it/s]
Epoch 00001: val_mDice improved from -inf to 0.54210, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 2/100
 - 7s - loss: 0.0064 - acc: 0.9972 - mDice: 0.6353 - val_loss: 0.0073 - val_acc: 0.9969 - val_mDice: 0.5735
Loading Dataset: train:  37%|███▋      | 55/148 [00:04<00:07, 11.93it/s]
Epoch 00003: val_mDice improved from 0.61294 to 0.64110, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/2-AV/sd2/best_model_weights.h5
Epoch 4/100
 - 7s - loss: 0.0022 - acc: 0.9991 - mDice: 0.7345 - val_loss: 0.0041 - val_acc: 0.9985 - val_mDice: 0.6373
Loading Dataset: train:  39%|███▊      | 57/148 [00:04<00:08, 11.32it/s]Loading Dataset: train:  40%|███▉      | 59/148 [00:04<00:08, 10.84it/s]Loading Dataset: train:  41%|████      | 61/148 [00:05<00:08, 10.52it/s]Loading Dataset: train:  43%|████▎     | 63/148 [00:05<00:08, 10.29it/s]Loading Dataset: train:  44%|████▍     | 65/148 [00:05<00:07, 10.69it/s]Loading Dataset: train:  45%|████▌     | 67/148 [00:05<00:06, 11.58it/s]Loading Dataset: train:  47%|████▋     | 69/148 [00:05<00:06, 12.25it/s]Loading Dataset: train:  48%|████▊     | 71/148 [00:06<00:06, 12.42it/s]Loading Dataset: train:  49%|████▉     | 73/148 [00:06<00:06, 12.42it/s]Loading Dataset: train:  51%|█████     | 75/148 [00:06<00:05, 12.37it/s]Loading Dataset: train:  52%|█████▏    | 77/148 [00:06<00:05, 12.47it/s]Loading Dataset: train:  53%|█████▎    | 79/148 [00:06<00:05, 12.60it/s]Loading Dataset: train:  55%|█████▍    | 81/148 [00:06<00:05, 12.80it/s]Loading Dataset: train:  56%|█████▌    | 83/148 [00:06<00:05, 12.89it/s]Loading Dataset: train:  57%|█████▋    | 85/148 [00:07<00:04, 12.92it/s]Loading Dataset: train:  59%|█████▉    | 87/148 [00:07<00:04, 12.74it/s]Loading Dataset: train:  60%|██████    | 89/148 [00:07<00:04, 12.60it/s]Loading Dataset: train:  61%|██████▏   | 91/148 [00:07<00:04, 12.65it/s]Loading Dataset: train:  63%|██████▎   | 93/148 [00:07<00:04, 12.66it/s]Loading Dataset: train:  64%|██████▍   | 95/148 [00:07<00:03, 13.32it/s]Loading Dataset: train:  66%|██████▌   | 98/148 [00:08<00:03, 14.89it/s]Loading Dataset: train:  68%|██████▊   | 101/148 [00:08<00:03, 15.32it/s]Loading Dataset: train:  70%|██████▉   | 103/148 [00:08<00:03, 14.06it/s]Loading Dataset: train:  71%|███████   | 105/148 [00:08<00:03, 13.29it/s]Loading Dataset: train:  72%|███████▏  | 107/148 [00:08<00:03, 12.86it/s]Loading Dataset: train:  74%|███████▎  | 109/148 [00:08<00:03, 12.71it/s]Loading Dataset: train:  75%|███████▌  | 111/148 [00:09<00:02, 12.53it/s]Loading Dataset: train:  76%|███████▋  | 113/148 [00:09<00:02, 13.18it/s]Loading Dataset: train:  78%|███████▊  | 115/148 [00:09<00:02, 14.65it/s]Loading Dataset: train:  79%|███████▉  | 117/148 [00:09<00:01, 15.93it/s]Loading Dataset: train:  80%|████████  | 119/148 [00:09<00:01, 15.08it/s]Loading Dataset: train:  82%|████████▏ | 121/148 [00:09<00:01, 13.67it/s]Loading Dataset: train:  83%|████████▎ | 123/148 [00:09<00:01, 12.73it/s]Loading Dataset: train:  84%|████████▍ | 125/148 [00:10<00:01, 12.08it/s]Loading Dataset: train:  86%|████████▌ | 127/148 [00:10<00:01, 11.44it/s]Loading Dataset: train:  87%|████████▋ | 129/148 [00:10<00:01, 11.01it/s]Loading Dataset: train:  89%|████████▊ | 131/148 [00:10<00:01, 11.07it/s]Loading Dataset: train:  90%|████████▉ | 133/148 [00:10<00:01, 11.42it/s]Loading Dataset: train:  91%|█████████ | 135/148 [00:10<00:01, 11.67it/s]Loading Dataset: train:  93%|█████████▎| 137/148 [00:11<00:00, 12.14it/s]Loading Dataset: train:  94%|█████████▍| 139/148 [00:11<00:00, 12.91it/s]Loading Dataset: train:  95%|█████████▌| 141/148 [00:11<00:00, 13.45it/s]Loading Dataset: train:  97%|█████████▋| 143/148 [00:11<00:00, 13.20it/s]Loading Dataset: train:  98%|█████████▊| 145/148 [00:11<00:00, 12.75it/s]Loading Dataset: train:  99%|█████████▉| 147/148 [00:11<00:00, 12.39it/s]Loading Dataset: train: 100%|██████████| 148/148 [00:11<00:00, 12.37it/s]
concatenating train images:   0%|          | 0/148 [00:00<?, ?it/s]concatenating train images:  19%|█▉        | 28/148 [00:00<00:00, 272.49it/s]concatenating train images:  38%|███▊      | 56/148 [00:00<00:00, 272.82it/s]concatenating train images:  57%|█████▋    | 84/148 [00:00<00:00, 272.67it/s]concatenating train images:  75%|███████▌  | 111/148 [00:00<00:00, 270.83it/s]concatenating train images:  95%|█████████▌| 141/148 [00:00<00:00, 277.06it/s]concatenating train images: 100%|██████████| 148/148 [00:00<00:00, 275.85it/s]
Loading Dataset: test:   0%|          | 0/5 [00:00<?, ?it/s]
Epoch 00002: val_mDice improved from 0.54210 to 0.57352, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 3/100
 - 6s - loss: 0.0055 - acc: 0.9976 - mDice: 0.6953 - val_loss: 0.0076 - val_acc: 0.9971 - val_mDice: 0.6059
Loading Dataset: test:  40%|████      | 2/5 [00:00<00:00, 11.83it/s]Loading Dataset: test:  80%|████████  | 4/5 [00:00<00:00, 13.14it/s]Loading Dataset: test: 100%|██████████| 5/5 [00:00<00:00, 11.69it/s]
concatenating train images:   0%|          | 0/5 [00:00<?, ?it/s]concatenating train images: 100%|██████████| 5/5 [00:00<00:00, 311.46it/s]
Epoch 00004: val_mDice did not improve from 0.64110
Epoch 5/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.7609 - val_loss: 0.0054 - val_acc: 0.9984 - val_mDice: 0.6003

Epoch 00045: val_mDice did not improve from 0.90637

Epoch 00045: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 46/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9477 - val_loss: 0.0135 - val_acc: 0.9957 - val_mDice: 0.9047

Epoch 00003: val_mDice improved from 0.57352 to 0.60594, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 4/100
 - 6s - loss: 0.0048 - acc: 0.9979 - mDice: 0.7364 - val_loss: 0.0088 - val_acc: 0.9969 - val_mDice: 0.6047

Epoch 00005: val_mDice did not improve from 0.64110
Epoch 6/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.7811 - val_loss: 0.0052 - val_acc: 0.9984 - val_mDice: 0.6326

Epoch 00004: val_mDice did not improve from 0.60594
Epoch 5/100
 - 6s - loss: 0.0044 - acc: 0.9981 - mDice: 0.7584 - val_loss: 0.0086 - val_acc: 0.9970 - val_mDice: 0.6295

Epoch 00006: val_mDice did not improve from 0.64110
Epoch 7/100
 - 7s - loss: 0.0016 - acc: 0.9993 - mDice: 0.7978 - val_loss: 0.0052 - val_acc: 0.9980 - val_mDice: 0.6515
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
slicingDim [2] Nuclei_Indexes [5.0] GPU:   7 Cascade
---------------------------------------------------------------
 Nucleus: 5.0  | GPU: 7  | SD 2  | Dropout 0.1  | LR 0.001  | NL 3  |  Cascade |  FM 20
SubExperiment: sE11_Cascade_FM20_DO0.1
---------------------------------------------------------------
InputDimensions [60, 88, 44]
(5722, 60, 88, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 60, 88, 1)    0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 60, 88, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 60, 88, 20)   200         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 88, 20)   3620        conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 44, 20)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 30, 44, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 30, 44, 20)   80          dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 44, 40)   7240        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 22, 40)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 15, 22, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 22, 80)   28880       dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 22, 80)   57680       conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 15, 22, 80)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 15, 22, 80)   320         dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 30, 44, 40)   12840       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 44, 80)   0           conv2d_transpose_1[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 44, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 44, 40)   14440       conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 30, 44, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30, 44, 40)   160         dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 60, 88, 20)   3220        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 60, 88, 40)   0           conv2d_transpose_2[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 60, 88, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 60, 88, 20)   3620        conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 60, 88, 20)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 60, 88, 2)    42          dropout_5[0][0]                  
==================================================================================================
Total params: 182,846
Trainable params: 182,564
Non-trainable params: 282
__________________________________________________________________________________________________
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp1/models/sE8_Cascade_FM20/5-VLa/sd2
------------------------------------------------------------------
Train on 5722 samples, validate on 188 samples
Epoch 1/100
 - 10s - loss: 0.0043 - acc: 0.9983 - mDice: 0.4206 - val_loss: 0.0037 - val_acc: 0.9985 - val_mDice: 0.4586

Epoch 00046: val_mDice did not improve from 0.90637
Epoch 47/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9480 - val_loss: 0.0134 - val_acc: 0.9958 - val_mDice: 0.9057

Epoch 00005: val_mDice improved from 0.60594 to 0.62952, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 6/100
 - 7s - loss: 0.0041 - acc: 0.9982 - mDice: 0.7756 - val_loss: 0.0094 - val_acc: 0.9969 - val_mDice: 0.6431

Epoch 00007: val_mDice improved from 0.64110 to 0.65147, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/2-AV/sd2/best_model_weights.h5
Epoch 8/100
 - 7s - loss: 0.0017 - acc: 0.9993 - mDice: 0.7950 - val_loss: 0.0056 - val_acc: 0.9982 - val_mDice: 0.6337

Epoch 00001: val_mDice improved from -inf to 0.45855, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 2/100
 - 6s - loss: 0.0028 - acc: 0.9988 - mDice: 0.5737 - val_loss: 0.0040 - val_acc: 0.9985 - val_mDice: 0.5068

Epoch 00006: val_mDice improved from 0.62952 to 0.64306, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 7/100
 - 6s - loss: 0.0039 - acc: 0.9983 - mDice: 0.7902 - val_loss: 0.0091 - val_acc: 0.9968 - val_mDice: 0.6542

Epoch 00008: val_mDice did not improve from 0.65147
Epoch 9/100
 - 7s - loss: 0.0015 - acc: 0.9994 - mDice: 0.8207 - val_loss: 0.0043 - val_acc: 0.9986 - val_mDice: 0.6794

Epoch 00002: val_mDice improved from 0.45855 to 0.50677, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 3/100
 - 7s - loss: 0.0022 - acc: 0.9991 - mDice: 0.6736 - val_loss: 0.0042 - val_acc: 0.9985 - val_mDice: 0.5387

Epoch 00007: val_mDice improved from 0.64306 to 0.65415, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 8/100
 - 7s - loss: 0.0036 - acc: 0.9984 - mDice: 0.8043 - val_loss: 0.0094 - val_acc: 0.9967 - val_mDice: 0.6581

Epoch 00009: val_mDice improved from 0.65147 to 0.67944, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/2-AV/sd2/best_model_weights.h5
Epoch 10/100
 - 7s - loss: 0.0014 - acc: 0.9994 - mDice: 0.8227 - val_loss: 0.0048 - val_acc: 0.9982 - val_mDice: 0.6583

Epoch 00047: val_mDice did not improve from 0.90637
Epoch 48/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9480 - val_loss: 0.0131 - val_acc: 0.9959 - val_mDice: 0.9067

Epoch 00003: val_mDice improved from 0.50677 to 0.53869, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 4/100
 - 7s - loss: 0.0020 - acc: 0.9992 - mDice: 0.7172 - val_loss: 0.0042 - val_acc: 0.9983 - val_mDice: 0.5772

Epoch 00008: val_mDice improved from 0.65415 to 0.65812, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 9/100
 - 6s - loss: 0.0035 - acc: 0.9985 - mDice: 0.8113 - val_loss: 0.0098 - val_acc: 0.9969 - val_mDice: 0.6591

Epoch 00010: val_mDice did not improve from 0.67944
Epoch 11/100
 - 6s - loss: 0.0014 - acc: 0.9994 - mDice: 0.8311 - val_loss: 0.0058 - val_acc: 0.9982 - val_mDice: 0.6460

Epoch 00004: val_mDice improved from 0.53869 to 0.57716, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 5/100
 - 6s - loss: 0.0018 - acc: 0.9993 - mDice: 0.7445 - val_loss: 0.0052 - val_acc: 0.9981 - val_mDice: 0.5908

Epoch 00009: val_mDice improved from 0.65812 to 0.65905, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 10/100
 - 7s - loss: 0.0034 - acc: 0.9986 - mDice: 0.8195 - val_loss: 0.0104 - val_acc: 0.9968 - val_mDice: 0.6626

Epoch 00011: val_mDice did not improve from 0.67944
Epoch 12/100
 - 7s - loss: 0.0013 - acc: 0.9994 - mDice: 0.8421 - val_loss: 0.0065 - val_acc: 0.9983 - val_mDice: 0.6235

Epoch 00005: val_mDice improved from 0.57716 to 0.59082, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 6/100
 - 7s - loss: 0.0016 - acc: 0.9993 - mDice: 0.7684 - val_loss: 0.0049 - val_acc: 0.9983 - val_mDice: 0.5889

Epoch 00010: val_mDice improved from 0.65905 to 0.66261, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 11/100
 - 7s - loss: 0.0033 - acc: 0.9986 - mDice: 0.8233 - val_loss: 0.0110 - val_acc: 0.9968 - val_mDice: 0.6605

Epoch 00048: val_mDice improved from 0.90637 to 0.90666, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.2/1-THALAMUS/sd2/best_model_weights.h5
Epoch 49/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9479 - val_loss: 0.0135 - val_acc: 0.9958 - val_mDice: 0.9058

Epoch 00012: val_mDice did not improve from 0.67944
Epoch 13/100
 - 7s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8499 - val_loss: 0.0068 - val_acc: 0.9982 - val_mDice: 0.6149

Epoch 00006: val_mDice did not improve from 0.59082
Epoch 7/100
 - 6s - loss: 0.0015 - acc: 0.9993 - mDice: 0.7789 - val_loss: 0.0041 - val_acc: 0.9987 - val_mDice: 0.6253

Epoch 00011: val_mDice did not improve from 0.66261
Epoch 12/100
 - 6s - loss: 0.0031 - acc: 0.9986 - mDice: 0.8305 - val_loss: 0.0108 - val_acc: 0.9968 - val_mDice: 0.6604

Epoch 00013: val_mDice did not improve from 0.67944
Epoch 14/100
 - 6s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8521 - val_loss: 0.0066 - val_acc: 0.9983 - val_mDice: 0.6417

Epoch 00007: val_mDice improved from 0.59082 to 0.62530, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 8/100
 - 7s - loss: 0.0015 - acc: 0.9994 - mDice: 0.7907 - val_loss: 0.0040 - val_acc: 0.9987 - val_mDice: 0.6107

Epoch 00012: val_mDice did not improve from 0.66261
Epoch 13/100
 - 6s - loss: 0.0031 - acc: 0.9986 - mDice: 0.8302 - val_loss: 0.0107 - val_acc: 0.9970 - val_mDice: 0.6599

Epoch 00014: val_mDice did not improve from 0.67944
Epoch 15/100
 - 7s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8532 - val_loss: 0.0058 - val_acc: 0.9985 - val_mDice: 0.6451

Epoch 00049: val_mDice did not improve from 0.90666

Epoch 00049: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 50/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9481 - val_loss: 0.0135 - val_acc: 0.9957 - val_mDice: 0.9057

Epoch 00008: val_mDice did not improve from 0.62530
Epoch 9/100
 - 7s - loss: 0.0014 - acc: 0.9994 - mDice: 0.7981 - val_loss: 0.0046 - val_acc: 0.9986 - val_mDice: 0.6221

Epoch 00013: val_mDice did not improve from 0.66261
Epoch 14/100
 - 7s - loss: 0.0030 - acc: 0.9987 - mDice: 0.8387 - val_loss: 0.0113 - val_acc: 0.9967 - val_mDice: 0.6684

Epoch 00015: val_mDice did not improve from 0.67944
Epoch 16/100
 - 7s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8576 - val_loss: 0.0063 - val_acc: 0.9983 - val_mDice: 0.6450

Epoch 00009: val_mDice did not improve from 0.62530
Epoch 10/100
 - 6s - loss: 0.0013 - acc: 0.9994 - mDice: 0.8081 - val_loss: 0.0047 - val_acc: 0.9985 - val_mDice: 0.6252

Epoch 00014: val_mDice improved from 0.66261 to 0.66842, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 15/100
 - 6s - loss: 0.0029 - acc: 0.9987 - mDice: 0.8435 - val_loss: 0.0110 - val_acc: 0.9965 - val_mDice: 0.6684

Epoch 00016: val_mDice did not improve from 0.67944
Epoch 17/100
 - 7s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8574 - val_loss: 0.0060 - val_acc: 0.9984 - val_mDice: 0.6661

Epoch 00010: val_mDice did not improve from 0.62530
Epoch 11/100
 - 7s - loss: 0.0013 - acc: 0.9994 - mDice: 0.8169 - val_loss: 0.0046 - val_acc: 0.9985 - val_mDice: 0.6233

Epoch 00015: val_mDice did not improve from 0.66842
Epoch 16/100
 - 7s - loss: 0.0029 - acc: 0.9988 - mDice: 0.8455 - val_loss: 0.0115 - val_acc: 0.9969 - val_mDice: 0.6656

Epoch 00050: val_mDice did not improve from 0.90666
Epoch 51/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9480 - val_loss: 0.0134 - val_acc: 0.9957 - val_mDice: 0.9052

Epoch 00017: val_mDice did not improve from 0.67944
Epoch 18/100
 - 7s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8638 - val_loss: 0.0064 - val_acc: 0.9984 - val_mDice: 0.6444

Epoch 00011: val_mDice did not improve from 0.62530
Epoch 12/100
 - 7s - loss: 0.0013 - acc: 0.9995 - mDice: 0.8183 - val_loss: 0.0049 - val_acc: 0.9986 - val_mDice: 0.6136

Epoch 00016: val_mDice did not improve from 0.66842
Epoch 17/100
 - 6s - loss: 0.0028 - acc: 0.9988 - mDice: 0.8499 - val_loss: 0.0116 - val_acc: 0.9968 - val_mDice: 0.6729

Epoch 00018: val_mDice did not improve from 0.67944
Epoch 19/100
 - 6s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8646 - val_loss: 0.0068 - val_acc: 0.9982 - val_mDice: 0.6557

Epoch 00012: val_mDice did not improve from 0.62530
Epoch 13/100
 - 6s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8286 - val_loss: 0.0045 - val_acc: 0.9987 - val_mDice: 0.6423

Epoch 00017: val_mDice improved from 0.66842 to 0.67287, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 18/100
 - 6s - loss: 0.0028 - acc: 0.9988 - mDice: 0.8511 - val_loss: 0.0110 - val_acc: 0.9966 - val_mDice: 0.6789

Epoch 00019: val_mDice did not improve from 0.67944
Epoch 20/100
 - 7s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8690 - val_loss: 0.0059 - val_acc: 0.9985 - val_mDice: 0.6606

Epoch 00051: val_mDice did not improve from 0.90666
Epoch 52/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9481 - val_loss: 0.0134 - val_acc: 0.9958 - val_mDice: 0.9053

Epoch 00013: val_mDice improved from 0.62530 to 0.64231, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 14/100
 - 7s - loss: 0.0012 - acc: 0.9995 - mDice: 0.8323 - val_loss: 0.0053 - val_acc: 0.9986 - val_mDice: 0.6225

Epoch 00018: val_mDice improved from 0.67287 to 0.67895, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 19/100
 - 6s - loss: 0.0028 - acc: 0.9988 - mDice: 0.8502 - val_loss: 0.0104 - val_acc: 0.9971 - val_mDice: 0.6777

Epoch 00020: val_mDice did not improve from 0.67944
Epoch 21/100
 - 7s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8709 - val_loss: 0.0060 - val_acc: 0.9984 - val_mDice: 0.6741

Epoch 00014: val_mDice did not improve from 0.64231
Epoch 15/100
 - 6s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8363 - val_loss: 0.0047 - val_acc: 0.9987 - val_mDice: 0.6383

Epoch 00019: val_mDice did not improve from 0.67895
Epoch 20/100
 - 6s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8601 - val_loss: 0.0118 - val_acc: 0.9970 - val_mDice: 0.6666

Epoch 00021: val_mDice did not improve from 0.67944
Epoch 22/100
 - 6s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8741 - val_loss: 0.0083 - val_acc: 0.9981 - val_mDice: 0.6213

Epoch 00015: val_mDice did not improve from 0.64231
Epoch 16/100
 - 7s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8413 - val_loss: 0.0052 - val_acc: 0.9986 - val_mDice: 0.6152

Epoch 00020: val_mDice did not improve from 0.67895
Epoch 21/100
 - 6s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8599 - val_loss: 0.0113 - val_acc: 0.9969 - val_mDice: 0.6731

Epoch 00022: val_mDice did not improve from 0.67944
Epoch 23/100
 - 7s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8742 - val_loss: 0.0059 - val_acc: 0.9984 - val_mDice: 0.6757

Epoch 00052: val_mDice did not improve from 0.90666
Epoch 53/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9482 - val_loss: 0.0133 - val_acc: 0.9958 - val_mDice: 0.9047

Epoch 00021: val_mDice did not improve from 0.67895
Epoch 22/100
 - 7s - loss: 0.0025 - acc: 0.9989 - mDice: 0.8639 - val_loss: 0.0117 - val_acc: 0.9968 - val_mDice: 0.6784

Epoch 00016: val_mDice did not improve from 0.64231
Epoch 17/100
 - 7s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8435 - val_loss: 0.0051 - val_acc: 0.9986 - val_mDice: 0.6231

Epoch 00023: val_mDice did not improve from 0.67944
Epoch 24/100
 - 7s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8754 - val_loss: 0.0069 - val_acc: 0.9984 - val_mDice: 0.6640

Epoch 00017: val_mDice did not improve from 0.64231
Epoch 18/100
 - 6s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8440 - val_loss: 0.0054 - val_acc: 0.9985 - val_mDice: 0.6284

Epoch 00022: val_mDice did not improve from 0.67895
Epoch 23/100
 - 6s - loss: 0.0025 - acc: 0.9989 - mDice: 0.8671 - val_loss: 0.0115 - val_acc: 0.9969 - val_mDice: 0.6782

Epoch 00024: val_mDice did not improve from 0.67944
Epoch 25/100
 - 7s - loss: 9.8463e-04 - acc: 0.9996 - mDice: 0.8796 - val_loss: 0.0070 - val_acc: 0.9984 - val_mDice: 0.6703

Epoch 00018: val_mDice did not improve from 0.64231
Epoch 19/100
 - 7s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8464 - val_loss: 0.0051 - val_acc: 0.9987 - val_mDice: 0.6265

Epoch 00023: val_mDice did not improve from 0.67895
Epoch 24/100
 - 7s - loss: 0.0025 - acc: 0.9989 - mDice: 0.8668 - val_loss: 0.0112 - val_acc: 0.9970 - val_mDice: 0.6822

Epoch 00053: val_mDice did not improve from 0.90666

Epoch 00053: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 54/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9484 - val_loss: 0.0137 - val_acc: 0.9957 - val_mDice: 0.9049

Epoch 00025: val_mDice did not improve from 0.67944
Epoch 26/100
 - 7s - loss: 9.9607e-04 - acc: 0.9996 - mDice: 0.8800 - val_loss: 0.0064 - val_acc: 0.9984 - val_mDice: 0.6513

Epoch 00019: val_mDice did not improve from 0.64231
Epoch 20/100
 - 6s - loss: 0.0011 - acc: 0.9995 - mDice: 0.8405 - val_loss: 0.0047 - val_acc: 0.9987 - val_mDice: 0.6306

Epoch 00024: val_mDice improved from 0.67895 to 0.68220, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 25/100
 - 6s - loss: 0.0026 - acc: 0.9989 - mDice: 0.8624 - val_loss: 0.0120 - val_acc: 0.9969 - val_mDice: 0.6847

Epoch 00026: val_mDice did not improve from 0.67944
Epoch 27/100
 - 6s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8743 - val_loss: 0.0058 - val_acc: 0.9984 - val_mDice: 0.6904

Epoch 00020: val_mDice did not improve from 0.64231
Epoch 21/100
 - 6s - loss: 0.0010 - acc: 0.9996 - mDice: 0.8512 - val_loss: 0.0050 - val_acc: 0.9987 - val_mDice: 0.6423

Epoch 00025: val_mDice improved from 0.68220 to 0.68469, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 26/100
 - 6s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8720 - val_loss: 0.0119 - val_acc: 0.9971 - val_mDice: 0.6803

Epoch 00027: val_mDice improved from 0.67944 to 0.69040, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/2-AV/sd2/best_model_weights.h5
Epoch 28/100
 - 7s - loss: 9.5454e-04 - acc: 0.9996 - mDice: 0.8845 - val_loss: 0.0079 - val_acc: 0.9983 - val_mDice: 0.6377

Epoch 00054: val_mDice did not improve from 0.90666
Epoch 55/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9484 - val_loss: 0.0133 - val_acc: 0.9958 - val_mDice: 0.9057

Epoch 00021: val_mDice did not improve from 0.64231
Epoch 22/100
 - 7s - loss: 0.0010 - acc: 0.9995 - mDice: 0.8503 - val_loss: 0.0053 - val_acc: 0.9987 - val_mDice: 0.6290

Epoch 00026: val_mDice did not improve from 0.68469
Epoch 27/100
 - 7s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8737 - val_loss: 0.0118 - val_acc: 0.9970 - val_mDice: 0.6825

Epoch 00028: val_mDice did not improve from 0.69040
Epoch 29/100
 - 7s - loss: 9.3662e-04 - acc: 0.9996 - mDice: 0.8860 - val_loss: 0.0080 - val_acc: 0.9983 - val_mDice: 0.6488

Epoch 00027: val_mDice did not improve from 0.68469
Epoch 28/100
 - 6s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8760 - val_loss: 0.0132 - val_acc: 0.9967 - val_mDice: 0.6756

Epoch 00022: val_mDice did not improve from 0.64231
Epoch 23/100
 - 7s - loss: 9.7970e-04 - acc: 0.9996 - mDice: 0.8617 - val_loss: 0.0050 - val_acc: 0.9987 - val_mDice: 0.6427

Epoch 00029: val_mDice did not improve from 0.69040
Epoch 30/100
 - 6s - loss: 9.1418e-04 - acc: 0.9996 - mDice: 0.8879 - val_loss: 0.0080 - val_acc: 0.9983 - val_mDice: 0.6472

Epoch 00028: val_mDice did not improve from 0.68469
Epoch 29/100
 - 7s - loss: 0.0024 - acc: 0.9990 - mDice: 0.8751 - val_loss: 0.0112 - val_acc: 0.9969 - val_mDice: 0.6829

Epoch 00023: val_mDice improved from 0.64231 to 0.64270, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 24/100
 - 7s - loss: 9.8568e-04 - acc: 0.9996 - mDice: 0.8600 - val_loss: 0.0053 - val_acc: 0.9986 - val_mDice: 0.6230

Epoch 00055: val_mDice did not improve from 0.90666
Epoch 56/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9484 - val_loss: 0.0138 - val_acc: 0.9957 - val_mDice: 0.9049

Epoch 00030: val_mDice did not improve from 0.69040
Epoch 31/100
 - 7s - loss: 9.3919e-04 - acc: 0.9996 - mDice: 0.8872 - val_loss: 0.0068 - val_acc: 0.9984 - val_mDice: 0.6892

Epoch 00029: val_mDice did not improve from 0.68469
Epoch 30/100
 - 7s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8760 - val_loss: 0.0122 - val_acc: 0.9970 - val_mDice: 0.6829

Epoch 00024: val_mDice did not improve from 0.64270
Epoch 25/100
 - 7s - loss: 9.9383e-04 - acc: 0.9996 - mDice: 0.8590 - val_loss: 0.0052 - val_acc: 0.9987 - val_mDice: 0.6491

Epoch 00031: val_mDice did not improve from 0.69040
Epoch 32/100
 - 7s - loss: 9.3084e-04 - acc: 0.9996 - mDice: 0.8872 - val_loss: 0.0070 - val_acc: 0.9983 - val_mDice: 0.6641

Epoch 00030: val_mDice did not improve from 0.68469
Epoch 31/100
 - 6s - loss: 0.0023 - acc: 0.9990 - mDice: 0.8763 - val_loss: 0.0119 - val_acc: 0.9970 - val_mDice: 0.6862

Epoch 00025: val_mDice improved from 0.64270 to 0.64908, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 26/100
 - 6s - loss: 9.6112e-04 - acc: 0.9996 - mDice: 0.8649 - val_loss: 0.0056 - val_acc: 0.9987 - val_mDice: 0.6193

Epoch 00032: val_mDice did not improve from 0.69040
Epoch 33/100
 - 7s - loss: 9.2334e-04 - acc: 0.9996 - mDice: 0.8880 - val_loss: 0.0074 - val_acc: 0.9984 - val_mDice: 0.6557

Epoch 00056: val_mDice did not improve from 0.90666
Epoch 57/100
 - 15s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9487 - val_loss: 0.0135 - val_acc: 0.9958 - val_mDice: 0.9058

Epoch 00031: val_mDice improved from 0.68469 to 0.68620, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 32/100
 - 7s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8862 - val_loss: 0.0127 - val_acc: 0.9970 - val_mDice: 0.6791

Epoch 00026: val_mDice did not improve from 0.64908
Epoch 27/100
 - 7s - loss: 9.2992e-04 - acc: 0.9996 - mDice: 0.8679 - val_loss: 0.0055 - val_acc: 0.9986 - val_mDice: 0.6410

Epoch 00033: val_mDice did not improve from 0.69040

Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 34/100
 - 7s - loss: 8.7536e-04 - acc: 0.9996 - mDice: 0.8928 - val_loss: 0.0077 - val_acc: 0.9983 - val_mDice: 0.6690

Epoch 00032: val_mDice did not improve from 0.68620
Epoch 33/100
 - 6s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8871 - val_loss: 0.0130 - val_acc: 0.9970 - val_mDice: 0.6794

Epoch 00027: val_mDice did not improve from 0.64908
Epoch 28/100
 - 7s - loss: 9.6674e-04 - acc: 0.9996 - mDice: 0.8648 - val_loss: 0.0050 - val_acc: 0.9986 - val_mDice: 0.6366

Epoch 00034: val_mDice did not improve from 0.69040
Epoch 35/100
 - 7s - loss: 8.2780e-04 - acc: 0.9996 - mDice: 0.8995 - val_loss: 0.0079 - val_acc: 0.9983 - val_mDice: 0.6689

Epoch 00033: val_mDice did not improve from 0.68620
Epoch 34/100
 - 6s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8888 - val_loss: 0.0130 - val_acc: 0.9970 - val_mDice: 0.6820

Epoch 00028: val_mDice did not improve from 0.64908
Epoch 29/100
 - 6s - loss: 9.5091e-04 - acc: 0.9996 - mDice: 0.8641 - val_loss: 0.0057 - val_acc: 0.9986 - val_mDice: 0.6212

Epoch 00057: val_mDice did not improve from 0.90666

Epoch 00057: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 58/100
 - 15s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9485 - val_loss: 0.0135 - val_acc: 0.9958 - val_mDice: 0.9047

Epoch 00035: val_mDice did not improve from 0.69040
Epoch 36/100
 - 7s - loss: 8.1628e-04 - acc: 0.9996 - mDice: 0.9009 - val_loss: 0.0076 - val_acc: 0.9984 - val_mDice: 0.6686

Epoch 00034: val_mDice did not improve from 0.68620
Epoch 35/100
 - 6s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8883 - val_loss: 0.0129 - val_acc: 0.9970 - val_mDice: 0.6864

Epoch 00029: val_mDice did not improve from 0.64908
Epoch 30/100
 - 7s - loss: 9.2540e-04 - acc: 0.9996 - mDice: 0.8674 - val_loss: 0.0054 - val_acc: 0.9987 - val_mDice: 0.6307

Epoch 00036: val_mDice did not improve from 0.69040
Epoch 37/100
 - 7s - loss: 8.1389e-04 - acc: 0.9996 - mDice: 0.9012 - val_loss: 0.0079 - val_acc: 0.9983 - val_mDice: 0.6718

Epoch 00035: val_mDice improved from 0.68620 to 0.68645, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 36/100
 - 7s - loss: 0.0020 - acc: 0.9991 - mDice: 0.8921 - val_loss: 0.0137 - val_acc: 0.9968 - val_mDice: 0.6825

Epoch 00030: val_mDice did not improve from 0.64908
Epoch 31/100
 - 7s - loss: 9.0344e-04 - acc: 0.9996 - mDice: 0.8713 - val_loss: 0.0052 - val_acc: 0.9987 - val_mDice: 0.6477

Epoch 00037: val_mDice did not improve from 0.69040
Epoch 38/100
 - 7s - loss: 8.0887e-04 - acc: 0.9996 - mDice: 0.9015 - val_loss: 0.0074 - val_acc: 0.9984 - val_mDice: 0.6742

Epoch 00036: val_mDice did not improve from 0.68645
Epoch 37/100
 - 7s - loss: 0.0021 - acc: 0.9991 - mDice: 0.8909 - val_loss: 0.0130 - val_acc: 0.9969 - val_mDice: 0.6892

Epoch 00031: val_mDice did not improve from 0.64908

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 32/100
 - 7s - loss: 8.5667e-04 - acc: 0.9996 - mDice: 0.8781 - val_loss: 0.0056 - val_acc: 0.9987 - val_mDice: 0.6347

Epoch 00058: val_mDice did not improve from 0.90666
Epoch 59/100
 - 15s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9487 - val_loss: 0.0140 - val_acc: 0.9957 - val_mDice: 0.9060

Epoch 00038: val_mDice did not improve from 0.69040
Epoch 39/100
 - 7s - loss: 8.0456e-04 - acc: 0.9997 - mDice: 0.9024 - val_loss: 0.0085 - val_acc: 0.9982 - val_mDice: 0.6537

Epoch 00037: val_mDice improved from 0.68645 to 0.68919, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 38/100
 - 7s - loss: 0.0020 - acc: 0.9991 - mDice: 0.8916 - val_loss: 0.0133 - val_acc: 0.9969 - val_mDice: 0.6827

Epoch 00032: val_mDice did not improve from 0.64908
Epoch 33/100
 - 7s - loss: 8.2558e-04 - acc: 0.9996 - mDice: 0.8837 - val_loss: 0.0058 - val_acc: 0.9986 - val_mDice: 0.6452

Epoch 00039: val_mDice did not improve from 0.69040

Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 40/100
 - 7s - loss: 7.7965e-04 - acc: 0.9997 - mDice: 0.9050 - val_loss: 0.0077 - val_acc: 0.9984 - val_mDice: 0.6778

Epoch 00038: val_mDice did not improve from 0.68919
Epoch 39/100
 - 6s - loss: 0.0020 - acc: 0.9991 - mDice: 0.8932 - val_loss: 0.0136 - val_acc: 0.9970 - val_mDice: 0.6834

Epoch 00033: val_mDice did not improve from 0.64908
Epoch 34/100
 - 7s - loss: 8.0598e-04 - acc: 0.9996 - mDice: 0.8875 - val_loss: 0.0055 - val_acc: 0.9988 - val_mDice: 0.6506

Epoch 00059: val_mDice did not improve from 0.90666
Epoch 60/100
 - 15s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9485 - val_loss: 0.0134 - val_acc: 0.9958 - val_mDice: 0.9055

Epoch 00039: val_mDice did not improve from 0.68919
Epoch 40/100
 - 6s - loss: 0.0020 - acc: 0.9991 - mDice: 0.8935 - val_loss: 0.0136 - val_acc: 0.9969 - val_mDice: 0.6795

Epoch 00040: val_mDice did not improve from 0.69040
Epoch 41/100
 - 7s - loss: 7.6944e-04 - acc: 0.9997 - mDice: 0.9060 - val_loss: 0.0080 - val_acc: 0.9984 - val_mDice: 0.6680

Epoch 00034: val_mDice improved from 0.64908 to 0.65060, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 35/100
 - 6s - loss: 8.0497e-04 - acc: 0.9996 - mDice: 0.8862 - val_loss: 0.0059 - val_acc: 0.9987 - val_mDice: 0.6271

Epoch 00040: val_mDice did not improve from 0.68919

Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 41/100
 - 7s - loss: 0.0020 - acc: 0.9992 - mDice: 0.8958 - val_loss: 0.0134 - val_acc: 0.9970 - val_mDice: 0.6874

Epoch 00041: val_mDice did not improve from 0.69040
Epoch 42/100
 - 7s - loss: 7.6182e-04 - acc: 0.9997 - mDice: 0.9082 - val_loss: 0.0083 - val_acc: 0.9983 - val_mDice: 0.6668

Epoch 00035: val_mDice did not improve from 0.65060
Epoch 36/100
 - 7s - loss: 8.0818e-04 - acc: 0.9996 - mDice: 0.8868 - val_loss: 0.0057 - val_acc: 0.9986 - val_mDice: 0.6567

Epoch 00041: val_mDice did not improve from 0.68919
Epoch 42/100
 - 6s - loss: 0.0019 - acc: 0.9992 - mDice: 0.8970 - val_loss: 0.0136 - val_acc: 0.9970 - val_mDice: 0.6905

Epoch 00042: val_mDice did not improve from 0.69040
Epoch 43/100
 - 6s - loss: 7.6557e-04 - acc: 0.9997 - mDice: 0.9067 - val_loss: 0.0083 - val_acc: 0.9983 - val_mDice: 0.6671

Epoch 00036: val_mDice improved from 0.65060 to 0.65668, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/5-VLa/sd2/best_model_weights.h5
Epoch 37/100
 - 6s - loss: 7.9387e-04 - acc: 0.9997 - mDice: 0.8888 - val_loss: 0.0060 - val_acc: 0.9987 - val_mDice: 0.6341

Epoch 00060: val_mDice did not improve from 0.90666
Epoch 61/100
 - 15s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9488 - val_loss: 0.0139 - val_acc: 0.9957 - val_mDice: 0.9047

Epoch 00042: val_mDice improved from 0.68919 to 0.69050, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5
Epoch 43/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.8987 - val_loss: 0.0138 - val_acc: 0.9969 - val_mDice: 0.6854

Epoch 00043: val_mDice did not improve from 0.69040
Epoch 44/100
 - 7s - loss: 7.6181e-04 - acc: 0.9997 - mDice: 0.9082 - val_loss: 0.0081 - val_acc: 0.9983 - val_mDice: 0.6677

Epoch 00037: val_mDice did not improve from 0.65668
Epoch 38/100
 - 7s - loss: 7.8942e-04 - acc: 0.9997 - mDice: 0.8897 - val_loss: 0.0058 - val_acc: 0.9987 - val_mDice: 0.6505

Epoch 00043: val_mDice did not improve from 0.69050
Epoch 44/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.8977 - val_loss: 0.0139 - val_acc: 0.9969 - val_mDice: 0.6872

Epoch 00044: val_mDice did not improve from 0.69040

Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 45/100
 - 7s - loss: 7.4400e-04 - acc: 0.9997 - mDice: 0.9092 - val_loss: 0.0078 - val_acc: 0.9984 - val_mDice: 0.6772

Epoch 00038: val_mDice did not improve from 0.65668
Epoch 39/100
 - 6s - loss: 8.1082e-04 - acc: 0.9996 - mDice: 0.8870 - val_loss: 0.0058 - val_acc: 0.9987 - val_mDice: 0.6419

Epoch 00044: val_mDice did not improve from 0.69050
Epoch 45/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.8989 - val_loss: 0.0139 - val_acc: 0.9970 - val_mDice: 0.6895

Epoch 00061: val_mDice did not improve from 0.90666

Epoch 00061: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 62/100
 - 14s - loss: 0.0045 - acc: 0.9981 - mDice: 0.9487 - val_loss: 0.0134 - val_acc: 0.9958 - val_mDice: 0.9057

Epoch 00045: val_mDice did not improve from 0.69040
Epoch 46/100
 - 7s - loss: 7.4122e-04 - acc: 0.9997 - mDice: 0.9106 - val_loss: 0.0078 - val_acc: 0.9984 - val_mDice: 0.6780

Epoch 00039: val_mDice did not improve from 0.65668
Epoch 40/100
 - 7s - loss: 7.8916e-04 - acc: 0.9997 - mDice: 0.8905 - val_loss: 0.0056 - val_acc: 0.9987 - val_mDice: 0.6500

Epoch 00045: val_mDice did not improve from 0.69050
Epoch 46/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.8986 - val_loss: 0.0139 - val_acc: 0.9970 - val_mDice: 0.6869

Epoch 00046: val_mDice did not improve from 0.69040
Epoch 47/100
 - 7s - loss: 7.3292e-04 - acc: 0.9997 - mDice: 0.9111 - val_loss: 0.0077 - val_acc: 0.9984 - val_mDice: 0.6808

Epoch 00040: val_mDice did not improve from 0.65668
Epoch 41/100
 - 7s - loss: 7.9330e-04 - acc: 0.9997 - mDice: 0.8886 - val_loss: 0.0059 - val_acc: 0.9987 - val_mDice: 0.6394

predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:01,  2.24it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.70it/s]predicting test subjects:  60%|██████    | 3/5 [00:00<00:00,  3.38it/s]predicting test subjects:  80%|████████  | 4/5 [00:00<00:00,  4.22it/s]predicting test subjects: 100%|██████████| 5/5 [00:00<00:00,  4.87it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:   1%|          | 1/148 [00:00<00:15,  9.46it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:15,  9.51it/s]predicting train subjects:   3%|▎         | 5/148 [00:00<00:14,  9.53it/s]predicting train subjects:   4%|▍         | 6/148 [00:00<00:14,  9.62it/s]predicting train subjects:   5%|▍         | 7/148 [00:00<00:15,  8.97it/s]predicting train subjects:   5%|▌         | 8/148 [00:00<00:15,  9.17it/s]predicting train subjects:   7%|▋         | 10/148 [00:01<00:14,  9.39it/s]predicting train subjects:   8%|▊         | 12/148 [00:01<00:14,  9.40it/s]predicting train subjects:   9%|▉         | 14/148 [00:01<00:14,  9.56it/s]predicting train subjects:  10%|█         | 15/148 [00:01<00:13,  9.62it/s]predicting train subjects:  11%|█▏        | 17/148 [00:01<00:12, 10.28it/s]predicting train subjects:  13%|█▎        | 19/148 [00:01<00:13,  9.61it/s]predicting train subjects:  14%|█▍        | 21/148 [00:02<00:13,  9.58it/s]predicting train subjects:  16%|█▌        | 23/148 [00:02<00:12,  9.88it/s]predicting train subjects:  16%|█▌        | 24/148 [00:02<00:13,  9.54it/s]
Epoch 00046: val_mDice did not improve from 0.69050
Epoch 47/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.8996 - val_loss: 0.0140 - val_acc: 0.9969 - val_mDice: 0.6914
predicting train subjects:  17%|█▋        | 25/148 [00:02<00:12,  9.46it/s]predicting train subjects:  18%|█▊        | 26/148 [00:02<00:13,  9.28it/s]predicting train subjects:  18%|█▊        | 27/148 [00:02<00:13,  9.22it/s]predicting train subjects:  20%|█▉        | 29/148 [00:03<00:12,  9.25it/s]predicting train subjects:  20%|██        | 30/148 [00:03<00:12,  9.39it/s]predicting train subjects:  21%|██        | 31/148 [00:03<00:12,  9.37it/s]predicting train subjects:  22%|██▏       | 33/148 [00:03<00:11,  9.59it/s]predicting train subjects:  23%|██▎       | 34/148 [00:03<00:12,  9.22it/s]predicting train subjects:  24%|██▎       | 35/148 [00:03<00:12,  9.26it/s]predicting train subjects:  25%|██▌       | 37/148 [00:03<00:13,  8.49it/s]predicting train subjects:  26%|██▌       | 38/148 [00:04<00:12,  8.78it/s]
Epoch 00041: val_mDice did not improve from 0.65668

Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 42/100
 - 7s - loss: 7.6226e-04 - acc: 0.9997 - mDice: 0.8940 - val_loss: 0.0057 - val_acc: 0.9987 - val_mDice: 0.6506
predicting train subjects:  27%|██▋       | 40/148 [00:04<00:11,  9.13it/s]predicting train subjects:  28%|██▊       | 42/148 [00:04<00:11,  9.15it/s]predicting train subjects:  29%|██▉       | 43/148 [00:04<00:11,  9.21it/s]predicting train subjects:  30%|██▉       | 44/148 [00:04<00:11,  9.12it/s]predicting train subjects:  30%|███       | 45/148 [00:04<00:11,  8.93it/s]predicting train subjects:  31%|███       | 46/148 [00:04<00:11,  8.97it/s]predicting train subjects:  32%|███▏      | 47/148 [00:05<00:11,  8.95it/s]predicting train subjects:  32%|███▏      | 48/148 [00:05<00:12,  8.16it/s]predicting train subjects:  33%|███▎      | 49/148 [00:05<00:11,  8.28it/s]predicting train subjects:  34%|███▍      | 51/148 [00:05<00:11,  8.78it/s]predicting train subjects:  36%|███▌      | 53/148 [00:05<00:10,  9.05it/s]predicting train subjects:  37%|███▋      | 55/148 [00:05<00:10,  9.18it/s]predicting train subjects:  38%|███▊      | 56/148 [00:06<00:09,  9.40it/s]predicting train subjects:  39%|███▊      | 57/148 [00:06<00:09,  9.57it/s]predicting train subjects:  39%|███▉      | 58/148 [00:06<00:09,  9.68it/s]predicting train subjects:  40%|███▉      | 59/148 [00:06<00:09,  8.94it/s]predicting train subjects:  41%|████      | 60/148 [00:06<00:10,  8.76it/s]predicting train subjects:  41%|████      | 61/148 [00:06<00:10,  8.53it/s]predicting train subjects:  42%|████▏     | 62/148 [00:06<00:10,  8.47it/s]predicting train subjects:  43%|████▎     | 63/148 [00:06<00:10,  8.11it/s]predicting train subjects:  43%|████▎     | 64/148 [00:06<00:10,  7.95it/s]predicting train subjects:  45%|████▍     | 66/148 [00:07<00:09,  8.59it/s]predicting train subjects:  45%|████▌     | 67/148 [00:07<00:09,  8.96it/s]
Epoch 00062: val_mDice did not improve from 0.90666
Epoch 63/100
 - 14s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9489 - val_loss: 0.0138 - val_acc: 0.9957 - val_mDice: 0.9051
predicting train subjects:  47%|████▋     | 69/148 [00:07<00:08,  9.52it/s]predicting train subjects:  48%|████▊     | 71/148 [00:07<00:07,  9.84it/s]predicting train subjects:  49%|████▉     | 73/148 [00:07<00:07,  9.70it/s]predicting train subjects:  50%|█████     | 74/148 [00:07<00:07,  9.56it/s]predicting train subjects:  51%|█████▏    | 76/148 [00:08<00:07,  9.67it/s]predicting train subjects:  53%|█████▎    | 78/148 [00:08<00:07,  9.74it/s]predicting train subjects:  54%|█████▍    | 80/148 [00:08<00:06, 10.31it/s]predicting train subjects:  55%|█████▌    | 82/148 [00:08<00:06, 10.49it/s]predicting train subjects:  57%|█████▋    | 84/148 [00:08<00:06,  9.99it/s]predicting train subjects:  58%|█████▊    | 86/148 [00:09<00:06, 10.07it/s]predicting train subjects:  59%|█████▉    | 88/148 [00:09<00:05, 10.26it/s]predicting train subjects:  61%|██████    | 90/148 [00:09<00:05, 10.28it/s]predicting train subjects:  62%|██████▏   | 92/148 [00:09<00:05, 10.39it/s]predicting train subjects:  64%|██████▎   | 94/148 [00:09<00:05, 10.09it/s]predicting train subjects:  65%|██████▍   | 96/148 [00:10<00:04, 10.89it/s]predicting train subjects:  66%|██████▌   | 98/148 [00:10<00:04, 11.90it/s]predicting train subjects:  68%|██████▊   | 100/148 [00:10<00:03, 12.68it/s]predicting train subjects:  69%|██████▉   | 102/148 [00:10<00:04, 11.47it/s]predicting train subjects:  70%|███████   | 104/148 [00:10<00:04, 10.64it/s]
Epoch 00047: val_mDice improved from 0.69050 to 0.69145, saving model to /array/ssd/msmajdi/experiments/keras/exp1/models/sE11_Cascade_FM20_DO0.1/4-VA/sd2/best_model_weights.h5

Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 48/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.9004 - val_loss: 0.0137 - val_acc: 0.9970 - val_mDice: 0.6901
predicting train subjects:  72%|███████▏  | 106/148 [00:10<00:04, 10.02it/s]predicting train subjects:  73%|███████▎  | 108/148 [00:11<00:04,  9.44it/s]predicting train subjects:  74%|███████▎  | 109/148 [00:11<00:04,  9.60it/s]predicting train subjects:  74%|███████▍  | 110/148 [00:11<00:03,  9.67it/s]predicting train subjects:  75%|███████▌  | 111/148 [00:11<00:03,  9.63it/s]predicting train subjects:  76%|███████▌  | 112/148 [00:11<00:03,  9.72it/s]predicting train subjects:  77%|███████▋  | 114/148 [00:11<00:03, 10.66it/s]predicting train subjects:  78%|███████▊  | 116/148 [00:11<00:02, 11.20it/s]predicting train subjects:  80%|███████▉  | 118/148 [00:12<00:02, 12.10it/s]predicting train subjects:  81%|████████  | 120/148 [00:12<00:02, 11.01it/s]predicting train subjects:  82%|████████▏ | 122/148 [00:12<00:02, 10.43it/s]
Epoch 00042: val_mDice did not improve from 0.65668
Epoch 43/100
 - 7s - loss: 7.4844e-04 - acc: 0.9997 - mDice: 0.8958 - val_loss: 0.0059 - val_acc: 0.9987 - val_mDice: 0.6471
predicting train subjects:  84%|████████▍ | 124/148 [00:12<00:02,  9.72it/s]predicting train subjects:  85%|████████▌ | 126/148 [00:12<00:02,  9.40it/s]predicting train subjects:  86%|████████▌ | 127/148 [00:13<00:02,  9.07it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:13<00:02,  8.51it/s]predicting train subjects:  87%|████████▋ | 129/148 [00:13<00:02,  8.19it/s]predicting train subjects:  88%|████████▊ | 130/148 [00:13<00:02,  7.81it/s]predicting train subjects:  89%|████████▊ | 131/148 [00:13<00:02,  8.25it/s]predicting train subjects:  90%|████████▉ | 133/148 [00:13<00:01,  8.69it/s]predicting train subjects:  91%|█████████ | 134/148 [00:13<00:01,  8.81it/s]predicting train subjects:  91%|█████████ | 135/148 [00:14<00:01,  9.10it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:14<00:01,  9.34it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:14<00:01,  9.99it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:14<00:00, 10.35it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:14<00:00, 10.75it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:14<00:00, 10.11it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:15<00:00,  9.69it/s]predicting train subjects:  99%|█████████▉| 147/148 [00:15<00:00,  9.42it/s]predicting train subjects: 100%|██████████| 148/148 [00:15<00:00,  9.20it/s]

Epoch 00047: val_mDice did not improve from 0.69040
Restoring model weights from the end of the best epoch
Epoch 00047: early stopping
{'val_loss': [0.0036498962980794146, 0.004130862643347776, 0.0037481033867423205, 0.0040935091971558459, 0.0053698891238804823, 0.0051989486896769797, 0.0052328857078038629, 0.0055923015770918509, 0.0043256218109200611, 0.0047616559239302544, 0.0058131768427630692, 0.0065023204252282352, 0.0068377509832065154, 0.0066017435685275716, 0.0058322864426101776, 0.0063484530559087052, 0.0059587872547513625, 0.0063866203987693534, 0.0067927716894352688, 0.0059173126566283247, 0.0060049961221979023, 0.0082962368119587297, 0.0058621482210273442, 0.0069210229520785043, 0.007037332499439412, 0.0063965132242029014, 0.0057573922850350113, 0.0078803257263721296, 0.0080236217720394444, 0.0080349979169191191, 0.0068333135977228909, 0.0070460115539583753, 0.0073507188799533437, 0.0076563733411913219, 0.0078500836215754776, 0.0075981372254008946, 0.0078740569544916462, 0.0074252882536421433, 0.0084617303763615329, 0.0076972125890724203, 0.0079524120751847619, 0.0082688978774116389, 0.0083122926546221089, 0.0081361182073646406, 0.0077637628989016758, 0.0078317308600278617, 0.0077249012847847125], 'val_acc': [0.99859715015330208, 0.99847628588372084, 0.99856949106175852, 0.9984510870690041, 0.99835386428427186, 0.99838463169463132, 0.99802196152666778, 0.99817858985129826, 0.99858912381720033, 0.99823654332059497, 0.99816449525508477, 0.99827431617899143, 0.99820176718082831, 0.99825818361120022, 0.99850347701539388, 0.99831156400924037, 0.99837303795713062, 0.99838158932137999, 0.99816246742897841, 0.99845765499358485, 0.99842639933241173, 0.99812267942631494, 0.9984198783306365, 0.99836294194485276, 0.99835539244590921, 0.99839722349288618, 0.99842238933482064, 0.99828941898143042, 0.9982979678093119, 0.99829395654353692, 0.99835488390415272, 0.99828488902842749, 0.99837048383469273, 0.99832464913104446, 0.99830502778925789, 0.99838663922979476, 0.99833071992752398, 0.99842992995647672, 0.99823801060940354, 0.99840776717409174, 0.99835188211278714, 0.99832014580990402, 0.99830400943756104, 0.9983281937051327, 0.99839265929891707, 0.99836646242344629, 0.99840928899480941], 'val_mDice': [0.57615524149955588, 0.61293765839110026, 0.64109530093822076, 0.63731658585528106, 0.60026887629894499, 0.63261289545830257, 0.65146544639100423, 0.63367284105179156, 0.67944179697239648, 0.65832863843187372, 0.64602812680792299, 0.62348503381647957, 0.61493809679721267, 0.64169715059564469, 0.6451116673489834, 0.64495943708622705, 0.66613834842722464, 0.64439963406704837, 0.65574913329266482, 0.66057258463920432, 0.67413769250220446, 0.62126201771675271, 0.67574883014597786, 0.66398573429026497, 0.67031072048430751, 0.65125292285959768, 0.69039873747115443, 0.63773378412774273, 0.64883791005357783, 0.64723561418817399, 0.68922336811714979, 0.66413227801627306, 0.65570455028655683, 0.66896708468173416, 0.66892124870990188, 0.66858070835154104, 0.67180424040936404, 0.6741901699532854, 0.65374502603043905, 0.67778216904782229, 0.66795477714944396, 0.66684884974297054, 0.66713732354184418, 0.66766529768071281, 0.67723876871961231, 0.67798181417140557, 0.68077157406096767], 'loss': [0.0042456836862524083, 0.0028179016905683441, 0.0025062720938579188, 0.002151079256314106, 0.001905051009452891, 0.0017671794060107264, 0.0016355858887039576, 0.001653121792141977, 0.0014639096806992507, 0.0014346605389979881, 0.0013647896026243898, 0.0012775178995168657, 0.0012129701709629344, 0.0012017636740487754, 0.0011930026442602886, 0.0011652710104996923, 0.0011619543848155286, 0.0011121402186175893, 0.0011062735010264654, 0.0010792892083638941, 0.0010515870002340245, 0.0010307345649236289, 0.0010341718710191699, 0.0010141807055771051, 0.00098462849721551462, 0.00099607149918024278, 0.0010159245301728137, 0.00095454448768265195, 0.00093662301622062915, 0.00091417963451428204, 0.0009391858889714823, 0.00093084001440925303, 0.00092334380382798906, 0.00087535741842491505, 0.0008278012521710144, 0.0008162845487280578, 0.00081389190709935837, 0.00080886630339213356, 0.00080456048931274793, 0.00077965378197973178, 0.0007694440879017552, 0.00076181644208244889, 0.00076556694793554947, 0.00076181443559662765, 0.00074399705276860669, 0.00074121798775219044, 0.000732920998795305], 'acc': [0.99836587353796025, 0.99878809917477285, 0.99893658588666756, 0.99908259172449576, 0.99918916674603619, 0.99924467317043841, 0.99929783165934039, 0.99929525345115966, 0.99937166229322716, 0.99938307905147139, 0.99941159325936002, 0.99944826701186584, 0.99947602233866717, 0.99948123558881774, 0.99948474186694281, 0.99949542084843579, 0.9994965558367066, 0.99952110065858879, 0.99952082105217932, 0.9995333964863844, 0.99954406932199469, 0.99955432296674263, 0.99955071295861886, 0.99956337068515078, 0.99957174508565794, 0.99957068559799278, 0.99955866987661068, 0.99958480194039134, 0.99959340957193599, 0.99960307204527521, 0.99959461653995085, 0.9995984266746798, 0.99959977853969029, 0.99962028303749706, 0.99964012705083916, 0.99964514244523595, 0.99964774498694509, 0.99964970739851966, 0.99965109693049881, 0.99966228824943482, 0.99966746987433164, 0.99966857952906596, 0.99967147253143829, 0.99966918874228594, 0.99967918405162837, 0.9996803110398349, 0.99968294406095526], 'mDice': [0.52018696684305665, 0.63579879841242837, 0.68852613200178014, 0.73450818194329681, 0.7609315238318809, 0.78107653877527639, 0.79782586921057219, 0.79501578979498855, 0.82074131879386514, 0.82267788993191449, 0.83109948450516502, 0.84209649861171421, 0.84989782277206904, 0.85212433427749334, 0.85317961709643531, 0.85757378680186858, 0.8574403521273779, 0.86383922470486774, 0.86461769222004847, 0.86898114572076357, 0.87086475635066563, 0.87407377650795259, 0.87424398416491167, 0.87540189016119496, 0.87963241109561352, 0.87999317892695261, 0.87433550752405098, 0.88449393632069451, 0.88598251286522989, 0.88792328318279545, 0.88721292069307456, 0.88715419509701698, 0.88801362453245358, 0.89275172505917055, 0.89948974344039534, 0.90087331476764887, 0.9011744910793178, 0.90148321911822826, 0.90243320884257916, 0.90498676334632444, 0.90597558113212817, 0.90824976731163354, 0.90673292949873852, 0.90824850075980745, 0.90921961477860336, 0.91064987931373198, 0.91112559396293102], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001]}

Epoch 00048: val_mDice did not improve from 0.69145
Epoch 49/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.9021 - val_loss: 0.0140 - val_acc: 0.9970 - val_mDice: 0.6899

Epoch 00043: val_mDice did not improve from 0.65668
Epoch 44/100
 - 7s - loss: 7.4411e-04 - acc: 0.9997 - mDice: 0.8976 - val_loss: 0.0059 - val_acc: 0.9987 - val_mDice: 0.6426

Epoch 00049: val_mDice did not improve from 0.69145
Epoch 50/100
 - 6s - loss: 0.0019 - acc: 0.9992 - mDice: 0.9018 - val_loss: 0.0140 - val_acc: 0.9970 - val_mDice: 0.6870

Epoch 00063: val_mDice did not improve from 0.90666
Epoch 64/100
 - 14s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9488 - val_loss: 0.0134 - val_acc: 0.9958 - val_mDice: 0.9061

Epoch 00044: val_mDice did not improve from 0.65668
Epoch 45/100
 - 7s - loss: 7.4618e-04 - acc: 0.9997 - mDice: 0.8969 - val_loss: 0.0061 - val_acc: 0.9987 - val_mDice: 0.6335

Epoch 00050: val_mDice did not improve from 0.69145
Epoch 51/100
 - 6s - loss: 0.0019 - acc: 0.9992 - mDice: 0.9025 - val_loss: 0.0141 - val_acc: 0.9970 - val_mDice: 0.6883

Epoch 00045: val_mDice did not improve from 0.65668
Epoch 46/100
 - 7s - loss: 7.3410e-04 - acc: 0.9997 - mDice: 0.8982 - val_loss: 0.0059 - val_acc: 0.9987 - val_mDice: 0.6480

Epoch 00051: val_mDice did not improve from 0.69145

Epoch 00051: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 52/100
 - 6s - loss: 0.0019 - acc: 0.9992 - mDice: 0.9023 - val_loss: 0.0141 - val_acc: 0.9969 - val_mDice: 0.6880

Epoch 00046: val_mDice did not improve from 0.65668

Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 47/100
 - 7s - loss: 7.2602e-04 - acc: 0.9997 - mDice: 0.8996 - val_loss: 0.0061 - val_acc: 0.9987 - val_mDice: 0.6376

Epoch 00064: val_mDice did not improve from 0.90666
Epoch 65/100
 - 15s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9489 - val_loss: 0.0135 - val_acc: 0.9958 - val_mDice: 0.9059

Epoch 00052: val_mDice did not improve from 0.69145
Epoch 53/100
 - 7s - loss: 0.0019 - acc: 0.9992 - mDice: 0.9026 - val_loss: 0.0141 - val_acc: 0.9970 - val_mDice: 0.6882

Epoch 00047: val_mDice did not improve from 0.65668
Epoch 48/100
 - 7s - loss: 7.1936e-04 - acc: 0.9997 - mDice: 0.9005 - val_loss: 0.0061 - val_acc: 0.9987 - val_mDice: 0.6440

Epoch 00053: val_mDice did not improve from 0.69145
Epoch 54/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9028 - val_loss: 0.0141 - val_acc: 0.9970 - val_mDice: 0.6898

Epoch 00048: val_mDice did not improve from 0.65668
Epoch 49/100
 - 7s - loss: 7.1715e-04 - acc: 0.9997 - mDice: 0.9005 - val_loss: 0.0060 - val_acc: 0.9987 - val_mDice: 0.6435

Epoch 00054: val_mDice did not improve from 0.69145
Epoch 55/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9030 - val_loss: 0.0142 - val_acc: 0.9969 - val_mDice: 0.6870

Epoch 00049: val_mDice did not improve from 0.65668
Epoch 50/100
 - 7s - loss: 7.1288e-04 - acc: 0.9997 - mDice: 0.9014 - val_loss: 0.0059 - val_acc: 0.9987 - val_mDice: 0.6512

Epoch 00065: val_mDice did not improve from 0.90666

Epoch 00065: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 66/100
 - 15s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9491 - val_loss: 0.0139 - val_acc: 0.9957 - val_mDice: 0.9056

Epoch 00055: val_mDice did not improve from 0.69145

Epoch 00055: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 56/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9033 - val_loss: 0.0141 - val_acc: 0.9970 - val_mDice: 0.6886

Epoch 00050: val_mDice did not improve from 0.65668
Epoch 51/100
 - 7s - loss: 7.2197e-04 - acc: 0.9997 - mDice: 0.9016 - val_loss: 0.0060 - val_acc: 0.9987 - val_mDice: 0.6473

Epoch 00056: val_mDice did not improve from 0.69145
Epoch 57/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9037 - val_loss: 0.0142 - val_acc: 0.9970 - val_mDice: 0.6865

Epoch 00051: val_mDice did not improve from 0.65668

Epoch 00051: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 52/100
 - 6s - loss: 7.1186e-04 - acc: 0.9997 - mDice: 0.9011 - val_loss: 0.0061 - val_acc: 0.9987 - val_mDice: 0.6440

Epoch 00057: val_mDice did not improve from 0.69145
Epoch 58/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9034 - val_loss: 0.0141 - val_acc: 0.9970 - val_mDice: 0.6883

Epoch 00066: val_mDice did not improve from 0.90666
Epoch 67/100
 - 15s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9494 - val_loss: 0.0136 - val_acc: 0.9958 - val_mDice: 0.9061

Epoch 00052: val_mDice did not improve from 0.65668
Epoch 53/100
 - 7s - loss: 7.0410e-04 - acc: 0.9997 - mDice: 0.9016 - val_loss: 0.0060 - val_acc: 0.9987 - val_mDice: 0.6512

Epoch 00058: val_mDice did not improve from 0.69145
Epoch 59/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9035 - val_loss: 0.0143 - val_acc: 0.9970 - val_mDice: 0.6864

Epoch 00053: val_mDice did not improve from 0.65668
Epoch 54/100
 - 7s - loss: 7.0178e-04 - acc: 0.9997 - mDice: 0.9033 - val_loss: 0.0060 - val_acc: 0.9987 - val_mDice: 0.6499

Epoch 00059: val_mDice did not improve from 0.69145

Epoch 00059: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 60/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9037 - val_loss: 0.0147 - val_acc: 0.9969 - val_mDice: 0.6859

Epoch 00054: val_mDice did not improve from 0.65668
Epoch 55/100
 - 7s - loss: 7.1030e-04 - acc: 0.9997 - mDice: 0.9018 - val_loss: 0.0061 - val_acc: 0.9987 - val_mDice: 0.6522

Epoch 00067: val_mDice did not improve from 0.90666
Epoch 68/100
 - 15s - loss: 0.0044 - acc: 0.9981 - mDice: 0.9494 - val_loss: 0.0137 - val_acc: 0.9957 - val_mDice: 0.9056

Epoch 00060: val_mDice did not improve from 0.69145
Epoch 61/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9032 - val_loss: 0.0144 - val_acc: 0.9970 - val_mDice: 0.6851

Epoch 00055: val_mDice did not improve from 0.65668

Epoch 00055: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 56/100
 - 7s - loss: 7.1184e-04 - acc: 0.9997 - mDice: 0.9018 - val_loss: 0.0060 - val_acc: 0.9987 - val_mDice: 0.6473

Epoch 00068: val_mDice did not improve from 0.90666
Restoring model weights from the end of the best epoch
Epoch 00068: early stopping
{'val_loss': [0.0097318420616480019, 0.010833231230767874, 0.0098015265539288521, 0.0092812010731834638, 0.0099711006220716703, 0.0098424666346265711, 0.010161306565770736, 0.01036080288199278, 0.011843434749887539, 0.010300560997655759, 0.010800358122931076, 0.01051349937915802, 0.010610010045079084, 0.010938002513005184, 0.011093872384383129, 0.01128561651477447, 0.010793027324745288, 0.012867469627123613, 0.011181176783373723, 0.010936858992163952, 0.012002831110014366, 0.01186507223890378, 0.011376934699141063, 0.012974860146641731, 0.012260304763913155, 0.012888279242011217, 0.013332726433873177, 0.013491956946941523, 0.012924937817912836, 0.013249445420045119, 0.013174599251494957, 0.013503851655584116, 0.013260748380651841, 0.013402150370753728, 0.01337648698916802, 0.013397683819326071, 0.012740590394689487, 0.012982723374779407, 0.013484409866997829, 0.013547974638640881, 0.013173061852844862, 0.013293081607956152, 0.013302718360836688, 0.013266477948771073, 0.013604361349000381, 0.013545256108045578, 0.013366341017759763, 0.013116802733678084, 0.013499547011004044, 0.013513571940935575, 0.013414106976527434, 0.013424400813304461, 0.013317174206559475, 0.013733336701989174, 0.013266731125230972, 0.013790380066403976, 0.013470310717821121, 0.013546031183348252, 0.013969592535151886, 0.013368376387426486, 0.013925854474879228, 0.013416242857392017, 0.013772979521980653, 0.013357145281938406, 0.01352847797366289, 0.013863479933486534, 0.013616501210400691, 0.013729898545604486], 'val_acc': [0.9960688215035659, 0.99548264650198126, 0.99579184330426729, 0.99604585537543666, 0.99570743395731998, 0.99583980670342076, 0.99570126258409941, 0.99570964849912202, 0.9951978646791898, 0.99582960513921881, 0.99557761962597191, 0.99588280457716727, 0.99583259912637567, 0.99570764028109038, 0.99568325739640451, 0.99577525945810175, 0.99603708432270932, 0.99541219839682948, 0.99592767770473778, 0.99598426543749297, 0.99567807179230905, 0.99579598811956549, 0.99595986879788911, 0.99555407579128563, 0.99574364607150734, 0.99567780586389398, 0.99562217180545509, 0.99552387457627511, 0.99574517745238089, 0.9957396067105807, 0.99567248729559088, 0.99566230407127965, 0.99574436591221738, 0.99568610007946312, 0.99575241712423468, 0.99572244974283075, 0.99587024633701027, 0.99580688659961403, 0.99565792083740234, 0.99570610890021694, 0.99579777167393613, 0.99573703454091, 0.99578070640563965, 0.99577325582504272, 0.99573135375976562, 0.99570409151223993, 0.99579055492694557, 0.99585803196980405, 0.99575669948871315, 0.99573180308708775, 0.99572101464638341, 0.99577136223132789, 0.99577492017012381, 0.99569864456470192, 0.99579516282448399, 0.99567519701444185, 0.99583471279877878, 0.99575833173898554, 0.99572636989446783, 0.99579682258459234, 0.9956724735406729, 0.99580261799005365, 0.99571133118409372, 0.99581800974332368, 0.99578521343377924, 0.99570931838108945, 0.99575727261029756, 0.99573571406877959], 'val_mDice': [0.8707363192851727, 0.87210556635489833, 0.87832760352354788, 0.88322774263528681, 0.88511150158368623, 0.88675590203358579, 0.89189238731677711, 0.88746710924001837, 0.88842761975068307, 0.89157886230028593, 0.8901830178040725, 0.89472570327612067, 0.89273843398460972, 0.89255275634618902, 0.89530135576541603, 0.89829830939953148, 0.90231289313389706, 0.89630326399436366, 0.90126666197409999, 0.9002142686110276, 0.89947449702482962, 0.90247800258489752, 0.90281343460083008, 0.89968606141897345, 0.89816582661408639, 0.90202878530208885, 0.90088939208250784, 0.90069844172551083, 0.90305950549932623, 0.90400623358212984, 0.90353766771463251, 0.90339192060323859, 0.90442806940812326, 0.90401583451491141, 0.9053732798649714, 0.90477635310246396, 0.9063687370373652, 0.90488902880595279, 0.90340996247071481, 0.90449764636846686, 0.90536050154612613, 0.90505952101487375, 0.90544662108788121, 0.90527614721885097, 0.9053807900502131, 0.90473527174729562, 0.90570705212079561, 0.90665532075441801, 0.90582006711226248, 0.90569254985222447, 0.90524377272679257, 0.90532660025816702, 0.90473159001423764, 0.90493799631412208, 0.90568094070141136, 0.90492891806822562, 0.90576537755819464, 0.90468557522847104, 0.90602031120887172, 0.90553585841105533, 0.90474423078390265, 0.90565299987792969, 0.9051074798290546, 0.90606604172633243, 0.90589497181085443, 0.90561825495499826, 0.90609206603123593, 0.90556470705912662], 'loss': [0.013377329954325638, 0.010540159762456522, 0.0096819853004021898, 0.0090145624920733858, 0.0085546584499329413, 0.0079733715352440972, 0.0077045229399306386, 0.0073660096496391859, 0.0071199882674522, 0.0069009411478936833, 0.0066774834645535099, 0.006496723784265474, 0.0064164192312237741, 0.0061821092698668796, 0.006075830069906845, 0.0060617157488787937, 0.0058003040201483518, 0.0057405232272427021, 0.0056446734888713325, 0.0055885644017930327, 0.0055212232460661375, 0.0055003316974477452, 0.0054005383083137451, 0.0051253935476967081, 0.0050146311134023564, 0.0049591493359085878, 0.004958209519601727, 0.0048876223134261916, 0.0047932901905715884, 0.0047593142506070895, 0.0047372624220771847, 0.0047284516760213949, 0.0046353180200681774, 0.0046229841704271768, 0.0046069980096383571, 0.0046076995455818243, 0.004600349335222069, 0.0045740755593541883, 0.0045697104979429094, 0.0045604923864674771, 0.0045570669466045597, 0.0045507362997486049, 0.0045345043857756319, 0.0045343129483493387, 0.0045288126595032837, 0.0045301428899526819, 0.004505344746957035, 0.0045042587790981988, 0.0045109166966271737, 0.00450457589342965, 0.0044986118984203108, 0.0044932927186432494, 0.0044881352737367042, 0.0044653452917756548, 0.0044767380394524041, 0.004466410736863477, 0.0044553080258043602, 0.004468053301893822, 0.0044459322817006416, 0.0044562957496862279, 0.0044399339575531575, 0.004451861509302389, 0.0044203415674876666, 0.00444260984282243, 0.0044295451097709068, 0.004416675084203691, 0.0043960360677473553, 0.0043941429782009449], 'acc': [0.99468772207410416, 0.99551378175238303, 0.99585471562417982, 0.99613820422487009, 0.99633909532718901, 0.99658976994651882, 0.99671068486817382, 0.99684578749661312, 0.99695623721937454, 0.99704609033679292, 0.9971465227800772, 0.99721892888135544, 0.99726039383518206, 0.99735612027785359, 0.99740070583880813, 0.99740975249218933, 0.9975159266994762, 0.99754512271428197, 0.99758673055198988, 0.99761395562486399, 0.99763785506092528, 0.99765148663996883, 0.9976874961443869, 0.99780472947402188, 0.99785014750567635, 0.99786779517684832, 0.99787235057360402, 0.99790341588399634, 0.99794340050033925, 0.99795412174508946, 0.99796237616814343, 0.99796764025865736, 0.99800870052493074, 0.99801440680921238, 0.99801982443074055, 0.9980216625569257, 0.99802327217183373, 0.99803457868723244, 0.9980377790674414, 0.99804060917832182, 0.99803986966899449, 0.99804335308280823, 0.99805275514456493, 0.99805236621862869, 0.99805233324789966, 0.99805212091640472, 0.99805907426624119, 0.9980626912034658, 0.99806060893305304, 0.99806338474677181, 0.99806492359523702, 0.99806219027355048, 0.99806904985785039, 0.99807951524974325, 0.99807508330826367, 0.99808169254018886, 0.9980829485802154, 0.99807964931998583, 0.99808763975204162, 0.99808477184544753, 0.99809218562749424, 0.99808653138871462, 0.99809650057917731, 0.9980872099424013, 0.99809441705426849, 0.99809973364061622, 0.99810953652993839, 0.99810671687319164], 'mDice': [0.84896828998955787, 0.87343266366689964, 0.88418610534729725, 0.89263822160021911, 0.89845919435370369, 0.90558928897814439, 0.90918573471770314, 0.91316337628552413, 0.91633062576900226, 0.91880241777854421, 0.92178756268810591, 0.92382907317383378, 0.92485493177088673, 0.92777813896255368, 0.92910690341070412, 0.92911968884825769, 0.93226299003466229, 0.93302353414019823, 0.93426315818550909, 0.93505047574534783, 0.93563192664644101, 0.93582945798322181, 0.93722171398735155, 0.94051062208346525, 0.94182966705019644, 0.94227737870270412, 0.94247982383610684, 0.94340712346711553, 0.94434313377173473, 0.9449082641941241, 0.94517529116922372, 0.94530387369928914, 0.94627749054610766, 0.94653866346372639, 0.94690169180912775, 0.94662906527840895, 0.94678366348284104, 0.94724604022612269, 0.94724031496356775, 0.94721135862183714, 0.94747263550179361, 0.94747788884867479, 0.94761819023733451, 0.94765871743929431, 0.94766426642907353, 0.94770588938506695, 0.94797373326224887, 0.94800061357388032, 0.94785727939281472, 0.94813328907416439, 0.9480249782671365, 0.94810090879459863, 0.94817865840563953, 0.94844520931557841, 0.94836356507335817, 0.94840930496366105, 0.94870382540945231, 0.94849525782075106, 0.94867809309035589, 0.9485118687635361, 0.9488293396030576, 0.94871768321594674, 0.94887566759466679, 0.94878724656488467, 0.9489339647007452, 0.94909887341506016, 0.94939506970671661, 0.94941919202490621], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]
Epoch 00061: val_mDice did not improve from 0.69145
Epoch 62/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9045 - val_loss: 0.0145 - val_acc: 0.9969 - val_mDice: 0.6857
predicting test subjects:  20%|██        | 1/5 [00:00<00:02,  1.71it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:02,  1.84it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.12it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.39it/s]predicting test subjects:  60%|██████    | 3/5 [00:00<00:00,  2.62it/s]predicting test subjects:  80%|████████  | 4/5 [00:00<00:00,  3.07it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  3.30it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.73it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:   1%|          | 1/148 [00:00<00:16,  9.16it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.41it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:   1%|▏         | 2/148 [00:00<00:17,  8.41it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:17,  8.45it/s]predicting train subjects:   1%|          | 1/148 [00:00<00:34,  4.23it/s]predicting train subjects:   3%|▎         | 4/148 [00:00<00:16,  8.78it/s]predicting train subjects:   3%|▎         | 5/148 [00:00<00:15,  9.03it/s]predicting train subjects:   1%|▏         | 2/148 [00:00<00:33,  4.38it/s]predicting train subjects:   5%|▍         | 7/148 [00:00<00:15,  9.09it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:32,  4.52it/s]predicting train subjects:   5%|▌         | 8/148 [00:00<00:16,  8.34it/s]predicting train subjects:   3%|▎         | 4/148 [00:00<00:33,  4.32it/s]predicting train subjects:   7%|▋         | 10/148 [00:01<00:15,  9.06it/s]predicting train subjects:   7%|▋         | 11/148 [00:01<00:14,  9.17it/s]predicting train subjects:   3%|▎         | 5/148 [00:01<00:31,  4.57it/s]predicting train subjects:   8%|▊         | 12/148 [00:01<00:15,  8.71it/s]predicting train subjects:   4%|▍         | 6/148 [00:01<00:30,  4.65it/s]predicting train subjects:   9%|▉         | 14/148 [00:01<00:14,  9.16it/s]predicting train subjects:  11%|█         | 16/148 [00:01<00:13,  9.85it/s]predicting train subjects:   5%|▍         | 7/148 [00:01<00:32,  4.35it/s]predicting train subjects:  12%|█▏        | 18/148 [00:01<00:13,  9.29it/s]predicting train subjects:   5%|▌         | 8/148 [00:01<00:31,  4.48it/s]predicting train subjects:  13%|█▎        | 19/148 [00:02<00:13,  9.42it/s]predicting train subjects:   6%|▌         | 9/148 [00:01<00:29,  4.69it/s]predicting train subjects:  14%|█▍        | 21/148 [00:02<00:13,  9.35it/s]predicting train subjects:   7%|▋         | 10/148 [00:02<00:27,  4.98it/s]predicting train subjects:  16%|█▌        | 23/148 [00:02<00:13,  9.46it/s]predicting train subjects:   7%|▋         | 11/148 [00:02<00:26,  5.13it/s]predicting train subjects:  16%|█▌        | 24/148 [00:02<00:13,  9.33it/s]predicting train subjects:   8%|▊         | 12/148 [00:02<00:27,  4.87it/s]predicting train subjects:  18%|█▊        | 26/148 [00:02<00:12,  9.47it/s]predicting train subjects:   9%|▉         | 13/148 [00:02<00:24,  5.61it/s]predicting train subjects:  19%|█▉        | 28/148 [00:03<00:12,  9.46it/s]predicting train subjects:   9%|▉         | 14/148 [00:02<00:26,  5.02it/s]predicting train subjects:  20%|██        | 30/148 [00:03<00:12,  9.49it/s]predicting train subjects:  10%|█         | 15/148 [00:03<00:25,  5.15it/s]predicting train subjects:  21%|██        | 31/148 [00:03<00:12,  9.59it/s]predicting train subjects:  11%|█         | 16/148 [00:03<00:22,  5.82it/s]predicting train subjects:  22%|██▏       | 32/148 [00:03<00:12,  9.48it/s]predicting train subjects:  11%|█▏        | 17/148 [00:03<00:23,  5.48it/s]predicting train subjects:  23%|██▎       | 34/148 [00:03<00:12,  9.47it/s]predicting train subjects:  24%|██▍       | 36/148 [00:03<00:11,  9.66it/s]predicting train subjects:  12%|█▏        | 18/148 [00:03<00:25,  5.14it/s]predicting train subjects:  25%|██▌       | 37/148 [00:03<00:12,  9.24it/s]predicting train subjects:  26%|██▌       | 38/148 [00:04<00:11,  9.30it/s]predicting train subjects:  13%|█▎        | 19/148 [00:03<00:27,  4.75it/s]predicting train subjects:  27%|██▋       | 40/148 [00:04<00:11,  9.36it/s]predicting train subjects:  14%|█▎        | 20/148 [00:04<00:26,  4.86it/s]predicting train subjects:  28%|██▊       | 42/148 [00:04<00:11,  9.51it/s]predicting train subjects:  14%|█▍        | 21/148 [00:04<00:25,  4.89it/s]predicting train subjects:  29%|██▉       | 43/148 [00:04<00:11,  9.40it/s]predicting train subjects:  15%|█▍        | 22/148 [00:04<00:24,  5.21it/s]predicting train subjects:  30%|██▉       | 44/148 [00:04<00:11,  9.41it/s]predicting train subjects:  30%|███       | 45/148 [00:04<00:11,  9.34it/s]predicting train subjects:  16%|█▌        | 23/148 [00:04<00:24,  5.07it/s]predicting train subjects:  31%|███       | 46/148 [00:04<00:10,  9.42it/s]predicting train subjects:  32%|███▏      | 47/148 [00:04<00:10,  9.49it/s]predicting train subjects:  16%|█▌        | 24/148 [00:04<00:24,  4.96it/s]predicting train subjects:  32%|███▏      | 48/148 [00:05<00:10,  9.37it/s]predicting train subjects:  17%|█▋        | 25/148 [00:05<00:25,  4.89it/s]predicting train subjects:  34%|███▍      | 50/148 [00:05<00:10,  9.40it/s]predicting train subjects:  34%|███▍      | 51/148 [00:05<00:11,  8.76it/s]predicting train subjects:  18%|█▊        | 26/148 [00:05<00:25,  4.84it/s]predicting train subjects:  35%|███▌      | 52/148 [00:05<00:10,  8.84it/s]predicting train subjects:  36%|███▌      | 53/148 [00:05<00:10,  8.94it/s]predicting train subjects:  18%|█▊        | 27/148 [00:05<00:25,  4.81it/s]predicting train subjects:  36%|███▋      | 54/148 [00:05<00:10,  8.88it/s]predicting train subjects:  37%|███▋      | 55/148 [00:05<00:10,  8.99it/s]predicting train subjects:  19%|█▉        | 28/148 [00:05<00:25,  4.74it/s]predicting train subjects:  38%|███▊      | 56/148 [00:06<00:10,  8.94it/s]predicting train subjects:  20%|█▉        | 29/148 [00:05<00:24,  4.77it/s]predicting train subjects:  39%|███▉      | 58/148 [00:06<00:09,  9.05it/s]predicting train subjects:  20%|██        | 30/148 [00:06<00:24,  4.77it/s]predicting train subjects:  40%|███▉      | 59/148 [00:06<00:09,  8.94it/s]predicting train subjects:  41%|████      | 60/148 [00:06<00:09,  8.87it/s]predicting train subjects:  21%|██        | 31/148 [00:06<00:24,  4.76it/s]predicting train subjects:  41%|████      | 61/148 [00:06<00:09,  8.82it/s]predicting train subjects:  42%|████▏     | 62/148 [00:06<00:09,  8.78it/s]predicting train subjects:  22%|██▏       | 32/148 [00:06<00:24,  4.70it/s]predicting train subjects:  43%|████▎     | 63/148 [00:06<00:09,  8.72it/s]predicting train subjects:  43%|████▎     | 64/148 [00:06<00:10,  8.35it/s]predicting train subjects:  22%|██▏       | 33/148 [00:06<00:24,  4.70it/s]predicting train subjects:  45%|████▍     | 66/148 [00:07<00:09,  9.05it/s]predicting train subjects:  23%|██▎       | 34/148 [00:06<00:24,  4.69it/s]predicting train subjects:  46%|████▌     | 68/148 [00:07<00:08,  9.76it/s]predicting train subjects:  24%|██▎       | 35/148 [00:07<00:23,  4.78it/s]predicting train subjects:  47%|████▋     | 70/148 [00:07<00:07, 10.10it/s]predicting train subjects:  24%|██▍       | 36/148 [00:07<00:23,  4.80it/s]
Epoch 00062: val_mDice did not improve from 0.69145
Epoch 63/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9041 - val_loss: 0.0144 - val_acc: 0.9970 - val_mDice: 0.6872
predicting train subjects:  49%|████▊     | 72/148 [00:07<00:07,  9.92it/s]predicting train subjects:  25%|██▌       | 37/148 [00:07<00:22,  4.86it/s]predicting train subjects:  50%|█████     | 74/148 [00:07<00:07,  9.67it/s]predicting train subjects:  51%|█████     | 75/148 [00:07<00:07,  9.47it/s]predicting train subjects:  26%|██▌       | 38/148 [00:07<00:22,  4.89it/s]predicting train subjects:  52%|█████▏    | 77/148 [00:08<00:07,  9.66it/s]predicting train subjects:  26%|██▋       | 39/148 [00:08<00:22,  4.89it/s]predicting train subjects:  53%|█████▎    | 79/148 [00:08<00:06, 10.06it/s]predicting train subjects:  27%|██▋       | 40/148 [00:08<00:22,  4.90it/s]predicting train subjects:  55%|█████▍    | 81/148 [00:08<00:06,  9.73it/s]predicting train subjects:  28%|██▊       | 41/148 [00:08<00:21,  4.94it/s]predicting train subjects:  56%|█████▌    | 83/148 [00:08<00:06, 10.05it/s]predicting train subjects:  28%|██▊       | 42/148 [00:08<00:21,  5.00it/s]predicting train subjects:  57%|█████▋    | 85/148 [00:08<00:06, 10.16it/s]predicting train subjects:  29%|██▉       | 43/148 [00:08<00:20,  5.09it/s]predicting train subjects:  30%|██▉       | 44/148 [00:08<00:20,  5.18it/s]predicting train subjects:  59%|█████▉    | 87/148 [00:09<00:06, 10.06it/s]predicting train subjects:  30%|███       | 45/148 [00:09<00:19,  5.25it/s]predicting train subjects:  60%|██████    | 89/148 [00:09<00:05, 10.16it/s]predicting train subjects:  31%|███       | 46/148 [00:09<00:19,  5.29it/s]predicting train subjects:  61%|██████▏   | 91/148 [00:09<00:05, 10.42it/s]predicting train subjects:  32%|███▏      | 47/148 [00:09<00:18,  5.36it/s]predicting train subjects:  63%|██████▎   | 93/148 [00:09<00:05, 10.35it/s]predicting train subjects:  32%|███▏      | 48/148 [00:09<00:18,  5.38it/s]predicting train subjects:  64%|██████▍   | 95/148 [00:09<00:05, 10.59it/s]predicting train subjects:  66%|██████▌   | 97/148 [00:10<00:04, 11.68it/s]predicting train subjects:  33%|███▎      | 49/148 [00:09<00:18,  5.42it/s]predicting train subjects:  67%|██████▋   | 99/148 [00:10<00:03, 12.41it/s]predicting train subjects:  34%|███▍      | 50/148 [00:10<00:17,  5.45it/s]predicting train subjects:  68%|██████▊   | 101/148 [00:10<00:03, 12.50it/s]predicting train subjects:  34%|███▍      | 51/148 [00:10<00:17,  5.41it/s]predicting train subjects:  70%|██████▉   | 103/148 [00:10<00:04, 11.19it/s]predicting train subjects:  35%|███▌      | 52/148 [00:10<00:17,  5.42it/s]predicting train subjects:  71%|███████   | 105/148 [00:10<00:04, 10.42it/s]predicting train subjects:  36%|███▌      | 53/148 [00:10<00:18,  5.26it/s]predicting train subjects:  72%|███████▏  | 107/148 [00:11<00:04, 10.10it/s]predicting train subjects:  36%|███▋      | 54/148 [00:10<00:18,  5.17it/s]predicting train subjects:  37%|███▋      | 55/148 [00:11<00:18,  5.13it/s]predicting train subjects:  74%|███████▎  | 109/148 [00:11<00:04,  9.32it/s]predicting train subjects:  38%|███▊      | 56/148 [00:11<00:18,  5.06it/s]predicting train subjects:  75%|███████▌  | 111/148 [00:11<00:03,  9.41it/s]predicting train subjects:  76%|███████▋  | 113/148 [00:11<00:03,  9.96it/s]predicting train subjects:  39%|███▊      | 57/148 [00:11<00:18,  5.04it/s]predicting train subjects:  78%|███████▊  | 115/148 [00:11<00:02, 11.03it/s]predicting train subjects:  39%|███▉      | 58/148 [00:11<00:18,  5.00it/s]predicting train subjects:  79%|███████▉  | 117/148 [00:11<00:02, 11.60it/s]predicting train subjects:  80%|████████  | 119/148 [00:12<00:02, 11.80it/s]predicting train subjects:  40%|███▉      | 59/148 [00:11<00:19,  4.64it/s]predicting train subjects:  82%|████████▏ | 121/148 [00:12<00:02, 10.82it/s]predicting train subjects:  41%|████      | 60/148 [00:12<00:19,  4.45it/s]predicting train subjects:  83%|████████▎ | 123/148 [00:12<00:02, 10.06it/s]predicting train subjects:  41%|████      | 61/148 [00:12<00:20,  4.32it/s]predicting train subjects:  84%|████████▍ | 125/148 [00:12<00:02,  9.24it/s]predicting train subjects:  42%|████▏     | 62/148 [00:12<00:20,  4.24it/s]predicting train subjects:  85%|████████▌ | 126/148 [00:12<00:02,  9.09it/s]predicting train subjects:  86%|████████▌ | 127/148 [00:13<00:02,  8.77it/s]predicting train subjects:  43%|████▎     | 63/148 [00:12<00:20,  4.16it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:13<00:02,  8.76it/s]predicting train subjects:  87%|████████▋ | 129/148 [00:13<00:02,  8.83it/s]predicting train subjects:  43%|████▎     | 64/148 [00:13<00:20,  4.15it/s]predicting train subjects:  88%|████████▊ | 130/148 [00:13<00:02,  8.80it/s]predicting train subjects:  89%|████████▊ | 131/148 [00:13<00:01,  9.02it/s]predicting train subjects:  44%|████▍     | 65/148 [00:13<00:17,  4.64it/s]predicting train subjects:  45%|████▍     | 66/148 [00:13<00:16,  4.91it/s]predicting train subjects:  90%|████████▉ | 133/148 [00:13<00:01,  9.38it/s]predicting train subjects:  45%|████▌     | 67/148 [00:13<00:15,  5.29it/s]predicting train subjects:  91%|█████████ | 135/148 [00:13<00:01,  9.44it/s]predicting train subjects:  46%|████▌     | 68/148 [00:13<00:14,  5.61it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:14<00:01,  9.08it/s]predicting train subjects:  47%|████▋     | 69/148 [00:13<00:13,  5.88it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:14<00:01,  9.79it/s]predicting train subjects:  47%|████▋     | 70/148 [00:14<00:12,  6.06it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:14<00:00, 10.23it/s]predicting train subjects:  48%|████▊     | 71/148 [00:14<00:13,  5.84it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:14<00:00, 10.77it/s]predicting train subjects:  49%|████▊     | 72/148 [00:14<00:13,  5.71it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:14<00:00,  9.94it/s]predicting train subjects:  49%|████▉     | 73/148 [00:14<00:13,  5.62it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:14<00:00,  9.89it/s]predicting train subjects:  50%|█████     | 74/148 [00:14<00:13,  5.56it/s]predicting train subjects: 100%|██████████| 148/148 [00:15<00:00,  9.79it/s]

Epoch 00056: val_mDice did not improve from 0.65668
Restoring model weights from the end of the best epoch
Epoch 00056: early stopping
{'val_loss': [0.0036745008497320593, 0.0040474107806035813, 0.0041778818307880395, 0.0042432033199262115, 0.0052189873036076412, 0.0049307288483102273, 0.0041339054029989747, 0.0039790710731548197, 0.0045601326694830937, 0.0046843238610853538, 0.0046428642731080666, 0.0049466702373737988, 0.0044876693926276046, 0.0053276298963960177, 0.004704462730900404, 0.0051677803449491234, 0.005138642798633652, 0.0054096216038662069, 0.0051445525456616219, 0.0047436190788891722, 0.00500944020979582, 0.0053395525591963151, 0.0050062103632916794, 0.0053117093828288801, 0.0051561413095035456, 0.0056104227068259358, 0.005546814484640639, 0.0050201718794538617, 0.0056756661828369533, 0.0053795547363288858, 0.0052217717104135674, 0.005584313445031009, 0.0057657377279185231, 0.0054500867275798573, 0.0058661999696112696, 0.0057495947273329216, 0.0059864968358677752, 0.0057804158769864987, 0.0057936170970664383, 0.0056297425180673599, 0.0059219815253101762, 0.0057118606476232094, 0.005920170479394654, 0.0058829723262881981, 0.0061098946198979587, 0.0058992044960564753, 0.006100481296790407, 0.0061297068134584329, 0.0060344568077237044, 0.0059244611617574033, 0.0059693324518330553, 0.0061126847137162026, 0.0060220122178818317, 0.0059919125261775992, 0.0060654658170298057, 0.0060210603566721397], 'val_acc': [0.99850801204113249, 0.9985352095137251, 0.99846015712048142, 0.99833574320407625, 0.99810153119107514, 0.99829443211251112, 0.99866820015805835, 0.99868027834182094, 0.99855637677172393, 0.99847274511418449, 0.99845463037490845, 0.99857803354872032, 0.99873416728161746, 0.99856846256459009, 0.99867173839122692, 0.99855738624613333, 0.99863848534036193, 0.9985387414059741, 0.99866517046664627, 0.99873315780720817, 0.99870145701347512, 0.9986797571182251, 0.99868230870429509, 0.99860824422633399, 0.99870900270786689, 0.9986651869530373, 0.99861730793689163, 0.99859665302520106, 0.99859917290667266, 0.99869288155373104, 0.99870296742053744, 0.99868229348608784, 0.99863242468935376, 0.99876493342379302, 0.9987296728377647, 0.99862032621464836, 0.99868582791470462, 0.99865006385965549, 0.99868936614787329, 0.99868331691052048, 0.9987034797668457, 0.99873166642290478, 0.99869537607152414, 0.99871251304098896, 0.99867828095212896, 0.99871756929032351, 0.99871806134568886, 0.99867778382402783, 0.99869993138820568, 0.99870297249327311, 0.99870648663094708, 0.99870800845166474, 0.99871906321099468, 0.99872008790361122, 0.99870094466716686, 0.99872411058304156], 'val_mDice': [0.45855192998622324, 0.50677134762419029, 0.53868647839160677, 0.5771563737950427, 0.59082196367547868, 0.58890238974956755, 0.62530388477000787, 0.61068226175105322, 0.62212579427881443, 0.62517542661504544, 0.62330544502177132, 0.61364479648306014, 0.64231414871012915, 0.62248209689525846, 0.63832962132514792, 0.61515892566518582, 0.62311325808788864, 0.62840184886404804, 0.62647223345776826, 0.63055422331424471, 0.64226566603843205, 0.628950168477728, 0.6426995827796611, 0.62301226879688021, 0.64908433214147043, 0.61928582445104075, 0.64102591225441463, 0.63657799299727091, 0.62124562517125559, 0.63073216220165818, 0.64772343889195871, 0.63474023468950957, 0.64522466380545429, 0.65059602514226389, 0.62706297509213715, 0.65668384952748071, 0.63408041380821389, 0.65054081095025895, 0.64189257520310428, 0.64998445739137367, 0.63935593214440856, 0.65063196547487945, 0.6471492863715963, 0.64262059013894268, 0.63347306657344737, 0.6480131719974761, 0.63757381540663693, 0.64403640462997114, 0.64347669926095519, 0.65119698960730366, 0.64732259131492453, 0.64397319580646273, 0.65123392800067337, 0.64989337768960509, 0.65217142916740256, 0.64726158532690492], 'loss': [0.0043006412675070365, 0.0028324208627769068, 0.0022313390740143621, 0.0019535685023367769, 0.0017580171307348359, 0.0016074330736474453, 0.0015355786261038399, 0.0014665996394507856, 0.0013938941993226212, 0.0013342529622533863, 0.0012829953243735264, 0.0012536404232993559, 0.0011901040304740595, 0.001175716076033533, 0.0011389431278672158, 0.0011045158206677596, 0.0010951803036457939, 0.0011012724128357117, 0.0010754542735091786, 0.0011182345020803188, 0.0010339144544645998, 0.001043180334697517, 0.00097970325887634219, 0.00098568365373022078, 0.00099382830092964035, 0.00096112256991745016, 0.00092991758662034111, 0.00096674445839628074, 0.00095090724465009822, 0.00092540407792249366, 0.00090344178318464328, 0.00085666900203940554, 0.00082557968255256135, 0.00080597559687554867, 0.00080497075263371741, 0.00080817810719805436, 0.00079387487780237004, 0.0007894225198432428, 0.00081082364929285279, 0.00078915624724025914, 0.00079330012257154269, 0.00076225556883481138, 0.00074844289765465708, 0.00074410774962008468, 0.00074618238552412615, 0.00073409885825958447, 0.00072601523215917306, 0.00071936290141417071, 0.00071715077228078539, 0.00071287937787039505, 0.00072196631548549926, 0.00071185705011023019, 0.00070409541482300186, 0.00070177819647792726, 0.00071029657012483831, 0.00071183851016659323], 'acc': [0.99834500039136309, 0.99880754943495653, 0.99906795596126408, 0.99917988252573098, 0.99925985158385622, 0.999317324586533, 0.99934655452932941, 0.99937219048496395, 0.99940337154757763, 0.99942940331888719, 0.9994474553386884, 0.99946437126604337, 0.99948717511562735, 0.99949173244384981, 0.99950557707573562, 0.9995230071947151, 0.99952741095920572, 0.99952276065107082, 0.99953267295974735, 0.99951777606942116, 0.99955143883944142, 0.99954680563561027, 0.9995746061294406, 0.99957032134507096, 0.99956620552038988, 0.99958384437021031, 0.99959552581807776, 0.99958263761053046, 0.99958523258967902, 0.99959197081039519, 0.99960519256228464, 0.99962508872187839, 0.999636289353389, 0.99964697858496421, 0.99964696373067841, 0.99964370849623374, 0.99965179347775279, 0.99965331828165038, 0.99964314007500754, 0.99965273729782345, 0.99965158070521265, 0.99966511727212737, 0.99967216628700317, 0.99967162592850212, 0.99967165724125329, 0.99967862423463782, 0.99968251176582101, 0.99968441638526528, 0.99968452949033926, 0.99968751103508846, 0.9996861819202606, 0.99968582237570991, 0.99969068710223152, 0.99969136792019253, 0.99968981155354175, 0.99968689130095079], 'mDice': [0.42056505044037462, 0.57374786586871473, 0.67360110434565834, 0.71719087286872152, 0.74446515883236075, 0.76840967781599567, 0.77892294239639526, 0.79067126017523492, 0.79806093692946212, 0.80805669810879444, 0.81685903998614606, 0.81827494171856585, 0.8286147928662918, 0.83229052866309394, 0.83634676072031666, 0.84132490229748291, 0.84345104570615459, 0.84401984674786401, 0.84642232681062413, 0.8404947361801105, 0.85121128768280896, 0.85034977348064966, 0.86172524197707434, 0.85996756133646535, 0.85897923138790799, 0.86490461866741453, 0.8679389328608268, 0.8647703460070354, 0.86411123686593461, 0.86739340302661683, 0.87129001201178302, 0.87812141197538596, 0.88374156709819018, 0.88750399634038935, 0.88622411169900794, 0.88678229013467658, 0.88875271383593524, 0.88973400612387443, 0.88701935128622311, 0.89050085971256265, 0.88864708217612853, 0.89399270380514151, 0.89580917910486202, 0.89763567881832507, 0.89687204775548568, 0.89816309902810498, 0.89955419639489398, 0.90048850522262991, 0.90052669523729811, 0.90144484314373519, 0.90158447782629147, 0.90109841655243195, 0.90157216087332637, 0.90327794095179037, 0.90181776463506269, 0.90181186083770637], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05]}
predicting train subjects:  51%|█████     | 75/148 [00:15<00:13,  5.53it/s]predicting train subjects:  51%|█████▏    | 76/148 [00:15<00:13,  5.50it/s]predicting train subjects:  52%|█████▏    | 77/148 [00:15<00:12,  5.57it/s]predicting train subjects:  53%|█████▎    | 78/148 [00:15<00:12,  5.66it/s]
Epoch 00063: val_mDice did not improve from 0.69145
Epoch 64/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9035 - val_loss: 0.0143 - val_acc: 0.9970 - val_mDice: 0.6882
predicting train subjects:  53%|█████▎    | 79/148 [00:15<00:12,  5.72it/s]predicting train subjects:  54%|█████▍    | 80/148 [00:15<00:11,  5.73it/s]predicting train subjects:  55%|█████▍    | 81/148 [00:16<00:11,  5.78it/s]predicting train subjects:  55%|█████▌    | 82/148 [00:16<00:11,  5.82it/s]predicting train subjects:  56%|█████▌    | 83/148 [00:16<00:11,  5.77it/s]predicting train subjects:  57%|█████▋    | 84/148 [00:16<00:11,  5.74it/s]predicting train subjects:  57%|█████▋    | 85/148 [00:16<00:10,  5.73it/s]predicting train subjects:  58%|█████▊    | 86/148 [00:16<00:10,  5.72it/s]predicting train subjects:  59%|█████▉    | 87/148 [00:17<00:10,  5.70it/s]predicting train subjects:  59%|█████▉    | 88/148 [00:17<00:10,  5.69it/s]predicting train subjects:  60%|██████    | 89/148 [00:17<00:10,  5.61it/s]predicting train subjects:  61%|██████    | 90/148 [00:17<00:10,  5.64it/s]predicting train subjects:  61%|██████▏   | 91/148 [00:17<00:10,  5.64it/s]predicting train subjects:  62%|██████▏   | 92/148 [00:17<00:09,  5.63it/s]predicting train subjects:  63%|██████▎   | 93/148 [00:18<00:09,  5.64it/s]predicting train subjects:  64%|██████▎   | 94/148 [00:18<00:09,  5.68it/s]predicting train subjects:  64%|██████▍   | 95/148 [00:18<00:08,  6.27it/s]predicting train subjects:  65%|██████▍   | 96/148 [00:18<00:07,  6.85it/s]predicting train subjects:  66%|██████▌   | 97/148 [00:18<00:06,  7.29it/s]predicting train subjects:  66%|██████▌   | 98/148 [00:18<00:06,  7.56it/s]predicting train subjects:  67%|██████▋   | 99/148 [00:18<00:06,  7.82it/s]predicting train subjects:  68%|██████▊   | 100/148 [00:19<00:05,  8.06it/s]predicting train subjects:  68%|██████▊   | 101/148 [00:19<00:06,  6.83it/s]predicting train subjects:  69%|██████▉   | 102/148 [00:19<00:07,  6.22it/s]predicting train subjects:  70%|██████▉   | 103/148 [00:19<00:07,  5.85it/s]predicting train subjects:  70%|███████   | 104/148 [00:19<00:07,  5.64it/s]predicting train subjects:  71%|███████   | 105/148 [00:20<00:07,  5.50it/s]predicting train subjects:  72%|███████▏  | 106/148 [00:20<00:07,  5.42it/s]predicting train subjects:  72%|███████▏  | 107/148 [00:20<00:07,  5.40it/s]predicting train subjects:  73%|███████▎  | 108/148 [00:20<00:07,  5.39it/s]predicting train subjects:  74%|███████▎  | 109/148 [00:20<00:07,  5.42it/s]predicting train subjects:  74%|███████▍  | 110/148 [00:20<00:07,  5.41it/s]predicting train subjects:  75%|███████▌  | 111/148 [00:21<00:06,  5.41it/s]predicting train subjects:  76%|███████▌  | 112/148 [00:21<00:06,  5.44it/s]predicting train subjects:  76%|███████▋  | 113/148 [00:21<00:05,  5.97it/s]predicting train subjects:  77%|███████▋  | 114/148 [00:21<00:05,  6.39it/s]predicting train subjects:  78%|███████▊  | 115/148 [00:21<00:04,  6.86it/s]predicting train subjects:  78%|███████▊  | 116/148 [00:21<00:04,  7.30it/s]predicting train subjects:  79%|███████▉  | 117/148 [00:21<00:04,  7.63it/s]predicting train subjects:  80%|███████▉  | 118/148 [00:22<00:03,  7.91it/s]predicting train subjects:  80%|████████  | 119/148 [00:22<00:04,  6.63it/s]predicting train subjects:  81%|████████  | 120/148 [00:22<00:04,  5.65it/s]predicting train subjects:  82%|████████▏ | 121/148 [00:22<00:05,  5.36it/s]predicting train subjects:  82%|████████▏ | 122/148 [00:22<00:05,  5.11it/s]predicting train subjects:  83%|████████▎ | 123/148 [00:23<00:05,  4.95it/s]predicting train subjects:  84%|████████▍ | 124/148 [00:23<00:04,  4.88it/s]predicting train subjects:  84%|████████▍ | 125/148 [00:23<00:04,  4.71it/s]
Epoch 00064: val_mDice did not improve from 0.69145
Epoch 65/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9034 - val_loss: 0.0144 - val_acc: 0.9970 - val_mDice: 0.6874
predicting train subjects:  85%|████████▌ | 126/148 [00:23<00:04,  4.63it/s]predicting train subjects:  86%|████████▌ | 127/148 [00:24<00:04,  4.56it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:24<00:04,  4.53it/s]predicting train subjects:  87%|████████▋ | 129/148 [00:24<00:04,  4.47it/s]predicting train subjects:  88%|████████▊ | 130/148 [00:24<00:04,  4.36it/s]predicting train subjects:  89%|████████▊ | 131/148 [00:24<00:03,  4.55it/s]predicting train subjects:  89%|████████▉ | 132/148 [00:25<00:03,  4.74it/s]predicting train subjects:  90%|████████▉ | 133/148 [00:25<00:03,  4.87it/s]predicting train subjects:  91%|█████████ | 134/148 [00:25<00:02,  4.95it/s]predicting train subjects:  91%|█████████ | 135/148 [00:25<00:02,  5.03it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:25<00:02,  5.12it/s]predicting train subjects:  93%|█████████▎| 137/148 [00:26<00:02,  5.49it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:26<00:01,  5.76it/s]predicting train subjects:  94%|█████████▍| 139/148 [00:26<00:01,  6.01it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:26<00:01,  6.19it/s]predicting train subjects:  95%|█████████▌| 141/148 [00:26<00:01,  6.31it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:26<00:00,  6.42it/s]predicting train subjects:  97%|█████████▋| 143/148 [00:26<00:00,  5.97it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:27<00:00,  5.67it/s]predicting train subjects:  98%|█████████▊| 145/148 [00:27<00:00,  5.50it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:27<00:00,  5.36it/s]predicting train subjects:  99%|█████████▉| 147/148 [00:27<00:00,  5.27it/s]predicting train subjects: 100%|██████████| 148/148 [00:27<00:00,  5.21it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  5.58it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  5.39it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  5.98it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  6.56it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  5.62it/s]
predicting train subjects sagittal:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects sagittal:   1%|          | 1/148 [00:00<00:31,  4.72it/s]predicting train subjects sagittal:   1%|▏         | 2/148 [00:00<00:30,  4.78it/s]predicting train subjects sagittal:   2%|▏         | 3/148 [00:00<00:29,  4.85it/s]predicting train subjects sagittal:   3%|▎         | 4/148 [00:00<00:28,  4.98it/s]predicting train subjects sagittal:   3%|▎         | 5/148 [00:00<00:27,  5.14it/s]predicting train subjects sagittal:   4%|▍         | 6/148 [00:01<00:27,  5.09it/s]predicting train subjects sagittal:   5%|▍         | 7/148 [00:01<00:29,  4.77it/s]predicting train subjects sagittal:   5%|▌         | 8/148 [00:01<00:26,  5.20it/s]predicting train subjects sagittal:   6%|▌         | 9/148 [00:01<00:26,  5.24it/s]predicting train subjects sagittal:   7%|▋         | 10/148 [00:01<00:25,  5.38it/s]predicting train subjects sagittal:   7%|▋         | 11/148 [00:02<00:25,  5.45it/s]predicting train subjects sagittal:   8%|▊         | 12/148 [00:02<00:24,  5.56it/s]predicting train subjects sagittal:   9%|▉         | 13/148 [00:02<00:21,  6.22it/s]predicting train subjects sagittal:   9%|▉         | 14/148 [00:02<00:22,  5.89it/s]predicting train subjects sagittal:  10%|█         | 15/148 [00:02<00:23,  5.72it/s]predicting train subjects sagittal:  11%|█         | 16/148 [00:02<00:20,  6.32it/s]predicting train subjects sagittal:  11%|█▏        | 17/148 [00:03<00:22,  5.80it/s]
Epoch 00065: val_mDice did not improve from 0.69145
Epoch 66/100
 - 7s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9044 - val_loss: 0.0145 - val_acc: 0.9969 - val_mDice: 0.6853
predicting train subjects sagittal:  12%|█▏        | 18/148 [00:03<00:25,  5.15it/s]predicting train subjects sagittal:  13%|█▎        | 19/148 [00:03<00:24,  5.18it/s]predicting train subjects sagittal:  14%|█▎        | 20/148 [00:03<00:23,  5.55it/s]predicting train subjects sagittal:  14%|█▍        | 21/148 [00:03<00:23,  5.38it/s]predicting train subjects sagittal:  15%|█▍        | 22/148 [00:04<00:22,  5.56it/s]predicting train subjects sagittal:  16%|█▌        | 23/148 [00:04<00:23,  5.23it/s]predicting train subjects sagittal:  16%|█▌        | 24/148 [00:04<00:24,  5.08it/s]predicting train subjects sagittal:  17%|█▋        | 25/148 [00:04<00:24,  4.99it/s]predicting train subjects sagittal:  18%|█▊        | 26/148 [00:04<00:24,  4.92it/s]predicting train subjects sagittal:  18%|█▊        | 27/148 [00:05<00:25,  4.71it/s]predicting train subjects sagittal:  19%|█▉        | 28/148 [00:05<00:25,  4.72it/s]predicting train subjects sagittal:  20%|█▉        | 29/148 [00:05<00:26,  4.49it/s]predicting train subjects sagittal:  20%|██        | 30/148 [00:05<00:25,  4.59it/s]predicting train subjects sagittal:  21%|██        | 31/148 [00:06<00:25,  4.67it/s]predicting train subjects sagittal:  22%|██▏       | 32/148 [00:06<00:24,  4.69it/s]predicting train subjects sagittal:  22%|██▏       | 33/148 [00:06<00:24,  4.73it/s]predicting train subjects sagittal:  23%|██▎       | 34/148 [00:06<00:24,  4.63it/s]predicting train subjects sagittal:  24%|██▎       | 35/148 [00:06<00:23,  4.74it/s]predicting train subjects sagittal:  24%|██▍       | 36/148 [00:07<00:23,  4.76it/s]predicting train subjects sagittal:  25%|██▌       | 37/148 [00:07<00:23,  4.81it/s]predicting train subjects sagittal:  26%|██▌       | 38/148 [00:07<00:22,  4.84it/s]predicting train subjects sagittal:  26%|██▋       | 39/148 [00:07<00:22,  4.84it/s]predicting train subjects sagittal:  27%|██▋       | 40/148 [00:07<00:22,  4.88it/s]predicting train subjects sagittal:  28%|██▊       | 41/148 [00:08<00:21,  4.95it/s]predicting train subjects sagittal:  28%|██▊       | 42/148 [00:08<00:21,  5.04it/s]predicting train subjects sagittal:  29%|██▉       | 43/148 [00:08<00:20,  5.14it/s]predicting train subjects sagittal:  30%|██▉       | 44/148 [00:08<00:19,  5.20it/s]predicting train subjects sagittal:  30%|███       | 45/148 [00:08<00:19,  5.20it/s]predicting train subjects sagittal:  31%|███       | 46/148 [00:09<00:19,  5.22it/s]predicting train subjects sagittal:  32%|███▏      | 47/148 [00:09<00:19,  5.29it/s]predicting train subjects sagittal:  32%|███▏      | 48/148 [00:09<00:18,  5.30it/s]predicting train subjects sagittal:  33%|███▎      | 49/148 [00:09<00:18,  5.31it/s]predicting train subjects sagittal:  34%|███▍      | 50/148 [00:09<00:18,  5.36it/s]predicting train subjects sagittal:  34%|███▍      | 51/148 [00:09<00:18,  5.38it/s]predicting train subjects sagittal:  35%|███▌      | 52/148 [00:10<00:17,  5.42it/s]predicting train subjects sagittal:  36%|███▌      | 53/148 [00:10<00:18,  5.26it/s]predicting train subjects sagittal:  36%|███▋      | 54/148 [00:10<00:18,  5.15it/s]predicting train subjects sagittal:  37%|███▋      | 55/148 [00:10<00:18,  5.08it/s]predicting train subjects sagittal:  38%|███▊      | 56/148 [00:10<00:18,  5.02it/s]
Epoch 00066: val_mDice did not improve from 0.69145

Epoch 00066: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 67/100
 - 6s - loss: 0.0018 - acc: 0.9992 - mDice: 0.9045 - val_loss: 0.0144 - val_acc: 0.9970 - val_mDice: 0.6894
predicting train subjects sagittal:  39%|███▊      | 57/148 [00:11<00:18,  4.92it/s]predicting train subjects sagittal:  39%|███▉      | 58/148 [00:11<00:18,  4.76it/s]predicting train subjects sagittal:  40%|███▉      | 59/148 [00:11<00:19,  4.53it/s]predicting train subjects sagittal:  41%|████      | 60/148 [00:11<00:20,  4.25it/s]predicting train subjects sagittal:  41%|████      | 61/148 [00:12<00:20,  4.20it/s]predicting train subjects sagittal:  42%|████▏     | 62/148 [00:12<00:20,  4.18it/s]predicting train subjects sagittal:  43%|████▎     | 63/148 [00:12<00:20,  4.16it/s]
Epoch 00067: val_mDice did not improve from 0.69145
Restoring model weights from the end of the best epoch
Epoch 00067: early stopping
{'val_loss': [0.0077809133824515847, 0.0073089467837138379, 0.0075780070446273117, 0.0088032243297772213, 0.0086121714693751742, 0.0093786740239630347, 0.0091001856675807461, 0.0093880856726714906, 0.0097502278599967355, 0.010434578311570148, 0.011041003536987812, 0.010802886904554164, 0.01072351715745444, 0.011266738731176295, 0.011017460811962473, 0.011522930254168968, 0.011624801785070846, 0.010984439620787793, 0.010424196502154178, 0.01182416919618845, 0.011298538760301914, 0.011703580380120177, 0.011538065791605635, 0.011188333556848638, 0.0119851655822168, 0.011933664771470618, 0.011752934115839766, 0.013221727466171092, 0.01117480272783878, 0.012209871050683743, 0.011917260891579568, 0.012676694351149367, 0.012967118953770779, 0.012984284417743379, 0.012879863282308933, 0.013735739613308552, 0.012981438870601197, 0.013265622423050251, 0.013603117455668907, 0.013575859229139824, 0.013432322169079426, 0.01357405351355989, 0.01381354053762365, 0.013859362757586419, 0.013867255538067919, 0.013904014166365278, 0.013954062827248523, 0.013695000353804294, 0.013999481348598257, 0.014048414403929356, 0.014106088516084438, 0.014110671277058886, 0.014147241480965564, 0.014131032921215321, 0.014231504337426195, 0.014119985889881215, 0.014151760198651477, 0.014119509151799882, 0.014278318178146444, 0.014661284163594246, 0.014430375016750173, 0.014542005005034995, 0.014359246364774857, 0.014337173128064642, 0.014355680370267401, 0.014481438403116895, 0.014393145614799032], 'val_acc': [0.99662013637258651, 0.9968931243774739, 0.99711425507322271, 0.99690925694526511, 0.99701199633009885, 0.99691832446037454, 0.99684679381390839, 0.99669871685352729, 0.99688509170045247, 0.99675660944999533, 0.99677174776158439, 0.99679442162209364, 0.9970205565716358, 0.99672792186128334, 0.99645691475969678, 0.99685133137601489, 0.99676720259037421, 0.99664832556501348, 0.99705683550936108, 0.99701604817775968, 0.9969127609374675, 0.99680650234222412, 0.99691376533914122, 0.99702157492333265, 0.99692385247413151, 0.99706843939233336, 0.99701249345819998, 0.99671178929349213, 0.99693091118589361, 0.99702309293949853, 0.99698330747320296, 0.99698580706373174, 0.99695053760041585, 0.996958112463038, 0.99698682795179649, 0.99682361141164255, 0.9969107724250631, 0.99694097802994097, 0.99695206956660498, 0.99692184874351986, 0.99698126442888946, 0.99697827151481144, 0.99692888209160335, 0.99689313325476137, 0.99696869799431331, 0.9969712229485207, 0.9969319231966709, 0.99697825122386852, 0.99695155595211271, 0.99697171500388615, 0.9969792619664618, 0.99693544367526443, 0.99697070045674097, 0.99696919892696623, 0.99694149418080102, 0.99696113074079473, 0.99696718631906711, 0.99699487584702517, 0.99696264241604093, 0.99690118749090961, 0.99695306635917502, 0.99691882032029178, 0.99695005949507365, 0.99695405681082538, 0.99696112947261084, 0.99692638757381036, 0.99696565815742977], 'val_mDice': [0.54210385109515902, 0.57351519833219811, 0.60594189801114673, 0.60472876974876888, 0.62952480290798429, 0.64305854350962532, 0.6541540521256467, 0.65811886432323052, 0.65905183553695679, 0.66261155300952024, 0.66052599409793289, 0.6604460883647838, 0.6599479343028779, 0.66842102243545209, 0.66837524480008059, 0.66564178466796875, 0.67287147679227466, 0.67894682351579061, 0.67766848269929281, 0.66664103497850136, 0.67314907718212047, 0.67841272404853337, 0.67824681134934117, 0.68220205509916265, 0.68469215707576025, 0.68034095079340828, 0.68247923445194325, 0.67559605963686675, 0.68293357909993924, 0.68286063062383773, 0.68619593914518962, 0.67910093576350106, 0.67937132906406483, 0.68197014737636485, 0.68644929819918699, 0.68252524036042239, 0.6891883383405969, 0.68271741207609782, 0.68344317979000979, 0.67949985823732739, 0.68738504927209088, 0.69050050796346463, 0.68535629358697447, 0.68720650419275808, 0.68950936008006969, 0.68693901122884549, 0.69144501711459871, 0.69014390605561282, 0.68988354408994634, 0.68695634857137156, 0.68829067844025638, 0.68795397687465587, 0.68815508294612804, 0.68976821163867386, 0.68699506876316474, 0.68856991732374151, 0.68648747814462541, 0.68827185224979481, 0.6863621739631004, 0.68586841669488463, 0.68510016608745494, 0.68570694010308453, 0.68721847711725437, 0.68815425609020475, 0.68738607903744309, 0.68534086232489733, 0.68935346983848733], 'loss': [0.0087849725332737431, 0.0064468732244006011, 0.0054809636431845466, 0.0048195812962682346, 0.0044174691452555249, 0.0041107153463021458, 0.0038654533554550829, 0.0035991530399688164, 0.0034912901877450675, 0.0033504105089554592, 0.0032640688350161801, 0.0031493251306017518, 0.0031426076545181163, 0.0030014343548023128, 0.0029234810788783369, 0.0028780610749009074, 0.0028073067611004612, 0.0027688894837857732, 0.0027961492362995012, 0.0026113727329053408, 0.0026194679118531415, 0.0025402963305838151, 0.0025003047479978707, 0.00250254037506144, 0.0025698685660022085, 0.0024061506577955911, 0.0023763239857659646, 0.0023420990575905741, 0.0023532950828224291, 0.0023228298701483941, 0.0023279463871825213, 0.0021402010560193259, 0.002113109456872641, 0.002103613041994305, 0.0020923696960296841, 0.0020471166535965873, 0.002064445515172032, 0.0020450385809380905, 0.0020241919701169606, 0.0020280789306575458, 0.001960457113175303, 0.0019485300756914086, 0.001922597240152704, 0.0019481470215120325, 0.001911521964221125, 0.0019296575695724181, 0.0019049768639301666, 0.0018859698565085008, 0.0018634796763921, 0.0018700083966892015, 0.0018576038122432334, 0.0018514433904326033, 0.0018567712790352592, 0.0018431916278720225, 0.0018427906958636954, 0.0018414637915027054, 0.0018423225449323941, 0.0018360586409684569, 0.0018371126857188742, 0.0018288162653700654, 0.001836994702716829, 0.0018249688797725748, 0.0018258836649017124, 0.0018352286225104719, 0.0018381496281185659, 0.0018183785242968321, 0.0018182441395925854], 'acc': [0.99633940447047642, 0.99719801539934749, 0.99762897021034636, 0.99792191147679254, 0.99809616199951412, 0.99822106514723719, 0.99833778268763418, 0.9984494640001339, 0.99849535605574935, 0.99855344450086281, 0.99859502131353095, 0.99864177339687832, 0.99864430641719659, 0.99870540580312839, 0.99873368580080502, 0.99875818374731118, 0.99878979085500907, 0.99879811692179521, 0.99879068527885018, 0.99886666220078635, 0.99886229429074991, 0.9988986376656056, 0.99891206371080032, 0.99891460968955581, 0.99888403925684022, 0.99895414788170489, 0.99896991417483827, 0.998985676363772, 0.99897900289357522, 0.99899083715516845, 0.99899135174262943, 0.99906885461430583, 0.99908232818071507, 0.99908900854680061, 0.99909142321200273, 0.999109826588789, 0.99910584234849675, 0.99911297513487785, 0.999118401053401, 0.99911983012736405, 0.99915205115665306, 0.99915529301599038, 0.99916654508541369, 0.99915800630858798, 0.99916932229519062, 0.99916233907250918, 0.99917248421638671, 0.9991831582603401, 0.99919123661680598, 0.99918676937263939, 0.99919493977151019, 0.99919811863034225, 0.99919368307392986, 0.9991961975107656, 0.99919834736134361, 0.99919868261403277, 0.99920392953112003, 0.99920131065594653, 0.99920106663315578, 0.99920447378548571, 0.99920201570326861, 0.99920774924925926, 0.99920760401892772, 0.99920137951066557, 0.99919888015728409, 0.99921051916731107, 0.9992086935068335], 'mDice': [0.53092783416580047, 0.63530841479139788, 0.69525188499295354, 0.73644793360685812, 0.75840981758295012, 0.77556731966732684, 0.79016759584350882, 0.80427989743881734, 0.81128969860426903, 0.81951949501404264, 0.82326978030466447, 0.83050869137038308, 0.83019148922183217, 0.8386845898269899, 0.8434840977004423, 0.84548703730835528, 0.84989417663865552, 0.85112769761647178, 0.85016778660420567, 0.86007653681476059, 0.85990880686517612, 0.86387339270810737, 0.86708120210568251, 0.86677710943728714, 0.86238889423795029, 0.87203241236265705, 0.87370432852365387, 0.87598353503509574, 0.87513141374161341, 0.87600658357414241, 0.87626648915893168, 0.88615859696515908, 0.88714759077320815, 0.88876053198198057, 0.88829668376129933, 0.89208558800752991, 0.89088385986806795, 0.8916435795036991, 0.89320889370543932, 0.8935446868156145, 0.89576078884170751, 0.89699123614903553, 0.89871154898990502, 0.89773973271547602, 0.89890139611689868, 0.89862831753061101, 0.89957447268253687, 0.90042939789434362, 0.90212756516174264, 0.90182852128384539, 0.90246182378044248, 0.90226396763087235, 0.90258024668868542, 0.9028099098642236, 0.90299263410074904, 0.90326978526137081, 0.90367256040883126, 0.9034109658313273, 0.90346696488468103, 0.90367302372504765, 0.90324796131548035, 0.90447553620393439, 0.90413818837295501, 0.90349661506001488, 0.90340200817614158, 0.90438742685134654, 0.90452783429010142], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00050000002, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00025000001, 0.00012500001, 0.00012500001, 0.00012500001, 0.00012500001, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05, 9.0000001e-05]}predicting train subjects sagittal:  43%|████▎     | 64/148 [00:12<00:20,  4.09it/s]predicting train subjects sagittal:  44%|████▍     | 65/148 [00:13<00:18,  4.60it/s]predicting train subjects sagittal:  45%|████▍     | 66/148 [00:13<00:16,  5.04it/s]predicting train subjects sagittal:  45%|████▌     | 67/148 [00:13<00:15,  5.37it/s]predicting train subjects sagittal:  46%|████▌     | 68/148 [00:13<00:14,  5.64it/s]predicting train subjects sagittal:  47%|████▋     | 69/148 [00:13<00:13,  5.89it/s]predicting train subjects sagittal:  47%|████▋     | 70/148 [00:13<00:12,  6.08it/s]predicting train subjects sagittal:  48%|████▊     | 71/148 [00:13<00:13,  5.84it/s]
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting train subjects sagittal:  49%|████▊     | 72/148 [00:14<00:13,  5.69it/s]predicting train subjects sagittal:  49%|████▉     | 73/148 [00:14<00:13,  5.60it/s]predicting train subjects sagittal:  50%|█████     | 74/148 [00:14<00:13,  5.54it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:01,  2.01it/s]predicting train subjects sagittal:  51%|█████     | 75/148 [00:14<00:13,  5.48it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.60it/s]predicting train subjects sagittal:  51%|█████▏    | 76/148 [00:14<00:13,  5.44it/s]predicting test subjects:  80%|████████  | 4/5 [00:00<00:00,  3.28it/s]predicting test subjects: 100%|██████████| 5/5 [00:00<00:00,  4.01it/s]
predicting train subjects:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects sagittal:  52%|█████▏    | 77/148 [00:15<00:12,  5.54it/s]predicting train subjects:   1%|          | 1/148 [00:00<00:16,  8.66it/s]predicting train subjects sagittal:  53%|█████▎    | 78/148 [00:15<00:12,  5.62it/s]predicting train subjects sagittal:  53%|█████▎    | 79/148 [00:15<00:12,  5.69it/s]predicting train subjects:   2%|▏         | 3/148 [00:00<00:17,  8.36it/s]predicting train subjects:   3%|▎         | 4/148 [00:00<00:16,  8.59it/s]predicting train subjects sagittal:  54%|█████▍    | 80/148 [00:15<00:11,  5.74it/s]predicting train subjects:   3%|▎         | 5/148 [00:00<00:16,  8.90it/s]predicting train subjects sagittal:  55%|█████▍    | 81/148 [00:15<00:11,  5.74it/s]predicting train subjects:   4%|▍         | 6/148 [00:00<00:16,  8.81it/s]predicting train subjects:   5%|▍         | 7/148 [00:00<00:16,  8.76it/s]predicting train subjects sagittal:  55%|█████▌    | 82/148 [00:15<00:11,  5.74it/s]predicting train subjects:   5%|▌         | 8/148 [00:00<00:15,  9.06it/s]predicting train subjects:   6%|▌         | 9/148 [00:01<00:15,  9.15it/s]predicting train subjects sagittal:  56%|█████▌    | 83/148 [00:16<00:11,  5.66it/s]predicting train subjects:   7%|▋         | 11/148 [00:01<00:13,  9.90it/s]predicting train subjects sagittal:  57%|█████▋    | 84/148 [00:16<00:11,  5.66it/s]predicting train subjects:   8%|▊         | 12/148 [00:01<00:15,  8.61it/s]predicting train subjects sagittal:  57%|█████▋    | 85/148 [00:16<00:11,  5.66it/s]predicting train subjects:   9%|▉         | 13/148 [00:01<00:16,  8.31it/s]predicting train subjects sagittal:  58%|█████▊    | 86/148 [00:16<00:10,  5.66it/s]predicting train subjects:   9%|▉         | 14/148 [00:01<00:15,  8.49it/s]predicting train subjects:  10%|█         | 15/148 [00:01<00:15,  8.75it/s]predicting train subjects sagittal:  59%|█████▉    | 87/148 [00:16<00:10,  5.68it/s]predicting train subjects:  11%|█▏        | 17/148 [00:01<00:13,  9.38it/s]predicting train subjects sagittal:  59%|█████▉    | 88/148 [00:17<00:10,  5.58it/s]predicting train subjects:  12%|█▏        | 18/148 [00:01<00:14,  9.05it/s]predicting train subjects sagittal:  60%|██████    | 89/148 [00:17<00:10,  5.53it/s]predicting train subjects:  14%|█▎        | 20/148 [00:02<00:13,  9.40it/s]predicting train subjects:  14%|█▍        | 21/148 [00:02<00:13,  9.49it/s]predicting train subjects sagittal:  61%|██████    | 90/148 [00:17<00:10,  5.53it/s]predicting train subjects:  16%|█▌        | 23/148 [00:02<00:12,  9.68it/s]predicting train subjects sagittal:  61%|██████▏   | 91/148 [00:17<00:10,  5.51it/s]predicting train subjects:  16%|█▌        | 24/148 [00:02<00:13,  9.48it/s]predicting train subjects sagittal:  62%|██████▏   | 92/148 [00:17<00:10,  5.52it/s]predicting train subjects:  17%|█▋        | 25/148 [00:02<00:12,  9.50it/s]predicting train subjects:  18%|█▊        | 26/148 [00:02<00:13,  9.34it/s]predicting train subjects sagittal:  63%|██████▎   | 93/148 [00:17<00:09,  5.55it/s]predicting train subjects:  18%|█▊        | 27/148 [00:02<00:12,  9.35it/s]predicting train subjects:  19%|█▉        | 28/148 [00:03<00:12,  9.28it/s]predicting train subjects sagittal:  64%|██████▎   | 94/148 [00:18<00:09,  5.51it/s]predicting train subjects sagittal:  64%|██████▍   | 95/148 [00:18<00:08,  6.16it/s]predicting train subjects:  20%|██        | 30/148 [00:03<00:12,  9.37it/s]predicting train subjects sagittal:  65%|██████▍   | 96/148 [00:18<00:07,  6.69it/s]predicting train subjects:  21%|██        | 31/148 [00:03<00:12,  9.21it/s]predicting train subjects sagittal:  66%|██████▌   | 97/148 [00:18<00:07,  7.15it/s]predicting train subjects:  22%|██▏       | 32/148 [00:03<00:12,  9.26it/s]predicting train subjects sagittal:  66%|██████▌   | 98/148 [00:18<00:06,  7.51it/s]predicting train subjects sagittal:  67%|██████▋   | 99/148 [00:18<00:06,  7.78it/s]predicting train subjects:  23%|██▎       | 34/148 [00:03<00:12,  9.44it/s]predicting train subjects sagittal:  68%|██████▊   | 100/148 [00:18<00:05,  8.06it/s]predicting train subjects:  24%|██▎       | 35/148 [00:03<00:11,  9.53it/s]predicting train subjects sagittal:  68%|██████▊   | 101/148 [00:19<00:06,  6.83it/s]predicting train subjects:  25%|██▌       | 37/148 [00:03<00:11,  9.36it/s]predicting train subjects:  26%|██▌       | 38/148 [00:04<00:11,  9.39it/s]predicting train subjects sagittal:  69%|██████▉   | 102/148 [00:19<00:07,  6.18it/s]predicting train subjects:  27%|██▋       | 40/148 [00:04<00:11,  9.69it/s]predicting train subjects sagittal:  70%|██████▉   | 103/148 [00:19<00:07,  5.74it/s]predicting train subjects:  28%|██▊       | 41/148 [00:04<00:11,  9.64it/s]predicting train subjects:  28%|██▊       | 42/148 [00:04<00:10,  9.72it/s]predicting train subjects sagittal:  70%|███████   | 104/148 [00:19<00:07,  5.55it/s]predicting train subjects:  30%|██▉       | 44/148 [00:04<00:10,  9.91it/s]predicting train subjects sagittal:  71%|███████   | 105/148 [00:19<00:07,  5.41it/s]predicting train subjects:  30%|███       | 45/148 [00:04<00:10,  9.76it/s]predicting train subjects sagittal:  72%|███████▏  | 106/148 [00:20<00:07,  5.30it/s]predicting train subjects:  32%|███▏      | 47/148 [00:04<00:10,  9.81it/s]predicting train subjects:  32%|███▏      | 48/148 [00:05<00:10,  9.75it/s]predicting train subjects sagittal:  72%|███████▏  | 107/148 [00:20<00:07,  5.31it/s]predicting train subjects:  33%|███▎      | 49/148 [00:05<00:10,  9.59it/s]predicting train subjects sagittal:  73%|███████▎  | 108/148 [00:20<00:07,  5.31it/s]predicting train subjects:  34%|███▍      | 51/148 [00:05<00:09,  9.74it/s]predicting train subjects sagittal:  74%|███████▎  | 109/148 [00:20<00:07,  5.33it/s]predicting train subjects:  36%|███▌      | 53/148 [00:05<00:09,  9.97it/s]predicting train subjects sagittal:  74%|███████▍  | 110/148 [00:20<00:07,  5.32it/s]predicting train subjects:  36%|███▋      | 54/148 [00:05<00:09,  9.51it/s]predicting train subjects:  37%|███▋      | 55/148 [00:05<00:10,  9.04it/s]predicting train subjects sagittal:  75%|███████▌  | 111/148 [00:20<00:06,  5.34it/s]predicting train subjects:  38%|███▊      | 56/148 [00:05<00:10,  9.14it/s]predicting train subjects sagittal:  76%|███████▌  | 112/148 [00:21<00:06,  5.35it/s]predicting train subjects:  39%|███▊      | 57/148 [00:06<00:10,  8.92it/s]predicting train subjects:  39%|███▉      | 58/148 [00:06<00:10,  8.95it/s]predicting train subjects sagittal:  76%|███████▋  | 113/148 [00:21<00:05,  5.98it/s]predicting train subjects:  40%|███▉      | 59/148 [00:06<00:10,  8.89it/s]predicting train subjects sagittal:  77%|███████▋  | 114/148 [00:21<00:05,  6.48it/s]predicting train subjects:  41%|████      | 60/148 [00:06<00:10,  8.53it/s]predicting train subjects sagittal:  78%|███████▊  | 115/148 [00:21<00:04,  6.93it/s]predicting train subjects:  41%|████      | 61/148 [00:06<00:10,  8.33it/s]predicting train subjects sagittal:  78%|███████▊  | 116/148 [00:21<00:04,  7.24it/s]predicting train subjects sagittal:  79%|███████▉  | 117/148 [00:21<00:04,  7.51it/s]predicting train subjects:  42%|████▏     | 62/148 [00:06<00:10,  8.17it/s]predicting train subjects sagittal:  80%|███████▉  | 118/148 [00:21<00:03,  7.69it/s]predicting train subjects:  43%|████▎     | 63/148 [00:06<00:10,  8.00it/s]predicting train subjects:  43%|████▎     | 64/148 [00:06<00:10,  8.13it/s]predicting train subjects sagittal:  80%|████████  | 119/148 [00:22<00:04,  6.45it/s]predicting train subjects:  45%|████▍     | 66/148 [00:07<00:09,  8.81it/s]predicting train subjects:  45%|████▌     | 67/148 [00:07<00:08,  9.11it/s]predicting train subjects sagittal:  81%|████████  | 120/148 [00:22<00:04,  5.82it/s]predicting train subjects:  46%|████▌     | 68/148 [00:07<00:08,  8.94it/s]predicting train subjects sagittal:  82%|████████▏ | 121/148 [00:22<00:04,  5.47it/s]predicting train subjects:  47%|████▋     | 69/148 [00:07<00:08,  8.99it/s]predicting train subjects:  48%|████▊     | 71/148 [00:07<00:08,  9.43it/s]predicting train subjects sagittal:  82%|████████▏ | 122/148 [00:22<00:04,  5.22it/s]predicting train subjects:  49%|████▊     | 72/148 [00:07<00:08,  9.28it/s]predicting train subjects sagittal:  83%|████████▎ | 123/148 [00:22<00:04,  5.04it/s]predicting train subjects:  50%|█████     | 74/148 [00:07<00:07,  9.42it/s]predicting train subjects:  51%|█████     | 75/148 [00:08<00:07,  9.41it/s]predicting train subjects sagittal:  84%|████████▍ | 124/148 [00:23<00:04,  4.82it/s]predicting train subjects:  51%|█████▏    | 76/148 [00:08<00:07,  9.35it/s]predicting train subjects sagittal:  84%|████████▍ | 125/148 [00:23<00:04,  4.61it/s]predicting train subjects:  53%|█████▎    | 78/148 [00:08<00:07,  9.84it/s]predicting train subjects:  54%|█████▍    | 80/148 [00:08<00:06, 10.05it/s]predicting train subjects sagittal:  85%|████████▌ | 126/148 [00:23<00:04,  4.49it/s]predicting train subjects:  55%|█████▌    | 82/148 [00:08<00:06, 10.61it/s]predicting train subjects sagittal:  86%|████████▌ | 127/148 [00:23<00:04,  4.45it/s]predicting train subjects:  57%|█████▋    | 84/148 [00:08<00:05, 11.01it/s]predicting train subjects:  58%|█████▊    | 86/148 [00:08<00:05, 11.34it/s]predicting train subjects sagittal:  86%|████████▋ | 128/148 [00:24<00:04,  4.38it/s]predicting train subjects:  59%|█████▉    | 88/148 [00:09<00:05, 11.59it/s]predicting train subjects sagittal:  87%|████████▋ | 129/148 [00:24<00:04,  4.36it/s]predicting train subjects:  61%|██████    | 90/148 [00:09<00:05, 11.37it/s]predicting train subjects sagittal:  88%|████████▊ | 130/148 [00:24<00:04,  4.35it/s]predicting train subjects:  62%|██████▏   | 92/148 [00:09<00:05, 10.98it/s]predicting train subjects sagittal:  89%|████████▊ | 131/148 [00:24<00:03,  4.58it/s]predicting train subjects:  64%|██████▎   | 94/148 [00:09<00:05, 10.69it/s]predicting train subjects sagittal:  89%|████████▉ | 132/148 [00:24<00:03,  4.75it/s]predicting train subjects:  65%|██████▍   | 96/148 [00:09<00:04, 11.92it/s]predicting train subjects:  66%|██████▌   | 98/148 [00:09<00:03, 12.62it/s]predicting train subjects sagittal:  90%|████████▉ | 133/148 [00:25<00:03,  4.88it/s]predicting train subjects:  68%|██████▊   | 100/148 [00:10<00:03, 13.26it/s]predicting train subjects sagittal:  91%|█████████ | 134/148 [00:25<00:02,  4.98it/s]predicting train subjects:  69%|██████▉   | 102/148 [00:10<00:03, 11.64it/s]predicting train subjects sagittal:  91%|█████████ | 135/148 [00:25<00:02,  5.06it/s]predicting train subjects:  70%|███████   | 104/148 [00:10<00:04, 10.81it/s]predicting train subjects sagittal:  92%|█████████▏| 136/148 [00:25<00:02,  5.07it/s]predicting train subjects sagittal:  93%|█████████▎| 137/148 [00:25<00:02,  5.46it/s]predicting train subjects:  72%|███████▏  | 106/148 [00:10<00:04, 10.42it/s]predicting train subjects sagittal:  93%|█████████▎| 138/148 [00:26<00:01,  5.77it/s]predicting train subjects:  73%|███████▎  | 108/148 [00:11<00:04,  9.41it/s]predicting train subjects sagittal:  94%|█████████▍| 139/148 [00:26<00:01,  5.94it/s]predicting train subjects:  74%|███████▎  | 109/148 [00:11<00:04,  9.30it/s]predicting train subjects sagittal:  95%|█████████▍| 140/148 [00:26<00:01,  6.09it/s]predicting train subjects:  75%|███████▌  | 111/148 [00:11<00:03,  9.43it/s]predicting train subjects sagittal:  95%|█████████▌| 141/148 [00:26<00:01,  6.20it/s]predicting train subjects:  76%|███████▌  | 112/148 [00:11<00:03,  9.59it/s]predicting train subjects sagittal:  96%|█████████▌| 142/148 [00:26<00:00,  6.32it/s]predicting train subjects:  77%|███████▋  | 114/148 [00:11<00:03, 10.60it/s]predicting train subjects:  78%|███████▊  | 116/148 [00:11<00:02, 11.85it/s]predicting train subjects sagittal:  97%|█████████▋| 143/148 [00:26<00:00,  5.87it/s]predicting train subjects:  80%|███████▉  | 118/148 [00:11<00:02, 12.85it/s]predicting train subjects sagittal:  97%|█████████▋| 144/148 [00:27<00:00,  5.56it/s]predicting train subjects:  81%|████████  | 120/148 [00:12<00:02, 11.56it/s]predicting train subjects sagittal:  98%|█████████▊| 145/148 [00:27<00:00,  5.14it/s]predicting train subjects:  82%|████████▏ | 122/148 [00:12<00:02, 10.69it/s]predicting train subjects sagittal:  99%|█████████▊| 146/148 [00:27<00:00,  5.07it/s]predicting train subjects:  84%|████████▍ | 124/148 [00:12<00:02, 10.14it/s]predicting train subjects sagittal:  99%|█████████▉| 147/148 [00:27<00:00,  5.08it/s]predicting train subjects:  85%|████████▌ | 126/148 [00:12<00:02,  9.43it/s]predicting train subjects sagittal: 100%|██████████| 148/148 [00:27<00:00,  5.04it/s]predicting train subjects:  86%|████████▌ | 127/148 [00:12<00:02,  9.38it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 89.60it/s]
saving BB  train1-THALAMUS:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:  86%|████████▋ | 128/148 [00:12<00:02,  8.88it/s]saving BB  train1-THALAMUS:   6%|▌         | 9/148 [00:00<00:01, 84.47it/s]predicting train subjects:  87%|████████▋ | 129/148 [00:13<00:02,  8.71it/s]saving BB  train1-THALAMUS:  13%|█▎        | 19/148 [00:00<00:01, 86.68it/s]predicting train subjects:  88%|████████▊ | 130/148 [00:13<00:02,  8.81it/s]saving BB  train1-THALAMUS:  20%|█▉        | 29/148 [00:00<00:01, 88.70it/s]predicting train subjects:  89%|████████▊ | 131/148 [00:13<00:01,  8.97it/s]saving BB  train1-THALAMUS:  26%|██▋       | 39/148 [00:00<00:01, 89.58it/s]predicting train subjects:  89%|████████▉ | 132/148 [00:13<00:01,  9.08it/s]saving BB  train1-THALAMUS:  33%|███▎      | 49/148 [00:00<00:01, 90.00it/s]predicting train subjects:  90%|████████▉ | 133/148 [00:13<00:01,  9.21it/s]saving BB  train1-THALAMUS:  40%|███▉      | 59/148 [00:00<00:00, 89.75it/s]saving BB  train1-THALAMUS:  46%|████▌     | 68/148 [00:00<00:00, 89.33it/s]predicting train subjects:  91%|█████████ | 135/148 [00:13<00:01,  9.34it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 78/148 [00:00<00:00, 90.85it/s]predicting train subjects:  92%|█████████▏| 136/148 [00:13<00:01,  9.35it/s]saving BB  train1-THALAMUS:  60%|██████    | 89/148 [00:00<00:00, 93.72it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 100/148 [00:01<00:00, 97.83it/s]predicting train subjects:  93%|█████████▎| 138/148 [00:14<00:00, 10.07it/s]saving BB  train1-THALAMUS:  74%|███████▍  | 110/148 [00:01<00:00, 92.12it/s]predicting train subjects:  95%|█████████▍| 140/148 [00:14<00:00, 10.45it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 121/148 [00:01<00:00, 96.52it/s]saving BB  train1-THALAMUS:  89%|████████▊ | 131/148 [00:01<00:00, 91.99it/s]predicting train subjects:  96%|█████████▌| 142/148 [00:14<00:00, 10.80it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 141/148 [00:01<00:00, 93.53it/s]saving BB  train1-THALAMUS: 100%|██████████| 148/148 [00:01<00:00, 92.73it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 98.95it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/148 [00:00<?, ?it/s]predicting train subjects:  97%|█████████▋| 144/148 [00:14<00:00, 10.25it/s]saving BB  train1-THALAMUS Sagittal:   7%|▋         | 10/148 [00:00<00:01, 92.08it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▎        | 20/148 [00:00<00:01, 92.63it/s]predicting train subjects:  99%|█████████▊| 146/148 [00:14<00:00,  9.94it/s]saving BB  train1-THALAMUS Sagittal:  20%|██        | 30/148 [00:00<00:01, 92.23it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▋       | 39/148 [00:00<00:01, 90.35it/s]predicting train subjects: 100%|██████████| 148/148 [00:14<00:00,  9.79it/s]

saving BB  train1-THALAMUS Sagittal:  32%|███▏      | 47/148 [00:00<00:01, 83.87it/s]saving BB  train1-THALAMUS Sagittal:  38%|███▊      | 56/148 [00:00<00:01, 84.64it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 64/148 [00:00<00:01, 80.95it/s]saving BB  train1-THALAMUS Sagittal:  50%|█████     | 74/148 [00:00<00:00, 83.86it/s]saving BB  train1-THALAMUS Sagittal:  56%|█████▌    | 83/148 [00:00<00:00, 83.30it/s]saving BB  train1-THALAMUS Sagittal:  63%|██████▎   | 93/148 [00:01<00:00, 86.84it/s]saving BB  train1-THALAMUS Sagittal:  70%|███████   | 104/148 [00:01<00:00, 90.55it/s]saving BB  train1-THALAMUS Sagittal:  76%|███████▋  | 113/148 [00:01<00:00, 88.73it/s]saving BB  train1-THALAMUS Sagittal:  84%|████████▍ | 124/148 [00:01<00:00, 91.40it/s]saving BB  train1-THALAMUS Sagittal:  91%|█████████ | 134/148 [00:01<00:00, 86.00it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 144/148 [00:01<00:00, 88.90it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 148/148 [00:01<00:00, 87.72it/s]

