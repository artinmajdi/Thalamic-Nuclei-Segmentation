2020-01-20 22:18:27.306163: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:29.413087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:29.413131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:29.833830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:29.833865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:29.833876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:29.834321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['a']
TypeExperiment 8
CrossVal ['a']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:32.903172: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:39.131489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:39.131554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:39.549249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:39.549320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:39.549332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:39.549798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 8
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:42.645613: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:45.590577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:45.590646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:46.008132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:46.008207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:46.008219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:46.008672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 8
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:49.146003: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:50.986251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:50.986318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:51.407723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:51.407789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:51.407802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:51.408246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 8
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:21:28.359668: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:21:32.282503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:21:32.282567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:21:32.687984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:21:32.688056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:21:32.688068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:21:32.688521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:53,  2.33it/s]Loading train:   1%|          | 2/266 [00:00<01:40,  2.62it/s]Loading train:   1%|          | 3/266 [00:00<01:28,  2.96it/s]Loading train:   2%|▏         | 4/266 [00:01<01:20,  3.24it/s]Loading train:   2%|▏         | 5/266 [00:01<01:17,  3.38it/s]Loading train:   2%|▏         | 6/266 [00:01<01:14,  3.50it/s]Loading train:   3%|▎         | 7/266 [00:01<01:12,  3.55it/s]Loading train:   3%|▎         | 8/266 [00:02<01:10,  3.64it/s]Loading train:   3%|▎         | 9/266 [00:02<01:08,  3.72it/s]Loading train:   4%|▍         | 10/266 [00:02<01:07,  3.78it/s]Loading train:   4%|▍         | 11/266 [00:03<01:06,  3.82it/s]Loading train:   5%|▍         | 12/266 [00:03<01:05,  3.85it/s]Loading train:   5%|▍         | 13/266 [00:03<01:05,  3.87it/s]Loading train:   5%|▌         | 14/266 [00:03<01:04,  3.89it/s]Loading train:   6%|▌         | 15/266 [00:04<01:04,  3.90it/s]Loading train:   6%|▌         | 16/266 [00:04<01:03,  3.91it/s]Loading train:   6%|▋         | 17/266 [00:04<01:03,  3.92it/s]Loading train:   7%|▋         | 18/266 [00:04<01:03,  3.92it/s]Loading train:   7%|▋         | 19/266 [00:05<01:02,  3.93it/s]Loading train:   8%|▊         | 20/266 [00:05<01:02,  3.93it/s]Loading train:   8%|▊         | 21/266 [00:05<01:02,  3.92it/s]Loading train:   8%|▊         | 22/266 [00:05<01:02,  3.90it/s]Loading train:   9%|▊         | 23/266 [00:06<01:02,  3.91it/s]Loading train:   9%|▉         | 24/266 [00:06<01:00,  3.97it/s]Loading train:   9%|▉         | 25/266 [00:06<01:00,  4.00it/s]Loading train:  10%|▉         | 26/266 [00:06<00:59,  4.02it/s]Loading train:  10%|█         | 27/266 [00:07<01:02,  3.85it/s]Loading train:  11%|█         | 28/266 [00:07<01:01,  3.87it/s]Loading train:  11%|█         | 29/266 [00:07<01:00,  3.94it/s]Loading train:  11%|█▏        | 30/266 [00:07<01:01,  3.82it/s]Loading train:  12%|█▏        | 31/266 [00:08<01:01,  3.84it/s]Loading train:  12%|█▏        | 32/266 [00:08<01:00,  3.86it/s]Loading train:  12%|█▏        | 33/266 [00:08<01:00,  3.88it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:59,  3.89it/s]Loading train:  13%|█▎        | 35/266 [00:09<00:59,  3.90it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:58,  3.91it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:58,  3.92it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:57,  3.94it/s]Loading train:  15%|█▍        | 39/266 [00:10<01:01,  3.67it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:59,  3.77it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:58,  3.86it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:54,  4.12it/s]Loading train:  16%|█▌        | 43/266 [00:11<00:51,  4.30it/s]Loading train:  17%|█▋        | 44/266 [00:11<00:49,  4.46it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:48,  4.54it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:47,  4.64it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.71it/s]Loading train:  18%|█▊        | 48/266 [00:12<00:46,  4.70it/s]Loading train:  18%|█▊        | 49/266 [00:12<00:46,  4.70it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:45,  4.73it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:45,  4.76it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:45,  4.74it/s]Loading train:  20%|█▉        | 53/266 [00:13<00:44,  4.76it/s]Loading train:  20%|██        | 54/266 [00:13<00:44,  4.74it/s]Loading train:  21%|██        | 55/266 [00:13<00:44,  4.73it/s]Loading train:  21%|██        | 56/266 [00:13<00:44,  4.73it/s]Loading train:  21%|██▏       | 57/266 [00:14<00:43,  4.75it/s]Loading train:  22%|██▏       | 58/266 [00:14<00:43,  4.74it/s]Loading train:  22%|██▏       | 59/266 [00:14<00:43,  4.78it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:43,  4.75it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:43,  4.70it/s]Loading train:  23%|██▎       | 62/266 [00:15<00:44,  4.62it/s]Loading train:  24%|██▎       | 63/266 [00:15<00:43,  4.64it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:43,  4.62it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:43,  4.63it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:43,  4.64it/s]Loading train:  25%|██▌       | 67/266 [00:16<00:42,  4.65it/s]Loading train:  26%|██▌       | 68/266 [00:16<00:42,  4.65it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:42,  4.66it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:42,  4.59it/s]Loading train:  27%|██▋       | 71/266 [00:17<00:42,  4.60it/s]Loading train:  27%|██▋       | 72/266 [00:17<00:41,  4.63it/s]Loading train:  27%|██▋       | 73/266 [00:17<00:41,  4.66it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:41,  4.67it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.67it/s]Loading train:  29%|██▊       | 76/266 [00:18<00:40,  4.66it/s]Loading train:  29%|██▉       | 77/266 [00:18<00:40,  4.65it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:42,  4.47it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:43,  4.32it/s]Loading train:  30%|███       | 80/266 [00:19<00:43,  4.26it/s]Loading train:  30%|███       | 81/266 [00:19<00:44,  4.18it/s]Loading train:  31%|███       | 82/266 [00:19<00:45,  4.08it/s]Loading train:  31%|███       | 83/266 [00:19<00:46,  3.94it/s]Loading train:  32%|███▏      | 84/266 [00:20<00:48,  3.77it/s]Loading train:  32%|███▏      | 85/266 [00:20<00:47,  3.79it/s]Loading train:  32%|███▏      | 86/266 [00:20<00:49,  3.67it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:47,  3.78it/s]Loading train:  33%|███▎      | 88/266 [00:21<00:45,  3.88it/s]Loading train:  33%|███▎      | 89/266 [00:21<00:44,  3.95it/s]Loading train:  34%|███▍      | 90/266 [00:21<00:44,  3.97it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:43,  4.01it/s]Loading train:  35%|███▍      | 92/266 [00:22<00:42,  4.05it/s]Loading train:  35%|███▍      | 93/266 [00:22<00:42,  4.06it/s]Loading train:  35%|███▌      | 94/266 [00:22<00:42,  4.08it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:41,  4.11it/s]Loading train:  36%|███▌      | 96/266 [00:23<00:40,  4.15it/s]Loading train:  36%|███▋      | 97/266 [00:23<00:42,  3.97it/s]Loading train:  37%|███▋      | 98/266 [00:23<00:42,  3.94it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:40,  4.12it/s]Loading train:  38%|███▊      | 100/266 [00:24<00:40,  4.14it/s]Loading train:  38%|███▊      | 101/266 [00:24<00:38,  4.24it/s]Loading train:  38%|███▊      | 102/266 [00:24<00:37,  4.33it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:37,  4.38it/s]Loading train:  39%|███▉      | 104/266 [00:25<00:36,  4.44it/s]Loading train:  39%|███▉      | 105/266 [00:25<00:36,  4.46it/s]Loading train:  40%|███▉      | 106/266 [00:25<00:35,  4.50it/s]Loading train:  40%|████      | 107/266 [00:25<00:35,  4.51it/s]Loading train:  41%|████      | 108/266 [00:25<00:35,  4.51it/s]Loading train:  41%|████      | 109/266 [00:26<00:34,  4.52it/s]Loading train:  41%|████▏     | 110/266 [00:26<00:34,  4.53it/s]Loading train:  42%|████▏     | 111/266 [00:26<00:35,  4.34it/s]Loading train:  42%|████▏     | 112/266 [00:26<00:34,  4.42it/s]Loading train:  42%|████▏     | 113/266 [00:27<00:34,  4.40it/s]Loading train:  43%|████▎     | 114/266 [00:27<00:34,  4.42it/s]Loading train:  43%|████▎     | 115/266 [00:27<00:34,  4.44it/s]Loading train:  44%|████▎     | 116/266 [00:27<00:33,  4.45it/s]Loading train:  44%|████▍     | 117/266 [00:27<00:33,  4.49it/s]Loading train:  44%|████▍     | 118/266 [00:28<00:32,  4.52it/s]Loading train:  45%|████▍     | 119/266 [00:28<00:34,  4.30it/s]Loading train:  45%|████▌     | 120/266 [00:28<00:35,  4.17it/s]Loading train:  45%|████▌     | 121/266 [00:28<00:35,  4.07it/s]Loading train:  46%|████▌     | 122/266 [00:29<00:35,  4.01it/s]Loading train:  46%|████▌     | 123/266 [00:29<00:36,  3.97it/s]Loading train:  47%|████▋     | 124/266 [00:29<00:36,  3.94it/s]Loading train:  47%|████▋     | 125/266 [00:29<00:36,  3.89it/s]Loading train:  47%|████▋     | 126/266 [00:30<00:36,  3.85it/s]Loading train:  48%|████▊     | 127/266 [00:30<00:36,  3.86it/s]Loading train:  48%|████▊     | 128/266 [00:30<00:35,  3.87it/s]Loading train:  48%|████▊     | 129/266 [00:30<00:35,  3.90it/s]Loading train:  49%|████▉     | 130/266 [00:31<00:34,  3.90it/s]Loading train:  49%|████▉     | 131/266 [00:31<00:34,  3.91it/s]Loading train:  50%|████▉     | 132/266 [00:31<00:34,  3.91it/s]Loading train:  50%|█████     | 133/266 [00:32<00:34,  3.89it/s]Loading train:  50%|█████     | 134/266 [00:32<00:34,  3.87it/s]Loading train:  51%|█████     | 135/266 [00:32<00:33,  3.87it/s]Loading train:  51%|█████     | 136/266 [00:32<00:33,  3.87it/s]Loading train:  52%|█████▏    | 137/266 [00:33<00:32,  4.00it/s]Loading train:  52%|█████▏    | 138/266 [00:33<00:31,  4.09it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:30,  4.15it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:29,  4.23it/s]Loading train:  53%|█████▎    | 141/266 [00:33<00:29,  4.24it/s]Loading train:  53%|█████▎    | 142/266 [00:34<00:29,  4.26it/s]Loading train:  54%|█████▍    | 143/266 [00:34<00:28,  4.29it/s]Loading train:  54%|█████▍    | 144/266 [00:34<00:28,  4.31it/s]Loading train:  55%|█████▍    | 145/266 [00:34<00:27,  4.33it/s]Loading train:  55%|█████▍    | 146/266 [00:35<00:27,  4.32it/s]Loading train:  55%|█████▌    | 147/266 [00:35<00:27,  4.29it/s]Loading train:  56%|█████▌    | 148/266 [00:35<00:27,  4.23it/s]Loading train:  56%|█████▌    | 149/266 [00:35<00:27,  4.22it/s]Loading train:  56%|█████▋    | 150/266 [00:36<00:27,  4.26it/s]Loading train:  57%|█████▋    | 151/266 [00:36<00:26,  4.30it/s]Loading train:  57%|█████▋    | 152/266 [00:36<00:26,  4.34it/s]Loading train:  58%|█████▊    | 153/266 [00:36<00:25,  4.35it/s]Loading train:  58%|█████▊    | 154/266 [00:36<00:25,  4.38it/s]Loading train:  58%|█████▊    | 155/266 [00:37<00:24,  4.61it/s]Loading train:  59%|█████▊    | 156/266 [00:37<00:22,  4.79it/s]Loading train:  59%|█████▉    | 157/266 [00:37<00:22,  4.89it/s]Loading train:  59%|█████▉    | 158/266 [00:37<00:21,  5.00it/s]Loading train:  60%|█████▉    | 159/266 [00:37<00:21,  5.07it/s]Loading train:  60%|██████    | 160/266 [00:38<00:20,  5.13it/s]Loading train:  61%|██████    | 161/266 [00:38<00:20,  5.17it/s]Loading train:  61%|██████    | 162/266 [00:38<00:20,  5.18it/s]Loading train:  61%|██████▏   | 163/266 [00:38<00:19,  5.18it/s]Loading train:  62%|██████▏   | 164/266 [00:38<00:19,  5.21it/s]Loading train:  62%|██████▏   | 165/266 [00:39<00:19,  5.23it/s]Loading train:  62%|██████▏   | 166/266 [00:39<00:19,  5.20it/s]Loading train:  63%|██████▎   | 167/266 [00:39<00:18,  5.23it/s]Loading train:  63%|██████▎   | 168/266 [00:39<00:18,  5.18it/s]Loading train:  64%|██████▎   | 169/266 [00:39<00:18,  5.18it/s]Loading train:  64%|██████▍   | 170/266 [00:40<00:18,  5.14it/s]Loading train:  64%|██████▍   | 171/266 [00:40<00:18,  5.16it/s]Loading train:  65%|██████▍   | 172/266 [00:40<00:18,  5.18it/s]Loading train:  65%|██████▌   | 173/266 [00:40<00:18,  5.03it/s]Loading train:  65%|██████▌   | 174/266 [00:40<00:18,  4.92it/s]Loading train:  66%|██████▌   | 175/266 [00:41<00:18,  4.83it/s]Loading train:  66%|██████▌   | 176/266 [00:41<00:18,  4.78it/s]Loading train:  67%|██████▋   | 177/266 [00:41<00:18,  4.76it/s]Loading train:  67%|██████▋   | 178/266 [00:41<00:25,  3.49it/s]Loading train:  67%|██████▋   | 179/266 [00:42<00:27,  3.22it/s]Loading train:  68%|██████▊   | 180/266 [00:42<00:29,  2.94it/s]Loading train:  68%|██████▊   | 181/266 [00:43<00:35,  2.40it/s]Loading train:  68%|██████▊   | 182/266 [00:43<00:38,  2.19it/s]Loading train:  69%|██████▉   | 183/266 [00:44<00:41,  2.02it/s]Loading train:  69%|██████▉   | 184/266 [00:44<00:41,  1.98it/s]Loading train:  70%|██████▉   | 185/266 [00:45<00:45,  1.79it/s]Loading train:  70%|██████▉   | 186/266 [00:46<00:45,  1.77it/s]Loading train:  70%|███████   | 187/266 [00:46<00:42,  1.84it/s]Loading train:  71%|███████   | 188/266 [00:47<00:36,  2.15it/s]Loading train:  71%|███████   | 189/266 [00:47<00:33,  2.27it/s]Loading train:  71%|███████▏  | 190/266 [00:47<00:32,  2.35it/s]Loading train:  72%|███████▏  | 191/266 [00:48<00:35,  2.13it/s]Loading train:  72%|███████▏  | 192/266 [00:48<00:34,  2.13it/s]Loading train:  73%|███████▎  | 193/266 [00:49<00:34,  2.13it/s]Loading train:  73%|███████▎  | 194/266 [00:49<00:31,  2.30it/s]Loading train:  73%|███████▎  | 195/266 [00:50<00:29,  2.42it/s]Loading train:  74%|███████▎  | 196/266 [00:50<00:26,  2.65it/s]Loading train:  74%|███████▍  | 197/266 [00:50<00:24,  2.77it/s]Loading train:  74%|███████▍  | 198/266 [00:50<00:23,  2.86it/s]Loading train:  75%|███████▍  | 199/266 [00:51<00:28,  2.32it/s]Loading train:  75%|███████▌  | 200/266 [00:52<00:30,  2.14it/s]Loading train:  76%|███████▌  | 201/266 [00:52<00:30,  2.16it/s]Loading train:  76%|███████▌  | 202/266 [00:52<00:26,  2.43it/s]Loading train:  76%|███████▋  | 203/266 [00:53<00:23,  2.65it/s]Loading train:  77%|███████▋  | 204/266 [00:53<00:22,  2.82it/s]Loading train:  77%|███████▋  | 205/266 [00:53<00:20,  2.97it/s]Loading train:  77%|███████▋  | 206/266 [00:54<00:19,  3.05it/s]Loading train:  78%|███████▊  | 207/266 [00:54<00:18,  3.11it/s]Loading train:  78%|███████▊  | 208/266 [00:54<00:18,  3.22it/s]Loading train:  79%|███████▊  | 209/266 [00:54<00:17,  3.19it/s]Loading train:  79%|███████▉  | 210/266 [00:55<00:17,  3.21it/s]Loading train:  79%|███████▉  | 211/266 [00:55<00:22,  2.43it/s]Loading train:  80%|███████▉  | 212/266 [00:56<00:26,  2.07it/s]Loading train:  80%|████████  | 213/266 [00:57<00:29,  1.78it/s]Loading train:  80%|████████  | 214/266 [00:58<00:32,  1.61it/s]Loading train:  81%|████████  | 215/266 [00:58<00:32,  1.55it/s]Loading train:  81%|████████  | 216/266 [00:59<00:29,  1.67it/s]Loading train:  82%|████████▏ | 217/266 [00:59<00:29,  1.64it/s]Loading train:  82%|████████▏ | 218/266 [01:00<00:29,  1.64it/s]Loading train:  82%|████████▏ | 219/266 [01:01<00:28,  1.63it/s]Loading train:  83%|████████▎ | 220/266 [01:01<00:27,  1.69it/s]Loading train:  83%|████████▎ | 221/266 [01:02<00:26,  1.72it/s]Loading train:  83%|████████▎ | 222/266 [01:02<00:24,  1.77it/s]Loading train:  84%|████████▍ | 223/266 [01:03<00:25,  1.70it/s]Loading train:  84%|████████▍ | 224/266 [01:04<00:25,  1.62it/s]Loading train:  85%|████████▍ | 225/266 [01:04<00:25,  1.59it/s]Loading train:  85%|████████▍ | 226/266 [01:05<00:24,  1.61it/s]Loading train:  85%|████████▌ | 227/266 [01:05<00:24,  1.59it/s]Loading train:  86%|████████▌ | 228/266 [01:06<00:24,  1.53it/s]Loading train:  86%|████████▌ | 229/266 [01:07<00:23,  1.56it/s]Loading train:  86%|████████▋ | 230/266 [01:07<00:21,  1.65it/s]Loading train:  87%|████████▋ | 231/266 [01:08<00:21,  1.60it/s]Loading train:  87%|████████▋ | 232/266 [01:09<00:21,  1.57it/s]Loading train:  88%|████████▊ | 233/266 [01:09<00:19,  1.68it/s]Loading train:  88%|████████▊ | 234/266 [01:10<00:19,  1.65it/s]Loading train:  88%|████████▊ | 235/266 [01:10<00:18,  1.64it/s]Loading train:  89%|████████▊ | 236/266 [01:11<00:18,  1.58it/s]Loading train:  89%|████████▉ | 237/266 [01:12<00:17,  1.62it/s]Loading train:  89%|████████▉ | 238/266 [01:12<00:16,  1.66it/s]Loading train:  90%|████████▉ | 239/266 [01:13<00:16,  1.66it/s]Loading train:  90%|█████████ | 240/266 [01:13<00:15,  1.67it/s]Loading train:  91%|█████████ | 241/266 [01:14<00:15,  1.65it/s]Loading train:  91%|█████████ | 242/266 [01:15<00:14,  1.63it/s]Loading train:  91%|█████████▏| 243/266 [01:15<00:13,  1.67it/s]Loading train:  92%|█████████▏| 244/266 [01:16<00:13,  1.67it/s]Loading train:  92%|█████████▏| 245/266 [01:16<00:12,  1.67it/s]Loading train:  92%|█████████▏| 246/266 [01:17<00:11,  1.78it/s]Loading train:  93%|█████████▎| 247/266 [01:18<00:11,  1.72it/s]Loading train:  93%|█████████▎| 248/266 [01:18<00:11,  1.63it/s]Loading train:  94%|█████████▎| 249/266 [01:19<00:10,  1.55it/s]Loading train:  94%|█████████▍| 250/266 [01:20<00:10,  1.55it/s]Loading train:  94%|█████████▍| 251/266 [01:20<00:09,  1.58it/s]Loading train:  95%|█████████▍| 252/266 [01:21<00:09,  1.51it/s]Loading train:  95%|█████████▌| 253/266 [01:22<00:08,  1.49it/s]Loading train:  95%|█████████▌| 254/266 [01:22<00:08,  1.50it/s]Loading train:  96%|█████████▌| 255/266 [01:23<00:07,  1.56it/s]Loading train:  96%|█████████▌| 256/266 [01:23<00:06,  1.60it/s]Loading train:  97%|█████████▋| 257/266 [01:24<00:05,  1.64it/s]Loading train:  97%|█████████▋| 258/266 [01:25<00:04,  1.68it/s]Loading train:  97%|█████████▋| 259/266 [01:25<00:04,  1.62it/s]Loading train:  98%|█████████▊| 260/266 [01:26<00:03,  1.64it/s]Loading train:  98%|█████████▊| 261/266 [01:26<00:03,  1.66it/s]Loading train:  98%|█████████▊| 262/266 [01:27<00:02,  1.59it/s]Loading train:  99%|█████████▉| 263/266 [01:28<00:01,  1.59it/s]Loading train:  99%|█████████▉| 264/266 [01:28<00:01,  1.66it/s]Loading train: 100%|█████████▉| 265/266 [01:29<00:00,  1.61it/s]Loading train: 100%|██████████| 266/266 [01:30<00:00,  1.63it/s]Loading train: 100%|██████████| 266/266 [01:30<00:00,  2.95it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 47.99it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 47.16it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 45.67it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 45.02it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 44.91it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 45.10it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:05, 44.93it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 45.81it/s]concatenating: train:  18%|█▊        | 47/266 [00:00<00:04, 49.66it/s]concatenating: train:  20%|█▉        | 53/266 [00:01<00:04, 52.22it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:03, 54.62it/s]concatenating: train:  25%|██▌       | 67/266 [00:01<00:03, 56.74it/s]concatenating: train:  28%|██▊       | 74/266 [00:01<00:03, 58.31it/s]concatenating: train:  30%|███       | 80/266 [00:01<00:03, 58.30it/s]concatenating: train:  32%|███▏      | 86/266 [00:01<00:03, 57.42it/s]concatenating: train:  35%|███▍      | 92/266 [00:01<00:03, 57.48it/s]concatenating: train:  37%|███▋      | 98/266 [00:01<00:02, 57.46it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:02, 59.49it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:02, 60.65it/s]concatenating: train:  45%|████▍     | 119/266 [00:02<00:02, 60.89it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 57.32it/s]concatenating: train:  50%|████▉     | 132/266 [00:02<00:02, 56.69it/s]concatenating: train:  52%|█████▏    | 138/266 [00:02<00:02, 56.81it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 58.02it/s]concatenating: train:  57%|█████▋    | 151/266 [00:02<00:02, 57.24it/s]concatenating: train:  59%|█████▉    | 158/266 [00:02<00:01, 59.77it/s]concatenating: train:  62%|██████▏   | 166/266 [00:02<00:01, 62.63it/s]concatenating: train:  65%|██████▌   | 173/266 [00:03<00:01, 64.66it/s]concatenating: train:  68%|██████▊   | 180/266 [00:03<00:01, 63.30it/s]concatenating: train:  70%|███████   | 187/266 [00:03<00:01, 59.55it/s]concatenating: train:  73%|███████▎  | 194/266 [00:03<00:01, 58.43it/s]concatenating: train:  75%|███████▌  | 200/266 [00:03<00:01, 55.97it/s]concatenating: train:  78%|███████▊  | 207/266 [00:03<00:01, 57.84it/s]concatenating: train:  80%|████████  | 213/266 [00:03<00:00, 57.82it/s]concatenating: train:  82%|████████▏ | 219/266 [00:03<00:00, 52.10it/s]concatenating: train:  85%|████████▍ | 225/266 [00:04<00:00, 49.29it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 46.54it/s]concatenating: train:  89%|████████▊ | 236/266 [00:04<00:00, 42.45it/s]concatenating: train:  91%|█████████ | 241/266 [00:04<00:00, 44.01it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 45.27it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 45.70it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 44.93it/s]concatenating: train:  98%|█████████▊| 261/266 [00:04<00:00, 44.93it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 44.98it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 53.09it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:01,  1.60it/s]Loading test:  50%|█████     | 2/4 [00:01<00:01,  1.67it/s]Loading test:  75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 61.47it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<03:25,  1.29it/s]Loading trainS:   1%|          | 2/266 [00:01<03:18,  1.33it/s]Loading trainS:   1%|          | 3/266 [00:02<03:02,  1.44it/s]Loading trainS:   2%|▏         | 4/266 [00:02<02:52,  1.52it/s]Loading trainS:   2%|▏         | 5/266 [00:03<02:48,  1.55it/s]Loading trainS:   2%|▏         | 6/266 [00:03<02:42,  1.60it/s]Loading trainS:   3%|▎         | 7/266 [00:04<02:40,  1.61it/s]Loading trainS:   3%|▎         | 8/266 [00:05<02:42,  1.59it/s]Loading trainS:   3%|▎         | 9/266 [00:05<02:41,  1.59it/s]Loading trainS:   4%|▍         | 10/266 [00:06<02:41,  1.58it/s]Loading trainS:   4%|▍         | 11/266 [00:07<02:47,  1.52it/s]Loading trainS:   5%|▍         | 12/266 [00:07<02:44,  1.54it/s]Loading trainS:   5%|▍         | 13/266 [00:08<02:44,  1.53it/s]Loading trainS:   5%|▌         | 14/266 [00:08<02:44,  1.53it/s]Loading trainS:   6%|▌         | 15/266 [00:09<02:45,  1.52it/s]Loading trainS:   6%|▌         | 16/266 [00:10<02:45,  1.51it/s]Loading trainS:   6%|▋         | 17/266 [00:10<02:42,  1.53it/s]Loading trainS:   7%|▋         | 18/266 [00:11<02:45,  1.50it/s]Loading trainS:   7%|▋         | 19/266 [00:12<02:33,  1.61it/s]Loading trainS:   8%|▊         | 20/266 [00:12<02:36,  1.57it/s]Loading trainS:   8%|▊         | 21/266 [00:13<02:42,  1.51it/s]Loading trainS:   8%|▊         | 22/266 [00:14<02:43,  1.49it/s]Loading trainS:   9%|▊         | 23/266 [00:14<02:37,  1.55it/s]Loading trainS:   9%|▉         | 24/266 [00:15<02:33,  1.58it/s]Loading trainS:   9%|▉         | 25/266 [00:16<02:35,  1.55it/s]Loading trainS:  10%|▉         | 26/266 [00:16<02:33,  1.57it/s]Loading trainS:  10%|█         | 27/266 [00:17<02:36,  1.53it/s]Loading trainS:  11%|█         | 28/266 [00:18<02:31,  1.57it/s]Loading trainS:  11%|█         | 29/266 [00:18<02:28,  1.59it/s]Loading trainS:  11%|█▏        | 30/266 [00:19<02:23,  1.65it/s]Loading trainS:  12%|█▏        | 31/266 [00:19<02:28,  1.58it/s]Loading trainS:  12%|█▏        | 32/266 [00:20<02:30,  1.55it/s]Loading trainS:  12%|█▏        | 33/266 [00:21<02:30,  1.55it/s]Loading trainS:  13%|█▎        | 34/266 [00:21<02:30,  1.54it/s]Loading trainS:  13%|█▎        | 35/266 [00:22<02:29,  1.55it/s]Loading trainS:  14%|█▎        | 36/266 [00:23<02:29,  1.54it/s]Loading trainS:  14%|█▍        | 37/266 [00:23<02:22,  1.60it/s]Loading trainS:  14%|█▍        | 38/266 [00:24<02:24,  1.58it/s]Loading trainS:  15%|█▍        | 39/266 [00:25<02:25,  1.56it/s]Loading trainS:  15%|█▌        | 40/266 [00:25<02:26,  1.55it/s]Loading trainS:  15%|█▌        | 41/266 [00:26<02:25,  1.55it/s]Loading trainS:  16%|█▌        | 42/266 [00:26<02:18,  1.62it/s]Loading trainS:  16%|█▌        | 43/266 [00:27<02:11,  1.70it/s]Loading trainS:  17%|█▋        | 44/266 [00:27<02:06,  1.75it/s]Loading trainS:  17%|█▋        | 45/266 [00:28<02:08,  1.73it/s]Loading trainS:  17%|█▋        | 46/266 [00:29<02:02,  1.79it/s]Loading trainS:  18%|█▊        | 47/266 [00:29<01:59,  1.83it/s]Loading trainS:  18%|█▊        | 48/266 [00:30<01:56,  1.88it/s]Loading trainS:  18%|█▊        | 49/266 [00:30<01:55,  1.87it/s]Loading trainS:  19%|█▉        | 50/266 [00:31<01:56,  1.86it/s]Loading trainS:  19%|█▉        | 51/266 [00:31<01:49,  1.97it/s]Loading trainS:  20%|█▉        | 52/266 [00:32<01:52,  1.91it/s]Loading trainS:  20%|█▉        | 53/266 [00:32<01:54,  1.87it/s]Loading trainS:  20%|██        | 54/266 [00:33<01:54,  1.86it/s]Loading trainS:  21%|██        | 55/266 [00:33<01:52,  1.87it/s]Loading trainS:  21%|██        | 56/266 [00:34<01:57,  1.79it/s]Loading trainS:  21%|██▏       | 57/266 [00:34<01:58,  1.76it/s]Loading trainS:  22%|██▏       | 58/266 [00:35<01:55,  1.80it/s]Loading trainS:  22%|██▏       | 59/266 [00:36<01:50,  1.87it/s]Loading trainS:  23%|██▎       | 60/266 [00:36<01:48,  1.90it/s]Loading trainS:  23%|██▎       | 61/266 [00:37<01:49,  1.87it/s]Loading trainS:  23%|██▎       | 62/266 [00:37<01:47,  1.90it/s]Loading trainS:  24%|██▎       | 63/266 [00:38<01:51,  1.82it/s]Loading trainS:  24%|██▍       | 64/266 [00:38<01:54,  1.76it/s]Loading trainS:  24%|██▍       | 65/266 [00:39<01:51,  1.81it/s]Loading trainS:  25%|██▍       | 66/266 [00:39<01:49,  1.82it/s]Loading trainS:  25%|██▌       | 67/266 [00:40<01:47,  1.84it/s]Loading trainS:  26%|██▌       | 68/266 [00:40<01:42,  1.93it/s]Loading trainS:  26%|██▌       | 69/266 [00:41<01:49,  1.80it/s]Loading trainS:  26%|██▋       | 70/266 [00:42<01:50,  1.78it/s]Loading trainS:  27%|██▋       | 71/266 [00:42<01:50,  1.77it/s]Loading trainS:  27%|██▋       | 72/266 [00:43<01:47,  1.80it/s]Loading trainS:  27%|██▋       | 73/266 [00:43<01:47,  1.80it/s]Loading trainS:  28%|██▊       | 74/266 [00:44<01:47,  1.79it/s]Loading trainS:  28%|██▊       | 75/266 [00:44<01:50,  1.73it/s]Loading trainS:  29%|██▊       | 76/266 [00:45<01:45,  1.80it/s]Loading trainS:  29%|██▉       | 77/266 [00:45<01:46,  1.77it/s]Loading trainS:  29%|██▉       | 78/266 [00:46<01:45,  1.79it/s]Loading trainS:  30%|██▉       | 79/266 [00:47<01:44,  1.80it/s]Loading trainS:  30%|███       | 80/266 [00:47<01:43,  1.80it/s]Loading trainS:  30%|███       | 81/266 [00:48<01:42,  1.81it/s]Loading trainS:  31%|███       | 82/266 [00:48<01:44,  1.76it/s]Loading trainS:  31%|███       | 83/266 [00:49<01:45,  1.74it/s]Loading trainS:  32%|███▏      | 84/266 [00:49<01:46,  1.71it/s]Loading trainS:  32%|███▏      | 85/266 [00:50<01:47,  1.68it/s]Loading trainS:  32%|███▏      | 86/266 [00:51<01:45,  1.70it/s]Loading trainS:  33%|███▎      | 87/266 [00:51<01:52,  1.60it/s]Loading trainS:  33%|███▎      | 88/266 [00:52<01:50,  1.62it/s]Loading trainS:  33%|███▎      | 89/266 [00:53<01:44,  1.69it/s]Loading trainS:  34%|███▍      | 90/266 [00:53<01:43,  1.70it/s]Loading trainS:  34%|███▍      | 91/266 [00:54<01:46,  1.64it/s]Loading trainS:  35%|███▍      | 92/266 [00:54<01:48,  1.60it/s]Loading trainS:  35%|███▍      | 93/266 [00:55<01:50,  1.57it/s]Loading trainS:  35%|███▌      | 94/266 [00:56<01:50,  1.56it/s]Loading trainS:  36%|███▌      | 95/266 [00:56<01:50,  1.55it/s]Loading trainS:  36%|███▌      | 96/266 [00:57<01:45,  1.61it/s]Loading trainS:  36%|███▋      | 97/266 [00:58<01:47,  1.57it/s]Loading trainS:  37%|███▋      | 98/266 [00:58<01:42,  1.64it/s]Loading trainS:  37%|███▋      | 99/266 [00:59<01:29,  1.86it/s]Loading trainS:  38%|███▊      | 100/266 [00:59<01:34,  1.76it/s]Loading trainS:  38%|███▊      | 101/266 [01:00<01:29,  1.84it/s]Loading trainS:  38%|███▊      | 102/266 [01:00<01:28,  1.84it/s]Loading trainS:  39%|███▊      | 103/266 [01:01<01:32,  1.76it/s]Loading trainS:  39%|███▉      | 104/266 [01:01<01:30,  1.79it/s]Loading trainS:  39%|███▉      | 105/266 [01:02<01:29,  1.79it/s]Loading trainS:  40%|███▉      | 106/266 [01:02<01:26,  1.84it/s]Loading trainS:  40%|████      | 107/266 [01:03<01:24,  1.88it/s]Loading trainS:  41%|████      | 108/266 [01:03<01:22,  1.92it/s]Loading trainS:  41%|████      | 109/266 [01:04<01:24,  1.87it/s]Loading trainS:  41%|████▏     | 110/266 [01:05<01:22,  1.89it/s]Loading trainS:  42%|████▏     | 111/266 [01:05<01:21,  1.91it/s]Loading trainS:  42%|████▏     | 112/266 [01:06<01:23,  1.84it/s]Loading trainS:  42%|████▏     | 113/266 [01:06<01:20,  1.90it/s]Loading trainS:  43%|████▎     | 114/266 [01:07<01:21,  1.86it/s]Loading trainS:  43%|████▎     | 115/266 [01:07<01:22,  1.84it/s]Loading trainS:  44%|████▎     | 116/266 [01:08<01:20,  1.86it/s]Loading trainS:  44%|████▍     | 117/266 [01:08<01:21,  1.83it/s]Loading trainS:  44%|████▍     | 118/266 [01:09<01:18,  1.88it/s]Loading trainS:  45%|████▍     | 119/266 [01:09<01:23,  1.75it/s]Loading trainS:  45%|████▌     | 120/266 [01:10<01:26,  1.68it/s]Loading trainS:  45%|████▌     | 121/266 [01:11<01:31,  1.58it/s]Loading trainS:  46%|████▌     | 122/266 [01:11<01:31,  1.58it/s]Loading trainS:  46%|████▌     | 123/266 [01:12<01:34,  1.52it/s]Loading trainS:  47%|████▋     | 124/266 [01:13<01:35,  1.49it/s]Loading trainS:  47%|████▋     | 125/266 [01:14<01:34,  1.50it/s]Loading trainS:  47%|████▋     | 126/266 [01:14<01:34,  1.49it/s]Loading trainS:  48%|████▊     | 127/266 [01:15<01:33,  1.49it/s]Loading trainS:  48%|████▊     | 128/266 [01:16<01:32,  1.49it/s]Loading trainS:  48%|████▊     | 129/266 [01:16<01:31,  1.50it/s]Loading trainS:  49%|████▉     | 130/266 [01:17<01:29,  1.52it/s]Loading trainS:  49%|████▉     | 131/266 [01:17<01:26,  1.56it/s]Loading trainS:  50%|████▉     | 132/266 [01:18<01:28,  1.52it/s]Loading trainS:  50%|█████     | 133/266 [01:19<01:25,  1.55it/s]Loading trainS:  50%|█████     | 134/266 [01:19<01:26,  1.52it/s]Loading trainS:  51%|█████     | 135/266 [01:20<01:26,  1.52it/s]Loading trainS:  51%|█████     | 136/266 [01:21<01:22,  1.57it/s]Loading trainS:  52%|█████▏    | 137/266 [01:21<01:17,  1.67it/s]Loading trainS:  52%|█████▏    | 138/266 [01:22<01:14,  1.73it/s]Loading trainS:  52%|█████▏    | 139/266 [01:22<01:15,  1.68it/s]Loading trainS:  53%|█████▎    | 140/266 [01:23<01:13,  1.72it/s]Loading trainS:  53%|█████▎    | 141/266 [01:23<01:09,  1.79it/s]Loading trainS:  53%|█████▎    | 142/266 [01:24<01:10,  1.75it/s]Loading trainS:  54%|█████▍    | 143/266 [01:25<01:08,  1.79it/s]Loading trainS:  54%|█████▍    | 144/266 [01:25<01:08,  1.77it/s]Loading trainS:  55%|█████▍    | 145/266 [01:26<01:09,  1.75it/s]Loading trainS:  55%|█████▍    | 146/266 [01:26<01:10,  1.70it/s]Loading trainS:  55%|█████▌    | 147/266 [01:27<01:07,  1.75it/s]Loading trainS:  56%|█████▌    | 148/266 [01:27<01:04,  1.83it/s]Loading trainS:  56%|█████▌    | 149/266 [01:28<01:02,  1.86it/s]Loading trainS:  56%|█████▋    | 150/266 [01:28<01:02,  1.87it/s]Loading trainS:  57%|█████▋    | 151/266 [01:29<01:02,  1.84it/s]Loading trainS:  57%|█████▋    | 152/266 [01:30<01:02,  1.83it/s]Loading trainS:  58%|█████▊    | 153/266 [01:30<01:00,  1.88it/s]Loading trainS:  58%|█████▊    | 154/266 [01:31<01:00,  1.85it/s]Loading trainS:  58%|█████▊    | 155/266 [01:31<00:56,  1.96it/s]Loading trainS:  59%|█████▊    | 156/266 [01:32<00:54,  2.02it/s]Loading trainS:  59%|█████▉    | 157/266 [01:32<00:52,  2.06it/s]Loading trainS:  59%|█████▉    | 158/266 [01:32<00:51,  2.09it/s]Loading trainS:  60%|█████▉    | 159/266 [01:33<00:49,  2.16it/s]Loading trainS:  60%|██████    | 160/266 [01:33<00:52,  2.01it/s]Loading trainS:  61%|██████    | 161/266 [01:34<00:50,  2.08it/s]Loading trainS:  61%|██████    | 162/266 [01:34<00:50,  2.05it/s]Loading trainS:  61%|██████▏   | 163/266 [01:35<00:49,  2.08it/s]Loading trainS:  62%|██████▏   | 164/266 [01:35<00:47,  2.13it/s]Loading trainS:  62%|██████▏   | 165/266 [01:36<00:47,  2.14it/s]Loading trainS:  62%|██████▏   | 166/266 [01:36<00:46,  2.14it/s]Loading trainS:  63%|██████▎   | 167/266 [01:37<00:46,  2.13it/s]Loading trainS:  63%|██████▎   | 168/266 [01:37<00:46,  2.10it/s]Loading trainS:  64%|██████▎   | 169/266 [01:38<00:43,  2.25it/s]Loading trainS:  64%|██████▍   | 170/266 [01:38<00:46,  2.07it/s]Loading trainS:  64%|██████▍   | 171/266 [01:39<00:44,  2.16it/s]Loading trainS:  65%|██████▍   | 172/266 [01:39<00:43,  2.16it/s]Loading trainS:  65%|██████▌   | 173/266 [01:40<00:44,  2.08it/s]Loading trainS:  65%|██████▌   | 174/266 [01:40<00:47,  1.96it/s]Loading trainS:  66%|██████▌   | 175/266 [01:41<00:43,  2.09it/s]Loading trainS:  66%|██████▌   | 176/266 [01:41<00:46,  1.92it/s]Loading trainS:  67%|██████▋   | 177/266 [01:42<00:48,  1.84it/s]Loading trainS:  67%|██████▋   | 178/266 [01:42<00:47,  1.85it/s]Loading trainS:  67%|██████▋   | 179/266 [01:43<00:48,  1.80it/s]Loading trainS:  68%|██████▊   | 180/266 [01:43<00:48,  1.78it/s]Loading trainS:  68%|██████▊   | 181/266 [01:44<00:45,  1.85it/s]Loading trainS:  68%|██████▊   | 182/266 [01:44<00:45,  1.84it/s]Loading trainS:  69%|██████▉   | 183/266 [01:45<00:44,  1.85it/s]Loading trainS:  69%|██████▉   | 184/266 [01:46<00:43,  1.90it/s]Loading trainS:  70%|██████▉   | 185/266 [01:46<00:43,  1.85it/s]Loading trainS:  70%|██████▉   | 186/266 [01:47<00:43,  1.83it/s]Loading trainS:  70%|███████   | 187/266 [01:47<00:45,  1.75it/s]Loading trainS:  71%|███████   | 188/266 [01:48<00:41,  1.88it/s]Loading trainS:  71%|███████   | 189/266 [01:48<00:39,  1.94it/s]Loading trainS:  71%|███████▏  | 190/266 [01:49<00:38,  1.98it/s]Loading trainS:  72%|███████▏  | 191/266 [01:49<00:38,  1.93it/s]Loading trainS:  72%|███████▏  | 192/266 [01:50<00:39,  1.85it/s]Loading trainS:  73%|███████▎  | 193/266 [01:50<00:38,  1.91it/s]Loading trainS:  73%|███████▎  | 194/266 [01:51<00:38,  1.87it/s]Loading trainS:  73%|███████▎  | 195/266 [01:51<00:38,  1.84it/s]Loading trainS:  74%|███████▎  | 196/266 [01:52<00:38,  1.84it/s]Loading trainS:  74%|███████▍  | 197/266 [01:53<00:38,  1.79it/s]Loading trainS:  74%|███████▍  | 198/266 [01:53<00:38,  1.78it/s]Loading trainS:  75%|███████▍  | 199/266 [01:54<00:36,  1.84it/s]Loading trainS:  75%|███████▌  | 200/266 [01:54<00:35,  1.84it/s]Loading trainS:  76%|███████▌  | 201/266 [01:55<00:33,  1.92it/s]Loading trainS:  76%|███████▌  | 202/266 [01:55<00:33,  1.94it/s]Loading trainS:  76%|███████▋  | 203/266 [01:56<00:30,  2.04it/s]Loading trainS:  77%|███████▋  | 204/266 [01:56<00:30,  2.07it/s]Loading trainS:  77%|███████▋  | 205/266 [01:57<00:30,  2.02it/s]Loading trainS:  77%|███████▋  | 206/266 [01:57<00:31,  1.93it/s]Loading trainS:  78%|███████▊  | 207/266 [01:58<00:31,  1.86it/s]Loading trainS:  78%|███████▊  | 208/266 [01:58<00:31,  1.82it/s]Loading trainS:  79%|███████▊  | 209/266 [01:59<00:32,  1.78it/s]Loading trainS:  79%|███████▉  | 210/266 [01:59<00:31,  1.81it/s]Loading trainS:  79%|███████▉  | 211/266 [02:00<00:31,  1.77it/s]Loading trainS:  80%|███████▉  | 212/266 [02:01<00:29,  1.82it/s]Loading trainS:  80%|████████  | 213/266 [02:01<00:27,  1.91it/s]Loading trainS:  80%|████████  | 214/266 [02:02<00:27,  1.90it/s]Loading trainS:  81%|████████  | 215/266 [02:02<00:26,  1.90it/s]Loading trainS:  81%|████████  | 216/266 [02:03<00:27,  1.83it/s]Loading trainS:  82%|████████▏ | 217/266 [02:03<00:27,  1.81it/s]Loading trainS:  82%|████████▏ | 218/266 [02:04<00:26,  1.81it/s]Loading trainS:  82%|████████▏ | 219/266 [02:04<00:26,  1.75it/s]Loading trainS:  83%|████████▎ | 220/266 [02:05<00:25,  1.82it/s]Loading trainS:  83%|████████▎ | 221/266 [02:06<00:26,  1.73it/s]Loading trainS:  83%|████████▎ | 222/266 [02:06<00:26,  1.65it/s]Loading trainS:  84%|████████▍ | 223/266 [02:07<00:25,  1.71it/s]Loading trainS:  84%|████████▍ | 224/266 [02:07<00:23,  1.77it/s]Loading trainS:  85%|████████▍ | 225/266 [02:08<00:22,  1.85it/s]Loading trainS:  85%|████████▍ | 226/266 [02:08<00:21,  1.88it/s]Loading trainS:  85%|████████▌ | 227/266 [02:09<00:19,  2.02it/s]Loading trainS:  86%|████████▌ | 228/266 [02:09<00:16,  2.30it/s]Loading trainS:  86%|████████▌ | 229/266 [02:09<00:14,  2.57it/s]Loading trainS:  86%|████████▋ | 230/266 [02:09<00:12,  2.87it/s]Loading trainS:  87%|████████▋ | 231/266 [02:10<00:12,  2.74it/s]Loading trainS:  87%|████████▋ | 232/266 [02:10<00:14,  2.40it/s]Loading trainS:  88%|████████▊ | 233/266 [02:11<00:14,  2.27it/s]Loading trainS:  88%|████████▊ | 234/266 [02:11<00:15,  2.12it/s]Loading trainS:  88%|████████▊ | 235/266 [02:12<00:15,  2.07it/s]Loading trainS:  89%|████████▊ | 236/266 [02:12<00:14,  2.06it/s]Loading trainS:  89%|████████▉ | 237/266 [02:13<00:14,  2.05it/s]Loading trainS:  89%|████████▉ | 238/266 [02:13<00:13,  2.09it/s]Loading trainS:  90%|████████▉ | 239/266 [02:14<00:12,  2.12it/s]Loading trainS:  90%|█████████ | 240/266 [02:14<00:12,  2.13it/s]Loading trainS:  91%|█████████ | 241/266 [02:15<00:11,  2.13it/s]Loading trainS:  91%|█████████ | 242/266 [02:15<00:11,  2.07it/s]Loading trainS:  91%|█████████▏| 243/266 [02:16<00:10,  2.15it/s]Loading trainS:  92%|█████████▏| 244/266 [02:16<00:09,  2.29it/s]Loading trainS:  92%|█████████▏| 245/266 [02:16<00:08,  2.47it/s]Loading trainS:  92%|█████████▏| 246/266 [02:17<00:07,  2.53it/s]Loading trainS:  93%|█████████▎| 247/266 [02:17<00:07,  2.41it/s]Loading trainS:  93%|█████████▎| 248/266 [02:18<00:07,  2.34it/s]Loading trainS:  94%|█████████▎| 249/266 [02:18<00:07,  2.27it/s]Loading trainS:  94%|█████████▍| 250/266 [02:19<00:07,  2.24it/s]Loading trainS:  94%|█████████▍| 251/266 [02:19<00:06,  2.18it/s]Loading trainS:  95%|█████████▍| 252/266 [02:20<00:06,  2.18it/s]Loading trainS:  95%|█████████▌| 253/266 [02:20<00:06,  2.05it/s]Loading trainS:  95%|█████████▌| 254/266 [02:21<00:06,  1.76it/s]Loading trainS:  96%|█████████▌| 255/266 [02:21<00:05,  2.02it/s]Loading trainS:  96%|█████████▌| 256/266 [02:22<00:05,  1.98it/s]Loading trainS:  97%|█████████▋| 257/266 [02:22<00:04,  1.89it/s]Loading trainS:  97%|█████████▋| 258/266 [02:23<00:04,  1.69it/s]Loading trainS:  97%|█████████▋| 259/266 [02:24<00:03,  1.88it/s]Loading trainS:  98%|█████████▊| 260/266 [02:24<00:03,  1.88it/s]Loading trainS:  98%|█████████▊| 261/266 [02:25<00:02,  1.95it/s]Loading trainS:  98%|█████████▊| 262/266 [02:25<00:01,  2.06it/s]Loading trainS:  99%|█████████▉| 263/266 [02:25<00:01,  2.18it/s]Loading trainS:  99%|█████████▉| 264/266 [02:26<00:00,  2.11it/s]Loading trainS: 100%|█████████▉| 265/266 [02:26<00:00,  2.16it/s]Loading trainS: 100%|██████████| 266/266 [02:27<00:00,  2.01it/s]Loading trainS: 100%|██████████| 266/266 [02:27<00:00,  1.81it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  1.83it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:01,  1.95it/s]Loading testS:  75%|███████▌  | 3/4 [00:01<00:00,  2.04it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]----------+++ 
CrossVal ['a']
TypeExperiment 8
CrossVal ['a']
(0/4) test vimp2_A_CSFn2
(1/4) test vimp2_ANON967_CSFn2
(2/4) test vimp2_B_CSFn2
(3/4) test vimp2_E_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 83s - loss: 0.0976 - acc: 0.9895 - mDice: 0.8103 - val_loss: 0.1122 - val_acc: 0.9923 - val_mDice: 0.4377

Epoch 00001: val_mDice improved from -inf to 0.43775, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 77s - loss: 0.0635 - acc: 0.9932 - mDice: 0.8766 - val_loss: 0.1277 - val_acc: 0.9923 - val_mDice: 0.4716

Epoch 00002: val_mDice improved from 0.43775 to 0.47155, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 77s - loss: 0.0552 - acc: 0.9940 - mDice: 0.8927 - val_loss: 0.1812 - val_acc: 0.9916 - val_mDice: 0.4707

Epoch 00003: val_mDice did not improve from 0.47155
Epoch 4/300
 - 78s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9010 - val_loss: 0.1294 - val_acc: 0.9932 - val_mDice: 0.5184

Epoch 00004: val_mDice improved from 0.47155 to 0.51840, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 79s - loss: 0.0462 - acc: 0.9948 - mDice: 0.9102 - val_loss: 0.1778 - val_acc: 0.9922 - val_mDice: 0.4926

Epoch 00005: val_mDice did not improve from 0.51840
Epoch 6/300
 - 80s - loss: 0.0452 - acc: 0.9949 - mDice: 0.9122 - val_loss: 0.0343 - val_acc: 0.9936 - val_mDice: 0.5038

Epoch 00006: val_mDice did not improve from 0.51840
Epoch 7/300
 - 80s - loss: 0.0437 - acc: 0.9950 - mDice: 0.9151 - val_loss: 0.1316 - val_acc: 0.9915 - val_mDice: 0.4887

Epoch 00007: val_mDice did not improve from 0.51840
Epoch 8/300
 - 79s - loss: 0.0421 - acc: 0.9952 - mDice: 0.9182 - val_loss: 2.5163e-04 - val_acc: 0.9941 - val_mDice: 0.5117

Epoch 00008: val_mDice did not improve from 0.51840
Epoch 9/300
 - 80s - loss: 0.0398 - acc: 0.9954 - mDice: 0.9228 - val_loss: 0.1134 - val_acc: 0.9937 - val_mDice: 0.4954

Epoch 00009: val_mDice did not improve from 0.51840
Epoch 10/300
 - 80s - loss: 0.0403 - acc: 0.9954 - mDice: 0.9217 - val_loss: -4.5842e-02 - val_acc: 0.9943 - val_mDice: 0.5011

Epoch 00010: val_mDice did not improve from 0.51840
Epoch 11/300
 - 80s - loss: 0.0385 - acc: 0.9955 - mDice: 0.9253 - val_loss: -1.0834e-02 - val_acc: 0.9941 - val_mDice: 0.5090

Epoch 00011: val_mDice did not improve from 0.51840
Epoch 12/300
 - 79s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9274 - val_loss: -4.2839e-02 - val_acc: 0.9941 - val_mDice: 0.4956

Epoch 00012: val_mDice did not improve from 0.51840
Epoch 13/300
 - 79s - loss: 0.0377 - acc: 0.9956 - mDice: 0.9267 - val_loss: 0.0450 - val_acc: 0.9940 - val_mDice: 0.4988

Epoch 00013: val_mDice did not improve from 0.51840
Epoch 14/300
 - 79s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9301 - val_loss: 0.0693 - val_acc: 0.9938 - val_mDice: 0.5057

Epoch 00014: val_mDice did not improve from 0.51840
Epoch 15/300
 - 80s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9290 - val_loss: 0.0781 - val_acc: 0.9927 - val_mDice: 0.4901

Epoch 00015: val_mDice did not improve from 0.51840
Epoch 16/300
 - 80s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9317 - val_loss: -1.6565e-02 - val_acc: 0.9921 - val_mDice: 0.4435

Epoch 00016: val_mDice did not improve from 0.51840
Epoch 17/300
 - 79s - loss: 0.0365 - acc: 0.9958 - mDice: 0.9290 - val_loss: -2.7723e-02 - val_acc: 0.9942 - val_mDice: 0.5120

Epoch 00017: val_mDice did not improve from 0.51840
Epoch 18/300
 - 80s - loss: 0.0344 - acc: 0.9958 - mDice: 0.9333 - val_loss: 0.0619 - val_acc: 0.9920 - val_mDice: 0.4219

Epoch 00018: val_mDice did not improve from 0.51840
Epoch 19/300
 - 80s - loss: 0.0349 - acc: 0.9959 - mDice: 0.9321 - val_loss: -5.1111e-03 - val_acc: 0.9942 - val_mDice: 0.4978

Epoch 00019: val_mDice did not improve from 0.51840

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 20/300
 - 79s - loss: 0.0318 - acc: 0.9961 - mDice: 0.9384 - val_loss: -1.0087e-02 - val_acc: 0.9942 - val_mDice: 0.5077

Epoch 00020: val_mDice did not improve from 0.51840
Epoch 21/300
 - 79s - loss: 0.0312 - acc: 0.9961 - mDice: 0.9395 - val_loss: 0.0324 - val_acc: 0.9942 - val_mDice: 0.5011

Epoch 00021: val_mDice did not improve from 0.51840
Epoch 22/300
 - 80s - loss: 0.0313 - acc: 0.9961 - mDice: 0.9392 - val_loss: -8.4202e-03 - val_acc: 0.9942 - val_mDice: 0.5054

Epoch 00022: val_mDice did not improve from 0.51840
Epoch 23/300
 - 81s - loss: 0.0304 - acc: 0.9962 - mDice: 0.9410 - val_loss: 0.0708 - val_acc: 0.9937 - val_mDice: 0.5023

Epoch 00023: val_mDice did not improve from 0.51840
Epoch 24/300
 - 82s - loss: 0.0303 - acc: 0.9962 - mDice: 0.9411 - val_loss: -4.8407e-02 - val_acc: 0.9941 - val_mDice: 0.5064

Epoch 00024: val_mDice did not improve from 0.51840
Epoch 25/300
 - 81s - loss: 0.0312 - acc: 0.9962 - mDice: 0.9395 - val_loss: 0.0688 - val_acc: 0.9941 - val_mDice: 0.5063

Epoch 00025: val_mDice did not improve from 0.51840
Epoch 26/300
 - 81s - loss: 0.0301 - acc: 0.9962 - mDice: 0.9417 - val_loss: -4.4839e-02 - val_acc: 0.9944 - val_mDice: 0.4997

Epoch 00026: val_mDice did not improve from 0.51840
Epoch 27/300
 - 81s - loss: 0.0296 - acc: 0.9963 - mDice: 0.9425 - val_loss: 0.0630 - val_acc: 0.9938 - val_mDice: 0.5184

Epoch 00027: val_mDice improved from 0.51840 to 0.51844, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300
 - 81s - loss: 0.0290 - acc: 0.9963 - mDice: 0.9438 - val_loss: 0.0299 - val_acc: 0.9939 - val_mDice: 0.5061

Epoch 00028: val_mDice did not improve from 0.51844
Epoch 29/300
 - 81s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9437 - val_loss: 0.0589 - val_acc: 0.9938 - val_mDice: 0.4952

Epoch 00029: val_mDice did not improve from 0.51844
Epoch 30/300
 - 81s - loss: 0.0289 - acc: 0.9963 - mDice: 0.9440 - val_loss: 0.0945 - val_acc: 0.9937 - val_mDice: 0.4991

Epoch 00030: val_mDice did not improve from 0.51844
Epoch 31/300
 - 80s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9436 - val_loss: -4.4243e-02 - val_acc: 0.9944 - val_mDice: 0.5055

Epoch 00031: val_mDice did not improve from 0.51844
Epoch 32/300
 - 79s - loss: 0.0292 - acc: 0.9963 - mDice: 0.9433 - val_loss: 0.0370 - val_acc: 0.9935 - val_mDice: 0.4921

Epoch 00032: val_mDice did not improve from 0.51844
Epoch 33/300
 - 80s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9435 - val_loss: -2.0968e-03 - val_acc: 0.9942 - val_mDice: 0.4919

Epoch 00033: val_mDice did not improve from 0.51844
Epoch 34/300
 - 79s - loss: 0.0294 - acc: 0.9963 - mDice: 0.9430 - val_loss: -3.8210e-02 - val_acc: 0.9942 - val_mDice: 0.4858

Epoch 00034: val_mDice did not improve from 0.51844

Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 35/300
 - 80s - loss: 0.0276 - acc: 0.9964 - mDice: 0.9465 - val_loss: 0.0572 - val_acc: 0.9934 - val_mDice: 0.4988

Epoch 00035: val_mDice did not improve from 0.51844
Epoch 36/300
 - 80s - loss: 0.0278 - acc: 0.9964 - mDice: 0.9460 - val_loss: -5.5872e-03 - val_acc: 0.9942 - val_mDice: 0.4983

Epoch 00036: val_mDice did not improve from 0.51844
Epoch 37/300
 - 79s - loss: 0.0271 - acc: 0.9965 - mDice: 0.9474 - val_loss: -4.5703e-02 - val_acc: 0.9943 - val_mDice: 0.5010
